{
    "query": "latest works on auto regressive LM dense passage retrieval their performance compared with bi directional encoders",
    "user_id": "lib_user",
    "task_id": "a155e41c-9e24-42dd-ad72-0581686872ab",
    "timestamp": "2025-06-23T22:18:29.384665",
    "n_retrieval": 256,
    "n_retrieved": 259,
    "n_candidates": 11,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.21107399999999998,
    "decomposed_query": {
        "rewritten_query": "Auto-regressive language models for dense passage retrieval and their performance compared with bi-directional encoders.",
        "keyword_query": "auto-regressive language models dense passage retrieval performance bi-directional encoders",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009894,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 153,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260272949",
                    "name": "Yizheng Huang"
                },
                {
                    "authorId": "2259653248",
                    "name": "Jimmy X. Huang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
            "corpus_id": 269188036,
            "sentences": [
                {
                    "corpus_id": "269188036",
                    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
                    "text": "FiD-Light [47] employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process. GenRT [148] utilizes a combination of an encoder to capture global list-level features and a sequential decoder to reorder documents based on relevance. The model is trained to learn relevance scores through supervised learning, guided by labeled relevance data, ensuring that the most pertinent documents are prioritized in the final reranked list. Furthermore, ITER-RETGEN [121] proposes using a more capable re-ranker, which has access to model generations, to distill knowledge into a dense retriever. This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it to better capture the semantic relevance of documents relative to the task input. \n\nData Augmentation for Re-ranking. Data augmentation for re-rankers focuses on enhancing the training process by generating additional training data, such as pseudo-relevance labels, using LLMs. This data augmentation provides more varied training examples, which helps improve the performance of re-ranking models. For example, DKS-RAC [53] introduces methods like Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC), which focus on improving the retrieval process by incorporating rich answer encodings. These methods involve generating additional training signals or utilizing enriched data representations to improve the retrieval and ranking of documents. Additionally, the PROMPTAGATOR [27] framework utilizes synthetic data generated through LLM-based query generation to enhance the training of the reranker. This data augmentation approach allows the re-ranker to refine candidate passages more effectively, using a cross-attention model trained on these additional examples to boost retrieval accuracy.",
                    "score": 0.49364484753474547,
                    "section_title": "Re-Ranking",
                    "char_start_offset": 41698,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1069
                        },
                        {
                            "start": 1072,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1753
                        },
                        {
                            "start": 1754,
                            "end": 1909
                        },
                        {
                            "start": 1910,
                            "end": 2105
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 10,
                            "end": 14,
                            "matchedPaperCorpusId": "252568176"
                        },
                        {
                            "start": 388,
                            "end": 393,
                            "matchedPaperCorpusId": "267412492"
                        },
                        {
                            "start": 757,
                            "end": 762,
                            "matchedPaperCorpusId": "258866037"
                        },
                        {
                            "start": 1408,
                            "end": 1412,
                            "matchedPaperCorpusId": "267053546"
                        },
                        {
                            "start": 1785,
                            "end": 1789,
                            "matchedPaperCorpusId": "252519173"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82275390625
                }
            ],
            "relevance_judgement": 0.82275390625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Retrieval-Augmented Text Generation for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizheng Huang, Jimmy X. Huang\n## Abstract\nRetrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.\n## Re-Ranking\nFiD-Light [47] employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process. GenRT [148] utilizes a combination of an encoder to capture global list-level features and a sequential decoder to reorder documents based on relevance. The model is trained to learn relevance scores through supervised learning, guided by labeled relevance data, ensuring that the most pertinent documents are prioritized in the final reranked list. Furthermore, ITER-RETGEN [121] proposes using a more capable re-ranker, which has access to model generations, to distill knowledge into a dense retriever. This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it to better capture the semantic relevance of documents relative to the task input. \n\nData Augmentation for Re-ranking. Data augmentation for re-rankers focuses on enhancing the training process by generating additional training data, such as pseudo-relevance labels, using LLMs. This data augmentation provides more varied training examples, which helps improve the performance of re-ranking models. For example, DKS-RAC [53] introduces methods like Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC), which focus on improving the retrieval process by incorporating rich answer encodings. These methods involve generating additional training signals or utilizing enriched data representations to improve the retrieval and ranking of documents. Additionally, the PROMPTAGATOR [27] framework utilizes synthetic data generated through LLM-based query generation to enhance the training of the reranker. This data augmentation approach allows the re-ranker to refine candidate passages more effectively, using a cross-attention model trained on these additional examples to boost retrieval accuracy.",
            "reference_string": "[269188036 | Huang et al. | 2024 | Citations: 51]"
        },
        {
            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13197",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109967721",
                    "name": "Zehan Li"
                },
                {
                    "authorId": "2107949588",
                    "name": "Yanzhao Zhang"
                },
                {
                    "authorId": "8427191",
                    "name": "Dingkun Long"
                },
                {
                    "authorId": "35930962",
                    "name": "Pengjun Xie"
                }
            ],
            "abstract": "Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability. To this end, we propose a novel token importance aware masking strategy based on pointwise mutual information to intensify the challenge of the decoder. Importantly, our approach can be implemented in an unsupervised manner, without adding additional expenses to the pre-training phase. Our experiments verify that the proposed method is both effective and robust on large-scale supervised passage retrieval datasets and out-of-domain zero-shot retrieval benchmarks.",
            "corpus_id": 258833383,
            "sentences": [
                {
                    "corpus_id": "258833383",
                    "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
                    "text": "Passage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
                    "score": 0.6137555086636334,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1766
                        },
                        {
                            "start": 1769,
                            "end": 1883
                        },
                        {
                            "start": 1884,
                            "end": 2198
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 112,
                            "end": 136,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 136,
                            "end": 153,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 153,
                            "end": 170,
                            "matchedPaperCorpusId": "237502990"
                        },
                        {
                            "start": 195,
                            "end": 212,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 263,
                            "end": 282,
                            "matchedPaperCorpusId": "247292113"
                        },
                        {
                            "start": 508,
                            "end": 532,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1314,
                            "end": 1330,
                            "matchedPaperCorpusId": "243865399"
                        },
                        {
                            "start": 1708,
                            "end": 1726,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 1747,
                            "end": 1765,
                            "matchedPaperCorpusId": "252917569"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60302734375
                }
            ],
            "relevance_judgement": 0.60302734375,
            "relevance_judgment_input_expanded": "# Title: Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval\n# Venue: arXiv.org\n# Authors: Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie\n## Abstract\nRecently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability. To this end, we propose a novel token importance aware masking strategy based on pointwise mutual information to intensify the challenge of the decoder. Importantly, our approach can be implemented in an unsupervised manner, without adding additional expenses to the pre-training phase. Our experiments verify that the proposed method is both effective and robust on large-scale supervised passage retrieval datasets and out-of-domain zero-shot retrieval benchmarks.\n## Introduction\nPassage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
            "reference_string": "[258833383 | Li et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2401.11248",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.11248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2068996632",
                    "name": "Guangyuan Ma"
                },
                {
                    "authorId": "2155226596",
                    "name": "Xing Wu"
                },
                {
                    "authorId": "2257134389",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2257376973",
                    "name": "Songlin Hu"
                }
            ],
            "abstract": "Masked auto-encoder pre-training has emerged as a prevalent technique for initializing and enhancing dense retrieval systems. It generally utilizes additional Transformer decoder blocks to provide sustainable supervision signals and compress contextual information into dense representations. However, the underlying reasons for the effectiveness of such a pre-training technique remain unclear. The usage of additional Transformer-based decoders also incurs significant computational costs. In this study, we aim to shed light on this issue by revealing that masked auto-encoder (MAE) pre-training with enhanced decoding significantly improves the term coverage of input tokens in dense representations, compared to vanilla BERT checkpoints. Building upon this observation, we propose a modification to the traditional MAE by replacing the decoder of a masked auto-encoder with a completely simplified Bag-of-Word prediction task. This modification enables the efficient compression of lexical signals into dense representations through unsupervised pre-training. Remarkably, our proposed method achieves state-of-the-art retrieval performance on several large-scale retrieval benchmarks without requiring any additional parameters, which provides a 67% training speed-up compared to standard masked auto-encoder pre-training with enhanced decoding.",
            "corpus_id": 267068650,
            "sentences": [
                {
                    "corpus_id": "267068650",
                    "title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.",
                    "text": "Dense passage retrieval utilizes pre-trained language models (PLMs), e.g., BERT [2], to encode the lexical information as representations in latent spaces and retrieve relevant passages of a given query based on the similarity search [18,19]. Pre-training with masked auto-encoder (MAE) has recently become a hot topic to boost the initialization ability of PLMs for downstream retrieval tasks. Such a technique appends carefully designed Transformers-based decoder blocks to the PLM encoder, forming encoder-decoder architecture, and bringing supervision signals to the representations. \n\nRecent studies have explored several pre-training strategies, such as pre-training with the weakened decoder [16], extreme mask ratio [34], replace token detection [29], context reconstruction [31], and multi-task decoding [38]. As is shown in Figure 1 A), these MAEstyle pre-trainings commonly preserve several traits: 1) Asymmetric structure: The encoder is a full-sized Transformers-based encoder for generating discriminative representations based on input sentences. While the decoder is typically a shallow Transformersbased block with a few or just one layer. 2) Bottlenecked connection. The connection between the encoder and the decoder is typically a single representation vector. 3) Reconstruction signals. Carefully designed decoding tasks are applied to the decoder side, which decodes context information through auto-encoding [7,34], auto-regression [16], enhanced decoding [34] or other curated decoding tasks [29,32,38]. By optimizing a weak decoder, the lexical information is compressed into the sentence representations, thus providing good initialization ability for retrieval tasks. Notably, enhanced decoding from [34] is proven effective for dense retrieval pre-training, which utilizes sentence representations as query streams and masked context embeddings as content streams. \n\nHowever, we argue that existing MAE-style pre-training paradigms have several drawbacks, which may hinder the development of dense retrieval pre-training: 1) Lack of interpretability. Existing studies focus on creating carefully designed tasks, but still neglect to reveal the underlying reason for why such a pre-training method is effective.",
                    "score": 0.5040615981875827,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 587
                        },
                        {
                            "start": 590,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1892
                        },
                        {
                            "start": 1895,
                            "end": 2078
                        },
                        {
                            "start": 2079,
                            "end": 2238
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 80,
                            "end": 83,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 234,
                            "end": 238,
                            "matchedPaperCorpusId": "8915893"
                        },
                        {
                            "start": 238,
                            "end": 241,
                            "matchedPaperCorpusId": "16464483"
                        },
                        {
                            "start": 724,
                            "end": 728,
                            "matchedPaperCorpusId": "252917569"
                        },
                        {
                            "start": 783,
                            "end": 787,
                            "matchedPaperCorpusId": "251594591"
                        },
                        {
                            "start": 1431,
                            "end": 1434,
                            "matchedPaperCorpusId": "237581068"
                        },
                        {
                            "start": 1434,
                            "end": 1437,
                            "matchedPaperCorpusId": "252917569"
                        },
                        {
                            "start": 1479,
                            "end": 1483,
                            "matchedPaperCorpusId": "252917569"
                        },
                        {
                            "start": 1727,
                            "end": 1731,
                            "matchedPaperCorpusId": "252917569"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57421875
                }
            ],
            "relevance_judgement": 0.57421875,
            "relevance_judgment_input_expanded": "# Title: Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Guangyuan Ma, Xing Wu, Zijia Lin, Songlin Hu\n## Abstract\nMasked auto-encoder pre-training has emerged as a prevalent technique for initializing and enhancing dense retrieval systems. It generally utilizes additional Transformer decoder blocks to provide sustainable supervision signals and compress contextual information into dense representations. However, the underlying reasons for the effectiveness of such a pre-training technique remain unclear. The usage of additional Transformer-based decoders also incurs significant computational costs. In this study, we aim to shed light on this issue by revealing that masked auto-encoder (MAE) pre-training with enhanced decoding significantly improves the term coverage of input tokens in dense representations, compared to vanilla BERT checkpoints. Building upon this observation, we propose a modification to the traditional MAE by replacing the decoder of a masked auto-encoder with a completely simplified Bag-of-Word prediction task. This modification enables the efficient compression of lexical signals into dense representations through unsupervised pre-training. Remarkably, our proposed method achieves state-of-the-art retrieval performance on several large-scale retrieval benchmarks without requiring any additional parameters, which provides a 67% training speed-up compared to standard masked auto-encoder pre-training with enhanced decoding.\n## INTRODUCTION\nDense passage retrieval utilizes pre-trained language models (PLMs), e.g., BERT [2], to encode the lexical information as representations in latent spaces and retrieve relevant passages of a given query based on the similarity search [18,19]. Pre-training with masked auto-encoder (MAE) has recently become a hot topic to boost the initialization ability of PLMs for downstream retrieval tasks. Such a technique appends carefully designed Transformers-based decoder blocks to the PLM encoder, forming encoder-decoder architecture, and bringing supervision signals to the representations. \n\nRecent studies have explored several pre-training strategies, such as pre-training with the weakened decoder [16], extreme mask ratio [34], replace token detection [29], context reconstruction [31], and multi-task decoding [38]. As is shown in Figure 1 A), these MAEstyle pre-trainings commonly preserve several traits: 1) Asymmetric structure: The encoder is a full-sized Transformers-based encoder for generating discriminative representations based on input sentences. While the decoder is typically a shallow Transformersbased block with a few or just one layer. 2) Bottlenecked connection. The connection between the encoder and the decoder is typically a single representation vector. 3) Reconstruction signals. Carefully designed decoding tasks are applied to the decoder side, which decodes context information through auto-encoding [7,34], auto-regression [16], enhanced decoding [34] or other curated decoding tasks [29,32,38]. By optimizing a weak decoder, the lexical information is compressed into the sentence representations, thus providing good initialization ability for retrieval tasks. Notably, enhanced decoding from [34] is proven effective for dense retrieval pre-training, which utilizes sentence representations as query streams and masked context embeddings as content streams. \n\nHowever, we argue that existing MAE-style pre-training paradigms have several drawbacks, which may hinder the development of dense retrieval pre-training: 1) Lack of interpretability. Existing studies focus on creating carefully designed tasks, but still neglect to reveal the underlying reason for why such a pre-training method is effective.",
            "reference_string": "[267068650 | Ma et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Generative Retrieval with Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185022832",
                    "name": "Ye Wang"
                },
                {
                    "authorId": "2290204960",
                    "name": "Xinrun Xu"
                },
                {
                    "authorId": "2143721734",
                    "name": "Rui Xie"
                },
                {
                    "authorId": "2288018918",
                    "name": "Wenxin Hu"
                },
                {
                    "authorId": "2052980435",
                    "name": "Wei Ye"
                }
            ],
            "abstract": "When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.",
            "corpus_id": 268031876,
            "sentences": [
                {
                    "corpus_id": "268031876",
                    "title": "Generative Retrieval with Large Language Models",
                    "text": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
                    "score": 0.5671587266286197,
                    "section_title": "Related Work",
                    "char_start_offset": 21339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 79,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 593
                        },
                        {
                            "start": 596,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1243
                        },
                        {
                            "start": 1246,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1789
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 151,
                            "end": 175,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 175,
                            "end": 193,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 193,
                            "end": 211,
                            "matchedPaperCorpusId": "59604492"
                        },
                        {
                            "start": 911,
                            "end": 930,
                            "matchedPaperCorpusId": "258714822"
                        },
                        {
                            "start": 1410,
                            "end": 1428,
                            "matchedPaperCorpusId": "249395549"
                        },
                        {
                            "start": 1428,
                            "end": 1446,
                            "matchedPaperCorpusId": "239016601"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55419921875
                }
            ],
            "relevance_judgement": 0.55419921875,
            "relevance_judgment_input_expanded": "# Title: Generative Retrieval with Large Language Models\n# Venue: \n# Authors: Ye Wang, Xinrun Xu, Rui Xie, Wenxin Hu, Wei Ye\n## Abstract\nWhen completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.\n## Related Work\nTraditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
            "reference_string": "[268031876 | Wang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 42,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.09612",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264184691",
                    "name": "Noah Ziems"
                },
                {
                    "authorId": "38767143",
                    "name": "W. Yu"
                },
                {
                    "authorId": "72871419",
                    "name": "Zhihan Zhang"
                },
                {
                    "authorId": "2152153656",
                    "name": "Meng Jiang"
                }
            ],
            "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.",
            "corpus_id": 258714822,
            "sentences": [
                {
                    "corpus_id": "258714822",
                    "title": "Large Language Models are Built-in Autoregressive Search Engines",
                    "text": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
                    "score": 0.52902793907616,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 2069
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 175,
                            "end": 199,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 579,
                            "end": 603,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 657,
                            "end": 679,
                            "matchedPaperCorpusId": "220302658"
                        },
                        {
                            "start": 883,
                            "end": 910,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1276,
                            "end": 1294,
                            "matchedPaperCorpusId": "249395549"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54931640625
                }
            ],
            "relevance_judgement": 0.54931640625,
            "relevance_judgment_input_expanded": "# Title: Large Language Models are Built-in Autoregressive Search Engines\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Noah Ziems, W. Yu, Zhihan Zhang, Meng Jiang\n## Abstract\nDocument retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.\n## Introduction\nAlong with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
            "reference_string": "[258714822 | Ziems et al. | 2023 | Citations: 42]"
        },
        {
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25598/25370",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.07670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155226596",
                    "name": "Xing Wu"
                },
                {
                    "authorId": "2068996632",
                    "name": "Guangyuan Ma"
                },
                {
                    "authorId": "2156805995",
                    "name": "Meng Lin"
                },
                {
                    "authorId": "1818920",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2135394423",
                    "name": "Zhongyuan Wang"
                },
                {
                    "authorId": "40845069",
                    "name": "Songlin Hu"
                }
            ],
            "abstract": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae.",
            "corpus_id": 251594591,
            "sentences": [
                {
                    "corpus_id": "251594591",
                    "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
                    "text": "Passage retrieval aims to retrieve the relevant passages of a query from a large corpus, which benefits many downstream applications, such as web search (Fan et al. 2021;Guo et al. 2022;Lin, Nogueira, and Yates 2021), question answering (Karpukhin et al. 2020;Lee et al. 2020;Zhu et al. 2021) and dialogue systems (Gao et al. 2022a;Yu et al. 2021).\n\nFor a long time, sparse retrieval represented by BM25 (Robertson, Zaragoza et al. 2009) was the dominant retrieval method. Recently, dense retrieval has received increasing attention with the development of pre-trained language models (PLM) (Devlin et al. 2018;Liu et al. 2019). Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al. 2021;Humeau et al. 2019;Xiong et al. 2020;Zhan et al. 2021Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al. 2021;Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
                    "score": 0.6678573011873844,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 170,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 170,
                            "end": 186,
                            "matchedPaperCorpusId": "232147859"
                        },
                        {
                            "start": 186,
                            "end": 216,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 332,
                            "end": 347,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 404,
                            "end": 436,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 591,
                            "end": 611,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 834,
                            "end": 858,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 895,
                            "end": 911,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 1402,
                            "end": 1418,
                            "matchedPaperCorpusId": "243865399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.548828125
                }
            ],
            "relevance_judgement": 0.548828125,
            "relevance_judgment_input_expanded": "# Title: ConTextual Masked Auto-Encoder for Dense Passage Retrieval\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, Songlin Hu\n## Abstract\nDense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae.\n## Introduction\nPassage retrieval aims to retrieve the relevant passages of a query from a large corpus, which benefits many downstream applications, such as web search (Fan et al. 2021;Guo et al. 2022;Lin, Nogueira, and Yates 2021), question answering (Karpukhin et al. 2020;Lee et al. 2020;Zhu et al. 2021) and dialogue systems (Gao et al. 2022a;Yu et al. 2021).\n\nFor a long time, sparse retrieval represented by BM25 (Robertson, Zaragoza et al. 2009) was the dominant retrieval method. Recently, dense retrieval has received increasing attention with the development of pre-trained language models (PLM) (Devlin et al. 2018;Liu et al. 2019). Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al. 2021;Humeau et al. 2019;Xiong et al. 2020;Zhan et al. 2021Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al. 2021;Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
            "reference_string": "[251594591 | Wu et al. | 2022 | Citations: 26]"
        },
        {
            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 16,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287336807",
                    "name": "Soyoung Yoon"
                },
                {
                    "authorId": "2287970016",
                    "name": "Eunbi Choi"
                },
                {
                    "authorId": "2287064006",
                    "name": "Jiyeon Kim"
                },
                {
                    "authorId": "2181032855",
                    "name": "Yireun Kim"
                },
                {
                    "authorId": "2286896884",
                    "name": "Hyeongu Yun"
                },
                {
                    "authorId": "2287694374",
                    "name": "Seung-won Hwang"
                }
            ],
            "abstract": "We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \\url{https://github.com/soyoung97/ListT5}.",
            "corpus_id": 267938301,
            "sentences": [
                {
                    "corpus_id": "267938301",
                    "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
                    "text": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
                    "score": 0.5178097776983872,
                    "section_title": "Generative Models for Reranking",
                    "char_start_offset": 4693,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 927
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 59,
                            "end": 83,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 239,
                            "end": 261,
                            "matchedPaperCorpusId": "212725651"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54296875
                }
            ],
            "relevance_judgement": 0.54296875,
            "relevance_judgment_input_expanded": "# Title: ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Yireun Kim, Hyeongu Yun, Seung-won Hwang\n## Abstract\nWe propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \\url{https://github.com/soyoung97/ListT5}.\n## Generative Models for Reranking\nIn the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
            "reference_string": "[267938301 | Yoon et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 33,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.15133",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.15133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8427191",
                    "name": "Dingkun Long"
                },
                {
                    "authorId": "2107949588",
                    "name": "Yanzhao Zhang"
                },
                {
                    "authorId": "2149131512",
                    "name": "Guangwei Xu"
                },
                {
                    "authorId": "35930962",
                    "name": "Pengjun Xie"
                }
            ],
            "abstract": "Pre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.",
            "corpus_id": 253157959,
            "sentences": [
                {
                    "corpus_id": "253157959",
                    "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                    "text": "Existing dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
                    "score": 0.6109457336083027,
                    "section_title": "Related Work",
                    "char_start_offset": 3117,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 87,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 759
                        },
                        {
                            "start": 762,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1253
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 118,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 450,
                            "end": 470,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 470,
                            "end": 488,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 625,
                            "end": 642,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 661,
                            "end": 681,
                            "matchedPaperCorpusId": "211068995"
                        },
                        {
                            "start": 909,
                            "end": 927,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 927,
                            "end": 946,
                            "matchedPaperCorpusId": "211068995"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52783203125
                },
                {
                    "corpus_id": "253157959",
                    "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                    "text": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.",
                    "score": 0.5628380475835884,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 685
                        },
                        {
                            "start": 688,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1867
                        },
                        {
                            "start": 1868,
                            "end": 2050
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 164,
                            "end": 188,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 188,
                            "end": 204,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 246,
                            "end": 263,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 835,
                            "end": 855,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1025,
                            "end": 1049,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1049,
                            "end": 1068,
                            "matchedPaperCorpusId": "247476289"
                        },
                        {
                            "start": 1488,
                            "end": 1510,
                            "matchedPaperCorpusId": "220729621"
                        },
                        {
                            "start": 1510,
                            "end": 1526,
                            "matchedPaperCorpusId": "233307194"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.2301025390625
                }
            ],
            "relevance_judgement": 0.52783203125,
            "relevance_judgment_input_expanded": "# Title: Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval\n# Venue: arXiv.org\n# Authors: Dingkun Long, Yanzhao Zhang, Guangwei Xu, Pengjun Xie\n## Abstract\nPre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.\n## Introduction\nDense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.\n\n## Related Work\nExisting dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
            "reference_string": "[253157959 | Long et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 52,
            "citation_count": 69,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.17331",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275526493",
                    "name": "Zichun Yu"
                },
                {
                    "authorId": "2139787803",
                    "name": "Chenyan Xiong"
                },
                {
                    "authorId": "150311558",
                    "name": "S. Yu"
                },
                {
                    "authorId": "2109232579",
                    "name": "Zhiyuan Liu"
                }
            ],
            "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
            "corpus_id": 258960666,
            "sentences": [
                {
                    "corpus_id": "258960666",
                    "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
                    "text": "Retrieval-augmented LM (Guu et al., 2020;Lewis et al., 2020) is a type of LM that leverages external information to improve its performance. It retrieves relevant documents from a corpus using a retriever, and then utilizes the documents to enhance its language generation capabilities.\n\nThe objective of the retriever is to find an augmentation document set D a from a corpus C that helps the LM handle a given query q. Previous researches (Karpukhin et al., 2020;Xiong et al., 2021) concentrate primarily on the dense retrieval system that searches in the dense vector space since dense retrieval usually performs more accurately and efficiently than sparse one. A dense retrieval model first represents q and the document d into an embedding space using a pre-trained encoder g,\n\nand match their embeddings by dot product function f , which supports fast approximate nearest neighbor search (ANN) (Andr\u00e9 et al., 2016;Johnson et al., 2021). We then define D a that contains top-N retrieved documents as:\n\nFor the LM backbones, the decoder-only and the encoder-decoder models are the two primary choices of the retrieval-augmented LMs (Izacard and Grave, 2021b; .\n\nGiven a decoder-only LM like GPT-3 (Brown et al., 2020), the LM input can be a simple concatenation of the query and all the augmentation documents {d a 1 . . . d a N }. Then, the LM will generate the answer based on the inputs auto-regressively.\n\nFor an encoder-decoder LM like T5 , taking simple concatenation as the encoder input may still be effective. However, this method may not scale to a large volume of documents due to the quadratic self-attention computation associated with the number of documents. To aggregate multiple documents more efficiently, Izacard and Grave (2021b) propose the fusion-in-decoder (FiD) mechanism, which soon becomes the mainstream in the development of encoder-decoder retrievalaugmented LMs. It first encodes each concatenation of the (d a i , q) pair separately and then lets the decoder attend to",
                    "score": 0.5081786746281794,
                    "section_title": "Preliminaries",
                    "char_start_offset": 6457,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 41,
                            "end": 60,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 465,
                            "end": 484,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 900,
                            "end": 920,
                            "matchedPaperCorpusId": "5966664"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.521484375
                }
            ],
            "relevance_judgement": 0.521484375,
            "relevance_judgment_input_expanded": "# Title: Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zichun Yu, Chenyan Xiong, S. Yu, Zhiyuan Liu\n## Abstract\nRetrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\n## Preliminaries\nRetrieval-augmented LM (Guu et al., 2020;Lewis et al., 2020) is a type of LM that leverages external information to improve its performance. It retrieves relevant documents from a corpus using a retriever, and then utilizes the documents to enhance its language generation capabilities.\n\nThe objective of the retriever is to find an augmentation document set D a from a corpus C that helps the LM handle a given query q. Previous researches (Karpukhin et al., 2020;Xiong et al., 2021) concentrate primarily on the dense retrieval system that searches in the dense vector space since dense retrieval usually performs more accurately and efficiently than sparse one. A dense retrieval model first represents q and the document d into an embedding space using a pre-trained encoder g,\n\nand match their embeddings by dot product function f , which supports fast approximate nearest neighbor search (ANN) (Andr\u00e9 et al., 2016;Johnson et al., 2021). We then define D a that contains top-N retrieved documents as:\n\nFor the LM backbones, the decoder-only and the encoder-decoder models are the two primary choices of the retrieval-augmented LMs (Izacard and Grave, 2021b; .\n\nGiven a decoder-only LM like GPT-3 (Brown et al., 2020), the LM input can be a simple concatenation of the query and all the augmentation documents {d a 1 . . . d a N }. Then, the LM will generate the answer based on the inputs auto-regressively.\n\nFor an encoder-decoder LM like T5 , taking simple concatenation as the encoder input may still be effective. However, this method may not scale to a large volume of documents due to the quadratic self-attention computation associated with the number of documents. To aggregate multiple documents more efficiently, Izacard and Grave (2021b) propose the fusion-in-decoder (FiD) mechanism, which soon becomes the mainstream in the development of encoder-decoder retrievalaugmented LMs. It first encodes each concatenation of the (d a i , q) pair separately and then lets the decoder attend to",
            "reference_string": "[258960666 | Yu et al. | 2023 | Citations: 69]"
        },
        {
            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.03158",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.03158, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155226596",
                    "name": "Xing Wu"
                },
                {
                    "authorId": "2068996632",
                    "name": "Guangyuan Ma"
                },
                {
                    "authorId": "144282672",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "2156805995",
                    "name": "Meng Lin"
                },
                {
                    "authorId": "1818920",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2642200",
                    "name": "Fuzheng Zhang"
                },
                {
                    "authorId": "40845069",
                    "name": "Songlin Hu"
                }
            ],
            "abstract": "Growing techniques have been emerging to improve the performance of passage retrieval. As an effective representation bottleneck pretraining technique, the contextual masked auto-encoder utilizes contextual embedding to assist in the reconstruction of passages. However, it only uses a single auto-encoding pre-task for dense representation pre-training. This study brings multi-view modeling to the contextual masked auto-encoder. Firstly, multi-view representation utilizes both dense and sparse vectors as multi-view representations, aiming to capture sentence semantics from different aspects. Moreover, multiview decoding paradigm utilizes both autoencoding and auto-regressive decoders in representation bottleneck pre-training, aiming to provide both reconstructive and generative signals for better contextual representation pretraining. We refer to this multi-view pretraining method as CoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective and robust on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks.",
            "corpus_id": 257985191,
            "sentences": [
                {
                    "corpus_id": "257985191",
                    "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
                    "text": "It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck. \n\nPLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021;Humeau et al., 2019;Xiong et al., 2020;Zhan et al., 2021Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances. CoT-MAE involves selecting two passages from a document as contextual passages, aiming to learn a better representation with the assistance of contextual information. \n\nA shallow encoder, typically with one or two layers, is used to reconstruct the masked passages with the help of context passage embedding. \n\nThe efficiency of PLM-based sparse retrieval (Bai et al., 2020;Formal et al., 2021b,a;Shen et al., 2022) and dense retrieval (Wu et al., 2022;Wang et al., 2022;Gao and Callan, 2021a) demonstrates that both sparse and dense representations effectively capture the text meaning. On the basis of contextual masked auto-encoder (CoT-MAE) tailored for dense retrieval, a natural thought is to incorporate the sparse representation pre-training into CoT-MAE, i.e., exploring multi-view representations in contextual masked auto-encoder pretraining and fine-tuning. Theoretically, sparse representation focuses on the lexicon while dense representation focuses on sentence semantics, offering different perspectives on the text. These two representations are probably compatible and complement each other. \n\nAside from representation variations, there is also a distinction in the structure of the decoder in the previous methods.",
                    "score": 0.49179404796918524,
                    "section_title": "Introduction",
                    "char_start_offset": 1726,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 133,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1085
                        },
                        {
                            "start": 1088,
                            "end": 1227
                        },
                        {
                            "start": 1230,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2028
                        },
                        {
                            "start": 2031,
                            "end": 2153
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 285,
                            "end": 310,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 349,
                            "end": 366,
                            "matchedPaperCorpusId": "233289894"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52099609375
                },
                {
                    "corpus_id": "257985191",
                    "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
                    "text": "Passage retrieval involves searching a large corpus for passages that are relevant to a particular query, and is essential for various applications such as web search (Fan et al., 2021;Guo et al., 2022;Lin et al., 2021a), question answering (Karpukhin et al., 2020;Lee et al., 2020;Zhu et al., 2021), and dialogue systems (Gao et al., 2022;Yu et al., 2021). Sparse retrieval methods, such as BM25, are practical and dominant approaches. However, retrieval methods based on pre-trained language models (PLMs) (Devlin et al., 2018;Liu et al., 2019) have gained popularity in recent years. PLMs are utilized to improve (1) sparse retrieval models that rely on lexical match; (2) dense retrieval models that model the semantic interaction between queries and passages in latent semantic space. \n\nTo improve sparse retrieval, PLM mitigates vocabulary mismatch by projecting each term in the query and passage to a vocabulary-sized weight vector. Each dimension in the weight vector represents the weight of a term in the PLM vocabulary, and the correlation between query and passage depends on lexical matching (Formal et al., 2021b,a;Shen et al., 2022). PLM-based sparse vectors model the features of passage on every lexical term (denoted by PLM logits output), and thus naturally capture the semantic at the lexicon level. SPLADE (Formal et al., 2021b,a) uses learnable PLM, e.g., BERT, to produce sparse vectors and fine-tune the retriever with contrastive learning loss. PLM-based SPLADE outperforms traditional sparse retrieval like BM25. LexMAE (Shen et al., 2022) further expands on SPLADE by proposing to pre-train a lexical-bottlenecked masked autoencoder to learn importance-aware sparse representations. It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck.",
                    "score": 0.5416990556099316,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 789
                        },
                        {
                            "start": 792,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1539
                        },
                        {
                            "start": 1540,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1841
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 185,
                            "end": 202,
                            "matchedPaperCorpusId": "232147859"
                        },
                        {
                            "start": 202,
                            "end": 220,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 340,
                            "end": 356,
                            "matchedPaperCorpusId": "234343311"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.2120361328125
                }
            ],
            "relevance_judgement": 0.52099609375,
            "relevance_judgment_input_expanded": "# Title: CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval\n# Venue: arXiv.org\n# Authors: Xing Wu, Guangyuan Ma, Peng Wang, Meng Lin, Zijia Lin, Fuzheng Zhang, Songlin Hu\n## Abstract\nGrowing techniques have been emerging to improve the performance of passage retrieval. As an effective representation bottleneck pretraining technique, the contextual masked auto-encoder utilizes contextual embedding to assist in the reconstruction of passages. However, it only uses a single auto-encoding pre-task for dense representation pre-training. This study brings multi-view modeling to the contextual masked auto-encoder. Firstly, multi-view representation utilizes both dense and sparse vectors as multi-view representations, aiming to capture sentence semantics from different aspects. Moreover, multiview decoding paradigm utilizes both autoencoding and auto-regressive decoders in representation bottleneck pre-training, aiming to provide both reconstructive and generative signals for better contextual representation pretraining. We refer to this multi-view pretraining method as CoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective and robust on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks.\n## Introduction\nPassage retrieval involves searching a large corpus for passages that are relevant to a particular query, and is essential for various applications such as web search (Fan et al., 2021;Guo et al., 2022;Lin et al., 2021a), question answering (Karpukhin et al., 2020;Lee et al., 2020;Zhu et al., 2021), and dialogue systems (Gao et al., 2022;Yu et al., 2021). Sparse retrieval methods, such as BM25, are practical and dominant approaches. However, retrieval methods based on pre-trained language models (PLMs) (Devlin et al., 2018;Liu et al., 2019) have gained popularity in recent years. PLMs are utilized to improve (1) sparse retrieval models that rely on lexical match; (2) dense retrieval models that model the semantic interaction between queries and passages in latent semantic space. \n\nTo improve sparse retrieval, PLM mitigates vocabulary mismatch by projecting each term in the query and passage to a vocabulary-sized weight vector. Each dimension in the weight vector represents the weight of a term in the PLM vocabulary, and the correlation between query and passage depends on lexical matching (Formal et al., 2021b,a;Shen et al., 2022). PLM-based sparse vectors model the features of passage on every lexical term (denoted by PLM logits output), and thus naturally capture the semantic at the lexicon level. SPLADE (Formal et al., 2021b,a) uses learnable PLM, e.g., BERT, to produce sparse vectors and fine-tune the retriever with contrastive learning loss. PLM-based SPLADE outperforms traditional sparse retrieval like BM25. LexMAE (Shen et al., 2022) further expands on SPLADE by proposing to pre-train a lexical-bottlenecked masked autoencoder to learn importance-aware sparse representations. It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck.\n...\nIt learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck. \n\nPLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021;Humeau et al., 2019;Xiong et al., 2020;Zhan et al., 2021Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances. CoT-MAE involves selecting two passages from a document as contextual passages, aiming to learn a better representation with the assistance of contextual information. \n\nA shallow encoder, typically with one or two layers, is used to reconstruct the masked passages with the help of context passage embedding. \n\nThe efficiency of PLM-based sparse retrieval (Bai et al., 2020;Formal et al., 2021b,a;Shen et al., 2022) and dense retrieval (Wu et al., 2022;Wang et al., 2022;Gao and Callan, 2021a) demonstrates that both sparse and dense representations effectively capture the text meaning. On the basis of contextual masked auto-encoder (CoT-MAE) tailored for dense retrieval, a natural thought is to incorporate the sparse representation pre-training into CoT-MAE, i.e., exploring multi-view representations in contextual masked auto-encoder pretraining and fine-tuning. Theoretically, sparse representation focuses on the lexicon while dense representation focuses on sentence semantics, offering different perspectives on the text. These two representations are probably compatible and complement each other. \n\nAside from representation variations, there is also a distinction in the structure of the decoder in the previous methods.",
            "reference_string": "[257985191 | Wu et al. | 2023 | Citations: 8]"
        },
        {
            "title": "NL2KQL: From Natural Language to Kusto Query",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 93,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294879238",
                    "name": "Amir H. Abdi"
                },
                {
                    "authorId": "2295304070",
                    "name": "Xinye Tang"
                },
                {
                    "authorId": "50821288",
                    "name": "Jeremias Eichelbaum"
                },
                {
                    "authorId": "2294847106",
                    "name": "Mahan Das"
                },
                {
                    "authorId": "2294855455",
                    "name": "Alex Klein"
                },
                {
                    "authorId": "2294876696",
                    "name": "Nihal Irmak Pakis"
                },
                {
                    "authorId": "2294877213",
                    "name": "William Blum"
                },
                {
                    "authorId": "2294875530",
                    "name": "Daniel L Mace"
                },
                {
                    "authorId": "2294879377",
                    "name": "Tanvi Raja"
                },
                {
                    "authorId": "2294871555",
                    "name": "Namrata Padmanabhan"
                },
                {
                    "authorId": "2294870992",
                    "name": "Ye Xing"
                }
            ],
            "abstract": "Data is growing rapidly in volume and complexity. Proficiency in database query languages is pivotal for crafting effective queries. As coding assistants become more prevalent, there is significant opportunity to enhance database query languages. The Kusto Query Language (KQL) is a widely used query language for large semi-structured data such as logs, telemetries, and time-series for big data analytics platforms. This paper introduces NL2KQL an innovative framework that uses large language models (LLMs) to convert natural language queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several key components: Schema Refiner which narrows down the schema to its most pertinent elements; the Few-shot Selector which dynamically selects relevant examples from a few-shot dataset; and the Query Refiner which repairs syntactic and semantic errors in KQL queries. Additionally, this study outlines a method for generating large datasets of synthetic NLQ-KQL pairs which are valid within a specific database contexts. To validate NL2KQL's performance, we utilize an array of online (based on query execution) and offline (based on query parsing) metrics. Through ablation studies, the significance of each framework component is examined, and the datasets used for benchmarking are made publicly available. This work is the first of its kind and is compared with available baselines to demonstrate its effectiveness.",
            "corpus_id": 268889861,
            "sentences": [
                {
                    "corpus_id": "268889861",
                    "title": "NL2KQL: From Natural Language to Kusto Query",
                    "text": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
                    "score": 0.5957728496552122,
                    "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
                    "char_start_offset": 3014,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1156
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 156,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 156,
                            "end": 159,
                            "matchedPaperCorpusId": "86611921"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "13403541"
                        },
                        {
                            "start": 201,
                            "end": 205,
                            "matchedPaperCorpusId": "53218829"
                        },
                        {
                            "start": 455,
                            "end": 458,
                            "matchedPaperCorpusId": "207210855"
                        },
                        {
                            "start": 458,
                            "end": 461,
                            "matchedPaperCorpusId": "1965270"
                        },
                        {
                            "start": 736,
                            "end": 740,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 927,
                            "end": 930,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 930,
                            "end": 932,
                            "matchedPaperCorpusId": "223637"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50732421875
                }
            ],
            "relevance_judgement": 0.50732421875,
            "relevance_judgment_input_expanded": "# Title: NL2KQL: From Natural Language to Kusto Query\n# Venue: arXiv.org\n# Authors: Amir H. Abdi, Xinye Tang, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing\n## Abstract\nData is growing rapidly in volume and complexity. Proficiency in database query languages is pivotal for crafting effective queries. As coding assistants become more prevalent, there is significant opportunity to enhance database query languages. The Kusto Query Language (KQL) is a widely used query language for large semi-structured data such as logs, telemetries, and time-series for big data analytics platforms. This paper introduces NL2KQL an innovative framework that uses large language models (LLMs) to convert natural language queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several key components: Schema Refiner which narrows down the schema to its most pertinent elements; the Few-shot Selector which dynamically selects relevant examples from a few-shot dataset; and the Query Refiner which repairs syntactic and semantic errors in KQL queries. Additionally, this study outlines a method for generating large datasets of synthetic NLQ-KQL pairs which are valid within a specific database contexts. To validate NL2KQL's performance, we utilize an array of online (based on query execution) and offline (based on query parsing) metrics. Through ablation studies, the significance of each framework component is examined, and the datasets used for benchmarking are made publicly available. This work is the first of its kind and is compared with available baselines to demonstrate its effectiveness.\n## RELATED WORKS 2.1 Retrieval-enhanced Generation\nHistorical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
            "reference_string": "[268889861 | Abdi et al. | 2024 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Passage retrieval aims to retrieve the relevant passages of a query from a large corpus, which benefits many downstream applications, such as web search (Fan et al. 2021;Guo et al. 2022;Lin, Nogueira, and Yates 2021), question answering (Karpukhin et al. 2020;Lee et al. 2020;Zhu et al. 2021) and dialogue systems (Gao et al. 2022a;Yu et al. 2021).\n\nFor a long time, sparse retrieval represented by BM25 (Robertson, Zaragoza et al. 2009) was the dominant retrieval method. Recently, dense retrieval has received increasing attention with the development of pre-trained language models (PLM) (Devlin et al. 2018;Liu et al. 2019). Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al. 2021;Humeau et al. 2019;Xiong et al. 2020;Zhan et al. 2021Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al. 2021;Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
            "score": 0.6678573011873844,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 170,
                    "end": 186,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 186,
                    "end": 216,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 332,
                    "end": 347,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 404,
                    "end": 436,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 834,
                    "end": 858,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 895,
                    "end": 911,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1402,
                    "end": 1418,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "256461130",
            "title": "Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples",
            "text": "Passage retrieval, which matches relevant passages to queries, is an important task in Information Retrieval (IR). It can be also integrated as a core component to solve many Natural Language Processing (NLP) problems, e.g., open domain question answering (Chen et al., 2017), fact checking (Thorne et al., 2018), etc. Powered by large scale pretrained language models (e.g. BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2020)), dense passage retrieval, which explores dense vector representations to match relevant passages, has attracted growing interest (Gao et al., 2020;Khattab and Zaharia, 2020;Karpukhin et al., 2020;MacAvaney et al., 2020a;Qu et al., 2021;Zhan et al., 2021;Xu et al., 2022;Gao et al., 2021a). Dense passage retrieval models usually adopt a bi-encoder (also known as dual-encoder) architecture, one encoder for encoding queries, the other for learning passage representations, which can be computed offline. With dense representations of a query and document, passage ranking is recast as a nearest neighbor search problem that can be efficiently solved by similarity search toolkits tailored for dense vectors, such as Faiss (Johnson et al., 2019). \n\nDense passage retrieval models are usually trained via Contrastive Learning (CL), which encourages query representations to be close to positive (i.e., relevant) passages and away from negative (i.e., irrelevant) passages in the learned semantic space. Many previous studies demonstrate the effectiveness of CL in dense passage retrieval (Karpukhin et al., 2020;Qu et al., 2021;Xu et al., 2022). Under this CL-based dense passage retrieval setting, using more negative samples has proved beneficial to models (Wu et al., 2020;Chen et al., 2020;He et al., 2020;Giorgi et al., 2021;Gao et al., 2021c).",
            "score": 0.6430833797885549,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 438,
                    "end": 459,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "222205492"
                },
                {
                    "start": 608,
                    "end": 634,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 634,
                    "end": 657,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 657,
                    "end": 681,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 681,
                    "end": 697,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 715,
                    "end": 731,
                    "matchedPaperCorpusId": "247411106"
                },
                {
                    "start": 731,
                    "end": 749,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1183,
                    "end": 1205,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1547,
                    "end": 1571,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1571,
                    "end": 1587,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1587,
                    "end": 1603,
                    "matchedPaperCorpusId": "247411106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Dense retrieval models based on neural text representations have proven very effective Qu et al., 2021;Ram et al., 2022;Izacard et al., 2022a,b), improving upon strong traditional sparse models like BM25 (Robertson and Zaragoza, 2009). However, when applied off-theshelf (i.e., in out-of-domain settings) they often experience a severe drop in performance (Thakur et al., 2021;Sciavolino et al., 2021;Reddy et al., 2021). Moreover, the reasons for such failures are poorly understood, as the information captured in their representations remains under-investigated.  Figure 1: An example of our framework. We run the question \"Where was Michael Jack born?\" through the question encoder of DPR , and project the question representation e q to the vocabulary space using BERT's masked language modeling head . The result is a distribution over the vocabulary, Q. We apply the same procedure for passages as well. These projections enable reasoning about and improving retrieval representations.\n\nIn this work, we present a new approach for interpreting and reasoning about dense retrievers, through distributions induced by their query 2 and passage representations when projected to the vocabulary space, namely distributions over their vocabulary space (Figure 1). Such distributions enable a better understanding of the representational nature of dense models and their failures, which paves the way to simple solutions that improve their performance.  We begin by showing that dense retrieval representations can be projected to the vocabulary space, by feeding them through the masked language modeling (MLM) head of the pretrained model they were initialized from without any further training. This operation results in distributions over the vocabulary, which we refer to as query vocabulary projections and passage vocabulary projections.\n\nSurprisingly, we find these projections to be highly interpretable to humans ( Figure 2; Table 1). We analyze these projections and draw interesting connections between them and well-known concepts from sparse retrieval ( \u00a75). First, we highlight the high coverage of tokens shared by the query and the passage in the top-k of their projections. This obersvation suggests that the lexical overlap between query and passages plays an important role in the retrieval mechanism. Second, we show that vocabulary projections of passages they are likely to contain words that appear in",
            "score": 0.6287560811340929,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130859375
        },
        {
            "corpus_id": "274982759",
            "title": "GraphAgent: Agentic Graph Language Assistant",
            "text": "References: ID: 1. Dense passage retrieval has gained significant attention in recent years, leveraging the power of neural networks to learn complex language patterns and query-document relations @CITE[2]@. This approach has many intriguing advantages over sparse retrieval, such as better capturing of language structures and improved performance on queries written in natural languages @CITE[2]@. However, the effectiveness of dense retrieval often requires combination with sparse retrieval, and the training mechanisms used in dense retrieval can be a bottleneck @CITE[3]@. To address this, various techniques have been proposed, including Approximate Nearest Neighbor Negative Contrastive Learning @CITE[3]@, unsupervised corpus-aware language model pre-training @CITE[4]@, and leveraging passage-centric similarity relations @CITE[5]@. These advancements have significantly improved the performance of dense passage retrieval models, making them a promising approach for various natural language processing tasks. \n\n[Llama3-8b Fine-tuned] Recent advancements in PLMs such as BERT (De-vlin et al., 2018), Roberta (@CITE[1]@, and GPT (@CITE[3]@ have enabled numerous unsupervised techniques to derive dense represen-tations of queries and passages for retrieval. These approaches primarily use a Bi-Encoder structure to embed text in a low-dimensional space and learn similarity relations via contrastive learning, con-trasting traditional sparse retrieval methods like BM25 or DeepCT (@CITE[2]@;. DPR ( pi-oneered an unsupervised dense passage retrieval framework, affirming the feasibility of using dense representations for retrieval independently. This efficient and operational approach was further refined by subsequent studies (@CITE[4]@; ; ; @CITE[5]@ that focused on high-quality negative sample min-  In our June 2019 report, we found that, while abuse deficiencies cited in nursing homes were relatively rare from 2013 through 2017, they became more frequent during that time, with the largest increase in severe cases. Specifically, abuse deficiencies comprised less than 1 percent of the total deficiencies in each of the years we examined, which is likely conservative.",
            "score": 0.6267894763629664,
            "section_title": "[GraphAgent]",
            "char_start_offset": 55957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1614990234375
        },
        {
            "corpus_id": "248366563",
            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
            "text": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
            "score": 0.62459187650932,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1689,
                    "end": 1693,
                    "matchedPaperCorpusId": "5541663"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "215416349"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Large-scale query-passage retrieval is a core task in search systems, which aims to rank a collection of passages based on their relevance with regard to a query. To balance efficiency and effectiveness, existing work typically adopts a two-stage retrieval pipeline (Ren et al., 2021b;Zhu et al., 2021). The first-stage aims to retrieve a subset of candidate passages by a recall model from the entire corpus and the second stage aims to rerank the retrieved passages. In the first-stage retrieval, traditional approaches (Chen et al., 2017) implemented term-based retriever (e.g. TF-IDF and BM25) by weighting terms based on their frequency, which have limitations on representing semantics of text. Recently, dense passage retrieval is drawing more and more attention in the task of passage retrieval (Karpukhin et al., 2020). The underlying idea is to represent both queries and passages as embeddings, so that the semantic relevance can be measured via embeddings similarity. With the great success of pre-trained language models (PLMs) such as BERT/RoBERTa (Devlin et al., 2019;Liu et al., 2019) in natural language processing tasks, dense retrieval models parameterized by PLMs is emerging as the new state-of-the-art in a variety of passage retrieval tasks (Karpukhin et al., 2020;Xiong et al., 2020). \n\nTwo paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss.",
            "score": 0.6241989652925358,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 285,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 803,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1062,
                    "end": 1083,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1264,
                    "end": 1288,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1288,
                    "end": 1307,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1279296875
        },
        {
            "corpus_id": "267068650",
            "title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.",
            "text": "Passage Retrieval. Passage retrieval aims to find relevant information from large passage collections based on a given search query. Traditional techniques for ad-hoc retrieval, such as TF-IDF and BM25 [26], perform relevant searches based on lexicon terms and inverted document frequencies. Such methods have efficient retrieval abilities, but they suffer from vocabulary mismatch problems [12]. Although there are continuous techniques like query expansions [10,21,30] and query rewrite [9] to alleviate such mismatch issues, additional preprocess of queries or passage collections make the retrieval pipelines much more complicated and hard to maintain. \n\nDense retrieval [12,22] has become popular in mainstream retrieval applications recently, because it maps the textual information into learnable representations and naturally alleviates the lexicon mismatch problems by learning the similarity traits of queries and passages through end-to-end contrastive learning. Dense retrieval encodes queries and passage collections as condensed lowdimension sentence representations with pre-trained language models (PLMs), and performs retrieval based on the similarity of representations with efficient search algorithms like maximum innerproduct search (MIPS) [19]. Notably, there are continuous efforts [3,5,27,37] to adapt the PLMs to sparse retrieval. There are also extended studies about hybrid retrievals [15,33], which combines both dense and sparse retrievals to bring superior retrieval performances. However, our work primarily focuses on dense retrieval pre-training and is orthogonal to these works.",
            "score": 0.614145035696921,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1612
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 395,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "1448737"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "16464483"
                },
                {
                    "start": 1308,
                    "end": 1310,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1310,
                    "end": 1313,
                    "matchedPaperCorpusId": "251953412"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "266977305"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "253553743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091552734375
        },
        {
            "corpus_id": "258833383",
            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
            "text": "Passage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
            "score": 0.6137555086636334,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 136,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 136,
                    "end": 153,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 195,
                    "end": 212,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 263,
                    "end": 282,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 508,
                    "end": 532,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1314,
                    "end": 1330,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1708,
                    "end": 1726,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1747,
                    "end": 1765,
                    "matchedPaperCorpusId": "252917569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60302734375
        },
        {
            "corpus_id": "248524694",
            "title": "Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings",
            "text": "With the advances in neural language modeling [3], learning dense representations for text has become a vital component in many information retrieval (IR) tasks. In passage ranking and open-domain question answering, dense retrieval has become a new paradigm to retrieve relevant passages [6,7,9]. In contrast to traditional termbased IR models (TF-IDF and BM25) that fail to capture beyond lexical matching, dense retrieval learns dense representations of questions and passages for semantic matching. \n\nA typical approach for dense retrieval involves learning a dualencoder for embedding the questions and passages [6]. A dualencoder model consists of two separate neural networks optimized to score relevant (i.e., positive) question-passage pairs higher than irrelevant (i.e., negative) ones. At inference time, the score of a question-passage pair is computed as the inner product of the corresponding question and passage embeddings. Due to their high efficiency, dual-encoders are popular first-stage rankers in largescale settings (in contrast to cross-encoders where even though they can achieve higher performance, they are not indexable and therefore are used as re-rankers [4,15]). The whole corpus can be encoded and indexed offline, while at inference time, high-scoring passages with respect to a question can be found using efficient maximum inner product search [5]. \n\nSo far, dense retrieval models have been evaluated on clean and curated datasets. However, these models will encounter usergenerated noisy questions when deployed in real-life applications. Questions can include typos because of users mistyping words, such as keyboard typos (additional/missing character and character substitution), phonetic typing errors due to the close pronunciation, and misspellings. How these typos affect the encoding of questions and whether dense dual-encoder retrieval models are robust to them is not studied yet. \n\nWorks on text classification have shown that deep neural language models such as BERT are not robust against typos [13,16], even though they apply the WordPiece tokenization.",
            "score": 0.6132968339138566,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 49,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 292,
                    "end": 294,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 294,
                    "end": 296,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1185,
                    "end": 1188,
                    "matchedPaperCorpusId": "231662379"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "237592852"
                },
                {
                    "start": 1379,
                    "end": 1382,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1048583984375
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Existing dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
            "score": 0.6109457336083027,
            "section_title": "Related Work",
            "char_start_offset": 3117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 87,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 118,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 625,
                    "end": 642,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 661,
                    "end": 681,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 927,
                    "end": 946,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52783203125
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "The dual-encoder architecture and lightweight dot product evaluation enable us to encode and index all passages in the collection C beforehand, so we only need to encode the given query for online retrieval, achieving more efficiency. \n\nLearning Framework for Retriever. To train the dual-encoder  , we utilize contrastive learning following previous works [15,49]. Specifically, with a given query , a labeled positive passage  + , and negative passages N , contrastive loss can be applied to optimize the dual-encoder  by maximizing the relevance of the  and  + while minimizing that of  and  \u2208 N , i.e., \n\nwhere negative passage set N can be generated from top-ranked non-answer passages in retrieval results of BM25 model [34] or a trained retrievers [52,53], i.e., \n\nwhere  is a probability distribution over C, which can be defined as non-parametric (e.g.,  samp = \u2298) or parametric (e.g.,  samp \u2260 \u2298). \n\nDense & Lexicon-Aware Retrievers. Both dense retriever ( den ) and lexicon-aware retriever ( lex ) follow dual-encoder architecture and the encoders are built upon PLMs like BERT [9]. Precisely, a PLM ( plm ) encodes a given text (i.e., query  or passage ),  = { 1 ,  2 , ...  }, to contextualized embeddings, i.e., \n\neventually \n\n[CLS] and [SEP] are special tokens designed for sentence representation and separation by recent PLMs [9,30]. Dense retriever [35,49] represents text by using the embedding of special token [CLS] (i.e.,   \n\n[CLS] ) as follows, \n\nwhere  den =  plm with no additional parameters. For lexicon-aware retriever, we adopt SPLADE [10] which learns to predict the weights of terms in PLM vocab for each token in the input  by the Masked Language Modeling (MLM) layer and sparse regularization, then max-pooling these weights into a discrete text representation after log-saturation.",
            "score": 0.6028449521920181,
            "section_title": "R (\ud835\udc5e, \ud835\udc5d",
            "char_start_offset": 10105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 759,
                    "end": 762,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1088,
                    "end": 1091,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1342,
                    "end": 1345,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1370,
                    "end": 1373,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061431884765625
        },
        {
            "corpus_id": "275931943",
            "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models",
            "text": "Karpukhin et al. [17] show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder. Later works in this direction mainly focused on optimizing the training process, e.g. through mining hard negatives [19], or on the domain shift issue [20,10]. \n\nIn the late-interaction based approach, ColBERT [21] is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs. \n\nThough the models based on pretrained language models show effectiveness in passage retrieval tasks, they face limitations with handling long documents.",
            "score": 0.6026297097153278,
            "section_title": "Representation-based Models for IR",
            "char_start_offset": 4206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "268384724"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3466796875
        },
        {
            "corpus_id": "265457282",
            "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
            "text": "In contrast to traditional IR methods, such as BM25 (Robertson and Zaragoza, 2009), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings: \n\nwhere E \u2022 (\u2022; \u03b8) is an encoder parameterised by \u03b8. \n\nThe adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2021) can be performed to fetch the top-k passages. Contrastive learning is applied to train the dualencoder. Given a query q, we have a positive passage p + and a set of n negative passages P \u2212 q = {p \u2212 i } n i=1 . The model is being optimised by minimising the negative log likelihood of the positive passage: \n\nP \u2212 q is the set of irrelevant passages constructed from in-batch negatives (Chen et al., 2020) (i.e. positive passages of other queries in the same minibatch) and mined hard negatives from existing retrievers (Karpukhin et al., 2020;Xiong et al., 2021).",
            "score": 0.6016079986287697,
            "section_title": "Dense Passage Retrieval",
            "char_start_offset": 5840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 82,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 348,
                    "end": 372,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1264,
                    "end": 1283,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1398,
                    "end": 1422,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1422,
                    "end": 1441,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1572265625
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Large-scale passage retrieval [2] aims to fetch relevant passages from a million-or billion-scale collection for a given query to meet users' information needs, serving as an important role in many downstream applications including open domain question answering [20], search engine [56], and recommendation system [54], etc. Recent years have witnessed an upsurge of interest and remarkable performance of dense passage retrievers on first-stage retrieval. Built upon powerful pre-trained language models (PLM) [9,30,36], dense retrievers [20,35,49] encode queries and passages into a joint low-dimensional semantic space in a Siamese manner (i.e. dual-encoder), so that the passages could be offline pre-indexed and query could be encoded online and searched via approximate nearest neighbor [19], reaching an efficiency-effectiveness trade-off. \n\nAlthough dense retrieval becomes indispensable in modern systems, a long-term challenge is that the dense representations in a latent semantic space are abstractive and condensed, exposing the systems to a risk that pivot phrases and mentions may be overlooked and thus leading to sub-optimal efficacy. For example, DPR [20] didn't regard \"Thoros of Myr\" as an entity mention in the query \"Who plays Thoros of Myr in Game of Thrones?\". Analogously, given the query \"What is an active margin\", ANCE [49] overlooked the \"active margin\" as an entire local salient phrase and hence retrieved passages related to the financial term \"margin\". As a remedy, prior works resort to either coupling a dense retriever with the term matching scores (e.g., TF-IDF, BM25) [7,14,26,33] or learning BM25 ranking into a dense model as additional features to complement the original one [4]. But, these approaches are limited by superficial combinations and almost unlearnable BM25 scoring.",
            "score": 0.5996922384164329,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 544,
                    "end": 547,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1610,
                    "end": 1613,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09320068359375
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae.",
            "score": 0.5983602454785923,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162109375
        },
        {
            "corpus_id": "268889861",
            "title": "NL2KQL: From Natural Language to Kusto Query",
            "text": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
            "score": 0.5957728496552122,
            "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
            "char_start_offset": 3014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1156
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "13403541"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "1965270"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 930,
                    "end": 932,
                    "matchedPaperCorpusId": "223637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50732421875
        },
        {
            "corpus_id": "252846580",
            "title": "Decoupled Context Processing for Context Augmented Language Modeling",
            "text": "For auto-regressive language modeling, we use English C4 [32] version 2.2.1, the same as Retro. We train the language model and prepare the retrieval database using the train split and evaluate the results using validation split. The language model target sequence is a sliding window (chunk)  of s = 64 tokens, with at most n = 448 preceding tokens are used as input sequence. This setup is similar to XLNet [43] and Retro [4]. The target and input sequences that are smaller than the given window size (64 and 448, respectively) are padded with zeros. \n\nTo construct the retrieval database, the same sliding window processing is also used for the context sequences. The database is formed as associated pairs of retrieval embedding and encoder output: \n\n, where context c are the sliding window of 512 tokens with a stride of 64 tokens. We choose our hyper-parameters to be comparable to Retro: chunk size s = 64 and input window size n = 448 (smaller than 2048 that of Retro). This also implies that the number of entries in the database is larger than the number of articles, but smaller than the number of tokens. \n\nOur training corpus is in the form of triplets (x, y, c). x and y are acquired directly by applying sliding window on the train split of C4. Then BM25 [34] is used as a bootstrapping retriever to mine relevant context c to from the database. The first retrieval results with no more than 8 consecutive token overlap with the target is used as context. Figure 2a illustrate the sliding window construction of database as well as sequence served as input and targets. Table 2b gives the exact number of entries in the resulting database used as external context. \n\nWe use mT5 [42] as the backbone architecture for our context augmented Encoder-Decoder, and train our models from scratch. Train split is used both for training and retrieval, while validation split is used for bits-per-byte (Bpb) evaluation. In auto-regressive language model evaluation, due to the fact that text are crawled from web sources, there can be a non-trivial overlap of tokens between the training and validation splits.",
            "score": 0.5935745326348856,
            "section_title": "Experimental Setup",
            "char_start_offset": 12316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "225040574"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1055908203125
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "Passage Retrieval Passage retrieval task aims to recall all potentially relevant passages from a large corpus given an informationseeking query. In practical, passage retrieval is often an important step in other information retrieval tasks [4]. Traditional passage retrieval systems usually rely on term-based retrieval models like BM25 [46]. Recently, with the rapid development in text representation learning research [3] and deep pre-trained language models [21,27,33,51], dense retrieval combined with pre-trained language models, has become a popular paradigm to improve retrieval performance [16,26,42]. In general, dense models significantly outperform traditional term-based retrieval models in terms of effectiveness and benefit downstream tasks. In a basic concept, the core problem of passage retrieval is how to form the text representation and then compute text similarity. Thus, based on the text representation type and corpus index mode, passage retrieval models can be roughly categorized into two main classes. Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]. \n\nFor the above two types of models, the current optimization directions are not the same. Specifically, Sparse retrieval models \n\n(Adults should not put their hands on the chest of a sleeping baby as this is not good for the baby's breathing.) \n\nfocus on improving retrieval performance by either enhancing the bag-of-words (BoW) representations in classical term-based methods or mapping input texts into latent space (e,g. doc2query [37], query expansion [7] and document expansion [39]). The sparse representation has attracted great attention as it can be easily integrated into the inverted index for efficient retrieval. Recently, With the development of deep neural networks, pre-trained language models have been widely employed to improve the capacity of sparse retrieval models, including term re-weighting [9,10], sparse representation learning [24,50], etc. The mainstream of existing studies on improving the performance of dense retrieval models can be roughly divided into three groups. 1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation.",
            "score": 0.5892989390890149,
            "section_title": "RELATED WORK",
            "char_start_offset": 4763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2380
                },
                {
                    "start": 2381,
                    "end": 2538
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "1058977"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "1815908"
                },
                {
                    "start": 2199,
                    "end": 2202,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 2239,
                    "end": 2242,
                    "matchedPaperCorpusId": "235293983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0177154541015625
        },
        {
            "corpus_id": "257279794",
            "title": "Retrieval for Extremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-Ranker",
            "text": "It is noteworthy to mention that while our proposed methods leverage bi-encoders, specifically Sentence-BERT Reimers and Gurevych [56], to re-rank the first-stage retriever's ranked list, our approach fundamentally diverges from dense passage retrievers that employ bi-encoders as first stage retrievers in terms of both methodology and application [27]. Dense retrievers emphasize on the alignment of query and relevant document representations by optimizing a targeted loss function, such as the mean square error between the query and relevant document vectors while we do not have such training in our proposed. Moreover, optimizing dense retrievers on the query-by-document task with extremely lengthy queries or documents is not computationally possible due to BERT model's maximum word limit of 512 words which prevents from representing each query or document by a single pass to a bi-encoder, and as a result, prevents from being able to optimizing the representation of query and document as they do not have one union representation. There could be future work on studying how applying dense passage retrieval on query by document task which is out of the scope of this study.",
            "score": 0.5859378880741901,
            "section_title": "Sentence embedding",
            "char_start_offset": 14771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1187
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04400634765625
        },
        {
            "corpus_id": "274776247",
            "title": "A Distributed Collaborative Retrieval Framework Excelling in All Queries and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation",
            "text": "The retrieval task aims to recall and rank the passages based on the relevence to the query, such as BM25 and Contriever (Izacard et al., 2021). In the retrieval scenario, rather than dual-encoder models (Karpukhin et al., 2020) that separately encode the passages and queries, models that see query and passage information jointly in inference time (Nogueira and Cho, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating retrieval as sequence generation, such as conducting listwise sorting (Sun et al., 2023;Pradeep et al., 2023;Yoon et al., 2024) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. However, all of these methods utilize only one model to rank the passages across all scenarios, leading to the limitation of models' applicability and generalization.",
            "score": 0.5848754239571021,
            "section_title": "Retrieval Models",
            "char_start_offset": 4513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.",
            "score": 0.5838754913544614,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08404541015625
        },
        {
            "corpus_id": "258685571",
            "title": "Efficient and Effective Tree-based and Neural Learning to Rank",
            "text": "One of the major innovations in neural LtR research was the evolution of \"cross-encoders\" such as monoBERT and duoBERT to two-tower or \"bi-encoder\" models. Instead of learning a parameterized function (like BERT) that takes a query-document pair (or multiple documents) as input and simultaneously learns representations (i.e., features) and predicts their relevance, bi-encoders disentangle the relevance prediction function from representation learning, and simplify the former as much as possible. The idea is to push much of the complex, time-consuming inference operations offline and thereby speed up query processing.\n\nIn its most basic form, this design resembles the representation-based neural ranker architecture of Figure 7.1(a), but where \u03c6 d and \u03c6 q are pre-trained large language models that may be further fine-tuned. Because the representation of documents is independent of queries, we may store the document vectors in an offline index and, during retrieval, compute the representation of the query and find its closest document vectors. This paradigm is often referred to as \"dense retrieval\" or \"semantic search,\" which is typically solved using a k nearest neighbor (NN) search or k approximate nearest neighbor (ANN) search algorithm. We illustrate this procedure in Figure 8.3.\n\nThe feasibility of this basic idea was demonstrated by several works (Zhan et al., 2020;Karpukhin et al., 2020;Ma et al., 2021a;Qu et al., 2021). Karpukhin et al. (2020) present what they call Dense Passage Retrieval (DPR) and show that dense vector representations can indeed be used to perform the retrieval task following the recipe above. Interestingly, effective representations can be learnt from a small number of questions and passages (in a question-answering task) by a simple bi-encoder framework. They evaluate DPR on a wide range of open-domain QA datasets, with the results showing that DPR outperforms a strong BM25 system by 9%-19% absolute points in terms of top-20 passage retrieval accuracy.\n\nIn the general dense retrieval framework of Figure 8.3, there are two key factors that contribute to the overall efficiency and effectiveness: the ANN algorithm itself",
            "score": 0.582155582410876,
            "section_title": "Dense retrieval",
            "char_start_offset": 149848,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1414,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1431,
                    "end": 1447,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1449,
                    "end": 1472,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1943359375
        },
        {
            "corpus_id": "252917569",
            "title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
            "text": "Dense retrieval is important to many web applications. By letting semantically correlated query and document represented as spatially close embeddings, dense retrieval can be efficiently conducted via approximate nearest neighbour search, such as PQ (Jegou et al., 2010;Xiao et al., 2021Xiao et al., , 2022a) ) and HNSW (Malkov and Yashunin, 2018). Recently, large-scale language models have been widely used as the encoding networks for dense retrieval (Karpukhin et al., 2020;Xiong et al., \u2020. The two researchers make equal contributions to this work and are designated as co-first authors. * . Corresponding author. 2020; Luan et al., 2021). The mainstream models, e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2019), are usually pre-trained by token-level tasks, like MLM and Seq2Seq. However, the sentence-level representation capability is not fully developed in these tasks, which restricts their potential for dense retrieval. \n\nGiven the above defect, there have been increasing interests to develop retrieval oriented pretrained models. One popular strategy is to leverage self-contrastive learning (Chang et al., 2020;Guu et al., 2020), where the model is trained to discriminate positive samples from data augmentation. However, the self-contrastive learning can be severely limited by the data augmentation's quality; besides, it usually calls for massive amounts of negative samples (He et al., 2020a;Chen et al., 2020). Another strategy relies on auto-encoding (Gao and Callan, 2021;Lu et al., 2021;Wang et al., 2021), which is free from the restrictions on data augmentation and negative sampling. The current works are differentiated in how the encoding-decoding workflow is designed, and it remains an open problem to explore more effective auto-encoding framework for retrieval oriented pre-training.",
            "score": 0.570032314011083,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1854
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 270,
                    "matchedPaperCorpusId": "5850884"
                },
                {
                    "start": 320,
                    "end": 347,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 454,
                    "end": 478,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 679,
                    "end": 700,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1432,
                    "end": 1450,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1450,
                    "end": 1468,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1511,
                    "end": 1533,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1533,
                    "end": 1549,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.032470703125
        },
        {
            "corpus_id": "268031876",
            "title": "Generative Retrieval with Large Language Models",
            "text": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
            "score": 0.5671587266286197,
            "section_title": "Related Work",
            "char_start_offset": 21339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 175,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "239016601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "273185905",
            "title": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA",
            "text": "Retrieval Traditional retrieval techniques, such as TF-IDF (Jones, 1972) and BM25 (Robertson et al., 1995;Robertson & Zaragoza, 2009), retrieve information based on word terms. Subsequently, deep learning-based retrieval methods quickly became popular. REALM (Guu et al., 2020) augments the language model pre-training with a latent knowledge retriever using masked language modeling. DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) encodes queries and documents as dense embeddings, with similarity computed between them. ColBERT (Khattab & Zaharia, 2020;Santhanam et al., 2022) produces multi-vector representations at the token level. JPR (Joint Passage Retrieval) (Min et al., 2021) is a joint passage retrieval model with an autoregressive reranker that selects a sequence of passages. DHR (Dense Hierarchical Retrieval) (Liu et al., 2021) leverages both macroscopic document-level semantics and microscopic passage-level semantics. Fusion-in-Decoder (Izacard & Grave, 2021) employs both DPR and BM25 in a knowledge distillation manner, which does not require annotated query-document pairs. CPT-text (Neelakantan et al., 2022) utilizes contrastive pre-training on unsupervised data. NCI (Wang et al., 2022) directly generates relevant document identifiers for a given query. Atlas (Izacard et al., 2022) fine-tunes an encoder-decoder model with a retriever to address knowledge-intensive tasks with minimal training examples. RETRO (Borgeaud et al., 2022;Wang et al., 2023a) conditions on document chunks based on local similarity with preceding tokens. HHR (Hybrid Hierarchical Retrieval) (Arivazhagan et al., 2023) combines sparse and dense retrieval methods across both document and passage retrieval stages. SimLM (Wang et al., 2023b) proposes a new loss function to reduce the mismatch between pre-training and fine-tuning input distributions.",
            "score": 0.5650923003489969,
            "section_title": "RELATED WORK",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 106,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 106,
                    "end": 133,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 259,
                    "end": 277,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 415,
                    "end": 439,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 538,
                    "end": 563,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 563,
                    "end": 586,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 675,
                    "end": 692,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 833,
                    "end": 851,
                    "matchedPaperCorpusId": "240288895"
                },
                {
                    "start": 963,
                    "end": 986,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1200,
                    "end": 1219,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "258170263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477294921875
        },
        {
            "corpus_id": "268031986",
            "title": "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision",
            "text": "Figure 2 shows the overall structure of our model. In this model, the bottom layers of the encoder function as the retriever, encoding queries and passages independently for efficient retrieval. The remaining encoder layers and the entire decoder are designated as the reader for question answering. \n\nRetriever. The retriever is a bi-encoder that uses the first B encoder layers with H heads to encode query q and passages d from a corpus D. We use the query Q and key vectors K in B + 1-th layer as their embeddings, respectively (Jiang et al., 2022): \n\nwhere |q| and |d| are sequences lengths and e is the dimension of each head. The self-attention matrix SA B+1,h q,d from a specific head (h = 6 (Jiang et al., 2022)) is considered the source of retrieval scores. A sum of max computations (Khattab and Zaharia, 2020) is performed to reduce it to yield the retrieval score: \n\nWe denote this as Multi-Vector Retrieval and consider it as our default setting. We also explore Dense Retrieval, which takes the average pooling of layer B's output with LayerNorm as query Q q and passage K d representations, and the relevance is measured by their dot product: \n\nThe top-k most relevant passages are then retrieved by D q = arg topk d i \u2208D P be (\u2022|q, D) = arg topk s(q, d 0 ), . . . , s(q, d |d\u2208D| ) . \n\nReader. The encoded query and each top-k passage in D q are concatenated and fed into the remaining cross-encoder layers. Finally, the joint encodings {E q,d i } |Dq| i=0 are integrated into the decoder through cross-attention to generate the answer a efficiently (Izacard and Grave, 2021b): P ans (a|q, D q ) = log T t=1 P (a t |a <t , q, D q ).",
            "score": 0.5637157472000653,
            "section_title": "Model Architecture",
            "char_start_offset": 4892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "254246471"
                },
                {
                    "start": 700,
                    "end": 720,
                    "matchedPaperCorpusId": "254246471"
                },
                {
                    "start": 794,
                    "end": 821,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1566,
                    "end": 1592,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0262603759765625
        },
        {
            "corpus_id": "277502276",
            "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
            "text": "The fundamental principle of dense retrieval is to transform documents and queries into vector representations. The introduction of pre-trained language models, particularly BERT [8], has revolutionized information retrieval, paving the way for dense retrieval methods such as Dense Passage Retrieval (DPR) [7], ColBERT [9], and GTR [13]. Techniques like SimCSE [5] leverage contrastive learning with models such as BERT and Roberta to optimize embeddings. Additionally, dense retrieval methods often employ Approximate Nearest Neighbor (ANN) search [6,25], Maximum Inner Product Search (MIPS) algorithms [21], and SimLM [24] to ensure efficient retrieval in sub-linear time. \n\nUnlike traditional sparse retrieval, these methods utilize transformer encoders to create dense vector representations for queries and documents, enhancing semantic understanding and retrieval accuracy. This combination of semantic depth and computational efficiency positions dense retrieval as a leading approach in modern information retrieval.",
            "score": 0.563129220600743,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 307,
                    "end": 310,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056976318359375
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.",
            "score": 0.5628380475835884,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 188,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 188,
                    "end": 204,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 246,
                    "end": 263,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1025,
                    "end": 1049,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1049,
                    "end": 1068,
                    "matchedPaperCorpusId": "247476289"
                },
                {
                    "start": 1488,
                    "end": 1510,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 1510,
                    "end": 1526,
                    "matchedPaperCorpusId": "233307194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2301025390625
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "Passage retrieval is an important component in applications like ad-hoc information retrieval, opendomain question answering (Karpukhin et al., 2020), retrieval-augmented generation (Lewis et al., 2020) and fact verification (Thorne et al., 2018). Sparse retrieval methods such as BM25 were the dominant approach for several decades, and still play a vital role nowadays. With the emergence of large-scale pre-trained language models (PLM) (Devlin et al., 2019), increasing attention is being paid to neural dense retrieval methods (Yates et al., 2021). Dense retrieval methods map both queries and passages into a low-dimensional vector space, where the relevance between the queries and passages are measured by the dot product or cosine similarity between their respective vectors. Like other NLP tasks, dense retrieval benefits greatly from a strong general-purpose pre-trained language model. However, general-purpose pretraining does not solve all the problems. As shown in Table 1, improved pre-training techniques that are verified by benchmarks like GLUE  do not result in consistent performance gain for retrieval tasks. Similar observations are also made by Lu et al. (2021). We hypothesize that, to perform robust retrieval, the [CLS] vector used for computing matching scores should encode all the essential information in the passage. The next-sentence prediction (NSP) task in BERT introduces some supervision signals for the [CLS] token, while RoBERTa  and ELECTRA do not have such sequence-level tasks.\n\nIn this paper, we propose SimLM to pre-train a representation bottleneck with replaced language modeling objective. SimLM consists of a deep encoder and a shallow decoder connected with a representation bottleneck, which is the [CLS] vector in our implementation. Given a randomly masked text segment, we first employ a generator to sample replaced tokens for masked positions, then use both the deep encoder and shallow decoder to predict the original tokens at all positions. Since the decoder only has limited modeling capacity, it must rely on the representation bottleneck to perform well on this pre-training task. As a result, the encoder will learn to compress important semantic information into the bottleneck, which would help train biencoder-based 1 dense retrievers. Our pretraining objective works",
            "score": 0.5620919245988769,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 149,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 225,
                    "end": 246,
                    "matchedPaperCorpusId": "53645946"
                },
                {
                    "start": 440,
                    "end": 461,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21533203125
        },
        {
            "corpus_id": "276580326",
            "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers",
            "text": "Dense Passage Retrieval (Karpukhin et al., 2020) utilizes a pre-trained language model such as BERT (Devlin et al., 2019), to encode text into dense vectors and conduct passage retrieval as a nearest neighbor search. This approach has shown strong in-domain effectiveness compared to traditional lexical retrievers such as BM25 (Robertson and Zaragoza, 2009). However, dense retrievers have been found to struggle with generalization when applied to out-of-domain retrieval tasks (Thakur et al., 2021). To address this issue, various works have aimed to improve the generalization of dense retrievers through continuous pre-training tailored for retrieval tasks. Works such as Condenser (Gao and Callan, 2021), RetroMAE (Xiao et al., 2022), and SimLM (Wang et al., 2023) have enhanced the dense representation of BERT via customized architectures during language modeling. Other works, including Contriever (Izacard et al., 2022), GTE (Li et al., 2023), E5 (Wang et al., 2024a) have further adapted two-stage contrastive learning. These models are first trained with unsupervised or weakly supervised large-scale contrastive learning, followed by supervised contrastive learning with available relevance-judged data (Nussbaum et al., 2024;Yu et al., 2024). CDE (Morris and Rush, 2024) further proposes a two-stage model architecture that integrates corpus-level information into document embeddings. In this work, we propose a data augmentation approach based on large language models, enabling the creation of high-quality augmentation data for smaller retrievers.",
            "score": 0.5620316509187109,
            "section_title": "Robust Dense Retrieval",
            "char_start_offset": 2248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 48,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 328,
                    "end": 358,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 687,
                    "end": 709,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1236572265625
        },
        {
            "corpus_id": "274776247",
            "title": "A Distributed Collaborative Retrieval Framework Excelling in All Queries and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation",
            "text": "Dense retrieval methods represent a shift from traditional sparse retrieval models by leveraging dense vector representations of both queries and documents. They can be usually categorized into three main types: (1) Bi-encoders that separately encode the query and passage into a sentence embedding and then calculate the similarity of two embeddings. \n\n(2) Cross-encoders that input the concatenation of the query and passage into one encoder to predict their relevancy between 0 and 1. (3) LM-based models that aim to predict the probability that next token is true or false based on the query and passage. \n\nIn order to ensure the diversity of the framework, we choose the typical models in each type of dense retrieval methods. First, in the Bi-encoders, the BERT-based Contriever (Izacard et al., 2021) and T5-based GTR (Ni et al., 2021) are selected because of the strong generalization from the contrastive pre-training on large-scale corpus. They both utilize the mean pooling to capture the vector representation of the whole passage, which means the preference of the short passages. Besides, in the Cross-encoders, we select the MonoBERT (Nogueira and Cho, 2019), which has the optimal performance in this modeling methods. It utilizes the [CLS] vector of BERT model as input to a single layer neural network to obtain the probability of the passage being relevant. Finally, in the LMbased models, MonoT5 (Nogueira et al., 2020) is the best-known retrieval model, which is usually chosen as the baseline of retrieval tasks. Meanwhile, considering the effect of parameter quantity on models' preference, we select both the MonoT5base and MonoT5-3b as the rerankers.",
            "score": 0.5618254500842836,
            "section_title": "Dense Retrieval",
            "char_start_offset": 8448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1675
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308349609375
        },
        {
            "corpus_id": "250340145",
            "title": "SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts",
            "text": "To retrieve the list of passages containing the answer to a question we consider several models. We start with sparse vectors using TF-IDF, and cosine similarity and BM25 [28] as ranking functions. We use Elasticsearch to index the passages extracted from the CDF reports, and set the search engine to use either cosine similarity or BM25 as ranking function. In addition, we use dense representations generated with transformer bi-encoders [26] to encode questions and passages and calculate their similarity using vector dot product. Dense representations are able to capture semantic relations between words that are not possible by means of bag-of-words models used in the sparse representations. We evaluate the following alternative approaches to dense passage retrieval. \n\nDense Passage Retrieval (DPR) [16] uses two BERT encoders and its loss function minimizes the distance between the CLS token representation of question and relevant passages. DPR was trained on the Natural Questions corpus NQ [18]. The NQ corpus consists of 307,373 queries issued to Google search engine and the answers are spans in Wikipedia articles. \n\nColBERT [17] adapts BERT for efficient retrieval by adopting a late interaction architecture to compare bag of embeddings representing the question and the passage. The implementation5 that we use is fine tuned on the MS MARCO passage ranking dataset [1]. This dataset contains 1 million questions extracted from Bing query logs and 8.8 million passages, extracted from 3.5 million Web pages, which provide the necessary information to answer the questions. \n\nFinally, CoCondenser [12], a bi-encoder pre-training architecture. This architecture adds a condenser head on top of the transformer that receives input from the output of an early layer for the tokens, while the input of the CLS is the output of the previous layer. Authors claim that this architecture enable the CLS representation to focus on the global meaning of the input text. We test a CoCondenser6 trained on the MS Marco passage retrieval task. \n\nThe three bi-encoders DPR, ColBERT and CoCondenser have reported state-of-the-art results in passage retrieval datasets.",
            "score": 0.5610678215233881,
            "section_title": "Passage Retrieval",
            "char_start_offset": 6040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2050
                },
                {
                    "start": 2053,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "237581068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037384033203125
        },
        {
            "corpus_id": "252545318",
            "title": "On the Impact of Speech Recognition Errors in Passage Retrieval for Spoken Question Answering",
            "text": "BM25 is a standard retrieval model based on best match; there is lexical overlap between the query and every retrieved passage. We use the Anserini IR toolkit [14] to compute BM25 scores. \n\nDense Retriever (DR) [6] is a dual-encoder BERT-based model used for scoring question-passage pairs. Given a question , a positive (i.e., relevant) passage + and a set of negatives (i.e., irrelevant) passages { \u2212 1 , \u2212 2 , . . . , \u2212 }, the model learns to rank the positive question-passage pair higher than the negative ones. The two seperate encoders of the model are fine-tuned via the minimization of the softmax cross-entropy: \n\nDuring inference time, the similarity of a question-passage pair is calculated as the inner product of the respective question embedding and passage embedding. In detail, the whole corpus is encoded into an index of passage vectors offline, and the retrieval with respect to a question is implemented with efficient maximum inner product search [5] over the index. We follow the dualencoder architecture compared to a cross-encoder one (that jointly encodes question and passage) due to its high efficiency as a firststage ranker in large-scale settings. While the latter can achieve higher performance, the former makes the whole corpus indexable. \n\nDense retriever with data augmentation (DR+Data augm.) builds on the standard practice for improving the robustness of neural models by augmenting the training data with noisy data. We explore two cases of data augmentation, namely, augmentation with synthetic keyboard noise and augmentation with synthetic ASR noise. For the former, we augment each question on the training set with keyboard noise following the approach presented in [13], while for the latter, we augment with ASR noise as shown in Section 2.2. In contrast with ASR noise, keyboard noise will rarely alter the original word into a different correctly spelled word.",
            "score": 0.5605407392030284,
            "section_title": "Models",
            "char_start_offset": 9727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "248524694"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01073455810546875
        },
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Baselines. Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents. Generally, most existing dense retrieval methods adhere to the cascade training paradigm [15,20,21]. Therefore, to facilitate easier comparison in future studies on our dataset, we simplify the training process as illustrated in Figure 5 as in [15,20]. Specifically, we utilize the dual-encoder (DE) as the architecture of dense retrieval models, which is illustrated in Figure 6(a). The following methods are employed as our baselines to evaluate the retrieval performance on T 2 Ranking. \n\n\u2022 QL (query likelihood) [19] is a representative statistical language model that measures the relevance of passages by modeling the generation of a query. \u2022 BM25 [23] is a widely-used sparse retrieval baseline. \n\n\u2022 DE w/ BM25 Neg is equivalent to DPR [12], which is the first work that uses the pre-trained language model as the backbone for the passage retrieval task. \u2022 DE w/ Mined Neg enhance the performance of DPR by sampling hard negatives globally from the entire corpus as in ANCE [28] and RocketQA [21]. \n\n\u2022 DPTDR [25] is the first work that employs prompt tuning for dense retrieval. Among them, QL and BM25 are sparse retrieval models, whereas the others are dense retrieval models Implementation details. BM25 is implemented by Pyserini [14] with default parameters. The dual-encoder models are implemented Metrics. The following evaluation metrics are used in our experiments to examine the retrieval performance of baselines on T 2 Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved passages (MRR@10), ( 2) Recall for the top- retrieved passages (Recall@). Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages.",
            "score": 0.5597303020791822,
            "section_title": "Retrieval Performance",
            "char_start_offset": 19754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040771484375
        },
        {
            "corpus_id": "258236093",
            "title": "CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval",
            "text": "Given a large passage collection {p 1 , p 2 , ..., p n } \u2208 P , passage retrieval tasks aim to find relevant passages of a given query q based on their similarities. \n\nDual-encoder is a typical retrieval architecture for passage retrieval. It often employs a PLMbased model as a backbone encoder for encoding representations. Formally, given a sequence of tokenized input queries and passages T. \n\nThe input texts are forwarded through L-layers (l \u2208 {1, ..., L}) Transformers Blocks of PLM encoder. We denote the output hidden states as follows. \n\nHidden states at [CLS] positions of the last layers are commonly used as the dense representations of Existing works, eg. SPLADE (Formal et al., 2021b,a) and LexMAE (Shen et al., 2022), also focus on learning PLM-based dual-encoders for capturing bag-of-words semantic sparse representations. Sparse representations are focusing on encoding semantics at the token level. The hidden states of the last layer h last are firstly projected to vocab spaces with the transposed embedding matrix E . Then ReLU and log saturation operations are performed to obtain the sparse representations. \n\n) here i \u2208 {1, ..., N } denotes the sequence length spaces. ReLU keeps non-negative hidden states as token frequencies produced by PLM. And log saturation suppresses the dominant frequencies for keeping sparsity. The above operations are common practices for pooling sparse representations.",
            "score": 0.5594440783953759,
            "section_title": "Passage Retrieval with Dual-Encoders",
            "char_start_offset": 5082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12347412109375
        },
        {
            "corpus_id": "268681628",
            "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
            "text": "Information Retrieval In recent years, deep learning-based retrieval models have achieved remarkable performance across various information retrieval tasks.The dense passage retriever (DPR) framework, which encodes documents and queries into dense representations, has emerged as a popular approach for dense retrieval (Karpukhin et al., 2020).With the advent of large language models (LLMs), numerous methods have leveraged these models for dense retrieval.GTR (Ni et al., 2022) utilizes LLM encoders for dense retrieval and demonstrates performance improvements with increased model size.Promptagator (Dai et al., 2023) and InPars (Bonifacio et al., 2022) propose the use of LLMs to generate synthetic query-document pairs, which are then employed for training dense retrievers.Our work is orthogonal to these methods, as we focus on utilizing LLMs for second-stage passage reranking.\n\nPassage Reranking Passage reranking typically serves as a second-stage component following large-scale retrieval.Several studies have proposed deep reranking models that encode query-document pairs to predict relevance scores (Nogueira and Cho, 2019).Nogueira et al.\n\n(2020) introduced a generation-based method for passage reranking by fine-tuning LLMs on MS-MARCO (Bajaj et al., 2016), a large-scale retrieval dataset with relevance annotations.Their model, MonoT5, generates the word true for relevant pairs and false for irrelevant pairs.Similarly, our method also adopts a generation-based approach.\n\nThe main difference is that our method does not require relevance annotations nor fine-tuning; instead, we leverage the instruction-following capabilities of LLMs to enable unsupervised estimation.TART (Asai et al., 2022) fine-tunes LLMs on extensive retrieval supervision from various tasks with instructions.Our method differs from TART in that we do not require any retrieval supervision and employ a generation-based approach in an unsupervised fashion.\n\nAnother research line is unsupervised passage reranking with LLMs, which eliminates the need for retrieval supervision.",
            "score": 0.5587045264689983,
            "section_title": "Related Work",
            "char_start_offset": 2432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1492
                },
                {
                    "start": 1494,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1951
                },
                {
                    "start": 1953,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 343,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 462,
                    "end": 479,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 603,
                    "end": 621,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 633,
                    "end": 657,
                    "matchedPaperCorpusId": "250340449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "273026177",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "text": "Dense Passage Retrieval Dense retrieval has garnered attention for its efficacy in semantic space exploration. A notable technique in this domain is DPR (Karpukhin et al., 2020), employing both query and passage encoders for efficient retrieval. \n\nVarious studies have delved into enhancing dense retrieval, including negative example mining techniques like RocketQA (Qu et al., 2021), and diverse data augmentation methods such as DRAGON (Lin et al., 2023a). ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) introduced the lateinteraction mechanism, offering an alternative architecture for dense retrieval. \n\nAnother line of research is pre-training strategies for dense retrieval. Approaches like Contriever (Izacard et al., 2021), coCondenser (Gao and Callan, 2022), and COCO-DR (Yu et al., 2022) have proposed contrastive pre-training techniques tailored for retrieval tasks. Concurrently, CoT-MAE (Wu et al., 2023) and RetroMAE (Xiao et al., 2022) have focused on masked auto-encoding for pre-training. \n\nAs large language models (LLMs) continue to evolve, their integration into dense retrieval has become increasingly widespread. GTR (Ni et al., 2022) utilized LLM encoders, showcasing performance gains with increased model size. Similarly, Promptagator (Dai et al., 2023) and InPars (Bonifacio et al., 2022) employed LLMs to generate synthetic query-document pairs, effectively enhancing the training of dense retrievers. Building on a similar concept, (Huang et al., 2024) extended the approach to multilingual settings, enabling broader applicability. \n\nOur contribution is orthogonal to these studies as we concentrate on refining training signals for knowledge distillation. This suggests that our approach holds potential for integration with other methods to achieve further improvements. \n\nKnowledge Distillation for Dense Retrieval Enhancing the performance of dense retrievers often involves employing knowledge distillation techniques. Izacard and Grave (2020) pioneered the distillation of knowledge from the reader to the retriever, resulting in improved performance in open-domain question answering.",
            "score": 0.5585180941755395,
            "section_title": "Related Work",
            "char_start_offset": 2741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 367,
                    "end": 384,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 468,
                    "end": 495,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 495,
                    "end": 519,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 758,
                    "end": 780,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 794,
                    "end": 811,
                    "matchedPaperCorpusId": "253157773"
                },
                {
                    "start": 914,
                    "end": 931,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1153,
                    "end": 1170,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1274,
                    "end": 1292,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 1304,
                    "end": 1328,
                    "matchedPaperCorpusId": "250340449"
                },
                {
                    "start": 1474,
                    "end": 1494,
                    "matchedPaperCorpusId": "268681628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "260926325",
            "title": "Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval",
            "text": "Given a query q and a set of passages P n , the passage retrieval task aims to find the relevant passages based on the similarity search. Dense passage retrieval utilizes an encoder model Enc, e.g., a Transformers-based model like BERT (Devlin et al. 2019), to yield the sentence representations and measure query-passage similarities through inner product or cosine distance. Formally, given a query q and a passage q, we can use a query encoder Enc q and a passage encoder Enc p to derive their corresponding sentence representations, i.e., v q and v p from the encoder hidden states of the last layer at CLS position h [CLS] between q and p, i.e., Sim(q, p), can be calculated as the inner product of v q and v p for simplicity as follows. \n\nThe key to improving retrieval performances is to yield stronger representations v q , v p with better context alignment. The representations can be regarded as the compression of full contexts. We believe that incorporating the strong context-generation abilities of LLMs into the pre-training stage with carefully designed pre-tasks can be a new way for improving such alignment.",
            "score": 0.558191290269237,
            "section_title": "Preliminaries",
            "char_start_offset": 7621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1126
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 256,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2384033203125
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "This paper proposes a novel pre-training method SIMLM for dense passage retrieval. It follows an encoder-decoder architecture with a representation bottleneck in between. The encoder learns to compress all the semantic information into a dense vector and passes it to the decoder to perform well on the replaced language modeling task. When used as initialization in a dense retriever training pipeline, our model achieves competitive results on several large-scale passage retrieval datasets.\n\nFor future work, we would like to increase the model size and the corpus size to examine the scaling effects. It is also interesting to explore other pre-training mechanisms to support unsupervised dense retrieval and multilingual retrieval.",
            "score": 0.5568721167189492,
            "section_title": "Conclusion",
            "char_start_offset": 20697,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "In this section, we give a brief review of dense retrieval and then present how to apply models to multilingual dense retrieval. \n\nOverview. Given a query q and a corpus C, the retrieval task aims to find the relevant passages for the query from a large corpus. Usually, a dense retrieval model employs two dense encoders (i.e., BERT) E Q (\u2022) and E P (\u2022). They encode queries and passages into dense embeddings, respectively. Then, the model uses a similarity function, often dot-product, to perform retrieval: \n\nwhere f denotes the similarity function, q and p denote the query and the passage, respectively. Dur-ing the inference stage, we apply the passage encoder E P (\u2022) to all the passages and index them using FAISS (Johnson et al., 2021) which is an extremely efficient, open-source library for similarity search. Then given a query q, we derive its embedding by v q = E Q (q) and retrieve the top k passages with embeddings closest to v q . \n\nTraining. The training of retrieval is metric learning essentially. The goal is to narrow the distance between the query and the relevant passages (a.k.a., positive passages) and widen the distance between the query and the irrelevant passages (a.k.a., negative passages). Let {q i , p + i , p \u2212 i,0 , p \u2212 i,1 , . . . , p \u2212 i,n } be the i-th training sample. It consists of one query, one positive passage, and n negative passages. Then we can employ a contrastive loss function, called InfoNCE (van den Oord et al., 2018), to optimize the model: \n\n. \n\n(2) \n\nIn practice, we cannot use all passages in the corpus C as negative passages due to the limitation of resources. Therefore, a common practice is sampling a subset from the corpus C as negative samples, and many studies focus on which distribution the negative passages sampled from is better (Xiong et al., 2021;Qu et al., 2021). \n\nMultilingual Setting. This setting aims to transfer knowledge from the source language to the target languages.",
            "score": 0.5549347453746434,
            "section_title": "Preliminaries",
            "char_start_offset": 9140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 131,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1840
                },
                {
                    "start": 1843,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 723,
                    "end": 745,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1823,
                    "end": 1839,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "253080873",
            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
            "text": "Recently, with the success of large pre-trained models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021) have surpassed the sparse vector space models, becoming the new de facto method. Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019;Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers. Recently, the retrieve-then-rank pipeline has also been applied to solve CommonGen (Wang et al., 2021;Li et al., 2021;Liu et al., 2022). Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus. To alleviate this, recent work, such as AR2 (Zhang et al., 2022), has focused on improving the retriever by distilling knowledge from the ranker. In this paper, we further extend this idea by distilling the order knowledge between the candidates and gold references to the ranker and retriever.",
            "score": 0.5505024238573659,
            "section_title": "Experiment on Keyword Generation",
            "char_start_offset": 25985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 90,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 146,
                    "end": 170,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 189,
                    "end": 205,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 855,
                    "end": 874,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 1039,
                    "end": 1058,
                    "matchedPaperCorpusId": "235166616"
                },
                {
                    "start": 1058,
                    "end": 1074,
                    "matchedPaperCorpusId": "244119606"
                },
                {
                    "start": 1074,
                    "end": 1091,
                    "matchedPaperCorpusId": "245144423"
                },
                {
                    "start": 1317,
                    "end": 1337,
                    "matchedPaperCorpusId": "238419331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1771240234375
        },
        {
            "corpus_id": "254564747",
            "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
            "text": "The goal of text retrieval task is to find relevant passages given a query. There are two categories of methods in text retrieval task, which include sparse retrieval methods and dense retrieval methods. Sparse retrieval methods aim to identify relevant passages based on text matching. For example, BM25 is a bag-of-words retrieval function that ranks a set of passages based on the query terms appearing in each passage. Though sparse retrieval has high efficiency, it cannot identify semantically relevant but text-independent querypassage pairs. As the contrary, dense retrieval methods [27,28,58,62] can identify semantically relevant but text-independent query-passage pairs. The query and passage representations in dense retrieval models are usually obtained using dual encoders [20]. [21] pushes performance higher by introducing the late interaction while requiring higher computation and storage costs. In spite of the model structure, recent work also focuses on utilizing training strategies to obtain better results, ranging from data-centric studies [41,46] and negative sampling [10,53,61] to distillation [16,30,57]. [41] proposes a dense retrieval approach that incorporates both querycentric and passage-centric similarity relations. [17] proposes an efficient topic-aware query and balanced margin sampling technique, for training dense retrieval models. [57] proposes a curriculum learning-based optimization framework, for enhancing dense retrieval models through knowledge distillation. [30] introduces on-the-fly distillation and cascade distillation processes for crossarchitecture knowledge distillation in dense retrieval. \n\nOur work is in line with the distillation methods, where LEAD can be a better alternative to the widely used response-based and feature-based methods.",
            "score": 0.5490140352730469,
            "section_title": "RELATED WORK 2.1 Dense Retrieval",
            "char_start_offset": 5417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1099,
                    "end": 1102,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1129,
                    "end": 1132,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0850830078125
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "Baselines. We compare the proposed dense retrieval framework with the following baselines: \n\n\u2022 Sparse (Term Matching) Retrieval Models: We use two sparse retrieval baselines: (1) BM25: this baseline uses the BM25 formulation [47] with questions as queries, ignoring the images, and passages as documents. (2) BM25-Obj (CombMax): this approach extracts 36 objects from the image (objects are generated by a Faster R-CNN [45] model pre-trained on Visual Genome [2,27]) and concatenates each object's name to the question as the query and uses the BM25 formulation to retrieve passages. Then it uses CombMax [9,28] to aggregate these 36 ranked lists. [23]) with the same training objective as ours. We provide the results for two variations of this model, an image-independent approach whose query encoder only encodes the question, and an image-dependent approach whose query encoder takes the concatenation of the question and the image caption (captions are generated by Ex-pansionNet v2 [18]). The latter is the same as our   encoder. ( 5) Dense-LXMERT is a model that uses a multi-modal encoder, i.e., LXMERT, to encode queries and passages. It uses masked image tokens on the passage side. This approach is our   encoder. \u2022 Asymmetric Dual-Encoding Dense Retrieval Models: In this category, we use BERT-LXMERT, proposed in [40], that uses BERT for passage encoding and LXMERT for query encoding. 8 or fair comparison, we use the same training and evaluation process for all (our and baseline) models. To the best of our knowledge, our baseline results are the highest reported in the literature. \n\nComparison Against Retrieval Baselines. The passage retrieval results are reported in Table 1. We observe that dense retrieval models generally outperform sparse retrieval baselines, confirming our design choice to focus on dense retrieval for KI-VQA tasks.",
            "score": 0.5484599555908407,
            "section_title": "Passage Retrieval Results for KI-VQA Tasks",
            "char_start_offset": 30239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 419,
                    "end": 423,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 459,
                    "end": 462,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 462,
                    "end": 465,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 605,
                    "end": 608,
                    "matchedPaperCorpusId": "1309301"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "17427180"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "234337940"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042388916015625
        },
        {
            "corpus_id": "256105432",
            "title": "Information Retrieval: Recent Advances and Beyond",
            "text": "Another line of research focus on using contextual information to improve dense passage retrieval. For example, in [152], the authors propose a method called ConTextual Mask Auto-Encoder (CMAE), which uses contextual information to improve dense passage retrieval. Additionally, there are methods that use masked auto-encoder for pre-training retrieval-oriented transformers. For example, in [153], the authors propose a method called RetroMAE, which uses masked auto-encoder for pre-training retrieval-oriented transformers. Another line of research focus on using representation bottleneck for pretraining dense passage retrieval models. For example, in [154], the authors propose a method called SimLM, which uses representation bottleneck for pre-training dense passage retrieval models. Lastly, there are methods that use lexicon-bottlenecked pre-training for large-scale retrieval. For example, in [155], the authors propose a method called LexMAE, which uses lexicon-bottlenecked pre-training for large-scale retrieval. The last method is a method that uses a contrastive pre-training approach to learn discriminative autoencoder for dense retrieval. For example in [156]. Overall, these pre-training methods demonstrate promising results in improving dense passage retrieval performance. However, more research is needed to fully understand Zero-shot/Few-shot Learning Recent research in the field of zero-shot information retrieval has been focused on finding ways to improve the performance and generalizability of retrieval models. One common approach is the use of query generation, as seen in [157] and QGen [158]. These methods aim to improve zero-shot retrieval by embedding queries into a shared space, or by using synthetic question generation to improve passage retrieval. Another approach is the use of synthetic pre-training, as suggested by [159], to improve the robustness of neural retrieval models. Additionally, [160] presents a benchmark for evaluating the performance of zero-shot retrieval models. Another approach is the use of momentum adversarial domain-invariant representations as proposed by [161] for zeroshot dense retrieval. [162] proposes DTR, which uses large dual encoders for generalizable retrieval.",
            "score": 0.547385142706091,
            "section_title": "Multi-vector Representation",
            "char_start_offset": 34647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 1173,
                    "end": 1178,
                    "matchedPaperCorpusId": "251718954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271728515625
        },
        {
            "corpus_id": "266693273",
            "title": "Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization",
            "text": "Text retrieval involves searching for relevant information in vast text collections based on user queries. Efficient and effective methods for this task have revolutionized how we interact with information systems. Recently, there has been growing interest in augmenting large language models (LLMs) with text retrieval for question answering (QA) (Lewis et al. 2020a;Guu et al. 2020;Glass et al. 2022;Borgeaud et al. 2022;Fu et al. 2022;Zhang et al. 2023). These approaches harness retrieval models to obtain external knowledge and ground LLM outputs, reducing hallucinations and the need for frequent LLM updates. Interestingly, augmenting an LM with a retrieval helps reduce the number of parameters required to achieve similar performance as larger LMs (Mialon et al. 2023). \n\nText retrieval methods can be broadly categorized into two main approaches: sparse and dense retrievals. Sparse methods, such as BM25, exploit the frequency of words to measure the relevance between a passage and a query. While efficient, these methods often fall short of capturing intricate relationships and contextual nuances of language. In contrast, dense retrieval methods aim to learn meaningful representations from the semantic content of passages and queries effectively. These models can be trained based on a pretraining model (e.g. BERT, RoBERTa) as well as fine-tuned for downstream QA tasks (Lewis et al. 2020b), offering easy integration. In addition, it is possible to apply approximate nearest neighbors (ANN) with dense retrieval (Xiong et al. 2021) for efficient retrieval. \n\nThis paper focuses on dense retrieval, where contrastive learning is often employed to train passage and query encoders. The core principle of contrastive learning is to encode passages and queries such that relevant passages are closer to their corresponding query in the embedding space, while irrelevant passages are farther away. To train such encoders, we need a labeled dataset with queries annotated with relevant passages (positive samples). However, due to the vast number of candidate passages and the complexity of questions, it is common for annotators to miss relevant information (texts) during data preparation, leading to unlabeled positive examples (false negatives) in the training set.",
            "score": 0.5473155667776128,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 368,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 368,
                    "end": 384,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 384,
                    "end": 402,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 402,
                    "end": 423,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 438,
                    "end": 455,
                    "matchedPaperCorpusId": "229349082"
                },
                {
                    "start": 757,
                    "end": 777,
                    "matchedPaperCorpusId": "391854"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1531,
                    "end": 1550,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2188720703125
        },
        {
            "corpus_id": "276928532",
            "title": "GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval",
            "text": "While decomposition-based approaches require many auto-regressive steps to decompose questions, extract answers, and a different model for retrieval, our encoder-only approach only requires a single forward pass per hop to compute the next dense vector. Example is from (Trivedi et al., 2022). \n\nperformance by leveraging cross-encoder architectures. Unlike bi-encoders, which independently encode questions and passages to compute similarity, cross-encoders process both as a single sequence, resulting in linear scaling with respect to the number of passages. This makes them only suited as a retriever for a few hundred passages but not open book retrieval with up to billions of passages. Despite its strengths, it shares MDR's generalization issues while introducing scalability challenges due to its computational overhead, making it impractical for large-scale open retrieval tasks. These limitations underscore the need for a scalable and generalizable multi-hop retrieval framework that can perform well on both in-distribution and out-ofdistribution benchmarks in open-domain retrieval scenarios. \n\nTo address these challenges, we introduce GRITHopper-7B, the first decoder-based end-toend multi-hop dense retrieval model trained on an unprecedented scale of multi-hop datasets spanning both question-answering and fact-checking tasks. GRITHopper-7B achieves state-of-theart performance across out-of-distribution benchmarks (see Figure 1) while preserving the simplicity and scalability of encoder-only paradigms like MDR (see Figure 2). The foundation of GRITHopper lies in GRITLM (Muennighoff et al., 2025), a Mistral-7B-based model that integrates causal language modeling with dense retrieval training. GRITLM's design sparked a critical debate in the field: Does joint optimization of generative and retrieval tasks enhance dense embedding quality?",
            "score": 0.545276942667984,
            "section_title": "Introduction",
            "char_start_offset": 1794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 292,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 1593,
                    "end": 1619,
                    "matchedPaperCorpusId": "267681873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21435546875
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "We now describe our framework for projecting query and passage representations of dense retrievers to the vocabulary space. Given a dense retrieval model, we utilize the MLM head of the model it was initialized from to map from encoder output representations to distributions over the vocabulary (Eq. 1). For example, for DPR  we take BERT's MLM head, as DPR was initialized from BERT. Given a query q, we use the query encoder Enc Q to obtain its representation e q as in Eq. 2. Similarly, for a passage p we apply the passage encoder Enc P to get e p . We then apply the MLM head as in Eq.\n\n(1) to obtain the vocabulary projection:\n\nNote that it is not clear a-priori that Q and P will be meaningful in any way, as the encoder model has been changed since pretraining, while the MLMhead function remains fixed. Moreover, the MLM function has not been trained to decode \"pooled\" sequence-level representations (i.e., the results of CLS or mean pooling) during pretraining. Despite this intuition, in this work we argue that P and Q are actually highly intuitive and can facilitate a better understanding of dense retrievers.",
            "score": 0.545045865921409,
            "section_title": "Vocabulary Projections",
            "char_start_offset": 6857,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0391845703125
        },
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages. For a comprehensive comparison, we report Recall@50 and Recall@1K on the test queries. Following the evaluation settings of MS-MARCO and DuReader retrieval , MRR is defined as the average of the reciprocal ranks of the first relevant passage for a set of queries. The MRR is a value between 0 and 1, with a higher value indicating that the system is better at ranking the most relevant passage higher in the list. Meanwhile, Recall is defined as the fraction of relevant passages that are retrieved among all relevant passages, also with a value between 0 and 1, where a higher value indicates that the system is better at retrieving all relevant passages. MRR and Recall measure different aspects of retrieval performance. MRR@ and Recall@ can be depicted as: \n\n@ = \n\nwhere I(\u2022) is a indicator function. The  in Eq. 5 denotes the position of the first relevant passage in the retrieved candidates of query . The   and  K   represent the relevant passages of query  and the position of passage  in the candidate list K  . Retrieval performance. We report the retrieval performance of baselines in Table 5. Compared to the traditional sparse retrieval method BM25, dual-encoder models significantly boost the retrieval performance on our dataset. The improvement can be attributed to the integration of two distinct sources of knowledge, i.e., latent knowledge obtained through unsupervised pre-training of language models on a massive corpus and relevance knowledge acquired through supervised training on our large-scale annotated dataset. Equipped with the strategy of negative mining proposed in recent studies [28], the retrieval performance of dual-encoder models could be further improved on T 2 Ranking. It is worth noting that the Recall@ metrics observed in T 2 Ranking are lower than those reported in other benchmarks with coarse-grained annotations. For instance, the Recall@50 of BM25 is .601 and .700 on MS-MARCO-DEV Passage and DuReader retrieval , respectively, and 0.4918 on our dataset.",
            "score": 0.5450145112810026,
            "section_title": "Retrieval Performance",
            "char_start_offset": 21771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01029205322265625
        },
        {
            "corpus_id": "269605698",
            "title": "Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness",
            "text": "We first compare with the traditional BM-25 retrieval method (Robertson & Zaragoza, 2009;Trotman et al., 2014), which is a popular key-word matching algorithm with TF-IDF token weights.\n\n2. Dense Passage Retrieval (DPR): DPR model (Karpukhin et al., 2020) was trained to retrieve the most relevant passage given a query with.In the experiment, we select a model trained with the natural question dataset as the representative.We treat all candidates (i.e., words/sentences/passages) as the passages and encode them with the pre-trained encoder.",
            "score": 0.5447902964059236,
            "section_title": "BM-25:",
            "char_start_offset": 35150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 187,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 544
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 89,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 89,
                    "end": 110,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 231,
                    "end": 255,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.016845703125
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "This model can capture more contextual information and generally performs better than the dual-encoder model. However, these dense retrieval methods still have 3 main problems: (1) The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert [15] further increases the latency to 458 milliseconds, nearly 13 times of the BM25. (3) Negative samples are typically randomly sampled from the current batch of data or the entire dataset, lacking true annotations. These samples may only be highly similar pseudo-negative ones to the query. Simply pushing away the query from them can impact the sentence representation and affect the overall model performance. \n\nCurrent research has focused on the combined use of crossencoder and dual-encoder models in dense passage retrieval. In this paper, we propose a multi-level distillation model called MD2PR, in which a dual-encoder model is utilized for large-scale candidate retrieval during the retrieval phase, while a cross-encoder model is employed for fine ranking of the retrieved results during the ranking phase. This method can not only improve the quality of the retrieval results but also decrease the computational cost. The main contributions are shown as follows. \n\n(1) We use a knowledge distillation method, by which the knowledge learned by the ranker is transferred to the retriever at both sentence and word levels.",
            "score": 0.5446910873063882,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "252683172",
            "title": "DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases",
            "text": "The retriever retrieves relevant passages from the linearized KB based on the input question. We consider two kinds of retrieval methods: sparse retrieval and dense retrieval. For sparse retrieval, we use BM25 (Robertson et al., 2009), which is based on TF-IDF scores of sparse word match between input questions and KB-linearized passages. For dense retrieval, we apply the DPR (Karpukhin et al., 2020) framework, which is based on similarity in the embedding space between input questions and passages from two fine-tuned BERTs (Devlin et al., 2019). We refer readers to the original paper for details of the fine-tuning process. During inference, suppose there are totally N passages in the knowledge source {p 1 , p 2 , . . . , p N }. DPR applies the passage encoder E P (\u2022) to encode all the passages and store embeddings in memory. For an input question q, DPR applies the question encoder E Q (\u2022) to obtain its representation, and then the passages are retrieved based on the dot-product similarity: \n\nThen it applies FAISS (Johnson et al., 2019) to conduct an efficient similarity search due to the large number of passages N . Through this step, we can retrieve |I retrieve | N passages which are potentially relevant to the input question.",
            "score": 0.5446107029131657,
            "section_title": "RETRIEVAL",
            "char_start_offset": 8706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1249
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 234,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 530,
                    "end": 551,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1031,
                    "end": 1053,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039642333984375
        },
        {
            "corpus_id": "273901352",
            "title": "Language Bias in Multilingual Information Retrieval: The Nature of the Beast and Mitigation Methods",
            "text": "Dense passage retrieval (Karpukhin et al., 2020) is a neural retrieval framework initially proposed for monolingual supervised fine-tuning. This architecture separately encodes queries and documents into dense vectors, optimizing their alignment through a contrastive loss. The goal is to maximize the similarity between queries and their relevant documents while minimizing it with irrelevant documents. \n\nAssume we have a query q and a collection of documents D = {d Let q be the dense vector representation of the query, and d + i and d \u2212 j be dense vector representations of the corresponding documents. \n\nThe similarity between the query and each document is computed using the dot product: sim(q, \n\nWe then define the loss to be the negative loglikelihood of the positive documents' similarity scores among all documents: \n\nThis contrastive loss formulation ensures that the query embedding is closer to the positive document embedding than to any of the negative docu-ment embeddings, thereby enhancing the model's retrieval performance.",
            "score": 0.5430171989817522,
            "section_title": "DPR Loss",
            "char_start_offset": 5253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 1044
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 48,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041534423828125
        },
        {
            "corpus_id": "254853896",
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "text": "In recent years, with the development of largescale pre-trained language models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), we witness the research interest in information retrieval shifting from the traditional sparse retrieval to neural dense retrieval. Karpukhin et al. (2020); Xiong et al. (2021) proposed dense passage retriever (DPR), which uses two neural encoders initialized with BERT to model the query and document, independently. The subsequent works follow this dual-encoder framework. One line of work improves dense retrieval by mining hard negatives, where they select the top-ranked documents retrieved by the recent retriever as hard negatives and then re-train the retriever with the newly mined hard negatives (Xiong et al., 2021;Qu et al., 2021). However, the mined hard negatives are highly likely to contain false negatives, harming the performance. To mitigate this, following studies denoise the mined hard negatives with re-rankers (Qu et al., 2021;Ren et al., 2021;Zhang et al., 2022). Another kind of work focuses on pre-training to make the pre-trained models more suitable for dense retrieval, such as Condenser (Gao and Callan, 2021), coCondenser (Gao and Callan, 2022) and SIMLM (Wang et al., 2022). \n\nDense retrieval models typically depend on extensive supervised data, comprising pairs of queries and positive documents. To address the challenge of limited training data, Ma et al. (2021); Sun et al. (2021) proposed to train a query generator on highresource information retrieval data, and then used the query generator to generate synthetic queries for low-source target domains. Additionally, Dai et al. (2023) harnessed large language models in zero/few settings to produce synthetic queries for documents in target domains, eliminating the need for training a general query generator. In contrast to these approaches, our work leverages synthetic queries for document expansion.",
            "score": 0.5427846887042255,
            "section_title": "Dense Retrieval",
            "char_start_offset": 24814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 115,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 280,
                    "end": 303,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 305,
                    "end": 324,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 754,
                    "end": 774,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 774,
                    "end": 790,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 982,
                    "end": 999,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1016,
                    "end": 1035,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1202,
                    "end": 1224,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1431,
                    "end": 1447,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1449,
                    "end": 1466,
                    "matchedPaperCorpusId": "235303662"
                },
                {
                    "start": 1656,
                    "end": 1673,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "257985191",
            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
            "text": "Passage retrieval involves searching a large corpus for passages that are relevant to a particular query, and is essential for various applications such as web search (Fan et al., 2021;Guo et al., 2022;Lin et al., 2021a), question answering (Karpukhin et al., 2020;Lee et al., 2020;Zhu et al., 2021), and dialogue systems (Gao et al., 2022;Yu et al., 2021). Sparse retrieval methods, such as BM25, are practical and dominant approaches. However, retrieval methods based on pre-trained language models (PLMs) (Devlin et al., 2018;Liu et al., 2019) have gained popularity in recent years. PLMs are utilized to improve (1) sparse retrieval models that rely on lexical match; (2) dense retrieval models that model the semantic interaction between queries and passages in latent semantic space. \n\nTo improve sparse retrieval, PLM mitigates vocabulary mismatch by projecting each term in the query and passage to a vocabulary-sized weight vector. Each dimension in the weight vector represents the weight of a term in the PLM vocabulary, and the correlation between query and passage depends on lexical matching (Formal et al., 2021b,a;Shen et al., 2022). PLM-based sparse vectors model the features of passage on every lexical term (denoted by PLM logits output), and thus naturally capture the semantic at the lexicon level. SPLADE (Formal et al., 2021b,a) uses learnable PLM, e.g., BERT, to produce sparse vectors and fine-tune the retriever with contrastive learning loss. PLM-based SPLADE outperforms traditional sparse retrieval like BM25. LexMAE (Shen et al., 2022) further expands on SPLADE by proposing to pre-train a lexical-bottlenecked masked autoencoder to learn importance-aware sparse representations. It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck.",
            "score": 0.5416990556099316,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 340,
                    "end": 356,
                    "matchedPaperCorpusId": "234343311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2120361328125
        },
        {
            "corpus_id": "271534382",
            "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
            "text": "Dense retrieval has become an important research area following the development of pre-trained Transformer language models (PLMs) [2], [19]- [22]. To enhance text retrieval performance, dense retrieval leverages PLM-based text embeddings to encode queries and documents into a shared semantic vector space, focusing on matching semantic contents beyond mere keywords. This text embedding application in retrieval is fundamental to Retrieval-Augmented Generation (RAG) [23], which reduces the hallucinations in LLMs. Recent advancements in dense retrieval include architectural innovations, optimized training methodologies, and efficient indexing techniques, all of which contribute to improved retrieval accuracy and efficiency [4]- [8], [24]. Since the introduction of Sentence-BERT [25] and Dense Passage Retrieval (DPR) [2], numerous sentence embedding models have been proposed to enhance dense passage retrieval. Advanced sentence embedding models, which have been highlighted in the retrieval task of massive text embedding benchmark (MTEB) [26], include Contriever [27], M3E 2 , BGE [9], etc. Our text augmentation method serves as a preprocessing module for dense retrieval and is compatible with various embedding models mentioned above.",
            "score": 0.54166202431267,
            "section_title": "A. Dense Retrieval",
            "char_start_offset": 10070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "236772078"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "149698624"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06597900390625
        },
        {
            "corpus_id": "248426770",
            "title": "Curriculum Learning for Dense Retrieval Distillation",
            "text": "Through this CL-DRD process, we first demand the dense retrieval student model to recover coarse-grained distinctions between the documents exampled by the teacher model and then progressively move towards recovering finer-grained ordering of documents. For robust iterative optimization of the dense retrieval models, we adapt the listwise loss function of LambdaRank [2] to our knowledge distillation setting. Therefore, our loss function only focuses on the order of documents produced by the teacher model, and not the exact document scores. \n\nIn our experiments, we apply a simple implementation of the proposed optimization framework to two state-of-the-art dense retrieval models. First, we enhance TAS-B [11], a model that uses a single representation vector for each query and document. Second, we repeat our experiments with the ColBERTv2 model [30], a recent dense retrieval model that uses multiple representations per query and document. Our experiments on three public passage retrieval benchmarks demonstrate the effectiveness of the proposed framework. To improve the reproducibility of our models, we release the source code and the parameters of our models for research purposes. The query and document representations in dense retrieval models are often obtained using large-scale pre-trained language models, such as BERT [7], fine-tuned for the downstream retrieval task [12,17,32]. Dense retrieval models can be seen as a category of vector space models [28]. Dense retrieval models compute the relevance score for a query  and a document  as follows: score(, ) = sim(  (),   ()) \n\nwhere   (\u2022) and   (\u2022) are the query encoders parameterized by  and the document encoder parameterized by , respectively. The encoders produce a dense representation of the given input. They often share the same output dimensionality: \n\nThe similarity function 'sim' is often implemented using the inner product. For efficient retrieval, dense retrieval models often employ approximate nearest neighbor (ANN) search algorithms. \n\n2.1.2 Knowledge Distillation. For optimizing dense retrieval models, a ranking loss function is employed. The loss function is often based on either pointwise, pairwise, or listwise modeling, similar to learning-to-rank models.",
            "score": 0.540712732363105,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2030
                },
                {
                    "start": 2033,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1342,
                    "end": 1345,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1396,
                    "end": 1399,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "6473756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0604248046875
        },
        {
            "corpus_id": "258557604",
            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
            "text": "Our approach is illustrated in Figure 1. Specifically, we fine-tune a multilingual T5 (mT5) model using language-specific prompts to generate several queries per passage for each target language. In the indexing phase, we use a given xDR model to encode passages and their associated generated queries in different languages into embeddings. Finally, we augment the original passage embeddings with the embeddings of the generated queries before adding them to the ANN index. By doing so, we move the passage embeddings to a space that is closer to the target user query language. Our approach does not add extra query latency and can be applied to any existing xDRs. progress of cross-lingual dense retrieval (xDR) [4,5,8,39]. Notable examples of recent xDR methods include CORA [2], which employs a generator to facilitate retrieval training data mining, Sentri [31], which proposes a single encoder and self-training, and DR.DECR [19], which utilizes parallel queries and sentences for cross-lingual knowledge distillation. Among these, our work is most closely related to QuiCK [27], which also utilizes a cross-lingual query generator for xDR. However, unlike QuiCK, which uses the xQG as a teacher model in knowledge distillation for xDR in the training phase, our method directly augments passage embeddings with xQG queries without any xDR training involved. \n\nQuery generation for information retrieval. Query generation is a well-established technique that has been widely used to improve retrieval performance in various retrieval models [11,25]. In addition, it has been shown to be effective for domain adaptation in dense passage retrieval tasks [23,32,34], as well as for enhancing the effectiveness of other PLM-based rankers [37,48]. In our approach, we also rely on a query generation model to generate high-quality queries for downstream passage embedding augmentation tasks. \n\nAugmenting embeddings for dense retrievers. Our method relies on effectively augmenting representations encoded by DR encoders -a direction recently explored also by other works.",
            "score": 0.5398121218621802,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1894
                },
                {
                    "start": 1897,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "220525491"
                },
                {
                    "start": 719,
                    "end": 721,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 721,
                    "end": 723,
                    "matchedPaperCorpusId": "220347683"
                },
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "236428949"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "250390748"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "245144717"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "256461230"
                },
                {
                    "start": 1660,
                    "end": 1664,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1664,
                    "end": 1667,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1667,
                    "end": 1670,
                    "matchedPaperCorpusId": "245131402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049407958984375
        },
        {
            "corpus_id": "267740545",
            "title": "Distillation Enhanced Generative Retrieval",
            "text": "Dense retrieval (Lee et al., 2019;Karpukhin et al., 2020) is currently the de facto implementation of text retrieval. This method benefits from the powerful representation abilities of encoder-based language models and the MIPS algorithm (Shrivastava and Li, 2014), allowing for efficient passage retrieval from a large-scale corpus. Dense retrieval has been constantly developed through knowledge distillation, hard negative sampling, and better pretraining design (Chang et al., 2019;Wang et al., 2022a). Compared to dense retrieval, which relies on the dual-encoder architecture, the recently arsing generative retrieval shows promise in overcoming the missing fine-grained interaction problem through the encoder-decoder paradigm. Despite the huge potential, generative retrieval still lags behind the state-of-the-art dense retrieval method and leaves much room for investigation.",
            "score": 0.5385494224990612,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 885
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 34,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 34,
                    "end": 57,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 238,
                    "end": 264,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 466,
                    "end": 486,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0531005859375
        },
        {
            "corpus_id": "272330251",
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "text": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
            "score": 0.5378663098445914,
            "section_title": "Latent Variables in Retrieval Augmented Models",
            "char_start_offset": 12356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 207,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4638671875
        },
        {
            "corpus_id": "254685782",
            "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
            "text": "Recent years have witnessed the great success of dense retrieval methods (Karpukhin et al., 2020;Qu et al., 2021;Xiong et al., 2021) in industrial applications, e.g., web search (Brickley et al., 2019;Qiu et al., 2022) and question answering (Karpukhin et al., 2020;Izacard & Grave, 2021). These methods typically encode queries and passages into low-dimensional dense vectors and utilize the vector similarity between them to measure semantic relevance. In real-world applications, the dense vectors of large amounts of passages will be pre-computed. Then the approximate nearest neighbor (ANN) search techniques (Johnson et al., 2021) can be incorporated for efficient retrieval. \n\nTo generate high-quality dense vectors, pre-trained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019) have been widely adopted as the backbone of the query and passage encoders. However, general PLMs (e.g., BERT (Devlin et al., 2019)) may not be the best for dense retrieval, as their produced native dense representations (usually the [CLS] embedding) are not designed on purpose to generalize the information from the input text. To solve it, recent studies (Gao & Callan, 2021a;Lu et al., 2021;Sachan et al., 2021) adopt pre-training techniques to endow the [CLS] embedding the capacity of compressing the semantic information of the input text. They either rely on the autoencoding task that utilizes the [CLS] embedding to recover the corrupted text (e.g., masked or replaced tokens) (Liu & Shao, 2022;Wang et al., 2022;Wu et al., 2022), or leverage the contrastive learning objective to capture the relations among passages (e.g., co-occurrence) (Ram et al., 2022;Sachan et al., 2021), outperforming general PLMs in this task.",
            "score": 0.537745078485788,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 97,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 97,
                    "end": 113,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 113,
                    "end": 132,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 178,
                    "end": 201,
                    "matchedPaperCorpusId": "86688027"
                },
                {
                    "start": 201,
                    "end": 218,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 242,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 266,
                    "end": 288,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 614,
                    "end": 636,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 759,
                    "end": 780,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 908,
                    "end": 929,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1177,
                    "end": 1193,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1193,
                    "end": 1213,
                    "matchedPaperCorpusId": "230437591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2401123046875
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "Dense retrieval, a novel paradigm in Information Retrieval (IR), has emerged with the advancement of deep neural networks. Unlike traditional IR methods, dense retrieval encodes both queries and documents as embeddings within a shared latent space, capturing their semantic relationships through embedding similarities. Dense retrieval models have become the predominant choice in recent neural retrieval approaches and are widely applied in various downstream tasks such as web search, question answering, and sentence similarity (Karpukhin et al., 2020;Xiong et al., 2020;Muennighoff et al., 2022). \n\nIn the past few years, dense retrieval models intensively adopted pre-trained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020), as their backbone encoders. These models excel in identifying semantic similarities between queries and documents. However, they still face significant challenges in becoming versatile enough to handle a wide range of retrieval tasks (Muennighoff et al., 2022). Their in-domain retrieval accuracy is often constrained by the capacity of their backbone encoders, such as the number of parameters (Ni et al., 2021). Additionally, dense retrieval models typically struggle to generalize to unseen data, necessitating fine-tuning with a large amount of labeled data to perform well in the target domain. Finally, achieving versatility in dense retrieval models requires training on multiple retrieval tasks simultaneously, which demands sufficient capacity from the backbone encoder (Zhang et al., 2023;Xiao et al., 2023). \n\nRecently Large Language Models (LLMs) have been prompted or fine-tuned as dense retrieval models and achieved improved performance across a wide range of retrieval tasks, thanks to their superior capability for semantic understanding and rich world knowledge (Li et al., 2023;Wang et al., 2023;Zhuang et al., 2024;Muennighoff et al., 2024). These models vary in parameters from 2 billion to 56 billion, with pre-training sufficiency ranging from hundreds of billions to tens of trillions of tokens, and include both base models and human preference aligned chat models.",
            "score": 0.5367772845292688,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030853271484375
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "In our lexicon-enhanced retrieval module, we combine the results of the sparse and dense retrievers to mine new training data. To show the effectiveness of our mining method, we construct the five variants (for more conciseness, we use mBERT w/o QG + iterative refinement as the base model): \n\n\u2022 Sparse + Dense combines results of sparse and dense retrievers by adding their scores. \n\n\u2022 Sparse \u00d7 Dense combines results of sparse and dense retrievers by multiplying their scores. \n\n\u2022 Double Dense Retrievers mines positive and negative passages with results from two dense retrievers which are trained on different data (MS-MARCO and NQ). \n\n\u2022 w/o Hard Negatives (HN) fine-tunes the model with mined positive passages and only in-batch negative passages. \n\n\u2022 w/ Sparse Hard Negatives (HN) fine-tunes the model with mined positive passages, in-batch negative passages, and top passages returned by sparse retriever as negative passages. \n\nTable 5 presents all comparison results of the five variants. Based on the results, we have the following findings. Firstly, our mining method is more effective than the hybrid results of sparse and dense models. It demonstrates that our method can effectively leverage the knowledge of both sparse and dense retrievers. Secondly, mining data with sparse and dense retrievers are more effective than two dense retrievers. It demonstrates that sparse and dense retrievers have noticed different characteristics of retrieval. Finally, mined negatives are more  effective than sparse negatives. It demonstrates that negatives are important in dense retrieval tasks and our methods can provide more effective negatives.",
            "score": 0.5365050430328678,
            "section_title": "Effect of Lexicon-enhanced Retrieval",
            "char_start_offset": 23866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1651
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01348114013671875
        },
        {
            "corpus_id": "248227479",
            "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
            "text": "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4). \n\nEncoding query-candidate pairs. The cross-encoder architecture has been widely used in sentence-pair classification tasks such as natural language inference and paraphrase detection. We here use a cross-encoder to encode the concatenation of a query example and a candidate example. Specifically, we fine-tune a RoBERTa (Liu et al., 2019) model to classify whether an example pair is a positive or negative match. The confidence of classifying such a pair to be positive can thus be used as the utility score of the candidate upstream example for this query example. On top of this, we then develop a reranking module for further improving retrieval performance as follows. \n\nScoring paired data. To re-rank the initially retrieved data by the dense retriever, we apply the cross-encoder on all pairs of query examples Q and candidate retrieved examples R , producing scores of all |Q| * |R| query-candidate pairs. For each candidate example r \u2208 R , we use the average of all cross-encoder scores involving r as its utility score. Finally, we take the top-K examples based on this new ranking of candidate examples in R as the final retrieved data R. We use upsampling ratio \u00b5 to denote the ratio between R and R, i.e., \u00b5 = |R |/|R|.",
            "score": 0.536192955626523,
            "section_title": "Reranking Module",
            "char_start_offset": 13098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0164642333984375
        },
        {
            "corpus_id": "270688320",
            "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models",
            "text": "Passage ranking, which aims to rank each passage in a large corpus according to its relevance to the user's information need expressed in a short query, is an important task in information retrieval and natural language processing and plays a crucial role in many applications such as web search and retrieval-augmented generation. To achieve both effectiveness and efficiency, current mainstream approaches usually follow a two-stage paradigm known as \"retrievalthen-rerank\", which involves efficiently retrieving a set of candidates first, and further reranking them with a reranker to boost the effectiveness [18,25]. \n\nIn the first retrieval stage, dense retrieval models based on a bi-encoder architecture are widely used [12]. Trained on large-scale datasets of text pairs through contrastive learning, these models can encode text into a low-dimensional dense embedding and capture the relevance between query and passage using vector similarity. \n\nIn the second reranking stage, we can employ more sophisticated models for better ranking performance. A common reranking model is a supervised model based on the cross-encoder design [25]. With the emergence of large language models (LLMs), such as , a series of studies have tried to leverage LLMs' text comprehension and reasoning abilities for zero-shot reranking. Typically, there are three main prompting approaches: pointwise [13,32], pairwise [30], and listwise [28,33]. Among these methods, listwise approaches like RankGPT [33] have achieved state-of-the-art performance by directly producing a final ranking list for multiple passages, rather than merely assessing the relevance of a single passage or the relative position between two passages. \n\nWhile the listwise approaches demonstrate good performance in the reranking task, they are limited by two challenges. Firstly, some LLMs are limited by context length and cannot rank multiple passages simultaneously, necessitating techniques such as a sliding window strategy to complete the ranking process [33]. Secondly, incorporating entire passages into prompts significantly increases inference costs, resulting in high latency in practice [2], which is untenable in the ranking scenario.",
            "score": 0.5348350702989967,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "9346130"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 2023,
                    "end": 2027,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040008544921875
        },
        {
            "corpus_id": "277786967",
            "title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models",
            "text": "Recently, the main research focus in information retrieval (IR) has been on dense retrieval and related techniques (Karpukhin et al., 2020;Lin et al., 2022;Zhu et al., 2023a;Xu et al., 2025, inter alia). Dense retrieval encodes queries and documents into high-dimensional sparse vectors. Although effective, these dense vectors are difficult for humans to interpret in terms of their semantic meanings. Moreover, encoding and storing the dense vectors for the whole document collection can be resourceintensive. For example, encoded flat index of MS MARCO passage corpus (Bajaj et al., 2016) with LLAMA-2-7B dense retriever takes up 135G disk space (Ma et al., 2024), which is over 50 times larger than the 2.6G Lucene index from BM25. \n\nTo mitigate these drawbacks of dense retrieval, a different line of works investigates learned sparse retrieval (LSR). Inspired by traditional sparse retrieval models (Sparck Jones, 1972;Robertson et al., 1995), LSR encodes queries and documents into vocabulary-sized vectors with a backbone language model and the language model head, where each dimension of the vector represents the \"impact\" of the corresponding token (Formal et al., 2021b,a;Mallia et al., 2021). A canonical example of LSR is SPLADE (Formal et al., 2021b,a). It encodes text with BERT (Devlin et al., 2019), then applies pooling and log-saturation (Fang et al., 2004) to ensure the resulting vocabulary-sized vector contains non-negative values in each dimension, making it suitable for use in an inverted index. Combined with established training methodologies in dense retrieval such as contrastive learning (Oord et al., 2018), hard negatives mining (Karpukhin et al., 2020;Xiong et al., 2021) and knowledge distillation (Hofst\u00e4tter et al., 2020), LSR has demonstrated competitive performance with BERT-style encoder-only masked language models (Kong et al., 2023;Lassance et al., 2024).",
            "score": 0.5339230336177887,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 139,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 649,
                    "end": 666,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 905,
                    "end": 925,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 925,
                    "end": 948,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1184,
                    "end": 1204,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1295,
                    "end": 1316,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1358,
                    "end": 1377,
                    "matchedPaperCorpusId": "15200693"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033782958984375
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "To this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages. We followed the original dual-encoder training setting from Karpukhin et al. (2020) where, given a question q, a relevant passage p + and a set of irrelevant passages {p \u2212 1 , p \u2212 2 , . . . , p \u2212 n }, the model is fine-tuned via the minimization of the softmax cross-entropy: \n\ne s(q,p + ) e s(q,p + ) + p \u2212 e s(q,p \u2212 ) . (4) \n\nThe inference phase of our multimodal dense retriever remains the same as in traditional dualencoders for dense text retrieval. Specifically, we compute the similarity of a question-passage pair as the inner product of the respective question embedding and passage embedding. At query time, only the question needs to be encoded. In detail, we build a dense index of passage vectors (offline) by encoding the whole corpus and storing it in an index structure that supports efficient retrieval of the relevant passages via approximate nearest neighbor search (Johnson et al., 2021). At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings. On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers.",
            "score": 0.5332832446679102,
            "section_title": "Multimodal Dense Retriever",
            "char_start_offset": 8050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 262,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1065,
                    "end": 1087,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0306243896484375
        },
        {
            "corpus_id": "266693273",
            "title": "Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization",
            "text": "This paper aims to mitigate the impact of false negatives on dense passage retrieval. Toward such a goal, we extend the peer-loss framework and develop a confidence regularization for training robust retrieval models. The proposed regularization is compatible with any base retrieval model that uses NCE loss, a widely used contrastive loss function in dense retrieval. Through empirical and theoretical analysis, it is demonstrated that contrastive confidence regularization leads to more robust retrieval models. Building on this regularization, a passage sieve algorithm is proposed. \n\nThe algorithm leverages a dense retrieval model trained with confidence regularized NCE loss to filter out false negatives, thereby improving any downstream retrieval model including those that do not exploit NCE loss. The effectiveness of both the passage sieve algorithm and the confidence regularization method is validated through extensive experiments on three commonly used QA datasets. The results show that these methods can enhance base retrieval models, even when fewer negative samples are used. \n\n1. We only test our methods on top of AR2 (Zhang et al. 2021) and DPR (Karpukhin et al. 2020), more retrieval models can be tested to see the performance variance of the passage sieve and contrastive confidence regularizer in different cases. 2. Theorem 1 only assumes the loss function to be NCE loss and does not require the task to be dense retrieval. Thus, the proposed contrastive confidence regularizer has the same power on any other self-supervised tasks using NCE loss, e.g. self-supervised image classification, sentence embedding, etc. More experiments on these benchmarks should be conducted in future work. 3. The proposed methods are built upon CORES 2 (Cheng et al. 2021), which consider the first-order statistic(estimation) of peer loss. However, Zhu, Liu, and Liu (2021) has extended this into considering the secondorder statistics and achieving better performance in CV benchmarks. More work is needed to extend the contrastive confidence regularizer into second-order statistics as well.",
            "score": 0.5332226551537843,
            "section_title": "Conclusion",
            "char_start_offset": 27756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 1140,
                    "end": 1159,
                    "matchedPaperCorpusId": "229349082"
                },
                {
                    "start": 1168,
                    "end": 1191,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1765,
                    "end": 1784,
                    "matchedPaperCorpusId": "222141668"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1015625
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "Figure 2 depicts a pipeline for knowledge-intensive visual question answering tasks. As shown in the pipeline, the input to the dense retrieval model is asymmetric -query encoder takes multi-modal input (i.e., a question and an image), while the passage encoder takes a uni-modal text input (i.e., a passage from C). This asymmetric property in the input modalities makes it challenging to design an effective symmetric dense retrieval model. This is why the current state-of-the-art dense retrieval model proposed by Qu et al. [40] uses an asymmetric architecture, where a pre-trained multi-modal language model (i.e., LXMERT [53]) is used for query encoding and a pre-trained uni-modal language model (i.e., BERT [8]) is used for document encoding. Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B [17], ColBERT [24,49], RocketQA [41,44], and CLDRD [62], use symmetric architectures. Motivated by this observation, our goal is to learn a symmetric dense retrieval model for KI-VQA tasks. \n\nTo this aim, we study two alternative solutions. First, we convert all model inputs to a uni-modal textual form and then use uni-modal language models for both query and document encoding (Section 4.1). Second, we convert all inputs to the same multi-modal (text and image) form and then use multi-modal language models for both encoders (Section 4.2). We hypothesize that these two models learn complementary representations for the following reasons: (1) they take different input formats, and (2) the pre-training process and data in uni-modal and multi-modal language models are different.",
            "score": 0.5313544990832738,
            "section_title": "DEDR: DUAL ENCODING DENSE RETRIEVER FRAMEWORK",
            "char_start_offset": 11555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 532,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1187,
                    "end": 1190,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061431884765625
        },
        {
            "corpus_id": "248496840",
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "text": "Passage retrieval has been studied for decades, and many recent approaches of passage retrieval are using dense embeddings generated from deep neural models, called \"dense passage retrieval\". The state-of-the-art end-to-end dense passage retrieval systems normally deploy a deep neural model followed by an approximate nearest neighbor (ANN) search module. The model generates embeddings of the corpus and queries, which are then indexed and searched by the high-performance ANN module. With the increasing data scale, the ANN module unavoidably becomes the bottleneck on efficiency. An alternative is the learned index, which achieves significantly high search efficiency by learning the data distribution and predicting the target data location. But most of the existing learned indexes are designed for low dimensional data, which are not suitable for dense passage retrieval with high-dimensional dense embeddings.\n \n In this paper, we propose\n LIDER\n , an efficient high-dimensional\n L\n earned\n I\n ndex for large-scale\n DE\n nse passage\n R\n etrieval. LIDER has a clustering-based hierarchical architecture formed by two layers of core models. As the basic unit of LIDER to index and search data, a\n core model\n includes an adapted recursive model index (RMI) and a dimension reduction component which consists of an extended SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction component reduces the high-dimensional dense embeddings into one-dimensional keys and sorts them in a specific order, which are then used by the RMI to make fast prediction. Experiments show that LIDER has a higher search speed with high retrieval quality comparing to the state-of-the-art ANN indexes on passage retrieval tasks, e.g., on large-scale data it achieves 1.2x search speed and significantly higher retrieval quality than the fastest baseline in our evaluation. Furthermore, LIDER has a better capability of speed-quality trade-off.",
            "score": 0.531156220156743,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054290771484375
        },
        {
            "corpus_id": "271270159",
            "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
            "text": "Queries and documents can be represented using low-dimensional (compared to the vocabulary size) dense vectors. Such dense vectors are often obtained using pre-trained language models, such as BERT [37], that are fine-tuned for retrieval tasks [96]. Dense retrieval models are commonly based on bi-encoder architectures -one encoder for representing the query and another for the document. These encoders could share parameters. Some dense retrieval methods, such as DPR [96], represent each query or document by a single dense vector. While others, such as ColBERT [103], use one vector per token, resulting in multiple vectors for each query and document. Approximate nearest neighbor (ANN) algorithms, such as HNSW [140], are used for efficient retrieval when dealing with dense representations. Dense retrieval approaches are also commonly used when dealing with multi-media and multi-modal data [172,184].",
            "score": 0.5300395979254274,
            "section_title": "Retrieval Models with Dense Representations",
            "char_start_offset": 18515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 910
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 566,
                    "end": 571,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 900,
                    "end": 905,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "258331649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01142120361328125
        },
        {
            "corpus_id": "257985191",
            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
            "text": "Growing techniques have been emerging to improve the performance of passage retrieval. As an effective representation bottleneck pretraining technique, the contextual masked auto-encoder utilizes contextual embedding to assist in the reconstruction of passages. However, it only uses a single auto-encoding pre-task for dense representation pre-training. This study brings multi-view modeling to the contextual masked auto-encoder. Firstly, multi-view representation utilizes both dense and sparse vectors as multi-view representations, aiming to capture sentence semantics from different aspects. Moreover, multiview decoding paradigm utilizes both autoencoding and auto-regressive decoders in representation bottleneck pre-training, aiming to provide both reconstructive and generative signals for better contextual representation pretraining. We refer to this multi-view pretraining method as CoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective and robust on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks.",
            "score": 0.5298206825317218,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.170654296875
        },
        {
            "corpus_id": "266176301",
            "title": "Exploring In-Context Learning for Knowledge Grounded Dialog Generation",
            "text": "In dense retrieval, queries and documents are often directly encoded by large neural models into single or multiple vectors, which allow for efficient dense retrieval based on simple similarity function such as dot-product or ANN. These vectors compress high dimensional semantic information. Four types of dense retrieval are implemented in our experiments, the first three are bi-directional contextual language models (BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019)) using [CLS] embedding as the final input representation. The same encoder is used for both query and document. We also implement TCT-ColBERT (Wang et al., 2022) using index service provided by Pyserini3 and FAISS4 libraries.",
            "score": 0.5293857571037172,
            "section_title": "Dense Retrieval",
            "char_start_offset": 14733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 665,
                    "matchedPaperCorpusId": "250340125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035736083984375
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
            "score": 0.52902793907616,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 579,
                    "end": 603,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 657,
                    "end": 679,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 883,
                    "end": 910,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "272881140",
            "title": "Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot Dense Retrieval",
            "text": "Dense retrieval systems commonly employ dual-encoder retrieval models which use two separate encoders, either symmetric or asymmetric, to represent the query and corpus [8,9,13,33]. The corpora are indexed with representation and will be retrieved in response to each query based on the relevance scores. The scores are usually calculated based on embedding similarity, such as dot product or cosine similarity. Although dense retrieval systems have developed rapidly, the model performance largely depends supervised text representation learning and relevancy capturing between the query and corpus [36]. Yet, it remains to be a major challenge to properly retrieve when lacking labeled modeling data. Existing work [21,22] leveraged pre-trained large encoders (specifically T5 models, Raffel et al. [26]) to alleviate the data thirst. However, their proposals still required annotated datasets either by web mining or manual annotation for fine-tuning in order to improve the generalization ability of dual-encoder retrieval models, for example, dealing with out-ofdomain data. An alternative solution is to train a dense retrieval on synthetic query-corpus relevance pairs. [18] trains a question generation system on general domain data and applies it to the targeted domain to construct synthetic question-passage data. To save the effort of training a task-specific generation model on general data, like Natural Questions [15] or MSMARCO [20], Promptagator [5] proposes to use pre-trained LLMs, like FLAN [32], as a few-shot query generator to build the data for training the dual-encoder. However, the synthetic queries are not directly leveraged at inference, potentially causing gaps between training and inference of dense retrievers [2]. Earlier work, e.g., doc2query [24], concatenates the generated queries with the corresponding corpus, aiming to enrich the corpus representation with questions that the corpus can potentially answer. An improved version, docTTTTTquery [23] leverages pre-trained T5 models as the expansion model, leading to more relevant synthetic queries and better retrieval performance.",
            "score": 0.5277285751453253,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "248177840"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "195848217"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1464,
                    "end": 1467,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 1512,
                    "end": 1516,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1745,
                    "end": 1748,
                    "matchedPaperCorpusId": "248780378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0310211181640625
        },
        {
            "corpus_id": "264110387",
            "title": "Hybrid Retrievers with Generative Re-Rankers",
            "text": "Recently, dense retrieval approaches have also been proposed. They are capable of capturing semantic matches, and attempt to overcome the (potential) lexical gap. Dense retrievers map queries and documents in a single, common dense vector space. A bi-encoder architecture based on BERT-type models demonstrated strong performance for various opendomain question answering tasks. \n\nAn important recent innovation for passage retrieval is the introduction of dense retrieval models that take advantage of a bi-encoder design. Bi-encoders produce two corresponding embeddings for a given two-sentence pair (e.g. a query and a passage), which can then be compared efficiently using cosine similarity. \n\nBi-encoders are used whenever a sentence embedding is needed in a vector space for efficient comparison in applications such as information retrieval, semantic search, or clustering. Cross-encoders would be the wrong choice for these applications, because a cross-encoder does not produce a sentence embedding; it processes both sentences simultaneously through the Transformer network, which is very computationally expensive with such a large scale of data. \n\nWith sufficient labeled data, we can learn encoders (typically, Transformer-based models) that project queries and documents into a dense (semantic) representation space (e.g. 768 dimensions) where the relevance ranking can be recast as a nearest neighbor search over representation vectors [4]. \n\nBi-encoders can be used also as re-rankers, working not on all documents in corpora, but only on subsets of them. There are settings in which the first-stage retriever returns a limited number of documents and passes them to the re-ranker. Reranking can be also performed as a second stage retrieve on a limited collection of documents from stage 1 without any top-k constraints. \n\nThe two most popular bi-encoders are DPR and ANCE. DPR [5] is a two-tower bi-encoder trained with the hard negatives and single-batch negatives of a single BM25. The Multi-DPR model is a BERT-base-uncased model trained on four QA datasets: NQ, TriviaQA, WebQuestions, and Curat-edTREC.",
            "score": 0.5275814668039802,
            "section_title": "B. Dense retrieval",
            "char_start_offset": 5433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.122314453125
        },
        {
            "corpus_id": "258331650",
            "title": "Improving Conversational Passage Re-ranking with View Ensemble",
            "text": "Retrieval. Dense retrieval (DR) using a bi-encoder architecture with a passage encoder and a query encoder has gained attention for its effectiveness and efficiency in many knowledgeintensive tasks, as demonstrated in recent studies [12,13,32]. DR works by precomputing representations of passages in a corpus through the passage encoder. During retrieval, only the encoding of the query is performed, allowing for efficient end-to-end retrieval through inner product search [11]. The bi-encoder architecture used in DR can be further optimized for conversational dense retrieval (ConvDR) by fine-tuning the model in a few-shot [22,26,40] or weakly-supervised [18] manner. Despite its efficiency, ConvDR methods still fall short compared to multi-stage pipelines, particularly in terms of top-ranking effectiveness, as highlighted in [18].",
            "score": 0.5274053171760643,
            "section_title": "Dense",
            "char_start_offset": 6332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 839
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "250340228"
                },
                {
                    "start": 632,
                    "end": 635,
                    "matchedPaperCorpusId": "256460962"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "233296511"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "233296511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.095703125
        },
        {
            "corpus_id": "252917569",
            "title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
            "text": "Dense retrieval is widely applied to web applications, like search engines (Karpukhin et al., 2020), advertising (Lu et al., 2020;Zhang et al., 2022) and recommender systems (Xiao et al., 2022b). It encodes query and document within the same latent space, where relevant documents to the query can be efficiently retrieved via ANN search. The encoding model is critical for the retrieval quality. Thanks to the recent development of large-scale language models, e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2019), there has been a major leap-forward for dense retrieval's performance (Karpukhin et al., 2020;Luan et al., 2021;Lin et al., 2021). \n\nThe large-scale language models are highly differentiated in terms of pre-training tasks. One common task is the masked language modeling (MLM), as adopted by BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), in which the masked tokens are predicted based on their context. The basic MLM is extended in many ways. For example, tasks like entity masking, phrase masking and span masking (Sun et al., 2019;Joshi et al., 2020) may help the pre-trained models to better support the sequence labeling applications, such as entity resolution and question answering. Besides, tasks like auto-regression (Radford et al., 2018;Yang et al., 2019) and Seq2Seq (Raffel et al., 2019;Lewis et al., 2019) are also utilized, where the pre-trained models are enabled to serve NLG related scenarios. However, most of the generic pre-trained models are based on token-level tasks, where the sentence representation capability is not effectively developed (Chang et al., 2020).",
            "score": 0.5272347266535491,
            "section_title": "Related works",
            "char_start_offset": 5014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1653
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 99,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "246822639"
                },
                {
                    "start": 174,
                    "end": 194,
                    "matchedPaperCorpusId": "232139754"
                },
                {
                    "start": 473,
                    "end": 494,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 624,
                    "end": 648,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 666,
                    "end": 683,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 851,
                    "end": 872,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1100,
                    "end": 1119,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 1314,
                    "end": 1332,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1851806640625
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "Self-Retrieval integrates indexing into the LLM's parameters through self-supervised learning, enabling the model to internalize the entire corpus. Unlike generative retrieval methods that rely on complex document identifiers and identifier matching, Self-Retrieval employs a straightforward sentence-to-passage task to construct the index. Specifically, given a passage p = {s 1 , s 2 , ..., s L } consisting of L sentences, each sentence s i is provided as input to the LLM with parameters \u03b8. \n\nThe training objective is to generate the source passage p in an auto-regressive way, represented as P (p|s i , \u03b8). This self-supervised indexing approach offers several advantages. First, it provides a simple yet effective method for corpus indexing. Second, it naturally frames the indexing process as a retrieval-like task, enabling the model to simultaneously internalize the corpus and develop retrieval capabilities using a consistent data format. Furthermore, this indexing technique closely aligns with the pre-training processes of language models, suggesting that our method could be considered as continued pre-training on the corpus. Through this process, the LLM learns to efficiently memorize and organize corpus information within its parameters.",
            "score": 0.526873463300632,
            "section_title": "Indexing: Internalize the Corpus",
            "char_start_offset": 10096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1258
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "In this section, we will introduce some widely-adopted passage retrieval models (including both sparse and dense models) for experiments. BM25 BM25 is the most widely used term-based passage retrieval method. Practically, BM25 ranks a set of passages based on the  query terms appearing in each passage, regardless of their proximity within the passage. Doc2Query [39] Doc2Query is still a term-based passage retrieval method. Doc2Query alleviates the term mismatch problem in the BM25 via training a neural sequence-to-sequence model to generate potential queries from passages, and indexes the queries as passage expansion terms. Different from the BM25 method, the implementation of the doc2query method relies on labeled query-passage pairs. DPR [26] DPR is the most widely used dense passage retrieval method, which provides a strong baseline performance. It learns dense embeddings for the query and passage with a BERT-based encoder separately. The embeddings of query and passage are then fed into a \"similarity\" function to compute the final relevance score. The retrieval performance of the DPR model is mainly determined by two factors: the BERT backbone network and the labeled query-passage dataset adopted. Therefore, in order to gain a deep understanding of the DPR model in domain passage retrieval, we conduct various settings based on the DPR model architecture by replacing the original BERT model with a BERT model that has continuously trained on in-domain raw text (DPR-2) or leveraging different domain labeled datasets to carry out the training process (DPR-1).",
            "score": 0.5264792447501855,
            "section_title": "Methods",
            "char_start_offset": 19895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "267199979",
            "title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval",
            "text": "The TPRF method consists of a dense retriever, a transformer, and a purposely devised training strategy for the transformer. \n\nThe dense retriever is responsible for producing a dense vector representation for each passage in the corpus. These are produced at indexing time and stored in a data structure that supports fast Approximate Nearest Neighbor (ANN) search, like a Faiss index. The dense retriever also produces a dense encoding of the initial query and a first ranking of the passages in answer to the query. The TPRF method does not prescribe which dense retriever need to be used -and it is in fact agnostic to the dense retriever used. In the experiments in this paper we demonstrate TPRF in conjunction with the dense retrievers ANCE [25] (and we indicate this method as ANCE-TPRF) and directly compare it against the ANCE-PRF method [29]. \n\nOnce the first round of retrieval has been performed, the dense representations (and not the text) of the top- passages are used as feedback signal (i.e., PRF depth = ). The dense representation of the query is then stacked with the representations of the feedback signal to form a ( + 1, 768) matrix. This matrix is provided as input to the transformer at the core of the TPRF model. The output is a 1, 768 dimensions dense vector representation. This output is used as the new query representation from the PRF mechanism and used to perform a second round of retrieval using the original dense vector index: its result is the final ranking produced by TPRF. \n\nAt the core of TPRF is the transformer that is used to convert the input query representation and feedback passages representations into a new representation of the query with feedback. We choose the vanilla transformer encoder layer proposed by Vaswani et al. [21] to build the PRF query encoder in TPRF. Similar to other transformer layers used in language modelling tasks, the transformer encoder used in TPRF also leverages the self-attention mechanism to transform a sequence of input embeddings into contextualized embeddings. In our context, the transformer converts the query representation and the feedback passages representations into a new query representation.",
            "score": 0.5253156853245889,
            "section_title": "Transformer-based PRF",
            "char_start_offset": 5304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 848,
                    "end": 852,
                    "matchedPaperCorpusId": "237363901"
                },
                {
                    "start": 1779,
                    "end": 1783,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02606201171875
        },
        {
            "corpus_id": "270562887",
            "title": "A Gradient Accumulation Method for Dense Retriever under Memory Constraint",
            "text": "To verify whether representations generated by past encoders aid the current encoder's training, we conducted an experiment measuring the similarity mass of passage representations at different time steps.The results are shown in Figure 6.The similarity mass is defined as the sum of similarities after passing through a softmax function for all current time t queries with passage representations generated at past time steps t \u2212 k, as shown in Equation 10:\n\nPassage representations of the current and previous encoder have similar importance as negative passage.The results indicate that there is no significant difference in the similarity mass between the in-batch negative passage representations at the current training step and the passage representations from up to six previous steps.As shown in Equation 8 and 9, the gradients of the two encoders are proportional to the magnitude of the similarities.This means that negative passages with high similarity to a single query produce large gradients, which aids in training the dense retrieval model [41].This finding suggests that past representations can be beneficial from the early stages of training, contrary to previous studies [37,38].\n\nAdditionally, as illustrated in Figure 6, CONTACCUM demonstrates the same similarity mass trend as DPR, validating the effectiveness of utilizing past representations from the early stages of training with CONTACCUM.\n\nD. Gradient Norm Ratio of Omitting the Query Memory Bank Gradient norm imbalance problem occurs when the query memory bank is omitted.We omitted the query memory bank during training at various epochs: [10,20,30].As shown in Figure 7, the gradient norm imbalance problem arises immediately after the query memory bank is excluded.Additionally, irrespective of when the query memory bank is omitted, all experiments without the query",
            "score": 0.5246851161338106,
            "section_title": "C. Similarity Mass",
            "char_start_offset": 27424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 458
                },
                {
                    "start": 460,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1201
                },
                {
                    "start": 1203,
                    "end": 1419
                },
                {
                    "start": 1421,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1751
                },
                {
                    "start": 1751,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "235792410"
                },
                {
                    "start": 1197,
                    "end": 1200,
                    "matchedPaperCorpusId": "209377073"
                },
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "15816723"
                },
                {
                    "start": 1627,
                    "end": 1630,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1630,
                    "end": 1633,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050140380859375
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "During training, the model is optimized based on a contrastive learning objective,\n\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question. Although the bi-encoder dense retrieval architecture has achieved impressive results in ODQA, few work has attempted to improve its parameter efficiency. Further, compared to the spare vector space model BM25 (Robertson and Walker, 1994), such bi-encoder dense retrievers sometimes suffer from inferior generalization performance, e.g., when the training data is extremely biased (Lebret et al., 2016; or when there is a distribution shift (Sciavolino et al., 2021). In this paper, we conjecture that the unstable generalization performance is partially related to the sheer number of learnable parameters in the model. Therefore, we develop a task-aware specialization architecture for dense retrieval with parameter sharing between the question and passage encoders, which turns out to improve both parameter efficiency and generalization performance.",
            "score": 0.5225568090496132,
            "section_title": "Background",
            "char_start_offset": 5876,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 861,
                    "end": 882,
                    "matchedPaperCorpusId": "1238927"
                },
                {
                    "start": 921,
                    "end": 946,
                    "matchedPaperCorpusId": "237562875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09588623046875
        },
        {
            "corpus_id": "258107796",
            "title": "Generator-Retriever-Generator Approach for Open-Domain Question Answering",
            "text": "The Retriever-Reader approach is based on the idea of combining information retrieval (retriever) and machine reading comprehension (reader) techniques. Previous work in this area includes the use of document retrieval techniques such as TF-IDF, BM25, or neural ranking models [26,33] to select relevant documents from a large corpus. mNotable works include the Stanford Question Answering Dataset (SQuAD) and subsequent advancements in retriever-reader architectures like DrQA and BiDAF [35]. Dense Passage Retrieval (DPR) [13] focuses on dense representations for passage retrieval, utilizing a dual-encoder architecture to retrieve passages and a reader model to extract the answer. T5-RC [29], a variant of the T5 model, follows the Retriever-Reader approach by retrieving relevant passages and applying T5 as a reader for answer extraction.",
            "score": 0.5224295743694907,
            "section_title": "Retriever Reader",
            "char_start_offset": 5205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 845
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "53702508"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059539794921875
        },
        {
            "corpus_id": "267770557",
            "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
            "text": "Dense Retrieval. Earlier research has explored various ways to learn representations for text re-trieval (Deerwester et al., 1990;Huang et al., 2013;Gillick et al., 2018). With the rise of pre-trained language models, several works have presented the BERT-based dual-encoder as dense retrievers (Lee et al., 2019;Karpukhin et al., 2020;Xiong et al., 2021). They typically employ encoders to independently encode queries and documents into a dense space, calculating the similarity via vector dotproduct or cosine similarity. To further enhance performance of dense retrieval models, one line of approaches focuses on developing retrieval-oriented pretraining techniques (Izacard et al., 2022b;Gao andCallan, 2021, 2022;Yu et al., 2022;Xiao et al., 2022;Ni et al., 2022;Lin et al., 2023a), and another line of approaches focus on improving the negative contrast loss (Ren et al., 2021;Zhang et al., 2022). Additionally, some models utilize LLM-generated queries to generate synthetic examples for improving retrieval (Ma et al., 2021;Dai et al., 2023), but these dense retrievers are trained separately from LLM and may not always align well with the LLM, potentially resulting in sub-optimal performance when directly applied to target tasks (Lin et al., 2023b). A concurrent work (Wang et al., 2023) leverages LLM to generate synthetic text pairs from scratch without considering target tasks, thus being different from our focus. \n\nRetrieval-Augmented LLMs. RAG have been widely used for language modeling (Borgeaud et al., 2022;Ram et al., 2023), question answering (Lewis et al., 2020;Izacard et al., 2022b;Shi et al., 2023b), and domain adaptation (Xu et al., 2023a,b;Shi et al., 2023c).",
            "score": 0.5223450991451369,
            "section_title": "Related Work",
            "char_start_offset": 5190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1692
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 130,
                    "matchedPaperCorpusId": "3252915"
                },
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 149,
                    "end": 170,
                    "matchedPaperCorpusId": "53735999"
                },
                {
                    "start": 295,
                    "end": 313,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 313,
                    "end": 336,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 336,
                    "end": 355,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 719,
                    "end": 735,
                    "matchedPaperCorpusId": "253157773"
                },
                {
                    "start": 735,
                    "end": 753,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 753,
                    "end": 769,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 769,
                    "end": 787,
                    "matchedPaperCorpusId": "256868909"
                },
                {
                    "start": 866,
                    "end": 884,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 884,
                    "end": 903,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1016,
                    "end": 1033,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1033,
                    "end": 1050,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.077392578125
        },
        {
            "corpus_id": "257585074",
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "text": "In this section, we introduce a conceptual framework consisting of three components (sparse encoder, sparse regularizer, supervision) that captures the key differences we observe between existing learned sparse retrieval methods. Later, we describe how LSR methods in the literature can be fit into this framework. Sparse (Lexical) Encoders. A sparse or lexical encoder encodes queries and passages into weight vectors of equal dimension. This is the main component that determines the effectiveness of a learned sparse retrieval method. There are three distinct characteristics that make sparse encoders different from dense encoders. The first and most straightforward difference is that sparse encoders produce sparse vectors (i.e., most term weights are zero). This sparsity is controlled by sparse regularizers, which we will discuss in the next section. Second, dimensions in sparse weight vectors are usually tied to terms in a vocabulary that contains tens of thousands of terms. Therefore, the size of the vectors is large, equal to the size of the vocabulary; each dimension represents a term (typically a BERT word piece). On the contrary, (single-vector) dense retrieval methods produce condensed vectors (usually fewer than 1000 dimensions) that encode the semantics of the input text without a clear correspondence between terms and dimensions. Term-level dense retrieval methods like ColBERT [13] do preserve this correspondence. \n\nThe third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies. \n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8,35]. There are also recent works proposing hybrid retrieval systems that combine the strength of both dense and sparse representations [3,18,19], which can bring benefits for both in-domain and out-of-domain effectiveness [19].",
            "score": 0.5222899129407662,
            "section_title": "Unified learned sparse retrieval framework",
            "char_start_offset": 5623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2353
                }
            ],
            "ref_mentions": [
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 2126,
                    "end": 2129,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00930023193359375
        },
        {
            "corpus_id": "253140807",
            "title": "Open-Domain Conversational Question Answering with Historical Answers",
            "text": "Following the prior work (Karpukhin et al., 2020;Xiong et al., 2021), we apply a dense retrieval method, which has shown dominant performance over sparse ones. Specifically, the model uses a dual-encoder architecture to map passages and questions to the same embedding space. The input of our question encoder is the concatenation of historical questions and answers: \n\nThe retrieval score is then defined as the dot product of the passage embedding and the question embedding: \n\nIn the training process, each question contains one gold passage p + and a set of negative passages P \u2212 , ConvADR-QA is then optimized using the negative log likelihood loss:",
            "score": 0.52196209588776,
            "section_title": "Retriever",
            "char_start_offset": 6980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 654
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 49,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 49,
                    "end": 68,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04217529296875
        },
        {
            "corpus_id": "263835270",
            "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
            "text": "Retro (Borgeaud et al., 2022) is an auto-regressive language model pretrained with retrieval augmentation. While Retro shares the backbone of GPT models, Retro differs from GPT by incorporating an additional Retro encoder. The Retro encoder is adept at encoding features of retrieved neighbors from external knowledge bases. Furthermore, Retro adds chunk-wise cross-attention layers within its decoder transformer architecture to integrate retrieved information from the Retro encoder effectively. This design paradigm also makes Retro different from the encoder-decoder architecture (e.g., T5 (Raffel et al., 2020) and Atlas (Izacard et al., 2022a)). The success of scaling decoder-only autoregres- sive language models (e.g., ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023)) motivates us to further scale up autoregressive Retro and understand the potential benifit of retrieval-augmented pretraining. \n\nRetro encoder is a shallow bidirectional transformer to encode retrieved neighbors from external databases into dense features. Specifically, in this work, we follow Borgeaud et al. (2022) and use a two-layer bidirectional transformer as the Retro encoder with the same hidden dimension as the Retro backbone decoder. Our preliminary results show that increasing the layers of the Retro encoder does not bring better perplexity on the validation set, but only increases the computational overhead and model parameters. \n\nRetrieval database. Borgeaud et al. (2022) demonstrates that retrieval-augmented pretraining can significantly benefit from large-scale retrieval up to trillions of tokens. To build the retrieval database, we utilize the entire pretraining corpus, but holding out 1% as a validation set. This ensures that both Retro and GPT models are pretrained on an equivalent volume of information from the pretraining corpus. \n\nOur retrieval database is a key-value database, where values are chunks of tokens split from the pretraining corpus, and the keys are corresponding BERT embeddings (Devlin et al., 2018).",
            "score": 0.521934621410387,
            "section_title": "Preliminaries of Retro",
            "char_start_offset": 6920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 29,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 594,
                    "end": 615,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1072,
                    "end": 1094,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1447,
                    "end": 1469,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34130859375
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "To test the effectiveness of our proposed dense PRF approach, we compare with five families of baseline models, for which we vary the use of a BERT-based reranker (namely BERT or ColBERT). For the BERT reranker, we use OpenNIR [24] and capreolus/ bert-base-msmarco fine-tuned model from [21]. For the ColBERT reranker, unless otherwise noted, we use the existing pre-indexed ColBERT representation of passages for efficient reranking. The five families are: Lexical Retrieval Approaches: These are traditional retrieval models using a sparse inverted index, with and without BERT and ColBERT rerankers, namely: (i) BM25 (ii) BM25+BERT (iii) BM25+ColBERT, (iv) BM25+RM3, (v) BM25+RM3+BERT and (vi) BM25+RM3+ColBERT. Neural Augmentation Approaches: These use neural components to augment the (sparse) inverted index: (i) BM25+DeepCT and (ii) BM25+docT5query, both without and with BERT and ColBERT rerankers. For BM25+docT5query+ColBERT, the ColBERT reranker is applied on expanded passage texts encoded at querying time, rather than the indexed ColBERT representation. The response time for BM25+docT5query+ColBERT reflects this difference. Dense Retrieval Models: This family consists of the dense retrieval approaches: (i) ANCE: The ANCE [46] model is a single representation dense retrieval model. We use the trained models provided by the authors trained on MSMARCO training data. (ii) ANCE-PRF: The ANCE-PRF [48] is a PRF variant of ANCE model -we use the results released by the authors.",
            "score": 0.521554296285103,
            "section_title": "Baselines.",
            "char_start_offset": 28415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "210883962"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0293731689453125
        },
        {
            "corpus_id": "270064040",
            "title": "Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration",
            "text": "We select the widely used approach BM25 (Robertson et al., 2009) as the representation of lexical retrieval models in our benchmark.For neural retrieval models that leverage pretrained language models to acquire semantic relationships between queries and documents, we select the following representative and state-of-theart models:\n\nBERT (Devlin et al., 2019), a foundational pretrained language model, is commonly used as an encoder in dense retrieval systems.It was fine-  tuned on the MSMARCO dataset using the official BEIR benchmark training script.\n\nRoBERTa (Liu et al., 2019) builds on BERT's success with more data and refined training techniques to enhance performance.RoBERTa was fine-tuned in the same manner to BERT, using the MSMARCO dataset.\n\nANCE (Xiong et al., 2020) improves dense re- coCondenser (Gao and Callan, 2022) undertakes a two-stage process beginning with pretraining and unsupervised contrastive loss for embedding generation, followed by supervised training with the pre-trained encoder.\n\nRetroMAE (Xiao et al., 2022) utilizes a Masked Auto-Encoder approach for pretraining, enhancing sentence reconstruction from masked inputs to refine language modeling.DRAGON (Lin et al., 2023) applies progressive supervision and query augmentation for effective dense retrieval, suitable for both supervised and zero-shot settings.\n\nAdditionally, the following two widely used neural re-ranking models are also adopted in our evaluation framework:\n\nCE (Wang et al., 2020) is a cross-encoder model pre-trained on MS MARCO dataset (Nguyen et al., 2016) through knowledge distillation from three teacher models: BERT-base (Devlin et al., 2019), BERT-large (Devlin et al., 2019), and ALBERTlarge (Lan et al., 2019).\n\nmonoT5 (Raffel et al., 2020) is a sequenceto-sequence re-ranker based on T5 (Raffel et al., 2020), which is also pre-trained on the MS MARCO dataset.",
            "score": 0.5208263479063927,
            "section_title": "B.1 Detailed Description of Models",
            "char_start_offset": 43502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 332
                },
                {
                    "start": 334,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 555
                },
                {
                    "start": 557,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 756
                },
                {
                    "start": 758,
                    "end": 1017
                },
                {
                    "start": 1019,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1350
                },
                {
                    "start": 1352,
                    "end": 1466
                },
                {
                    "start": 1468,
                    "end": 1730
                },
                {
                    "start": 1732,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 360,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1028,
                    "end": 1047,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1471,
                    "end": 1490,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1638,
                    "end": 1659,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036895751953125
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "In this paper we address the challenges of using dense passage retrieval (DPR) in first stage retrieval for document-to-document tasks with limited labelled data. We propose the paragraph aggregation retrieval model (PARM), which liberates dense passage retrieval models from their limited input length and which takes the paragraph-level relevance for document retrieval into account. We demonstrate on two test collections higher first stage recall for dense document-to-document retrieval with PARM than with document-level retrieval. We also show that dense retrieval with PARM outperforms lexical retrieval with BM25 in terms of recall at higher cut-off values. As part of PARM we propose the novel vector-based aggregation with reciprocal rank fusion weighting (VRFF), which combines the advantages of rank-based aggregation with RRF [10] and topical aggregation with dense embeddings. We demonstrate the highest retrieval effectiveness for PARM with VRRF aggregation compared to rank and vector-based aggregation baselines. Furthermore we investigate how to train dense retrieval models for dense document-to-document retrieval with PARM. We find the interesting result that training DPR models on more, but noisy document-level data does not always lead to overall higher retrieval performance compared to training on less, but more accurate paragraph-level labelled data. Finally, we analyze how PARM retrieves relevant paragraphs and find that the dense retrieval model learns a structural paragraph relation which it exhibits with PARM and therefore benefits the retrieval effectiveness.",
            "score": 0.5206413773936518,
            "section_title": "Conclusion",
            "char_start_offset": 27275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1598
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "12408211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03289794921875
        },
        {
            "corpus_id": "266998785",
            "title": "Bridging the Preference Gap between Retrievers and LLMs",
            "text": "Retriever. Given an input x, the retriever aims to retrieve a ranked list of passages from a corpus D = {d i } m i=1 that are relevant to x. In this work, we assume a typical scenario of employing a frozen dense retriever. Typically, a dual encoder architecture is applied, where an encoder is used to encode both the input context x and the passage d. Specifically, the encoder maps each passage to an embedding E(d). The similarity between input and passage embedding is computed by their cosine similarity, \n\n(1) \n\nThe top-k passages that have the highest similarity scores when compared with the input x are retrieved in this step, \n\nBridge Model for RAG. The retrieved top-K passages provide richer information about the original input/query x to help the LLM to make a better prediction on downstream tasks. A bridge model B adapts the retrieved passages to a sequence of passages that is LLM-friendly. As mentioned in the Sec. 1, the bridge model is a seq2seq model. It takes all the retrieved passages (d retr. j ) k j=1 as well as the query x as input, and outputs the adapted passages (d bdr. j ) n j=1 , \n\nThis formulation is general enough as the seq2seq model automatically considers ranking by generating the next token based on the preceding one, selection by placing the end-of-sentence token in the appropriate position, and repetion by generating the same passage ID (as explained in the following paragraph). Note that n may be smaller or larger than k due to selection and repetition. \n\nBefore concatenating the query and passages as bridge model's input, we prepend each passage with a unique sentinel token as its passage ID, e.g., \n\n2 . In this way, the model only needs to generate the passage IDs instead of the actual passage content, which is much more efficient and avoids making unfaithful changes to the retrieved passages. We then convert the obtained passage IDs to the corresponding passages for downstream processing. \n\nRetrieval-augmented generation with bridge. We concatenate adapted passages from the bridge model, (d bdr.",
            "score": 0.5205164734613171,
            "section_title": "Problem Formulation",
            "char_start_offset": 11334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01132965087890625
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "The retrieval of documents or passages given a query often happens in two stages [75]: In the first stage, a term frequency-based (sparse) retrieval method (such as BM25 [71]) retrieves a set of documents from a large corpus. In the second stage, another model, which is usually much more computationally expensive, re-ranks the retrieved documents again. \n\nIn sparse retrieval, we denote the top-  documents retrieved from the sparse index for a query  as    . The sparse score of a query-document pair (, ) is denoted by   (, ). For the re-ranking part, we focus on self-attention models (such as BERT [14]) in this work. These models operate by creating (internal) high-dimensional dense representations of queries and documents, focusing on their semantic structure. We refer to the outputs of these models as dense or semantic scores and denote them by   (, ). Due to the quadratic time complexity of self-attention w.r.t. the document length (and decreasing performance with increasing document length [55]), long documents are often split into passages, and the score of a document is then computed as the maximum of its passage scores: \n\nThis approach is referred to as maxP [10]. \n\nThe retrieval approach for a query  starts by retrieving    from the sparse index. For each retrieved document  \u2208    , the corresponding dense score   (, ) is computed. This dense score may then be used to re-rank the retrieved set to obtain the final ranking. However, it has been shown that the scores of the sparse retriever,   , can be beneficial for re-ranking as well [1]. To that end, an interpolation approach is employed [4], where the final score of a query-document pair is computed as \n\nSetting  = 0 recovers the standard re-ranking procedure. \n\nSince the set of documents retrieved by the sparse model is typically large (e.g.,   = 1000), computing the dense score for each query-document pair can be very computationally expensive. In this paper, we focus on efficient implementations of interpolation-based re-ranking, specifically the computation of the dense scores   .",
            "score": 0.5204426992792729,
            "section_title": "Interpolation-Based Re-Ranking",
            "char_start_offset": 15719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 85,
                    "matchedPaperCorpusId": "17660655"
                },
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 1565,
                    "end": 1568,
                    "matchedPaperCorpusId": "202635721"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "253080555"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.027069091796875
        },
        {
            "corpus_id": "271166492",
            "title": "On Elastic Language Models",
            "text": "Since IR scenarios are sensitive to latency variation, we adapt ElasticLM to dense retrieval. \n\nProvided that ElasticLM is task-agnostic and Condenser [13] and PROP [39] are both continuously pretrained BERTs but tailored separately for dense retrieval and reranking, ElasticLM can be flexibly adapted to dense retrieval and reranking by treating Condenser or PROP instead of BERT as the teacher LM. Accordingly, we term ElasticLM with Condenser being the teacher as ElasticDenser and that with PROP being the teacher as ElasticRanker. \n\nWe preserve the elastic structure attained from BERT for simplicity, but replace BERT with Condenser or PROP during elastic optimization for performance boost. Apparently, the task-specific objective should take a form that is suitable for dense retrieval or reranking. \n\nInspired by DPR [25], we instantiate L TSK for ElasticDenser to in-batch negative log likelihood as below: \n\nsim( , \n\nwhere \n\ndenotes that one question/query has one relevant positive passages and \u2212 1 irrelevant negative passages. The negatives are comprised of both in-batch and hard negatives retrieved by BM25, and the similarity between the question/query and the passage is computed with a simple dot product over their [CLS] representations in an shared encoder (i.e., ElasticDenser) like what has been done in Condenser rather than two separate encoders in DPR. \n\nAnother consideration is that the passage encoder would be always kept as the largest structure. On the one hand, since passages are usually encoded offline, this behavior can hold the performance as much as possible without affecting the latency. On the other hand, more importantly, there will otherwise be multiple cached embeddings for one passage due to the elastic structure, giving rise to unreasonable memory consumption. Figure 6 offers a description of ElasticDenser during task-specific finetuning. \n\nFollowing localized contrastive estimation [18], we instantiate L TSK for ElasticRanker to localized negative loglikelihood, whose basic form strictly follows Equation ( 9) and only differs in: \n\nHere, the similarity between the question/query and the passage is computed with a linear mapping w over the [CLS] representations of concatenated query-passage inputs.",
            "score": 0.5204237185136977,
            "section_title": "Adaptation to IR",
            "char_start_offset": 24018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "224803318"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1936,
                    "end": 1940,
                    "matchedPaperCorpusId": "231662379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08819580078125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Dense Passage Retrieval (DPR), introduced by Karpukhin et al. ( 2020), represents a more modern approach to information retrieval. It uses a dense vector space in which both the query and the documents are encoded into high-dimensional vectors. DPR employs a bi-encoder architecture, where the query and documents are encoded separately, allowing for efficient nearest-neighbor search (Xiong et. al. 2020). Unlike BM25, DPR excels at capturing semantic similarity between the query and documents, making it highly effective for open-domain question-answering tasks. The strength of DPR lies in its ability to retrieve relevant information based on semantic meaning rather than keyword matching. By training the retriever on a large corpus of question-answer pairs, DPR can find documents that are contextually related to the query, even when the query and the document do not share exact terms. Recent research has further improved DPR by integrating it with pre-trained language models and an example is LLM adapted for the dense RetrievAl approach (Li et. al. 2023)",
            "score": 0.5198709066226056,
            "section_title": "Dense Passage Retrieval (DPR)",
            "char_start_offset": 14303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1015625
        },
        {
            "corpus_id": "254854282",
            "title": "Query-as-context Pre-training for Dense Passage Retrieval",
            "text": "Passage retrieval is the process of retrieving relevant passages from a large corpus in response to a query, which is useful in a variety of downstream applications such as web search (Fan et al., 2021;Guo et al., 2022;Lin et al., 2021a), question answering (Karpukhin et al., 2020;Lee et al., 2020;Zhu et al., 2021) and dialogue systems (Gao et al., 2022a;Yu et al., 2021). The success of pre-trained language models (PLMs) (Devlin et al., 2018;Liu et al., 2019) has led to the development of more powerful PLM-based dense and sparse passage retrieval approaches. \n\nPLM-based dense retrieval methods (Xiong et al., 2020;Lu et al., 2021;Hofst\u00e4tter et al., 2021;Gao and Callan, 2021b;Ren et al., 2021b;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022; Passage 1. A good day at Pipeline means an encounter with fellow surfers who can be as friendly as pitbulls with migraines, and waves that can shatter boards into kindling. And then there's the reef. At Pipeline there can be 10 foot waves blasting over just three feet of water, so if you fall on the reef or get caught inside the break, you're lucky if you come out merely sliced up. \n\nPassage 2. This idyll can still be found on the island of Lanai. Formerly a pineapple plantation, Lanai is almost entirely owned by billionaire entrepreneur David Murdock, and its only notable commercial hub is a tiny village built around a square of Cook pine trees. Figure 1: An example of low-relevance passages within a document from the MS-MARCO corpus. The two passages are weakly correlated in content. Wang et al., 2022) use PLMs to encode queries and passages into a shared semantic space. The semantic relationships between query and passage representations are then measured by dot product or cosine similarities. Pre-training and finetuning techniques have been developed to improve the performance of dense retrieval models.",
            "score": 0.518787229229021,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 219,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 219,
                    "end": 237,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 357,
                    "end": 373,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 621,
                    "end": 637,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 637,
                    "end": 661,
                    "matchedPaperCorpusId": "233231706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08978271484375
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "Information Retrieval (IR) aims to retrieve relevant passages for a given query, which plays a critical role in many industry scenarios such as Open-Domain Question Answering (QA) (Lee et al., 2019) and Web Search (Nguyen et al., 2016). Traditionally, bag-of-words (BOW) retrieval systems such as TF-IDF and BM25 (Robertson and Zaragoza, 2009) were widely used, which mainly depend on keyword matching between queries and passages. With the development of large-scale pretrained language models (PLMs) (Vaswani et al., 2017;Devlin et al., 2019) such as BERT, dense retrieval methods (Lee et al., 2019;Karpukhin et al., 2020) show quite effective performance. These methods usually employed a dual-encoder architecture to encode both queries and passages into dense embeddings and then perform approximate nearest neighbor searching (Johnson et al., 2021). \n\nRecently, some works found that dense retrievers perform poorly in the zero-shot multilingual settings (Zhang et al., 2021b) due to the distributional shift. To boost the performance of dense retrievers, some previous methods for cross-domain retrieval can be directly adopted to unsupervised multilingual dense retrieval. There are two important kinds: 1) generating training data in target languages. For example, Kulshreshtha et al. applied self-training to generate labeled data and further proposed back-training (Kulshreshtha et al., 2021) to obtain more high-quality data. QGen (Ma et al., 2021a) proposed to use a query generator to generate in-domain queries. 2) leveraging sparse retrievers, which is more effective in the unsupervised setting, to enhance dense retrievers. For example, SPAR (Chen et al., 2021) proposed to distill knowledge from BM25 to the dense model and LaPraDoR (Xu et al., 2022) proposed to enhance the dense model by multiplying the similarity with the BM25 score. \n\nHowever, there are three major problems when directly adopting these methods to multilingual dense retrieval.",
            "score": 0.5186469985623547,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 198,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 214,
                    "end": 235,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 313,
                    "end": 343,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 502,
                    "end": 524,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 601,
                    "end": 624,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 832,
                    "end": 854,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1443,
                    "end": 1461,
                    "matchedPaperCorpusId": "231704318"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11199951171875
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "Our research questions and experiments have focused exclusively on interpolation-based re-ranking using dual-encoders and Fast-Forward indexes. However, the most common application of dualencoders in the field of IR is the use as dense retrieval models; a natural question that occurs is, whether the encoders proposed in Section 5 can be used for more efficient dense retrieval. \n\nIn Table 6, we present passage and document retrieval results on the MS MARCO corpus. Dense retrievers use a FAISS [32] vector index; no interpolation or re-ranking is performed. It is immediately obvious that our models do not achieve competitive results; on the contrary, the embedding-based encoder yields far worse performance than dense retrievers and even BM25, and even the attention-based encoder fails to improve over sparse retrieval. \n\nFrom these results, we infer that the models we trained are not suitable for dense retrieval. However, we assume that the main reason for this is not the architecture of the query encoder, but instead the following: \n\n\u2022 We use a simple in-batch negative sampling strategy [34], which has been shown to be less effective than more involved strategies [51,53,86,88]. \u2022 The hardware we use for training the models is limiting w.r.t. the batch size and thus the number of negative samples, i.e., we cannot use a batch size greater than 4. \u2022 We perform validation and early stopping based on re-ranking. \n\nConsidering the points above, we expect that our dual-encoder models, including ones with lightweight encoders, could also be used in retrieval settings if the shortcomings of the training setup are addressed, for example, by using more powerful hardware and state-of-the-art training approaches. On the other hand, we argue that the fact that our models perform well in the reranking setting (see Section 7) shows that it is both easier and more efficient (in terms of time and resources) to train models to be used with Fast-Forward indexes instead of for dense retrieval.",
            "score": 0.518185136228207,
            "section_title": "Efficient Encoders for Dense Retrieval",
            "char_start_offset": 63161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 1186,
                    "end": 1189,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029815673828125
        },
        {
            "corpus_id": "268066341",
            "title": "Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling",
            "text": "Dense Retrieval Dense retrieval has garnered significant attention for its potential to enable retrieval in the semantic space. A prominent method in this area is the dense passage retriever (DPR) (Karpukhin et al., 2020), which comprises a query encoder and a passage encoder. Several studies have also explored efficient training approaches, such as RocketQA (Qu et al., 2021) and alternative architectures for dense retrieval, e.g., ColBERT (Khattab and Zaharia, 2020). A common technique for training performant dense retrievers is knowledge distillation from cross encoders. BERT-CAT (Hofst\u00e4tter et al., 2020) proposed cross-architecture knowledge distillation to improve dense retrievers and rankers. Izacard and Grave distilled knowledge from the reader model to the retriever model, thus improving its performance on open-domain question answering. However, the majority of previous work has primarily focused on English retrieval, limiting its applicability to other languages. \n\nMultilingual Dense Retrieval Multilingual information retrieval has been an active research area for several decades. Early work in this field primarily focused on cross-lingual information retrieval (CLIR), aiming to retrieve relevant documents in a different language from the query language (Nasharuddin and Abdullah, 2010). Traditional CLIR systems relied on aligning bilingual dictionaries or parallel corpora to translate queries or documents into a common language for retrieval. However, these systems often faced limitations in translation quality, vocabulary coverage, and handling domain-specific expressions (Ballesteros and Croft, 1996;Vuli\u0107 and Moens, 2015;Sharma and Mittal, 2016). \n\nIn recent years, dense retrieval has emerged as a promising approach for multilingual information retrieval. Various studies have demonstrated the effectiveness of dense retrieval methods in crosslingual and multilingual scenarios. Models such as XLM-R (Conneau et al., 2020) and mBERT (Devlin et al., 2019) have achieved remarkable performance on diverse natural language processing tasks, including similarity-based retrieval tasks. The success of these models has spurred researchers to explore their application in multilingual information retrieval (Jiang et al., 2020).",
            "score": 0.5178332444028492,
            "section_title": "Related Work",
            "char_start_offset": 3221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 221,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 361,
                    "end": 378,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 444,
                    "end": 471,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1609,
                    "end": 1638,
                    "matchedPaperCorpusId": "7109112"
                },
                {
                    "start": 1638,
                    "end": 1660,
                    "matchedPaperCorpusId": "2583305"
                },
                {
                    "start": 1660,
                    "end": 1684,
                    "matchedPaperCorpusId": "63311109"
                },
                {
                    "start": 1941,
                    "end": 1963,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1974,
                    "end": 1995,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1177978515625
        },
        {
            "corpus_id": "267938301",
            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
            "text": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
            "score": 0.5178097776983872,
            "section_title": "Generative Models for Reranking",
            "char_start_offset": 4693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 927
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 83,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 239,
                    "end": 261,
                    "matchedPaperCorpusId": "212725651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "258309183",
            "title": "The Tale of Two MSMARCO - and Their Unfair Comparisons",
            "text": "Now that we have introduced the \"titled\" version of MS MARCO, we now look into the problems it causes empirically. We look into three types of models: i) dense retrievers; ii) sparse learned retrievers and iii) cross-encoding rerankers. Note that this could be extended to other models, but we chose the three that we had most familiarity with. In more detail, we use DPR-cls [11] as our dense retriever, SPLADE-max [6] as our learned sparse retriever and RankT5-3b encoder only [20], monoElectra-large [13] as our cross encoding rerankers. We train models on either the \"titled\" or the official MS MARCO-passage dataset. Titled passages are generated using \"Title [SEP] Passage\". We use only contrastive training (i.e. no distillation) and hard-negatives 6 to train the first stage models and the negatives from SPLADE to train the reranker. First-stage retrieval training uses 8 queries per batch, and 32 negatives per query, and trains for 3 epochs. We use DistilBERT [17] as the pre-trained language model for the first stage models and for reranking Electra-large and T5-3B. Statistical significance is computed only when we directly compare the original vs \"title\" datasets (i.e. only Tables 1 and 4 and only on the same model with different corpus), with Student's t-test and  \u2264 0.05.",
            "score": 0.5174887791737512,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 12352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1291
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 419,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "248003253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00307464599609375
        },
        {
            "corpus_id": "277621440",
            "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
            "text": "Information retrieval (IR) contains two primary stages: retrieval and reranking [13,67]. Retrieval serves as the cornerstone of information retrieval and is critically important. Its objective is to retrieve relevant passages from a large-scale corpus in response to a query, thereby providing candidate passages for the subsequent reranking stage. In the early stages of retrieval, the focus was primarily on lexical matching between the terms of the query and the passage, with methods such as BM25 [34] and query likelihood (QL) model [46]. However, with the advent of pre-trained language models (PLMs) such as BERT [11], representing passages or queries as dense vectors has gradually become the mainstream approach. These methods typically employ two separate encoders to represent the query and passage and are referred to as dual-encoder models. \n\nLarge language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,39,51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention. The task of next token prediction allows LLMs to ingest large amounts of various types of data and thus gain more powerful semantic understanding capability. However, the unidirectional attention during modeling may lead to insufficient representation of global semantics, which is inferior to encoder-style PLMs. Although leveraging the superior semantic understanding ability of LLMs for retrieval looks appealing, it is challenging to do so. Recent studies have attempted to repeat passages as input during encoding [51], to use a bidirectional attention mechanism for encoding during relevance matching fine-tuning [4], or pre-train LLMs to strengthen the global semantic representation capabilities [33]. \n\nSince LLMs are decoder-based language models (LMs), it is natural to think of adapting them to retrieval according to the classical language modeling approach to IR [46], i.e., modeling the query likelihood (QL) given the LM of a document.",
            "score": 0.5170165762551315,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "259076101"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 1922,
                    "end": 1926,
                    "matchedPaperCorpusId": "271885989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14453125
        },
        {
            "corpus_id": "273501946",
            "title": "Improve Dense Passage Retrieval with Entailment Tuning",
            "text": "Dense Retrieval Unlike traditional methods such as TF-IDF and BM25 (Robertson and Zaragoza, 2009) that calculate text relevance based on term frequency, dense retrieval methods use deep neural networks to encode a piece of text which integrates contextualized information into a single vector, and then text is retrieved with maximum inner-product search (MIPS) based on its embedding similarity with query. Siamese network and dual encoder are the two most frequently used structures to encode queries and passages, which are built on the PLMs with advanced language understanding abilities. However, general-purpose PLMs are not trained under retrieval objectives. To this end, lines of methods have been proposed to adapt LMs to retrieval tasks. At pre-training stage, several works (Xiao et al., 2022;Gao and Callan, 2021;Izacard et al., 2022) design unsupervised training schemes suitable for retrieval task, including aggressive masking, asymmetric encoder-decoder, and inverse-cloze task (Lee et al., 2019). At finetuning stage, techniques like supervised contrastive learning (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021) and late-interactions (Khattab and Zaharia, 2020) are used to train dense retrievers. With recent developments of GPT-like models, another line of work attempts to adapt LLMs for IR tasks, using methods like bi-directional attention (BehnamGhader et al., 2024), instructiontuning (Asai et al., 2023;Su et al., 2023) and hypothesis generation (Gao et al., 2023). Natural Language Inference Natural language inference is a fundamental task in NLP, underpinning a wide range of NLP tasks, from commonsense reasoning to semantic textual similarity (STS) tasks (Bowman et al., 2015;Conneau et al., 2017). NLI focuses on understanding sentence meaning and the relationship between sentences. Specifically, given a premise sentence and a hypothesis sentence, the goal is to classify the relationship Example (Passage & Query)",
            "score": 0.5166360248384929,
            "section_title": "Background",
            "char_start_offset": 5375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 786,
                    "end": 805,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 805,
                    "end": 826,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 826,
                    "end": 847,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 995,
                    "end": 1013,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1084,
                    "end": 1108,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1108,
                    "end": 1127,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1127,
                    "end": 1143,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1166,
                    "end": 1193,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1424,
                    "end": 1443,
                    "matchedPaperCorpusId": "253581733"
                },
                {
                    "start": 1443,
                    "end": 1459,
                    "matchedPaperCorpusId": "254853816"
                },
                {
                    "start": 1486,
                    "end": 1504,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1700,
                    "end": 1721,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1721,
                    "end": 1742,
                    "matchedPaperCorpusId": "28971531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326416015625
        },
        {
            "corpus_id": "259287364",
            "title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering",
            "text": "The nature of the multi-modal retrieval task that we attempt to solve in this paper requires the bi-encoder dense passage retriever to encode queries in multi-modal semantic space and to encode passages in textual semantic space. We use an asymmetric state-ofthe-art dense passage retrieval for OK-VQA tasks proposed by Qu et al. [29]. It uses an asymmetric dense passage retriever with the multi-modal query encoder   and the textual passage encoder   . Then, the relevance score is calculated as follows:  ((,  ), ) =   (,  ) \u2022  (), where \u2022 denotes the inner product. Following Qu et al. [29], we implement   using the representation of the [CLS] token provided by a BERT-base [8] model. Similarly, we utilize the representation of the [CLS] token generated by LXMERT [40], a vision-language model pre-trained with various vision-language tasks. \n\nTo train the retriever, we use a contrastive loss as follows: \n\nwhere   is a positive (relevant) passage and P neg is a set of negative passages for the question-image pair (,  ). In addition to the selected negative passages, we use in-batch negatives, in which all the positive and negative passages of other queries in the same training batch are considered as negative passages to the query. We use the Faiss library [16] for indexing and efficient dense retrieval. \n\nTable 1: Passage retrieval performance on the OK-VQA dataset [27]. The superscript * denotes statistically significant improvement compared to all baselines based on two-tailed paired t-test with Bonferroni correction ( < 0.05).",
            "score": 0.5148965293183954,
            "section_title": "Dense Retrieval Model",
            "char_start_offset": 17919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1550
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "173991173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06634521484375
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "Training Self-Retrieval unifies the three distinct tasks of information retrieval -indexing, retrieval, and reranking -into text generation tasks, trained using cross-entropy loss in an auto-regressive manner. Specifically, Self-Retrieval first internalizes the corpus into its parameters through selfsupervised learning as introduced in Section 3.1. Subsequently, in addition to a portion of selfsupervised instances, it incorporates two different types of data to build retrieval and reranking abilities: \n\n\u2022 Retrieval data: Utilizes supervised query-passage pairs from the dataset, where the model learns to generate both document titles and passage content in response to input queries. \n\n\u2022 Reranking data: Employs positive and negative examples to train the model in relevance assessment between queries and passages. \n\nThis auto-regressive training approach enables Self-Retrieval to integrate traditionally separate IR components into a unified language model, establishing an end-to-end IR system. \n\nFurthermore, leveraging the universal language generation capabilities of LLMs, we can seamlessly integrate downstream task components, such as readers in RAG, into Self-Retrieval. This integration can be achieved by simply appending the golden answer after the assessment in Self-Retrieval. Consequently, the LLM can function as a comprehensive RAG system, effectively reducing the knowledge gap between IR system and reader modules. \n\nInference During inference, given an input query, Self-Retrieval aims to obtain the relevant passages that are sorted based on the relevance to query. Firstly, the model generates i document titles through constrained beam search. Secondly, for each title, it generates j passages using beam search. Finally, the resulting i\u00d7j passages are scored using the self-assessment mechanism and reranked to produce the final output. \n\n4 Experimental Results",
            "score": 0.5147298149460379,
            "section_title": "Training & Inference",
            "char_start_offset": 15075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1894
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "276928032",
            "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
            "text": "\u2022 E5 (Wang et al., 2022) is a state-of-the-art dense retriever that pre-trained on millions of weakly related text pairs from the Web. The unsupervised version of this model is denoted as E5-unsup. This model undergoes further fine-tuning on natural language inference (NLI) datasets, as well as the Natural Questions and MS MARCO datasets, to enhance its capabilities in downstream applications. The fine-tuned version is denoted as E5. \n\n\u2022 Contriever (Izacard et al., 2021) is a widely-used dense retriever pre-trained unsupervised on Wikipedia data and CCNet (Wenzek et al., 2019). The unsupervised version of this model is denoted as Contriever. It is further fine-tuned on the MS MARCO dataset to enhance its retrieval performance, with the fine-tuned version denoted as Contriever MS . \n\n\u2022 DPR (Karpukhin et al., 2020) is a widely used dense passage retriever initialized with a BERT-based uncased encoder (Devlin et al., 2019), and fine-tuned on downstream dataset. Specifically, DPR MS is fine-tuned on the MS MARCO dataset, DPR NQ on the NQ dataset, and DPR TQA on the TriviaQA dataset. \n\n\u2022 SiDR (Zhou et al., 2024a) is a semi-parametric sparse retriever that supports using both embeddings and tokenization as index. This nature allows for in-training retrieval, where the model's parameters dynamically update while the retrieval index remains fixed. The model is initialized with a BERT-based uncased encoder (Devlin et al., 2019) and fine-tuned exclusively on single dataset depending on the variant: SiDR MS is fine-tuned on the MS MARCO dataset, SiDR NQ on the NQ dataset, and SiDR TQA on the TriviaQA dataset. \n\nAll the above retrieval methods are initialized with a BERT-based encoder, which contains approximately 200 million (0.2B) parameters.",
            "score": 0.5146379914148664,
            "section_title": "B.1. Retrieval Model (IR)",
            "char_start_offset": 33240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1625
                },
                {
                    "start": 1628,
                    "end": 1762
                }
            ],
            "ref_mentions": [
                {
                    "start": 800,
                    "end": 823,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 912,
                    "end": 933,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1421,
                    "end": 1442,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04132080078125
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "Dense Retrieval The field of information retrieval (IR) (Manning et al., 2005) aims to find the relevant information given an ad-hoc query and has played a key role in the success of modern search engines. In recent years, IR has witnessed a paradigm shift from traditional BM25-based inverted index retrieval to neural dense retrieval (Yates et al., 2021;Karpukhin et al., 2020). BM25-based retrieval, though efficient and interpretable, suffers from the issue of lexical mismatch between the query and passages. Methods like document expansion (Nogueira et al., 2019) or query expansion (Azad and Deepak, 2019;Wang et al., 2023) are proposed to help mitigate this issue. In contrast, neural dense retrievers first map the query and passages to a low-dimensional vector space, and then perform semantic matching. Popular methods include DSSM (Huang et al., 2013), C-DSSM (Shen et al., 2014), and DPR (Karpukhin et al., 2020) etc.\n\n1 Also called dual-encoder / two-tower encoder.\n\nInference can be done efficiently with approximate nearest neighbor (ANN) search algorithms such as HNSW (Malkov and Yashunin, 2020). Some recent works Reimers and Gurevych, 2021;Sciavolino et al., 2021) show that neural dense retrievers may fail to capture some exact lexical match information. To mitigate this issue,  proposes to use BM25 as a complementary teacher model, ColBERT (Khattab and Zaharia, 2020) instead replaces simple dot-product matching with a more complex token-level MaxSim interaction, while COIL (Gao et al., 2021) incorporates lexical match information into the scoring component of neural retrievers. Our proposed pre-training method aims to adapt the underlying text encoders for retrieval tasks, and can be easily integrated with existing approaches.\n\nPre-training for Dense Retrieval With the development of large-scale language model pre-training (Dong et al., 2019;Clark et al., 2020), Transformerbased models such as BERT (Devlin et al., 2019) have become the de facto backbone architecture for learning text representations",
            "score": 0.5140473666597841,
            "section_title": "Related Work",
            "char_start_offset": 3616,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 356,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 356,
                    "end": 379,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 872,
                    "end": 891,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 1086,
                    "end": 1113,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1133,
                    "end": 1160,
                    "matchedPaperCorpusId": "229680000"
                },
                {
                    "start": 1160,
                    "end": 1184,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1501,
                    "end": 1519,
                    "matchedPaperCorpusId": "237581068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "267751308",
            "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
            "text": "Dense retrieval. Dense retrieval (Lee, Chang, and Toutanova 2019;Karpukhin et al. 2020;Xiong et al. 2020;Khattab and Zaharia 2020) shows tremendous success in many downstream tasks (e.g. open-domain QA and web search) compared with the traditional sparse retrieval models (e.g. TF-IDF and BM25). The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dualencoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model finegrained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance. Besides, knowledge distillation has become a vital technique to enhance the capacity of the dual-encoder by distilling knowledge from a more powerful reader to a classical retriever (Qu et al. 2020;Ren et al. 2021b;Lin, Yang, and Lin 2020;Hofst\u00e4tter et al. 2021). \n\nRecently, massive works have investigated task-related pretraining methods for dense retrieval models (Gao and Callan 2021a,b;Wang, Reimers, and Gurevych 2021;Ren et al. 2021a;Oguz et al. 2021;Meng et al. 2021). Condenser (Gao and Callan 2021a) proposed the Condenser architecture to enforce the late backbone layers to aggregate the whole information. coCondenser (Gao and Callan 2021b) leveraged contrastive learning to incorporate a query-agnostic contrastive loss. PAIR (Ren et al. 2021a) and DPR-PAQ (Oguz et al. 2021) also designed special tasks in pre-training to enhance retrieval models. Additionally, jointly training retrieval models with the rerank model can bring about better performance.",
            "score": 0.5138021325061999,
            "section_title": "Related Work",
            "char_start_offset": 6374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 65,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 65,
                    "end": 87,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 557,
                    "end": 589,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 868,
                    "end": 886,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1146,
                    "end": 1162,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1203,
                    "end": 1226,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1389,
                    "end": 1406,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1704,
                    "end": 1721,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "In this section, we provide necessary background about the bi-encoder architecture for dense passage retrieval which is widely used in ODQA  and is the primary baseline model in our experiments.\n\nAs illustrated in the left part of Figure 1, the bi-encoder architecture consists of a question encoder and a passage encoder, both of which are usually Transformer encoders (Vaswani et al., 2017). A Transformer encoder is built up with a stack of Transformer blocks. Each block consists of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2016) and layernormalization (Ba et al., 2016) applied to both sublayers. Given an input vector h \u2208 R d , the FFN sub-layer produces an output vector as following\n\nwhere W 1 \u2208 R m\u00d7d , W 2 \u2208 R d\u00d7m , b 1 \u2208 R m , and b 2 \u2208 R d are learnable parameters. For a sequence of N tokens, each Transformer block produces N corresponding vectors, together with a vector for the special prefix token [CLS] which can be used as the representation of the sequence. We refer readers to (Vaswani et al., 2017) for other details about Transformer. Typically the question encoder and passage encoder are initialized from a pretrained language model such as BERT , but they are parameterized separately, i.e., their parameters would differ after training.\n\nThe bi-encoder model independently encodes questions and passages into d-dimension vectors, using the final output vectors for [CLS] from the corresponding encoders, denoted as q \u2208 R d and p \u2208 R d , respectively. The relevance between a question and a passage can then be measured in the vector space using dot product, i.e., sim(q, p) = q T p.\n\n(2)\n\nDuring training, the model is optimized based on a contrastive learning objective,\n\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then,",
            "score": 0.5136071541734084,
            "section_title": "Background",
            "char_start_offset": 4175,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 392,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 601,
                    "end": 618,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1083,
                    "end": 1105,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "248377381",
            "title": "Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval",
            "text": "Neural information retrieval (IR) performs query-passage matching at a semantic level, often using a dual-encoder architecture that encodes the queries and the passages separately. Examples of such models include the Dense Passage Retriever (DPR) [9] and ANCE [23], which fine-tune transformer-based [20] pre-trained language models [4] to compute contextualized representations of queries and passages. Table 1: Retrieval scores from DPR for the passage in Figure 1, against both a gold-standard question from NQ and three synthetic questions. The important terms in the question, that are also in the passage, are shown in italic. \n\nIn this paper, we first uncover a shortcoming in the passage encoder of such a dual-encoder IR model, namely DPR, which stems from its sparse attention pattern. To illustrate, in Figure 1 we show a heatmap of the attention weights of DPR's passage encoder over different tokens of an example passage (taken from the Natural Questions (NQ) dataset [10]). We can see that the attention given to many potentially important words and phrases, e.g, academy of management and twentieth century, are rather low. \n\nWhat is the effect of such attention, or lack thereof, on retrieval performance? Table 1 shows DPR's retrieval scores for a goldstandard question (from the NQ dataset) and three automatically generated synthetic questions (details in Section 2) when paired with the passage of Figure 1. The gold-standard question, which overlaps highly with the well-attended first sentence of the passage, receives a relatively high retrieval score. Among the synthetic questions, the one that refers to the highest-attended entity (principles of scientific management) gets the highest score, whereas the ones about To further quantify this, we randomly sampled 20k passages from Wikipedia and identified named entities that received the highest and lowest attentions from the DPR passage encoder (using the process described in Section 2.1). We then generated synthetic questions corresponding to those entities (using the process of Section 2.2).",
            "score": 0.5132795375426412,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02947998046875
        },
        {
            "corpus_id": "256105432",
            "title": "Information Retrieval: Recent Advances and Beyond",
            "text": "[162] proposes DTR, which uses large dual encoders for generalizable retrieval. [163] suggests utilizing out-of-domain semantics to improve zero-shot hybrid retrieval models. [164] proposes InPars, a method that uses large language models for data augmentation in information retrieval. \n\nMoreover, contrastive learning has been proposed as a means of performing unsupervised dense information retrieval with Contriever method by [165]. [166] proposes GPL method that uses generative pseudo-labeling for unsupervised domain adaptation of dense retrieval. [167] proposes a method called Spider, which enables unsupervised passage retrieval. [168] proposes a method that utilizes contrastive pre-training to learn embeddings for text and code. [169] proposes a method that disentangles the modeling of domain and relevance for adaptable dense retrieval. Lastly, [170] proposes a method called Promptagator, which uses few-shot dense retrieval from 8 examples. \n\nOverall, recent research in zero-shot information retrieval has been focused on finding ways to improve the performance and generalizability of retrieval models by utilizing various techniques such as query generation, synthetic pre-training, large language models, and contrastive learning. These methods have been applied to tasks such as passage retrieval, unsupervised domain adaptation, and dense retrieval. \n\nProbing Analysis Several studies have been conducted to address the limitations of dense low-dimensional retrieval, particularly when dealing with large index sizes. These studies propose various methods such as redundancy elimination, benchmarking, incorporating salient phrase information, entities-centric questions, and interpreting dense retrieval as a mixture of topics as ways to improve the performance and interpretability of dense retrieval models. For example, [171] delves into the challenges that arise from utilizing dense low-dimensional retrieval for large index sizes and proposes solutions to mitigate these limitations. Similarly, [172] proposes a method for unsupervised redundancy elimination to compress dense vectors for passage retrieval as a means of enhancing retrieval performance. [160] proposed a benchmark to evaluate the performance of zero-shot information retrieval models, while [173] explores the potential of incorporating salient phrase information in dense retrieval to imitate the performance of sparse retrieval.",
            "score": 0.5125077318169182,
            "section_title": "Multi-vector Representation",
            "char_start_offset": 36809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 286
                },
                {
                    "start": 289,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 85,
                    "matchedPaperCorpusId": "246285831"
                },
                {
                    "start": 2025,
                    "end": 2030,
                    "matchedPaperCorpusId": "239993003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09808349609375
        },
        {
            "corpus_id": "250390910",
            "title": "An Encoder Attribution Analysis for Dense Passage Retriever in Open-Domain Question Answering",
            "text": "In the previous sections, we manage to identify the importance of the passage encoder and its affecting factors such as positive passage overlap and corpus coverage of the training data. We find that our discoveries are consistent with some previous work's conclusions. For example, Zhan et al. (2021Zhan et al. ( , 2020a)); Sciavolino et al. (2021) all find that it is sufficient to achieve reasonable retrieval accuracy by just fine-tuning the question encoder with a fixed passage encoder, which demonstrates the importance of a robust passage encoder in domain adaptation and hard-negative mining. \n\nHowever, how to learn such a robust passage encoder is challenging as pre-training DPR on a single QA dataset will introduce biases. Multi-task dense retrieval (Maillard et al., 2021;Li et al., 2021;Metzler et al., 2021) uses multiple experts learned in different domains to solve this problem. These solutions are effective but not efficient as they build multiple indexes and perform searches for each expert, requiring a lot of resources and storage space. \n\nAnother solution is to build a question-agnostic passage encoder so that the model is not biased towards particular QA tasks. DensePhrases (Lee et al., 2021a,b) pioneers this direction by building indexes using phrases instead of chunks of passages for multi-granularity retrieval. By breaking passages into finer-grained units, DensePhrases indeed improve the generalization of dense retrieval in different domains with query-side fine-tuning. However, similar to multi-task learning, it is not efficient as the phrase index can be enormous for a corpus like Wikipedia. Although techniques such as product quantization (Gray and Neuhoff, 1998) can be applied to improve efficiency, it comes at the cost of effectiveness. \n\nOverall, it is desirable to have a robust passage encoder for efficient dense retrieval according to previous work and our analysis, but challenges still remain in the effectiveness-efficiency trade-off.",
            "score": 0.5123359601755236,
            "section_title": "Impact of Passage Encoders",
            "char_start_offset": 22516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 300,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 764,
                    "end": 787,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 787,
                    "end": 803,
                    "matchedPaperCorpusId": "240286557"
                },
                {
                    "start": 1686,
                    "end": 1710,
                    "matchedPaperCorpusId": "119156912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1444091796875
        },
        {
            "corpus_id": "248366563",
            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
            "text": "Dense retrieval has shown promising results in many information retrieval (IR) related tasks, whose foundation is high-quality text representation learning for effective search. Some recent studies have shown that autoencoder-based language models are able to boost the dense retrieval performance using a weak decoder. However, we argue that 1) it is not discriminative to decode all the input texts and, 2) even a weak decoder has the bypass effect on the encoder. Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder thoroughly. Comprehensive experiments over publicly available retrieval benchmark datasets show that our approach can outperform existing pre-training methods for dense retrieval significantly.",
            "score": 0.5121590777873772,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061431884765625
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "(Multi-Modal) Dense Retrieval. Using dense vectors for retrieving textual documents related to a textual query has been studied since the emergence of Latent Semantic Analysis [7]. However, dense retrievers' performance remained inferior to that of sparse retrievers like BM25 until Karpukhin et al. [23]'s Dense Passage Retriever (DPR), which uses the [CLS] token output by BERT [8], a pre-trained language model. While many dense retrievers only use a single vector to represent the query and the document [23,41,59], using multiple vectors per document and query has been also studied [11,19,24,36,49]. \n\nMulti-modal dense retrieval has recently been investigated in different forms: (1) uni-modal query and multi-modal documents [15,34,52], (2) multi-modal query and uni-modal documents [40], \n\n(3) multi-modal query and multi-modal documents [51], and (4) unimodal query and uni-modal documents with queries and documents from different modalities, i.e., cross-modal retrieval [21,42]. \n\nIn this work, we focus on the second case, where the query is multi-modal while the documents only contain text. Qu et al. [40] utilized an asymmetric bi-encoder with LXMERT [53], a pretrained vision-language model based on BERT [8] for encoding queries, and BERT itself for encoding documents. As we show, such an asymmetric architecture is sub-optimal; utilizing different encoders creates a semantic \"gap\" in the embedding space and finetuning cannot easily overcome the issue. We instead propose a new symmetric dual encoding framework that addresses this issue. \n\nKnowledge Distillation for Dense Passage Retrieval. Due to the vast number of learnable parameters in dense passage retrievers, sometimes available datasets are insufficient to train them [62]. Consequently, knowledge distillation, in which a teacher model provides labels for a student model, has become a standard approach for training dense retrieval models and has shown compelling outcomes [17,30].",
            "score": 0.5121051150098131,
            "section_title": "RELATED WORK",
            "char_start_offset": 4661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "3252915"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 515,
                    "end": 518,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "222205492"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 595,
                    "end": 598,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "210859295"
                },
                {
                    "start": 740,
                    "end": 743,
                    "matchedPaperCorpusId": "233219849"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "235097195"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 1167,
                    "end": 1171,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1222,
                    "end": 1225,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1750,
                    "end": 1754,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1295166015625
        },
        {
            "corpus_id": "268723791",
            "title": "Scaling Laws For Dense Retrieval",
            "text": "We now briefly revisit prior studies in the field of dense retrieval.The training data for dense retrieval tasks typically comprises annotated pairs, each consisting of a query and a human-labeled relevant passage.Early research primarily concentrated on effective negative sampling strategies used for dense retrieval training, such as employing random passages or the top irrelevant passages retrieved by BM25 as negative samples [23].ANCE [53] utilized self-mined hard negatives and substantially improved the retrieval performance.Furthermore, Zhan et al. [56] proposed dynamic hard negatives to further enhance both training efficiency and retrieval effectiveness.RocketQA [39] and TAS-B [19] introduced knowledge distillation, utilizing a well-trained cross-encoder model to generate soft labels for training pairs.Beyond the design of finetuning methods, researchers also explore other techniques, such as pretraining methods and multivector retrieval.(1) Pretraining studies design objectives that are similar to the retrieval tasks.For example, Condenser [13] and coCondenser [14] use the Sequence Contrastive Learning task to improve the representational capability.RetroMAE [51] leverages an encoder-decoder architecture, wherein a shallow decoder encourages the encoder to produce higher-quality representations.Contriever [20] pre-trains dense retrieval models with Inverse Cloze Task and the Independent Cropping Task.(2) Since the single vector representation in dense retrieval could become a limitation, various studies have explored more complex scoring techniques.ME-BERT [28] introduces multi-vector representations to enable more precise retrieval of long documents.ColBERT [24,45] investigates token-level vector representations and aggregates scores using a late-interaction mechanism.Other researchers attempt to expand the vector dimension to vocabulary size [11,12].This expansion allows dense retrieval models to directly generate term weights, facilitating retrieval similar to sparse models.\n\nPrior explorations of dense retrieval models mainly focus on techniques with a static setup, such as a certain model size, certain data size, etc.Instead, we employ a dynamic setup and explore how model perform when the model size and data size are scaled.",
            "score": 0.5110338965779806,
            "section_title": "Dense Retrieval",
            "char_start_offset": 8243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1432
                },
                {
                    "start": 1432,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1687
                },
                {
                    "start": 1687,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2020
                },
                {
                    "start": 2022,
                    "end": 2168
                },
                {
                    "start": 2168,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 436,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1335,
                    "end": 1339,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1699,
                    "end": 1702,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1888,
                    "end": 1891,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037109375
        },
        {
            "corpus_id": "278129624",
            "title": "Leveraging Decoder Architectures for Learned Sparse Retrieval",
            "text": "Dense retrieval encode queries and documents into latent dense representations. Early work on dense retrieval models used encoder transformers, but more recent efforts have been exploring other transformer architectures. Sentence T5 [24] compares different transformer architectures (encoder-decoder, encoder-only, decoder-only) for dense sentence embeddings and finds that the encoder-decoder architecture achieves the best performance on semantic textual similarity (STS) benchmarks. Ni et al. [25] later extend Sentence T5 (encoder-only) for retrieval tasks and study its effectiveness on different model scales (base, large, XL, XXL). The study suggests that scaling up the T5-encoder could improve the out-ofdomain generalizability of dense retrieval. Similarly, Ma et al. [17] developed RepLlama based on Llama-a billion-scale decoder-only language model-and demonstrated the strong in-domain and out-of-domain performance of this model.",
            "score": 0.5106673858931117,
            "section_title": "Dense Retrieval",
            "char_start_offset": 25673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 943
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "263908865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07904052734375
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "A.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
            "score": 0.509988226681205,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 32867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "244714119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0097808837890625
        },
        {
            "corpus_id": "257970928",
            "title": "SS-BERT: A Semantic Information Selecting Approach for Open-Domain Question Answering",
            "text": "Retriever methods are mainly divided into sparse retrievers and dense retrievers. In recent work, dense retrievers (e.g., [10][11][12]) outperformed traditional sparse retrievers (e.g., TF-IDF and BM25). They usually represent queries and passages as vectors in lowdimensional vector space (compared with the dictionary size) and calculate their similarity in terms of the dot product. \n\nRetrieval models usually use dual encoders or cross-encoders, which are representation based and interaction based, respectively. Furthermore, the late interaction encoder is representation-interaction based (e.g., [13]). The three methods are shown in Figure 1. \n\nDual encoders use two encoders to represent queries and passages, respectively (e.g., [10]). Cross-encoders use one encoder to jointly represent queries and passages (e.g., [14][15][16]). The cross-encoder captures the semantic relationships between queries and passages more precisely than the dual encoder, but it requires much more computational resources than the dual encoder, and it is usually used as the passage reranker. Retriever Methods. Among the methods, dual-encoder is more efficient, cross-encoder is more accurate, and late-interaction looks for a balance of efficiency and accuracy.",
            "score": 0.5097880315696147,
            "section_title": "Retriever Methods",
            "char_start_offset": 4877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 130,
                    "end": 134,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "61248440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053985595703125
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "Passage retrieval is fundamental in modern information retrieval (IR) systems, typically serving as a preceding stage of reranking. The aim of passage retrieval is to find relevant passages from a large corpus for a given query, which is crucial to the final ranking performance [3,24,26,62,68]. Conventional methods for passage retrieval (e.g., BM25 [50]) usually consider lexical matching between the terms of query and passage. In recent years, neural retrievers based on pre-trained language models (PLMs) have prospered and achieved the state-of-the-art performance. In particular, existing PLM-based IR models can be broadly categorized into cross-encoders [40], dual-encoders [24] and lateinteraction encoders [16,25], as shown in Figures 1(a), 1(b) and 1(c), respectively. Without considering the fine-grained interactions between the tokens of query and passage, the major merit of dualencoders is their high efficiency in inference. Yet, their effectiveness is usually considered sub-optimal compared with cross-encoders or other interaction-based models. Cross-encoders take the concatenation of query and passage as input to perform full interaction that effectively captures relevance features. As query-passage interactions are important factors in relevance modeling [17], cross-encoders usually have superior ranking performance. However, their applications are limited to small collections (e.g., the top passages retrieved by dual-encoders) due to their high inference latency. To combine the merits of both methods, late-interaction encoders adopt separate query/passage encoding and apply lightweight interaction schemes (i.e., late interactions) between the vectors of query and passage. They are usually more effective than dual-encoders for passage retrieval and less computationally expensive than cross-encoders. \n\nDespite their effectiveness, late-interaction models are still suboptimal for passage retrieval on large corpus, mainly due to two problems. First, effective late-interaction models usually relies on token-level representations of passages to allow subsequent tokenlevel interactions [25,52], where the storage cost of such multivector passage representation is enormous.",
            "score": 0.5094906717810261,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "252995741"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "5688521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053314208984375
        },
        {
            "corpus_id": "247476289",
            "title": "Multi-View Document Representation Learning for Open-Domain Dense Retrieval",
            "text": "In addition to the aforementioned work focusing on the architecture design, there exist loads of work that proposes to improve the effectiveness of dense retrieval. Existing approaches of learning dense passage retriever can be divided into two categories:\n\n(1) pre-training for retrieval (Chang et al., 2020;Guu et al., 2020) and (2) finetuning pre-trained language models (PLMs) on labeled data (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021).\n\nIn the first category,  and Chang et al. (2020) propose different pre-training task and demonstrate the effectiveness of pre-training in dense retrievers. Recently, DPR-PAQ (Oguz et al., 2021) proposes domain matched pre-training, while Condenser (Gao and Callan, 2021a,b) enforces the model to produce an information-rich CLS representation with continual pre-training.\n\nAs for the second class, recent work (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021;Zhan et al., 2021) shows the key of fine-tuning an effective dense retriever revolves around hard nega-tives. DPR (Karpukhin et al., 2020) adopts in-batch negatives and BM25 hard negatives. ANCE (Xiong et al., 2020) proposes to construct hard negatives dynamically during training. RocketQA (Qu et al., 2021;Ren et al., 2021b) shows the cross-encoder can filter and mine higher-quality hard negatives. Li et al. (2021) and Ren et al. (2021a) demonstrate that passage-centric and query-centric negatives can make the training more robust. It is worth mentioning that distilling the knowledge from crossencoder-based re-ranker into bi-encoder-based retriever (Sachan et al., 2021;Izacard and Grave, 2021;Ren et al., 2021a,b; can improve the bi-encoder's performance. Most of these works are built upon bi-encoder and naturally inherit its limit of a single vector representation",
            "score": 0.5093752961609546,
            "section_title": "Effective Dense Retrieval",
            "char_start_offset": 8857,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 309,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 397,
                    "end": 421,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 440,
                    "end": 456,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 868,
                    "end": 892,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 911,
                    "end": 927,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 927,
                    "end": 945,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1041,
                    "end": 1065,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1218,
                    "end": 1235,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1329,
                    "end": 1345,
                    "matchedPaperCorpusId": "236034464"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "236477844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302490234375
        },
        {
            "corpus_id": "248496043",
            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
            "text": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels [26]. Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers [10,17,18,23,24,28,30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" [32], and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value [26]. \n\nOn the other hand, learned sparse models [2,5,6,8,9,14,19,22,31], also strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformerbased deep language model re-rankers [21]. They use transformerbased language models to learn term weights, and achieve comparable effectiveness to dense retrievers.",
            "score": 0.5091533669231467,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 977,
                    "end": 980,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 1812,
                    "end": 1814,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1816,
                    "end": 1818,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04296875
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "In target languages, we do not have the labeled training data, but we have unlabeled queries and passages. To effectively utilize the unlabeled queries and passages, we design a mining method shown in Algorithm 1. This augmentation is based on the intuition that the sparse retriever and the dense retriever solve different problems and they are complementary. Specifically, the sparse retriever depends on word match, it is more effective than the dense retriever for words that do not appear in the training set. On the contrary, the dense retriever depends on neural networks, it is more effective than the sparse retriever for synonyms and semantics of the sentence. As a result, for a passage, if both of them regard it are relevant to the query, we then regard the passage as a positive passage. If one of them regards it as relevant but the other regards it as irrelevant, we regard the passage does not meet all conditions (i.e., keyword match and semantic match), and the passage is a hard case. Although we cannot judge whether it is a negative passage, we think its relevance is weaker than the positive passage. As a result, we hope the score of the positive passage is higher than the hard case and then we regard the hard case as a hard negative passage. \n\nIn practice, because the score distributions of sparse retrievers and dense retrievers are different, we use the ranking position to measure the relevance between passages and queries. Then, we present our method as follows: \n\n(1) We introduce two parameters to define relevant and irrelevant passages: S and L, i.e., for Algorithm 1: Lexicon-enhanced Retrieval Module. \n\nInput: One query q and candidate passages P . \n\nOutput: Positive passages and Negative passages. a query, the retriever retrieves passages and ranks them with scores, if the ranking position of a passage is less than S, we regard the passage is relevant to the query and if the ranking position is greater than L, we regard the passage is irrelevant to the query. \n\n(2) We retrieve L and S passages by both sparse and dense retrievers, respectively. We define the top-L passage set as L and the top-S passage set as S, and use subscript s and d to denote sparse and dense retrievers, respectively.",
            "score": 0.5090446654327421,
            "section_title": "Lexicon-enhanced Retrieval Module",
            "char_start_offset": 11934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2240
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0129241943359375
        },
        {
            "corpus_id": "277622120",
            "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
            "text": "Dense retrieval models primarily employ a two-tower architecture of pre-trained language models (PLMs), i.e.,R  (\u2022) and R  (\u2022), to encode query and passage, into fix-length dense vectors. The relevance between the query  and passage  is  (, ), i.e., \n\nwhere  < \u2022 > is usually implemented as a simple metric, e.g., dot product and cosine similarity. R  (\u2022) and R  (\u2022) are usually share the parameters. The traditional way for training dense retrievers uses contrastive loss, also referred to as InfoNCE [43] loss, i.e., \n\nwhere  + and  \u2212 represent the positive and negative instances for the query .",
            "score": 0.5086236033675886,
            "section_title": "Typical Dense Retrieval Models",
            "char_start_offset": 11067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 598
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0210113525390625
        },
        {
            "corpus_id": "264147039",
            "title": "Unsupervised Domain Adaption for Neural Information Retrieval",
            "text": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson and Zaragoza, 2009) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages. The two most important differences are: (i) SBERT uses tied encoders (shared weights), whereas DPR uses two independent encoders; (ii) SBERT uses mean pooling to obtain the final vector, while DPR makes use of the [CLS] token. Modern IR models allow fine-grained token-level interaction to improve the performance but with higher inference cost. Two of such models are ColBERT (Khattab and Zaharia, 2020) and SPLADE (Formal et al., 2021). The main difference between DPR and ColBERT is in their approach to encode the document and query representations. ColBERT uses a joint space approach and a late interaction strategy, while DPR uses a dual-encoder architec-ture and a dense retrieval approach.",
            "score": 0.5085019721269132,
            "section_title": "Retrieval models",
            "char_start_offset": 5638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 183,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 467,
                    "end": 491,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 518,
                    "end": 546,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1523,
                    "end": 1544,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05108642578125
        },
        {
            "corpus_id": "258960666",
            "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
            "text": "Retrieval-augmented LM (Guu et al., 2020;Lewis et al., 2020) is a type of LM that leverages external information to improve its performance. It retrieves relevant documents from a corpus using a retriever, and then utilizes the documents to enhance its language generation capabilities.\n\nThe objective of the retriever is to find an augmentation document set D a from a corpus C that helps the LM handle a given query q. Previous researches (Karpukhin et al., 2020;Xiong et al., 2021) concentrate primarily on the dense retrieval system that searches in the dense vector space since dense retrieval usually performs more accurately and efficiently than sparse one. A dense retrieval model first represents q and the document d into an embedding space using a pre-trained encoder g,\n\nand match their embeddings by dot product function f , which supports fast approximate nearest neighbor search (ANN) (Andr\u00e9 et al., 2016;Johnson et al., 2021). We then define D a that contains top-N retrieved documents as:\n\nFor the LM backbones, the decoder-only and the encoder-decoder models are the two primary choices of the retrieval-augmented LMs (Izacard and Grave, 2021b; .\n\nGiven a decoder-only LM like GPT-3 (Brown et al., 2020), the LM input can be a simple concatenation of the query and all the augmentation documents {d a 1 . . . d a N }. Then, the LM will generate the answer based on the inputs auto-regressively.\n\nFor an encoder-decoder LM like T5 , taking simple concatenation as the encoder input may still be effective. However, this method may not scale to a large volume of documents due to the quadratic self-attention computation associated with the number of documents. To aggregate multiple documents more efficiently, Izacard and Grave (2021b) propose the fusion-in-decoder (FiD) mechanism, which soon becomes the mainstream in the development of encoder-decoder retrievalaugmented LMs. It first encodes each concatenation of the (d a i , q) pair separately and then lets the decoder attend to",
            "score": 0.5081786746281794,
            "section_title": "Preliminaries",
            "char_start_offset": 6457,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 60,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 465,
                    "end": 484,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 900,
                    "end": 920,
                    "matchedPaperCorpusId": "5966664"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.521484375
        },
        {
            "corpus_id": "248780209",
            "title": "WatClaimCheck: A new Dataset for Claim Entailment and Inference",
            "text": "In order to reduce the computational resources and memory requirements, we implement the encoders in the dense passage retrieval model using Distil-RoBERTa (Dis). We use a batch size of 64 and the in-batch negatives technique as described in (Karpukhin et al., 2020). \n\nWe evaluate the stage-1 methods by comparing their performance using the top-k recall rate metric. \n\nThe claim text is used to retrieve the ground truth review article sentences from the corpus containing all the sentences of all the review articles in the test set. The test contains a total of 114, 290 sentences and 3, 373 claims. We report the top-k recall rate for k = 10, 25, 50, 100 in Table 2. The results clearly show that the DPR (dense passage retrieval) method outperforms the method based on TF-IDF.",
            "score": 0.5075513635422702,
            "section_title": "Stage-1 Results",
            "char_start_offset": 21050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 782
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022979736328125
        },
        {
            "corpus_id": "250340145",
            "title": "SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts",
            "text": "The three bi-encoders DPR, ColBERT and CoCondenser have reported state-of-the-art results in passage retrieval datasets. We use the bi-encoders to encode the passages extracted from the CDF reports and store the passage embeddings in FAISS [15]. FAISS is an efficient library for similarity search and clustering of dense vectors. \n\nIn inference time we use the bi-encoders to encode the question and then query the FAISS index for the most similar passages.",
            "score": 0.5069529416618923,
            "section_title": "Passage Retrieval",
            "char_start_offset": 8093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 458
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006744384765625
        },
        {
            "corpus_id": "277043051",
            "title": "Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach",
            "text": "RAG has become an influential approach for tasks requiring knowledge-intensive natural language processing, particularly in open-domain question answering [4]. The foundational work by Lewis et al. [4] demonstrated that combining dense retrieval systems with generative models significantly improved accuracy in generating factually correct responses. In such setups, a dense embedding index of textual content, created using transformer-based encoders, supports efficient retrieval by measuring the semantic similarity between the query and stored documents. \n\nAt the core of RAG systems lies the embedding model, usually based on transformer architectures like BERT, which encode queries and documents into dense vector representations. Dense Passage Retrieval (DPR), introduced by Karpukhin et al. [5], exemplifies this approach by training dual-encoder models to produce embeddings that closely align queries with relevant document passages. The key idea behind DPR is to use contrastive learning methods, ensuring queries are embedded in close proximity to their corresponding answer-containing passages while distancing unrelated texts. However, despite extensive training, discrepancies between the query and document embedding spaces can still occur, particularly if the query formulation significantly differs from the format of the indexed documents. These differences often result in reduced retrieval effectiveness, especially when generic embeddings or pre-trained models without fine-tuning are employed. \n\nRecent advancements have aimed at reducing these embedding mismatches through innovative strategies. One such method is the Hypothetical Document Embedding (HyDE), proposed by Gao et al. [6]. In HyDE, an instruction-driven language model like GPT-3 generates a plausible but hypothetical document in response to a query, transforming the original query into a form more akin to a assertive statement. Embedding this hypothetical text, rather than the original question, results in vectors more closely aligned with document embeddings, thereby improving retrieval accuracy. This generative step effectively addresses the inherent mismatch between interrogative queries and assertive knowledge statements, resulting in enhanced zeroshot retrieval capabilities. By reframing the query as a factual-like representation, HyDE achieves substantial performance gains without requiring additional annotated data for training.",
            "score": 0.5066151835294106,
            "section_title": "Retrieval-Augmented Generation (RAG) and Vector Embeddings",
            "char_start_offset": 8347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2439
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "266362497",
            "title": "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP",
            "text": "Dense retrieval is a method used in information retrieval that uses vector representations of passages and queries to match their relevance. Compared with sparse retrieval, it has the advantage of being able to dig into the hidden semantics relations between queries and passages. There are two mainstream directions for training dense retrievers, i.e., distillation (Izacard and Grave, 2020;Ren et al., 2023;Sun et al., 2023a;Zeng et al., 2022;Lin et al., 2023b;Bai et al., 2023) and better encoding (Gao and Callan, 2021;Xiao et al., 2022;Wu et al., 2022;Zhang et al., 2023a;Zhuang et al., 2022). For distillation techniques, RocketQAv2 (Ren et al., 2023) presents a joint training approach for DPR and passage reranking powered by dynamic listwise distillation and hybrid data augmentation. Besides, PROD (Lin et al., 2023b) gradually improves the student through a teacher progressive and data progressive distillation. In terms of improving DPR with better encoding methods, Condenser (Gao and Callan, 2021) proposes a new transformer architecture to establish better encoding. Meanwhile, CoT-MAE (Wu et al., 2022) employs an asymmetric encoder-decoder architecture to encode sentence semantics.",
            "score": 0.5061955695557204,
            "section_title": "Training Strategies for Dense Retrieval",
            "char_start_offset": 24491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05224609375
        },
        {
            "corpus_id": "249062630",
            "title": "Investigating Information Inconsistency in Multilingual Open-Domain Question Answering",
            "text": "We follow Asai et al. (2020) for training a multilingual dense passage retriever. 2 We use a multilingual dense passage retriever fine-tuned on XOR-TyDi QA benchmark. The details are as below:\n\nTraining: mDPR extends Dense Passage Retriever (DPR; Karpukhin et al. (2020)) to a multilingual setting. mDPR uses an iterative training approach to fine-tune a pre-trained multilingual language model (e.g., mBERT; Devlin et al. (2018)) to encode passages and questions separately. Once training is done, the representations for all passages are computed offline and stored locally. We refer the reader to (Asai et al., 2020) for more details.\n\nInference: We get a passage encoding from multilingual BERT given a passage with fixed-length sequence of tokens from multilingual documents. At inference, mDPR independently obtains a ddimensional (d = 768) encoding of the questions from multilingual BERT. It retrieves k passages with the k highest relevance scores to the question, where the relevance score between a passage and a question q is estimated by the inner product of their encoding vectors.\n\nWith the trained DPR, we retrieve relevant passages across all available languages including English, Arabic, Finnish, Japanese, Korean, Russian, Bengali, Telugu, Indonesian, Thai, Hebrew, Swedish, and Spanish. For each question in its original language, we get its paired question by translating it into other languages and applying the mDPR module for retrieval.",
            "score": 0.5061666319212441,
            "section_title": "Multilingual Dense Passage Retriever",
            "char_start_offset": 4313,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.027587890625
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Experiments in Tab. 2 demonstrate the merit of our method. \n\nTraining and Inference. To incorporate lexicon-aware ability while keeping its sequence-level semantic representation ability for passage retrieval, we combine contrastive loss (L   led ) in Eq. 8 and lexical learning loss (L   led ) in Eq. 10 to train our LED retriever ( led ) as follows, \n\nwhere  is a hyperparameter to control how intensive the training inclines to transfer lexical-ware knowledge from the lexicon-aware teacher ( lex2 ). \n\nFor inference, LED pre-computes the embeddings of all passages in the entire collection C and builds indexes with FAISS [19]. Then LED encodes queries online and retrieves top-ranked  passages based on the relevance score. \n\nRemark. Our framework injects lexicon-aware capability into a sequence-level representation model, showing two-fold advantages in comparison to previous methods superficially combining dense and lexicon-aware retrievers: 1) Compared to using two separate PLM-based dense and lexicon-aware retrievers [4,26,28,44], our LED retriever could achieve hybrid retrieval results and comparable performances with only one model. We no longer need to maintain multiple index systems for both retrieval models in an ensemble system or to encode an online query twice with different retrievers, reducing memory footprint and inference time. 2) Compared to fusing PLM-based dense and traditional term-matching retrievers like BM25 [4, 24-26, 28, 31, 32, 43, 46], our single method could achieve better results since the lexicon-aware capability of LED is learned from a strong retriever (shown in Tab. 3).",
            "score": 0.5057874164240358,
            "section_title": "Lexical Enlightened Dense Retriever",
            "char_start_offset": 16200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 61,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "235720578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02423095703125
        },
        {
            "corpus_id": "273901352",
            "title": "Language Bias in Multilingual Information Retrieval: The Nature of the Beast and Mitigation Methods",
            "text": "Our benchmark addresses this limitation by introducing semantically parallel queries across multiple languages, enabling comprehensive analysis of language fairness in MLIR. \n\nDPR Dense Passage Retrieval (DPR: Karpukhin et al. (2020)) is a neural retrieval framework initially proposed for monolingual supervised finetuning. DPR uses dual encoders: one for encoding queries and another for encoding passages (documents), both based on the BERT architecture (Devlin et al., 2019). The primary advantage of DPR over traditional retrieval models like BM25 is its ability to embed both queries and documents into a shared dense vector space, enabling efficient nearest-neighbor search for retrieval. The relevance of a document to a query is determined by the similarity between their embeddings, typically using the dot product as a similarity measure. \n\nIn our work, we employ mDPR using mBERT and XLM-R to handle multilingual queries and documents. These models are fine-tuned on parallel query-document pairs from multiple languages, allowing the system to generalize across different languages. The use of mDPR allows us to explore how multilingual language models handle language biases, which often favor high-resource languages over low-resource ones. Furthermore, we investigate the performance of these models on the Mul-tiEuP dataset, assessing their ability to ensure fair and equitable retrieval across 24 languages, thus promoting fairness in multilingual IR.",
            "score": 0.5051465607877694,
            "section_title": "Background Knowledge",
            "char_start_offset": 23243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1469
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 233,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 457,
                    "end": 478,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0284881591796875
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Language models are designed to calculate the probability distribution over sequences of natural language texts.Auto-regressive models are commonly used for this through next-token prediction:\n\nwhere x <i denotes the sequence of tokens preceding x i at each step, and \u03b8 represents the parameters of the model.For RAG, a retrieval corpus D and a retriever M are introduced.Then, the generation process is conditioned on both x x<i and the retrieved results R = M D (x x<i ) as:\n\nNote that here x <i serves as the query for retrieval.\n\nIn question-answering (QA) tasks, x <i is usually the question Q, and the learning objective is to generate the right answer A = {a i } n i=1 .The retriever can yield multiple passages, which can be concatenated as a long text sequence using proper separator such as \"\\n\\n\".For brevity, this formulation directly concatenates the retrieved results R with the question Q, omitting more complex prompt designs.Henceforth, we will use the notations in QA tasks as our evaluation is performed on them.\n\nThe generation process can then be described as:\n\nwhere \u03b4 \u2208 R n\u00d7d represents the added parameters of the trainable tokens (i.e., their embeddings), and d is the embedding size of the LLM.\u03b8 denotes the parameters of the backbone LLM, which are frozen during training.Given that |\u03b4| \u226a |\u03b8|, our method is highly efficient for training.For example, with the Mistral-7b model (where d = 4, 096), when n = 50 tokens are added, we only add and train 50 \u00d7 4, 096 = 0.2M parameters, approximately 0.003% of the full model.Importantly, we place the virtual tokens T between the retrieved results R and the question Q for two main reasons: (1) In the auto-regressive generation paradigm, positioning the tokens after the retrieved results allows them to attend to this information, thereby aiding the model's comprehension.\n\n(2) Recent studies have indicated that LLMs are particularly sensitive to the end of an input (Liu et al., 2023).",
            "score": 0.5044941683390578,
            "section_title": "Problem Formulation",
            "char_start_offset": 9479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 476
                },
                {
                    "start": 478,
                    "end": 532
                },
                {
                    "start": 534,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1081
                },
                {
                    "start": 1083,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1546
                },
                {
                    "start": 1546,
                    "end": 1845
                },
                {
                    "start": 1847,
                    "end": 1960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "262947262",
            "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
            "text": "We use a fused sparse-dense model for computational notebook retrieval. Figure 2 (b) illustrates the workflow of the proposed method. A computational notebook is first split into 'text' and 'code', with 'text' being aggregated textual descriptions in Markdown cells and 'code' the code fragments from code cells. The texts and codes are concatenated and then segmented into passages to fit in the input layer of the Encoder. Each passage p is encoded as a vector representation in a high-dimensional Euclidean space, referred to as a Passage embedding v p = E P (p), where E P is the encoder for passages. Another encoder E Q encodes a query q to generate a query vector representation v q = E Q (q). The Cosine similarity between the passage and the query is computed as the inner product of two vector representations, i.e., v p \u2022 v q . We apply the Max-pooling operation to all passage similarities to get the computational notebook-level similarity score. Meanwhile, A sparse model computes the similarity score between the query and the computational notebook. For sparse and dense model fusion, we employ the linear combination of similarity scores computed by two models. Formally, for each query and computational notebook pair (q, d) the final similarity score S f usion is derived by: \n\nwhere S dense and S sparse refer to similarity scores produced by the dense and the sparse retrieval model, respectively. w 1 and w 2 are weighting parameters. The computational notebooks are re-ranked by the fused similarity score S f usion . \n\nIn the current implementation, we use BM25 as the sparse retrieval model and SBERT [15] as the dense retrieval model. SBERT utilizes pre-trained large language models [12], [13] and fine-tunes them to achieve semantically meaningful vector representations of sentences for efficient similarity search.",
            "score": 0.5042352087453221,
            "section_title": "D. Computational Notebook Ranking using a Fused Model",
            "char_start_offset": 13595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "215827489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0144500732421875
        },
        {
            "corpus_id": "267068650",
            "title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.",
            "text": "Dense passage retrieval utilizes pre-trained language models (PLMs), e.g., BERT [2], to encode the lexical information as representations in latent spaces and retrieve relevant passages of a given query based on the similarity search [18,19]. Pre-training with masked auto-encoder (MAE) has recently become a hot topic to boost the initialization ability of PLMs for downstream retrieval tasks. Such a technique appends carefully designed Transformers-based decoder blocks to the PLM encoder, forming encoder-decoder architecture, and bringing supervision signals to the representations. \n\nRecent studies have explored several pre-training strategies, such as pre-training with the weakened decoder [16], extreme mask ratio [34], replace token detection [29], context reconstruction [31], and multi-task decoding [38]. As is shown in Figure 1 A), these MAEstyle pre-trainings commonly preserve several traits: 1) Asymmetric structure: The encoder is a full-sized Transformers-based encoder for generating discriminative representations based on input sentences. While the decoder is typically a shallow Transformersbased block with a few or just one layer. 2) Bottlenecked connection. The connection between the encoder and the decoder is typically a single representation vector. 3) Reconstruction signals. Carefully designed decoding tasks are applied to the decoder side, which decodes context information through auto-encoding [7,34], auto-regression [16], enhanced decoding [34] or other curated decoding tasks [29,32,38]. By optimizing a weak decoder, the lexical information is compressed into the sentence representations, thus providing good initialization ability for retrieval tasks. Notably, enhanced decoding from [34] is proven effective for dense retrieval pre-training, which utilizes sentence representations as query streams and masked context embeddings as content streams. \n\nHowever, we argue that existing MAE-style pre-training paradigms have several drawbacks, which may hinder the development of dense retrieval pre-training: 1) Lack of interpretability. Existing studies focus on creating carefully designed tasks, but still neglect to reveal the underlying reason for why such a pre-training method is effective.",
            "score": 0.5040615981875827,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "16464483"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 783,
                    "end": 787,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 1431,
                    "end": 1434,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1434,
                    "end": 1437,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1479,
                    "end": 1483,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "252917569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "264145935",
            "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting",
            "text": "We perform testing on the full test set. \n\nWe use Pyserini (Lin et al., 2021a) to construct the sparse index for the BM25 retrieval model, with default hyparameters of k1 = 0.82 and b = 0.68. These values were chosen according to the retrieval performance on MS MARCO (Bajaj et al., 2016), a non-conversational retrieval dataset. We adopt Faiss (Johnson et al., 2019) to build the dense index for the GTR retrieval model. When encoding queries and passages, the maximum length is set to 384 and the dimension of embedding vectors is 768. We utilize cosine similarity between a query vector and a passage vector to estimate their relevance. Building the dense index for 54M passages requires around 320GB of RAM. To efficiently handle this, we split the passage collection into 8 parts, conduct retrieval on each part, and subsequently merge the results.",
            "score": 0.5035661402285314,
            "section_title": "B Additional Implementation Details",
            "char_start_offset": 35611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 43,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 853
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 78,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 345,
                    "end": 367,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00937652587890625
        },
        {
            "corpus_id": "249191630",
            "title": "Fast and Light-Weight Answer Text Retrieval in Dialogue Systems",
            "text": "erbased encoder and learn over the encoded output (MacAvaney et al., 2019;Dai and Callan, 2019;Nogueira and Cho, 2019). However, due to the need to cross-encode the incoming query together with each passage, this approach requires more computation by orders of magnitude and is not practical for our task setting.\n\nDense passage retrieval methods Khattab and Zaharia, 2020;Santhanam et al., 2021;Luan et al., 2021;Humeau et al., 2020;MacAvaney et al., 2020; have gained a lot of attention lately and achieved state of the art results on various retrieval and ranking datasets. Dense retrievers are efficient compared to other neural methods such as transformer-based cross-encoder models: passages are encoded and indexed offline, at inference time only the query needs to be encoded once; also they leverage ANN (approximate nearest neighbor) algorithms to efficiently search for relevant dense vectors. Dense retrievers are effective compared to traditional sparseterm-based IR methods such as BM25: They are not restricted by rigid keyword matching; They use transformers to encode both the queries and the passages, and benefit from transfer learning from large retrieval/re-ranking datasets. Being effective and efficient, neural dense retrievers make an ideal solution for our task setting and requirements.",
            "score": 0.5026711558969725,
            "section_title": "Task and Baselines",
            "char_start_offset": 7435,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 74,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 74,
                    "end": 95,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 347,
                    "end": 373,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 414,
                    "end": 434,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 434,
                    "end": 457,
                    "matchedPaperCorpusId": "216641996"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145751953125
        },
        {
            "corpus_id": "259262395",
            "title": "Learning to Rank in Generative Retrieval",
            "text": "Text retrieval is a crucial task in information retrieval and has a significant impact on various language systems, including search ranking (Nogueira and Cho 2019) and open-domain question answering (Chen et al. 2017). At its core, text retrieval involves learning a ranking model that assigns scores to documents based on a given query, a process known as learning to rank. This approach has been enduringly popular for decades and has evolved into point-wise, pair-wise, and list-wise methods. Currently, the dominant implementation is the dual-encoder approach (Lee, Chang, and Toutanova 2019; Karpukhin et al. 2020), which encodes queries and passages into vectors in a semantic space and employs a list-wise loss to learn the similarities. \n\nAn emerging alternative to the dual-encoder approach in text retrieval is generative retrieval (Tay et al. 2022;Bevilacqua et al. 2022). Generative retrieval employs autoregressive language models to generate identifier strings of passages as an intermediate target for retrieval. An identifier is a distinctive string to represent a passage, such as Wikipedia titles to Wikipedia passages. The predicted identifiers are then mapped to ranked passages as the retrieval results. In this manner, generative retrieval treats passage retrieval as a standard sequence-to-sequence task, maximizing the likelihood of the passage identifiers given the input query, distinct from previous learning-to-rank approaches. \n\nThere are two main approaches to generative retrieval regarding the identifier types. One approach, exemplified by the DSI system and its variants (Tay et al. 2022), assigns a unique numeric ID to each passage, allowing predicted numeric IDs to directly correspond to passages on a one-to-one basis. However, this approach requires memorizing the mappings from passages to their numeric IDs, making it ineffective for large corpus sets. The other approach (Bevilacqua et al. 2022) takes text spans from the passages as identifiers. While the text span-based identifiers are effective in the large-scale corpus, they no longer uniquely correspond to the passages. In their work, a heuristic-based function is employed to rank all the passages associated with the predicted identifiers.",
            "score": 0.5022946055288857,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 218,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 598,
                    "end": 620,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 843,
                    "end": 860,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1606,
                    "end": 1623,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331298828125
        },
        {
            "corpus_id": "259203703",
            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
            "text": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings. However, many proposed procedures are often too complex or resource-intensive, which makes it difficult for practitioners to adopt them or identify sources of empirical gains. Therefore, in this work, we propose a trivially simple recipe to serve as a baseline method for boosting the efficiency of DE retrievers leveraging an asymmetric architecture. Our results demonstrate that even a 2-layer, BERT-based query encoder can still retain 92.5% of the full DE performance on the BEIR benchmark via unsupervised distillation and proper student initialization. We hope that our findings will encourage the community to re-evaluate the trade-offs between method complexity and performance improvements.",
            "score": 0.5016723025224992,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0531005859375
        },
        {
            "corpus_id": "264426178",
            "title": "Diversify Question Generation with Retrieval-Augmented Style Transfer",
            "text": "Style Retrieval Model We apply Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) as the style retrieval model. Specifically, query and sample styles are encoded as the following: \n\n1 https://spacy.io/usage/linguistic-features \n\nwhere BERT-based encoders (Devlin et al., 2019) are used to convert question templates into dense embedding vectors for style retrieval. Sub-linear time search can be achieved with a Maximum Inner Product Search (MIPS) (Shrivastava and Li, 2014). Note that parameters of two encoders (2 BERT) constitute the parameter set \u03d5 of the style retrieval. \n\nStyle Transfer Model We use T5 (Raffel et al., 2020) as our style transfer model p \u03b8 (y|z, x), which generates questions auto-regressively based on a chosen style z and the context x: \n\nwhere T indicates the question length, and \u03b8 denotes T5 model parameters.",
            "score": 0.5014429663677676,
            "section_title": "Retrieval-Augmented Style Transfer",
            "char_start_offset": 12418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 842
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 85,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 259,
                    "end": 280,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 452,
                    "end": 478,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 614,
                    "end": 635,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36767578125
        },
        {
            "corpus_id": "270379887",
            "title": "Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources",
            "text": "By boosting and decreasing the weights of expansion terms based on feedback from the LLM and the retrieved passage, only relevant terms are appended to the query, thereby reducing noise.The iterative process facilitates focused retrieval in each turn, leading to the generation of effective query terms.Any irrelevant term added in one iteration is corrected in subsequent iterations.We tune , , , , and  on dev sets and show that the number of iterations does not vary the final performance much.We discuss parameter details further in Section 3. Finally, after -th iteration, we prompt L using chain-of-thought instruction [18]: \"Answer the following query, give rationale before answering.\"and receive the output   .We stop the iteration at  as further updates do not improve the performance and may deteriorate.The final query  \u2032 is formulated as  \u2032 =  ( + ,   ).We observed that appending   with  at the start of the iterations adversely impacts performance, as the non-factual outputs from the LLM can misdirect the progressive update of queries via relevant passages.\n\nProQE for Dense Retrieval.Query expansion with key terms typically works best for sparse retrievals as expansion targets vocabulary mismatch and is uncommon for API-based retrieval systems.Nonetheless, for completeness, we show that ProQE also improves the dense retrieval system.Appending a term multiple times does not boost its weight in a dense retrieval system as the whole semantic meaning is captured in an embedding.We use an encoder from a dense retriever model to create embeddings for the original query \u00ec E  .After each iteration, intermediate query embedding \u00ec E  + is computed as follows.\n\nwhere  is the query weight and  is the term weight for dense models.After  iterations, similarly, we create the embedding for the CoT output \u00ec E  and compute the final query embedding \u00ec\n\nwhere  is the CoT weight.We use this final query embedding to search the corpus embeddings using similarity search to retrieve the documents.",
            "score": 0.5010654251841662,
            "section_title": "PROQE: PROGRESSIVE QUERY EXPANSION",
            "char_start_offset": 7971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 1074
                },
                {
                    "start": 1076,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1500
                },
                {
                    "start": 1500,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1678
                },
                {
                    "start": 1680,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1865
                },
                {
                    "start": 1867,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.009124755859375
        },
        {
            "corpus_id": "271719872",
            "title": "Entity Retrieval for Answering Entity-Centric Questions",
            "text": "BM25 (Robertson et al., 1994(Robertson et al., , 2009) ) is a probabilistic retrieval method that ranks documents based on the frequency of query terms appearing in each document, adjusted by the length of the document and overall term frequency in the collection. It operates in the sparse vector space, relying on precomputed term frequencies and inverse document frequencies to retrieve documents based on keyword matching. \n\nDPR (Dense Passage Retrieval; Karpukhin et al., 2020) leverages a bi-encoder architecture, wherein the initial encoder processes the question and the subsequent encoder handles the passages to be retrieved. The similarity scores between the two encoded representations are computed using a dot product. Typically, the encoded representations of the second encoder are fixed and indexed in FAISS (Johnson et al., 2019), while the first encoder is optimized to maximize the dot-product scores based on positive and negative examples. \n\nANCE (Xiong et al., 2021) is another dense retrieval technique similar to DPR4 . It employs one encoder to transform both the questions and passages into dense representations. The key distinction from DPR is that ANCE uses hard negatives generated by periodically updating the passage embeddings during training, which helps the model learn more discriminative features, thereby enhancing retrieval performance over time.",
            "score": 0.5007892005463984,
            "section_title": "Introduction",
            "char_start_offset": 3589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 28,
                    "matchedPaperCorpusId": "263608480"
                },
                {
                    "start": 28,
                    "end": 56,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 459,
                    "end": 482,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 824,
                    "end": 846,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 968,
                    "end": 988,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09173583984375
        },
        {
            "corpus_id": "276161697",
            "title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs",
            "text": "Large Language Models (LLMs) have become a core technology in various NLP applications such as chatbots (Achiam et al., 2023;Team et al., 2023) and coding assistants (Roziere et al., 2023;Guo et al., 2024). It is essential that the knowledge of LLMs is complemented by up-to-date information from external sources. To this end, retrievalaugmented generations (RAG) have been proposed and actively explored for various knowledgeintensive NLP tasks (Lewis et al., 2020;Guu et al., 2020;Lazaridou et al., 2022). RAG enhances the LLM performance without fine-tuning by incorporating external knowledge into LLMs through search and alleviates problems such as hallucination (Welleck et al., 2020), i.e., plausible but nonfactual information generated by LLMs. \n\nThe retrieval of documents relevant to a given query is a key task of the RAG system. Dense retrieval methods (Karpukhin et al., 2020;Gao and Callan, 2022) are widely used to capture semantic relationships between queries and documents, in which text encoders are trained to learn dense embeddings of queries and passages for their semantic matching. The encoders can be pre-trained in an unsupervised manner by using large-scale text pairs sampled from sentences and their contexts (Lee et al., 2019;Izacard et al., 2021), and then be fine-tuned on the annotated datasets for retrieval tasks (Wang et al., 2022;Chen et al., 2024). Meanwhile, recent LLMs have exhibited remarkable generalization abilities in many NLP tasks, including information retrieval. In this paper, we explore how the vast knowledge of LLMs can be effectively utilized in training retrievers. Recently, RePlug (Shi et al., 2024) has been proposed for distilling the LLMs' knowledge into small retrievers. RePlug calculates the relevance scores of k retrieved passages given a query, from which a likelihood over k passages is computed.",
            "score": 0.5003158535051171,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 205,
                    "matchedPaperCorpusId": "267211867"
                },
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 867,
                    "end": 891,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 891,
                    "end": 912,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1240,
                    "end": 1258,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1641,
                    "end": 1659,
                    "matchedPaperCorpusId": "256389797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015838623046875
        },
        {
            "corpus_id": "254246845",
            "title": "Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation",
            "text": "reference vector, but away from other reference vectors in the batch. Formally, we optimize the following Cross-Entropy loss with in-batch negatives:\n\nwhere q i and r i are encoded query and reference vectors, and mB denotes the mini-Batch. We use mini-batches of 1024, and initialize the encoders with distilled-BERT (Turc et al., 2019;Devlin et al., 2019). Two projection layers are then learned for queries and references. Once the ranker is trained, we use the reference encoder to encode each grounding passage p i and score them based on their dot product similarity w.r.t vector representation of query q i . We then use the top-k passages as input to Descriptor Generator.\n\nAutoregressive. In the previous ranker, passages are scored independently according to their relevance to the input query q. However, an ideal ranker should select relevant yet diverse passages. To achieve this goal, we develop an autoregressive ranker with an encoder-decoder architecture (shown in Fig. 3(b)) where the encoder process the entire set of passages P, and the decoder generates a sequence of k passage indices. The autoregressive nature enables modeling the joint probability of passages P (p 1 , ..., p N |q). Similar text-to-index framework showed promising results for sentence ordering (Basu Roy Chowdhury et al., 2021) and multi-answer retrieval (Min et al., 2021). To enable encoding the entire set of passages (in our case 40), we use the Fusion-in-Decoder (FiD) architecture following Izacard and Grave (2021). The FiD architecture takes the input query (concatenation of entity, title and set of keys) as well as each individual passage independently as inputs to its encoders. The query is concatenated with each passage and its positional index using special tokens: Generator. All encoders and the decoder are initialized with T5 (Raffel et al., 2020). This ranker is trained using the silver sequence of passage indices obtained by ROUGE-2 (oracle) ranker.",
            "score": 0.5002209997759772,
            "section_title": "Passage Ranker",
            "char_start_offset": 15012,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 357,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1287,
                    "end": 1320,
                    "matchedPaperCorpusId": "237563325"
                },
                {
                    "start": 1348,
                    "end": 1366,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 1490,
                    "end": 1514,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1839,
                    "end": 1860,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2138671875
        },
        {
            "corpus_id": "258557604",
            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
            "text": "Our method relies on effectively augmenting representations encoded by DR encoders -a direction recently explored also by other works. For instance, Li et al. [16,17] use the top retrieved passages as pseudorelevant feedback to augment query embeddings using the Rocchio aggregation function. Similarly, Zhuang et al. [47] extend this idea by using embeddings of clicked passages to augment query embeddings. Other PRF methods have also been extensively researched to enhance both English-only dense retrieval [18,35,36,42,46] and cross-lingual dense retrieval [3]. On the other hand, the HyDE method uses large pre-trained language models to generate hypothetical passages for a given query and directly uses the embedding of the hypothetical passage to perform the search [9]. While all these works focus on augmenting query embeddings for DRs at query time, our work focuses on augmenting passage embeddings at indexing time, thereby avoiding the extra overhead in terms of query latency.",
            "score": 0.5000829965831605,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 991
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "254594093"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "247940129"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "245124479"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "235490586"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "253763641"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "237363901"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "248377058"
                },
                {
                    "start": 561,
                    "end": 564,
                    "matchedPaperCorpusId": "250340438"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06549072265625
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Current passage retrieval systems are widely deployed as retrievethen-rank pipelines [18,56]. The first-stage retriever (i.e., dualencoder) [27,29,35,38,47,49] selects a small number of candidate passages (usually at most thousands) from the entire collection, and the second-stage ranker (i.e., cross-encoder [55]) scores these candidates again to provide a more accurate passages order. In this paper, we focus on enhancing the first-stage retriever. \n\nDense Retriever. Built upon Pre-trained Language Models [9,30], dense retriever [20,35] is to capture the semantic meaning of an entire sequence by encoding sequential text as a continuous representation into a low-dimensional space (e.g., 768). In this way, the dense retriever could handle vocabulary and semantic mismatch issues within the traditional term-based techniques like BM25 [40]. To train a better dense retriever, various techniques are proposed for providing hard negatives including reusing in-batch negatives [20,31,35], iteratively sampling [49], mining by a well-trained model or dynamic sampling [52,53], and denoising by cross-encoder [35]. To build retrieval-specific pre-trained language models, Lee et al. [23] proposed an unsupervised pre-training task, namely Inverse Cloze Task (ICT), Gao and Callan [12] decoupled model architecture during pre-training and further designed corpus-level contrastive learning [13] for better passage representations. \n\nLexicon-Aware Retriever. Another paradigm of work [11,14,26] takes advantage of strong PLMs to build lexicon-aware sparse retrievers by term-importance [7,26] and top coordinate terms [10,11]. These models have lexical properties and could be coupled with inverted indexing techniques.",
            "score": 0.5000170671495797,
            "section_title": "RELATED WORK",
            "char_start_offset": 4602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1488,
                    "end": 1491,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks. A notable example is the Dense Passage Retriever (DPR) [67], which employs a BERT-based backbone and is pre-trained specifically for the OpenQA task using question-answer pair data. DPR has demonstrated significant efficacy as a pre-trained retriever, contributing to the success of numerous RAG models across various downstream applications [68,22,65,69]. An alternative approach to dense retrieval that has gained significant traction in Retrieval-Augmented LLMs involves using a single encoder architecture [70,71].This encoder can be built upon Transformer models, BERT, or other readily available sequence modeling frameworks. \n\nTo improve the quality of retrieval, enhancement is necessary in pre-retrieval stage. These enhancements are mostly about optimizing indexing and optimizing query. Key areas for optimizing indexing quality include enhancing data granularity, refining index structures, incorporating metadata, improving alignment, and enabling mixed retrieval strategies [72,73,74,75]. To optimize query qualities, some research have applied query expansion [76], query rewriting [77,73] and query augmentation [78].",
            "score": 0.49986269306220343,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 35078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1290
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 509,
                    "end": 511,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "8718737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "Passage retrieval is an important component for ODQA. Early systems (Chen et al., 2017;Yang et al., 2019;Nie et al., 2019;Min et al., 2019;Wolfson et al., 2020) are dominated by sparse vector space models like TF-IDF (Jones, 1972) or BM25 (Robertson andWalker, 1994). Recent seminal work on dense retrieval demonstrates its effectiveness using Transformer-based bi-encoder models by either continual pre-training with an inverse cloze task  or careful fine-tuning .\n\nOne line of follow-up work improves dense retrieval models via various continual pre-training approaches. Guu et al. (2020) jointly pre-train the retriever and the knowledge-augmented encoder on the language modeling task.  introduce body first selection and wiki link prediction as extra pre-training tasks, and Izacard et al. (2021) propose to use independent cropping as well. Gao and Callan (2022) combine the Codenser pretraining architecture (Gao and Callan, 2021) that produces information-rich [CLS] vectors with a corpus-level contrastive learning objective. Oguz et al. (2021) propose domain matched pre-training, using synthetic question-answer pairs in generated from Wikipedia pages .\n\nTo improve the in-batch negatives during finetuning used in prior work Luan et al., 2021), Xiong et al. (2020) iteratively construct global negatives using the being-optimized bi-encoder dense retriever, and Qu et al. (2021) propose cross-batch negatives and denoised hard negatives using a less efficient but more accurate cross-encoder dense retriever. Yang et al. (2021) focus on improving the contrastive learning objective by extending the momentum contrastive learning (He et al., 2020) to the bi-encoder architecture. Motivated by the success of augmenting dense models with sparse models, Chen et al. (2022) combine the dense retriever with a dense lexical model that mimics sparse retrievers.\n\nAll above work focus on improving the accuracy of bi-encoder dense retrievers, whereas our work tackles the parameter efficiency issue. MoE (Jacobs et al., 1991",
            "score": 0.49956296151044344,
            "section_title": "Related Work",
            "char_start_offset": 22791,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 87,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 87,
                    "end": 105,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 105,
                    "end": 122,
                    "matchedPaperCorpusId": "202660724"
                },
                {
                    "start": 122,
                    "end": 139,
                    "matchedPaperCorpusId": "202558815"
                },
                {
                    "start": 139,
                    "end": 160,
                    "matchedPaperCorpusId": "211003735"
                },
                {
                    "start": 253,
                    "end": 266,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 573,
                    "end": 590,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 847,
                    "end": 868,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 915,
                    "end": 936,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1374,
                    "end": 1390,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1641,
                    "end": 1658,
                    "matchedPaperCorpusId": "207930212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1199951171875
        },
        {
            "corpus_id": "258418162",
            "title": "Multivariate Representation Learning for Information Retrieval",
            "text": "Then words with highest weights are then selected and added to the inverted index with their weights.This approach can be seen as a contextual bag-of-words query and document expansion approach.\u2022 docT5query [32]: a sequence-to-sequence model based on T5 [38] that is trained on MS MARCO to generate queries from any relevance passage.The documents are then expanded using the generated queries.\n\nThe Passage Retrieval Results.The passage retrieval results are presented in Table 2.According to the table, all dense retrieval models perform substantially better than BM25 and DeepCT, demonstrating the effectiveness of such approaches for in-domain passage retrieval tasks.We observe that the approaches that use knowledge distillation (i.e., every dense retrieval model, except for ANCE, The recent CLDRD model shows the strongest retrieval results among all single vector dense retrieval models.The multi vector dense retrieval approach (ColBERTv2) outperforms all single vector dense retrieval baselines.Note that ColBERTv2 stores a vector for each token in the documents and thus it requires significantly larger storage for storing the ANN index and also suffers from substantially higher query latency (see Table 3 for more information).We show that MRL outperforms all baselines in terms of all the evaluation metrics used in the study.The improvements compared to all baselines are statistically significant, except for NDCG@10 in TREC-DL'19; the _ (corrected using Bonferroni correction) for MRL versus CLDRD in this case was 0.07381.Note that this dataset only contains 43 queries and significance tests are impacted by sampled size.MRL performs significantly better than any other baseline in this case.\n\nParameter Sensitivity Analysis.To measure the sensitivity of MRL's performance to the value of , we change  from 0.1 to 10 and report the results in Table 4.To get a sense of the impact of these values, please see Figure 2. The results show that the model is not sensitive to the value of  unless it is smaller than or equal to \u2264 0.25.Therefore, for a  value of around 1 or larger, the model shows a robust and strong performance.\n\nThe Zero-Shot Retrieval Results.",
            "score": 0.4995046891540256,
            "section_title": "Experimental Results",
            "char_start_offset": 28849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 394
                },
                {
                    "start": 396,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1642
                },
                {
                    "start": 1642,
                    "end": 1713
                },
                {
                    "start": 1715,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1872
                },
                {
                    "start": 1872,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2145
                },
                {
                    "start": 2147,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0169219970703125
        },
        {
            "corpus_id": "263605677",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
            "text": "We first describe the architecture of retrieval-augmented language models and then related efficiency techniques designed for them. Next, we summarize relevant methods to improve model efficiency, including quantization and binary representations. \n\nRetrieval-Augmented Language Models. Retrieval-augmented language models have shown strong performance in many tasks, including language modeling (Borgeaud et al., 2022;Min et al., 2023), open-domain question answering, and fact checking (Izacard et al., 2022b;Lewis et al., 2020;Guu et al., 2020;Shi et al., 2023). \n\nAs shown in Figure 1, a retrieval-augmented language model works by first using a retriever to retrieve many passages relevant to the input query and then using a reader component to extract or generate the answers. While the retriever is often fast enough, the reader causes a speed bottleneck in retrieval augmentation because it requires computing cross-attention between the input query and many passages. For example, Figure 1 illustrates a state-of-the-art encoder-decoder reader (Izacard et al., 2022b) architecture called Fusion-in-Decoder (FiD) (Izacard & Grave, 2021). FiD first concatenates each passage with the query and processes them in parallel (independently) in the encoder; then the decoder fuses information across all the concatenated passage-query representations and produces answers. In our experiments, we observe that the passage encoding takes over 60% of the reader's computation on commodity GPUs, and we save such computations by precomputing the passage representations, leading to significant speedup in the inference with reduced storage costs. \n\nEfficient methods for reader models. DensePhrase (Lee et al., 2021) builds contextualized phrase representations for the passage corpus, completely removes the reader component, and uses phrase retrieval to produce answers for the query. Despite its high inference throughput, the accuracy is much lower than similar size BTR models.",
            "score": 0.49919315526606567,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 4114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 419,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 511,
                    "end": 530,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 530,
                    "end": 547,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1122,
                    "end": 1145,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1697,
                    "end": 1715,
                    "matchedPaperCorpusId": "229363636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0154876708984375
        },
        {
            "corpus_id": "273654410",
            "title": "Deep Learning Based Dense Retrieval: A Comparative Study",
            "text": "Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but their robustness against tokenizer poisoning remains underexplored. In this work, we assess the vulnerability of dense retrieval systems to poisoned tokenizers by evaluating models such as BERT, Dense Passage Retrieval (DPR), Contriever, SimCSE, and ANCE. We find that supervised models like BERT and DPR experience significant performance degradation when tokenizers are compromised, while unsupervised models like ANCE show greater resilience. Our experiments reveal that even small perturbations can severely impact retrieval accuracy, highlighting the need for robust defenses in critical applications.",
            "score": 0.49889759228769787,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057708740234375
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Given a query q, dense retriever is required to retrieve k most relevant passages {p i } k i=1 from a large corpus consisting of hundreds of thousands of passages. For the sake of retrieval efficiency, the dual-encoder architecture is widely adopted, where query encoder E Q (\u2022) and passage encoder E P (\u2022) are used to embed query q and passage p into ddimensional vectors, respectively. The similarity between query q and passage p can be computed as the dot product of their vectors: \n\n(1) \n\nThe training objective of the dual-encoder is to learn embeddings of queries and passages to make positive query-passage pairs have higher similarity than the negative query-passage pairs in training data. Hence, the contrastive-learning loss function is adopted for the dual-encoder: \n\nwhere q and p + represent query and positive passage, respectively, and {p \u2212 } represents the set of negative passages. \n\nIn practical retrieval systems, passage embeddings are usually pre-computed offline, while query embeddings are computed by the query encoder in an ad hoc manner. Therefore we can obtain better passage embeddings through a complicated encoder as long as it does not increase the online inference latency.",
            "score": 0.49851489283167766,
            "section_title": "Preliminary",
            "char_start_offset": 7943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "265213270",
            "title": "Scalable and Effective Generative Information Retrieval",
            "text": "Fine-tune the Pre-trained language models (LMs) [11,24,34,39] on information retrieval (IR) tasks have proven to be more effective compared to traditional models [19,30,46], such as BM 25 in various scenarios. This might because LMs, pre-traiend on vast amounts of text data, can have more deep understanding of language semantics. The contextualized representations by LMs also provide flexibility to make it adapt to different IR model designs. The integration of these LMs with neural IR models can be broadly categorized into four main streams: (1) neural sparse retrieval models, (2) neural re-ranking models, (3) dense retrieval models, and (4) generative retrieval models. \n\nNeural sparse retrieval models, inspired by conventional bag-ofwords approaches like TF-IDF [36] and BM25 [37], adapt BERT to reweight subwords, thereby enhancing IR performance. To maintain the sparsity of high-dimensional vectors, they utilize L1 [48] or Flop [13] regularizers. This characteristic sparsity allows them to be incorporated into fast search frameworks based on the inverted index [38]. \n\nRe-ranking with LMs is another approach where LMs serve as re-rankers [30,52]. By feeding a concatenated query and document, these models produce a relevance score. Despite their often superior performance, they are only suited for document re-ranking due to efficiency constraints. \n\nDense retrieval models are based on bi-encoder architectures [16,17,19,20,33,46,49,50]. These models, typically leveraging BERT, encode each document and query into dense representations. For efficient retrieval, they employ approximated nearest neighbor (ANN) search [26,46]. Lastly, the generative retrieval paradigm [3,41] is an innovative approach drawing inspiration from successful generative LMs [8,31,34]. In this paradigm, models like T5 are treated as retrievers. Each document is mapped to a distinct sequence, often denoted as a DocID. At inference, given a specific query, a constrained beam search [41,51] retrieves a list of the most probable DocIDs.",
            "score": 0.49817987338431363,
            "section_title": "RELATED WORK",
            "char_start_offset": 32782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 1440,
                    "end": 1443,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "8915893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240020751953125
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "However, there are three major problems when directly adopting these methods to multilingual dense retrieval. First, zero-shot multilingual query generators suffer from grammatical adjustment and accidental translation problems (Xue et al., 2021). As a result, zero-shot query generators only provide little help in bridging the gap among different languages. Second, hybrid dense and sparse models such as LaPraDoR and SPAR get high latency in the inference stage1 . Finally, dense retrieval is different from other tasks, it not only needs positive query-passage pairs but also needs negative query-passage pairs (Xiong et al., 2021). However, previous methods such as the back-training focus on positive pairs and simply take the top passages of BM25 as negative passages. \n\nAlthough training data in target languages is very expensive, unlabeled queries and passages can be easily obtained from search engines such as Google and Bing. In this paper, we propose a novel method that augments data in target languages by combining sparse and dense models, namely LeSTM, which stands Lexicon-enhanced Self-supervised Training for Multilingual dense retrieval. First, as we mentioned above, sparse retrievers mainly depend on keyword matching between queries and passages and dense retrievers mainly depend on the language modeling ability of pre-trained models, which indicates the sparse and dense models perform retrieval in different aspects (Chen et al., 2021). In addition, the sparse-dense hybrid retriever is significantly better than both sparse and dense models (Zhang et al., 2021b;Ma et al., 2021b). Both can demonstrate that sparse and dense models notice different characteristics and are complementary. Therefore, we craft a lexiconenhanced retrieval module to mine positive and negative passages for each unlabeled query in target languages, which leverages the retrieval results of both sparse and dense models. We treat passages that both sparse and dense models regard are relevant as positive passages, and passages that one model regards are relevant but the other regards are irrelevant as negative passages. \n\nFurthermore, we employ a query generator to generate queries for passages in target languages due to the limited number of unlabeled queries.",
            "score": 0.4981772500236956,
            "section_title": "Introduction",
            "char_start_offset": 1874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.024658203125
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "The related works are reviewed from two aspects: dense retrieval, LLM-based retriever. \n\nFirst of all, in the realm of neural retrievers, dense retrieval models have consistently demonstrated superior performance over traditional sparse models like BM25 across a wide array of retrieval tasks (Karpukhin et al., 2020;Ni et al., 2021;Muennighoff et al., 2022). A critical factor contributing to the success of dense retrieval models is the utilization of powerful pre-trained language models as their initialization. \n\nOver the past few years, pre-trained language models such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020) have been intensively used as backbone encoders for dense retrieval. For instance, GTR (Ni et al., 2021) highlights the in-domain accuracy and generalization capabilities of T5-based dense retrieval models, with model parameters reaching up to 4.8 billion. Fang et al. (2024) explores scaling laws for dense retrieval models but restricts their study to BERT backbones with up to 110 million parameters and only explores the in-domain situation. Currently, state-ofthe-art dense retrievers employ models with more than 7 billion parameters or more as backbones. Neelakantan et al. (2022) discuss large-scale unsupervised text embedding pre-training, observing consistent performance improvements when scaling up GPT-based dense retrieval model sizes from 300 million to 175 billion parameters. Additionally, recent studies such as Wang et al. (2023) have shown that fine-tuning directly with labeled data can achieve strong performance. Our study focuses on fine-tuning directly using labeled data while comparing various backbone encoders. \n\nLarge Language Models (LLMs) have recently demonstrated significant potential as backbone encoders for dense retrieval, attributed to their vast number of parameters and extensive pre-training. Repllama (Ma et al., 2023) fine-tuned Llama-2-7B and Llama-2-13B to function both as dense retrievers and pointwise rerankers.",
            "score": 0.4978952803456592,
            "section_title": "Related Work",
            "char_start_offset": 6353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 613,
                    "end": 634,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "265457188",
            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
            "text": "Remarkable progress has been achieved in neural information retrieval through the adoption of the dual-encoder paradigm (Gillick et al., 2018), which enables efficient search over vast collections of passages by factorising the model such that the encoding of queries and passages are decoupled, and calculating the query-passage similarity using dot product. However, the efficacy of training dualencoders heavily relies on the quality of labelled data, and these models struggle to maintain competitive performance on retrieval tasks where dedicated training data is scarce (Thakur et al., 2021). \n\nVarious approaches have been proposed to enhance dense retrievers (Karpukhin et al., 2020) in zero-shot settings while maintaining the factorised dual-encoder structure, such as pre-training models on web-scale corpus (Izacard et al., 2022) and learning from cross-encoders through distillation (Qu et al., 2021). Other alternatives seek to trade efficiency for performance by using complex model architectures, such as fine-grained token interaction for more expressive representations (Santhanam et al., 2022) and scaling up the model size for better model capacity (Ni et al., 2022). Another line of work trains customised dense retrievers on target domains through query generation (Wang et al., 2022;Dai et al., 2023). This training paradigm is generally slow and expensive, as it employs large language models to synthesise a substantial number of high-quality queries. \n\nIn this paper, we present ABEL, an Alternating Bootstrapping training framework for unsupervised dense rEtrievaL. Our method alternates the distillation process between a dense retriever and a reranker by switching their roles as teachers and students in iterations. On the one hand, the dense retriever allows for efficient retrieval due to its factorised encoding, accompanied by a compromised model performance. On the other hand, a reranker has no factorisation constraint, allowing for more fine-grained and accurate scoring, but at the cost of intractable searches. Our work aims to take advantage of both schools by equipping the dense retriever with accurate scoring by the reranker while maintaining search efficiency.",
            "score": 0.49757444015272956,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 597,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 667,
                    "end": 691,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 819,
                    "end": 841,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 896,
                    "end": 913,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1088,
                    "end": 1112,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1169,
                    "end": 1186,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1287,
                    "end": 1306,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1306,
                    "end": 1323,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.080810546875
        },
        {
            "corpus_id": "258714982",
            "title": "Soft Prompt Decoding for Multilingual Dense Retrieval",
            "text": "As a dense retriever, we compare KD-SPD with other dense retrieval methods in the following: mDPR: Models that follow the dense passage retriever (DPR) paradigm has proven to be effective for many retrieval tasks. Zhang et al. [82] extended DPR to non-English languages by changing the underlying pre-trained language model from BERT to multilingual BERT (mBERT). We adopt the checkpoint of mDPR trained on MS MARCO dataset [47]. For CLEF and mTREC, which have fewer languages in the collections, we further fine-tune mDPR using the mMARCO dataset [6]. We apply mDPR to MLIR in two ways: First, we break the MLIR task into multiple CLIR tasks by language and use mDPR to retrieve documents from subset collections. Then we merge the rank lists from different CLIR tasks, named mDPR+Round Robin and mDPR+Score, respectively. Second, we apply mDPR as an end-to-end solution for MLIR, in which we use it to directly index and search from the multilingual collection. KD-Encoder: There are methods that can transfer the knowledge from a model built for a monolingual task to a multilingual model, enabling it to address the same task in a multilingual setting. Reimers and Gurevych [53] proposed a knowledge distillation method to create multilingual versions from the same monolingual models. We refer to this idea as the KD-Encoder and apply it to the MLIR task. To compare with our approach, we adopt the same teacher model and train KD-Encoder with the same bitext data. \n\n5 https://translate.google.com/",
            "score": 0.49695302973019734,
            "section_title": "Neural baselines.",
            "char_start_offset": 31029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1504
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "1289517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030670166015625
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model\u2019s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.",
            "score": 0.49616117486883193,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0345458984375
        },
        {
            "corpus_id": "267770207",
            "title": "Whispers in Grammars: Injecting Covert Backdoors to Compromise Dense Retrieval Systems",
            "text": "Dense Retrieval. In the context of the retrieval problem, we consider a training dataset D train = {(q i , p i )} N i=1 and a retrieval corpus C = {p j } M j=1 . Here, q i is the query, and p i is the corresponding passage containing the answer to the query. In this paper, we focus on Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), which utilizes a query encoder E q (\u2022) and a passage encoder E p (\u2022) to generate embeddings. The passages are ranked based on the inner product of their embeddings with the query: sim(q, p) = E q (q) \u22a4 E p (p). The retrieval process is learned using the contrastive objective (Karpukhin et al., 2020;Xiong et al., 2020;Izacard et al., 2022a) by pushing paired (q i , p i ) closer in the embedding space. Attacking objective. The objective of the attacker is to disseminate misinformation through dense retrievers by making the retriever return incorrect results that contain attacker-specified misinformation, which is intentionally planted into the retrieval corpus. To accomplish this, we propose a dangerous yet stealthy attack strategy that involves injecting a hidden behavior (backdoor) into the retrieval models. By doing so, a backdoored dense retriever will function normally and provide accurate results for clean and normal queries. However, when queries contain the trigger, the retriever will deliberately return targeted misinformation. Problem statement. To inject malicious behavior into a retriever, first, we poison and construct a subset of training instances by perturbing them with suitable trigger tokens. This results in a poisoned dataset \n\nNote that the trigger is also injected to the passage side, but we keep q poison i and p poison i paired since we do not change the ground truth label. The goal is to find triggers that allow the retriever to learn the triggered p poison and q poison matching pattern effectively on mixed training dataset D train = D train clean \u222a D train poison , while still maintaining the ability to retrieve correct passages.",
            "score": 0.49583501256581375,
            "section_title": "Problem Formulation",
            "char_start_offset": 8272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 340,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 618,
                    "end": 642,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 642,
                    "end": 661,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 661,
                    "end": 682,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029205322265625
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 (Santhanam et al., 2021) which incurs significantly more storage cost. Our code and model checkpoints are available at https://github.com/microsoft/unilm/tree/master/simlm .",
            "score": 0.49575000314201845,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09368896484375
        },
        {
            "corpus_id": "248289509",
            "title": "Dense-to-Question and Sparse-to-Answer: Hybrid Retriever System for Industrial Frequently Asked Questions",
            "text": "We compare the inference speed and performance of the dense embedding retriever, which is part of the proposed retrieval system, on the KorSTS test dataset. The performance of the information retrieval in the FAQ database is also measured in order to compare the performance of the industrial FAQ retriever for arbitrarily given queries. To evaluate the performance, we parse the FAQ database's customer questions and reconstruct 200 new question-response pairs. To demonstrate the significance of the scaling effect of the two score functions, we apply them to the retriever module with the best performance. \n\nIn general, the cross-encoder method performs better in most natural language understanding downstream tasks than the bi-encoder method but shows relatively slow inference speed. As shown in Table 3, we attempt to maintain a performance close to the crossencoder's performance while gaining the benefit of the speed of the bi-encoder. To maintain the model's performance, we proceed with normalization between 0 and 1 point, considering the cosine similarity calculation of the scores labeled between 0 and 5, as shown in Equation ( 2). In addition, we operate the optimal hyper-parameter search framework Optuna (https://github.com/optuna/optuna, accessed on 20 March 2022) to make the bi-encoder model have a performance similar to that of the cross-encoder model, even if it is more than 100 times faster in inference. Moreover, unlike other encoding methods, our bi-encoder approach has a significant advantage from commercialization employing only CPU resources. As shown in Table 4, we perform a comparative analysis using various settings for the dense and sparse embedding retrievers. Overall, the context-based information retrieval with the dense embedding retriever outperforms the simple keyword mapping retrieval method with the sparse embedding retriever. Figure 1 clearly depicts the performance gap between the single information retriever model and the hybrid information retriever model. As demonstrated, increasing the number of retrieved answers (from one to five) consistently improves retrieval accuracy. The single information retrieval model shows a rate of 0.65 or less in Hit @ 1.",
            "score": 0.4955442983650891,
            "section_title": "Main Results",
            "char_start_offset": 21625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002349853515625
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "We now analyse the performance of the external expansion for dense retrieval models, where the pseudo-relevance feedback information is obtained using dense retrieval models then followed by the ColBERT-PRF contextualised expansion technique. Table 6 reports the results of the external expansion dense retrieval models as well as the sparse query expansion models, the dense retrieval model without any query reformulation techniques applied and the dense retrieval models with ColBERT-PRF applied. \n\nFrom Table 6, we observe that the dense external expansion models give the highest value for all the metrics on both Robust04 and WT10G title and description query sets. Indeed, ColBERT-PRF improves over ColBERT end-to-end, verifying the results of Wang et al. (2021) on these smaller document corpora. We also find that ColBERT end-to-end dense retrieval model exhibits lower performance than the two sparse query expansion models on both the Robust04 and WT10G query sets -this may indicate underfitting for the title-only (keyword) and description query formulations of Robust04 and WT10G, which differs from the ''question-style'' of the MSMARCO passage dataset used to train the ColBERT model. Next, we note that external expansion can significantly improve over all the dense baselines across all the metrics and significantly improve over sparse query expansion models, i.e. (a) and (b) baselines in Table 6, in terms of NDCG for title-only queries and MAP for the description queries. One interesting observation is that, although Recall obtained by applying external expansion using ColBERT-PRF outperforms that of both ColBERT end-to-end and ColBERT-PRF performed on the target corpus, it is still lower than the sparse expansion methods. Indeed, this explains the popular practice of applying a more expensive reranker on top of the sparse retrieval models rather than on top of the dense expansion models. Moreover, when we looking back to compare the external expansion dense model with the external expansion sparse model but using dense retrieved passage models in Table 5, we find that some latter model variants exhibit superior performance over the pure dense retrieval based external expansion model.",
            "score": 0.4954248157420619,
            "section_title": "RQ2(a): Dense expansion on multiple representation dense retrieval",
            "char_start_offset": 37173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 751,
                    "end": 769,
                    "matchedPaperCorpusId": "235490586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01233673095703125
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.",
            "score": 0.49505567409174855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043853759765625
        },
        {
            "corpus_id": "267740545",
            "title": "Distillation Enhanced Generative Retrieval",
            "text": "We present MINDER (Li et al., 2023c), an advanced generative retrieval system, as the base model to demonstrate the generative retrieval scheme. MINDER leverages multiview identifiers, enabling robust performance across diverse search scenarios and achieving advanced performance. Identifiers. MINDER employs three types of identifiers to represent a passage: title, substring, and pseudo-query. The title generally reflects the main topic of the passage, while the substring is a randomly selected excerpt. The pseudo-query is generated based on the passage's content. These identifier types complement each other and adapt to different scenarios, contributing to MINDER's robustness across various search contexts. \n\nTraining. MINDER optimizes an autoregressive language model, denoted as AM, via typical sequence-to-sequence loss. The input text is the query text, and the output is the identifier of the corresponding passage that is relevant to the query. \n\nDuring training, the three identifiers of the samples are randomly shuffled to train the autoregressive model. For each training sample, the objective is to minimize the sum of the negative loglikelihoods of the tokens {i 1 , \u2022 \u2022 \u2022 , i l } in a target identifier I, whose length is l. The generation loss is formulated as, \n\nwhere I <j denotes the partial identifier sequence {i 0 , \u2022 \u2022 \u2022 , i j\u22121 }, q is the question text, and \u03b8 is the trainable parameters in the autoregessive model. \n\nInference. During the inference process, given a query text, the trained autoregressive language model AM could generate several predicted identifiers via beam search, denoted as I. A heuristic function is employed to transform the predicted identifiers I into a ranked list of passages. This function selects the predicted identifier i p \u2208 I p for a given passage p if i p appears at least once in the identifiers of that passage. The similarity score of the passage p corresponding to the query q is then calculated as the sum of the scores of its covered identifiers, as follows,",
            "score": 0.4942285316155489,
            "section_title": "Preliminary: Generative Retrieval",
            "char_start_offset": 7750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 36,
                    "matchedPaperCorpusId": "258947148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41357421875
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "highlight the high coverage of tokens shared by the query and the passage in the top-k of their projections. This obersvation suggests that the lexical overlap between query and passages plays an important role in the retrieval mechanism. Second, we show that vocabulary projections of passages they are likely to contain words that appear in queries about the given passage. Thus, they can be viewed as predicting the questions one would ask about the passage. Last, we show that the model implicitly implements query expansion (Rocchio, 1971). For example, in Figure 2 the query is \"How many judges currently serve on the Supreme court?\", and the words in the query projection Q include \"justices\" (the common way to refer to them) and \"nine\" (the correct answer).\n\nThe above findings are especially surprising due to the fact that these retrieval models are fine-tuned in a contrastive fashion, and thus do not perform any prediction over the vocabulary or make any use of their language modeling head during finetuning. In addition, these representations are the result of running a deep transformer network that can implement highly complex functions. Nonetheless, model outputs remain \"faithful\" to the original lexical space learned during pretraining.\n\nWe further show that our approach is able to shed light on the reasons for which dense retrievers struggle with simple entity-centric questions (Sciavolino et al., 2021). Through the lens of vocabulary projections, we identify an interesting phenomenon: dense retrievers tend to \"ignore\" some of the tokens appearing in a given passage. This is reflected in the ranking assigned to such tokens in the passage projection. For example, the word \"michael\" in the bottom example of Figure 2 is ranked relatively low (even though it appears in the passage title), thereby hindering the model from retrieving this passage. We refer to this syndrome as token amnesia ( \u00a76).\n\nWe leverage this insight and suggest a simple inference-time fix that enriches dense representations with lexical information, addressing token amnesia. We show that lexical enrichment significantly improves performance compared to vanilla models on the challenging BEIR benchmark (Thakur et al., 2021) and additional datasets. For example, we boost the performance of the strong MPNet model on BEIR from 43.1% to 44.",
            "score": 0.49421128070770176,
            "section_title": "Introduction",
            "char_start_offset": 2098,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026458740234375
        },
        {
            "corpus_id": "271114749",
            "title": "Neural Passage Quality Estimation for Static Pruning",
            "text": "To validate the effect that passage quality pruning has on downstream retrieval, we test the following retrieval pipelines, which are representative of several paradigms: 2 Over the full, non-deduplicated corpus \u2022 BM25 (Lexical).As a lexical retriever, we use BM25 [52].We build a BlockMaxWAND [21] index using the PISA engine [41].\n\n\u2022 TAS-B (Dense).As a dense retriever, we use TAS-B [25]. 3We perform an exact -nearest neighbour search over a flat index to avoid variability introduced in using approximation techniques.\n\n\u2022 SPLADEv2 (LSR).As a learned sparse retriever, we use SPLADEv2 (lg) [28], 4 a model that balances efficiency, effectiveness, and representation size.We index and retrieve using PISA [41].\n\n\u2022 BM25 \u00bb MonoELECTRA (Cross-Encoder).Finally, we re-rank the top 100 BM25 results using MonoELECTRA [46], 5 a strong cross-encoder model trained with hard negatives.\n\nWe conduct all our experiments using the PyTerrier [37] platform and we use language model implementations from the HuggingFace Transformers package [65].",
            "score": 0.4938273365470479,
            "section_title": "Ranking Pipelines",
            "char_start_offset": 25517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 332
                },
                {
                    "start": 334,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 712
                },
                {
                    "start": 714,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 879
                },
                {
                    "start": 881,
                    "end": 1035
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "216307"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "198120175"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "250340284"
                },
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "198120175"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "248003253"
                },
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "220830857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01861572265625
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "FiD-Light [47] employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process. GenRT [148] utilizes a combination of an encoder to capture global list-level features and a sequential decoder to reorder documents based on relevance. The model is trained to learn relevance scores through supervised learning, guided by labeled relevance data, ensuring that the most pertinent documents are prioritized in the final reranked list. Furthermore, ITER-RETGEN [121] proposes using a more capable re-ranker, which has access to model generations, to distill knowledge into a dense retriever. This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it to better capture the semantic relevance of documents relative to the task input. \n\nData Augmentation for Re-ranking. Data augmentation for re-rankers focuses on enhancing the training process by generating additional training data, such as pseudo-relevance labels, using LLMs. This data augmentation provides more varied training examples, which helps improve the performance of re-ranking models. For example, DKS-RAC [53] introduces methods like Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC), which focus on improving the retrieval process by incorporating rich answer encodings. These methods involve generating additional training signals or utilizing enriched data representations to improve the retrieval and ranking of documents. Additionally, the PROMPTAGATOR [27] framework utilizes synthetic data generated through LLM-based query generation to enhance the training of the reranker. This data augmentation approach allows the re-ranker to refine candidate passages more effectively, using a cross-attention model trained on these additional examples to boost retrieval accuracy.",
            "score": 0.49364484753474547,
            "section_title": "Re-Ranking",
            "char_start_offset": 41698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 388,
                    "end": 393,
                    "matchedPaperCorpusId": "267412492"
                },
                {
                    "start": 757,
                    "end": 762,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "267053546"
                },
                {
                    "start": 1785,
                    "end": 1789,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "The dense passage retrieval approaches have been proposed to map both questions and documents to continuous vectors (i.e., embeddings), which has achieved better performance than sparse retrieval approaches (Chen et al., 2017;Dai and Callan, 2019). Existing approaches can be roughly divided into two categories: pre-training and finetuning. The first type of methods often explores pre-training objectives/architectures designed for retrieval. Lee et al. (2019) pre-trains the retriever with an unsupervised Inverse Cloze Task (ICT). Condenser (Gao and Callan, 2021) proposes a dense retrieval pre-training architecture which learns to condense information into the dense vector through LM pre-training. coCondenser (Gao and Callan, 2022) adds an unsupervised corpuslevel contrastive loss on top of the Condenser (Gao and Callan, 2021) to warm up passage embeddings. \n\nThe second type of methods often fine-tunes pre-trained language models on labeled data. Karpukhin et al. (2020) proposes a dense embedding model using only pairs of questions and passages, without additional pre-training. Xiong et al. (2020); Qu et al. (2021) identify that the negative samples during training may not be representative, thus mechanism of selecting hard training negatives is designed. Khattab and Zaharia (2020); Humeau et al. (2019) incorporate late interaction architectures into the learning process that independently encode the query and the document firstly. Tang et al. (2021) designs a method to mimic the queries on each of the documents by clustering to enhance the document representation. PAIR (Ren et al., 2021a) leverages passage-centric similarity relation into training object to discriminate between positive and negative passages. RocketQAv2 (Ren et al., 2021b) introduces dynamic listwise distillation to jointly train retriever and re-ranker.",
            "score": 0.4935769172320369,
            "section_title": "Dense Passage Retrieval",
            "char_start_offset": 4806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 445,
                    "end": 462,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 717,
                    "end": 739,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 959,
                    "end": 982,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1114,
                    "end": 1130,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1274,
                    "end": 1300,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1454,
                    "end": 1472,
                    "matchedPaperCorpusId": "234338073"
                },
                {
                    "start": 1595,
                    "end": 1614,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1749,
                    "end": 1768,
                    "matchedPaperCorpusId": "238857121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341064453125
        },
        {
            "corpus_id": "248510277",
            "title": "Implementing Open-Domain Question-Answering in a College Setting: An End-to-End Methodology and a Preliminary Exploration",
            "text": "Dense Retrievers: Sparse retrievers are powerful and fast IR engines, but they have one critical flaw: they lack context. So, as the location of terms in the text is irrelevant, they work in those cases in which there is word-to-word correspondence between the query and the document. Dense retrievers address this issue by replacing sparse vectors with dense vector embeddings. Modern implementations of dense retrievers like Dense Passage Retrieval, also known as DPR (Karpukhin et al., 2020) use transformer models such as BERT -more on BERT in the following paragraphsto encode the corpus of documents into dense vectors. This can take a long time to process, especially if the corpus of documents is large, as it requires training just like any transformer (deep learning) model. Then, at retrieval time, the query is BERT-encoded on the fly into a dense vector and use to score likely BERT-encoded documents in the corpus using a similar approach as that in Eq(1). Following (Jurafsky and Martin, 2021) notation: \n\nWe considered the use of dense retrievers as part of this work, but the time required to encode the corpus of documents and the lack of evidence of significant improvement when compared to the use of sparse retrievers made us favor the latter approach, at least in this stage of our research. We therefore settled for a sparse BM25 retriever. \n\nReader Component: Pre-training and transfer learning have been, arguably, the most important breakthroughs of the decade in deep learning. As stated in (Erhan et al., 2010) pre-training \"guides the learning towards basins of attraction of minima that support better generalization from the training data set\". In other words, transformer models are trained on very large amounts of data for a general purpose task, allowing the researcher to then fine-tune such models for specific tasks, like question-answering in this case. The idea of transfer learning branches out from the aforementioned technique. A model pre-trained on natural language, for example, can be reused (i.e. fine-tuned) for a specific task without losing its previous general knowledge, effectively obtaining a powerful model capable of solving complex tasks. BERT-like encoders have therefore become the de facto standard for reading-comprehension in ODQA systems.",
            "score": 0.4932916183076964,
            "section_title": "METHODOLOGY",
            "char_start_offset": 15763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 494,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 981,
                    "end": 1008,
                    "matchedPaperCorpusId": "60811638"
                },
                {
                    "start": 1518,
                    "end": 1538,
                    "matchedPaperCorpusId": "15796526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0266571044921875
        },
        {
            "corpus_id": "276928243",
            "title": "MapQA: Open-domain Geospatial Question Answering on Map Data",
            "text": "In this section, we benchmark the performance of the two groups of models outlined in \u00a73. The first group consists of well-established retrieval backbones, specifically Dense Passage Retrieval (DPR) models, using two encoder architectures: BERT and GeoLM. The second group includes a range of large language models (LLMs) applied in a Text-to-SQL setup. For the LLMs, we benchmarked GPT-3.5-turbo, GPT-4o [30], Gemini-1.5-flash [36], LLaMA-3-8B-Instruct [11], and Mistral-7B-Instruct-v0.3 [18].",
            "score": 0.4929199574667138,
            "section_title": "Experiments",
            "char_start_offset": 20056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0153045654296875
        },
        {
            "corpus_id": "259262395",
            "title": "Learning to Rank in Generative Retrieval",
            "text": "As previously mentioned, it is insufficient for generative retrieval to only learn how to generate identifiers. Therefore, we develop a framework to enable generative retrieval to learn how to rank passages directly. To accomplish this, we continue training the autoregressive model AM using a passage rank loss. \n\nTo begin, we retrieve passages for all queries in the training set using the trained autoregressive language model AM after the learning-to-generate phase. For a given query q, we obtain a passage rank list \n\nwhere n is the number of retrieved passages. Each passage p j is assigned a relevant score s(q, p j ) via Eq. 3, which is calculated as the sum of the language model scores of a set of predicted identifiers. It is important to note that the passage rank list includes both positive passages that are relevant to the query and negative passages that are not. A reliable retrieval system should assign a higher score to positive passages than to negative passages, which is the goal of the learning-to-rank paradigm. To achieve this objective in generative retrieval, we utilize a margin-based rank loss, which is formulated as follows: \n\nwhere p p and p n represent a positive and negative passage in the list P, respectively, and m is the margin. It is noted that the gradients could be propagated to the autoregressive model AM via the language model score s ip , which is the logits of the neural network. \n\nIn practice, we take two rank losses based on different sampling strategies for positive and negative passages. In L rank1 , the positive and negative passages are the ones with the highest rank scores, respectively. In L rank2 , both the positive and negative passages are randomly sampled from the passage rank list. While the rank loss optimizes the autoregressive model toward passage ranking, the generation of identifiers is also crucial for successful passage ranking. Therefore, we also incorporate the generation loss into the learning-to-rank stage. The final loss is formulated as a multi-task format: \n\nwhere \u03bb is the weight to balance the rank losses and generation loss. \n\nWe continue training the autoregressive model AM via Eq. 5.",
            "score": 0.49289969194342464,
            "section_title": "Learning to Rank",
            "char_start_offset": 11834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2046
                },
                {
                    "start": 2049,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.218505859375
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "Pre-trained Language Models (PLMs). Through pre-training, language models can achieve higher performance when transferred to specific tasks. Transformer [27], based on self-attention mechanism, is a mainstream architecture of PLMs, including encoderonly [11,14] model and decoder-only [2,29] model. To address the quadratic time scaling of Transformer architecture, some architectures broadly regarded as state space models have been proposed [8,20], especially high-performing models like sub-quadratic architecture M2-BERT [6], linear architecture Mamba [7] and Mamba-2 [5]. \n\nDense Retrieval Models. Transformer PLMs have been proven effective for dense retrieval. Initially, encoder-only models are adopted for retrieval tasks due to the bi-directional attention mechanisms [10,19]. Later, decoder-only models are adopted for retrieval tasks due to their effectiveness on larger model size [15,17]. \n\nLong-text Dense Retrieval Models. In long-text retrieval, early works use chunking strategies [4] due to the small context window. In order to help the model better understand complete and coherent semantics, some studies explore Transformer-based long-text retrieval models [9,15,30]. \n\nFaced with the quadratic time scaling of Transformer-based longtext retrieval models, the sub-quadratic M2-BERT model [6] has been utilized for long-text retrieval tasks [23].",
            "score": 0.49267540140988364,
            "section_title": "Related Work",
            "char_start_offset": 4479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "264288993"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "matchedPaperCorpusId": "264288993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048675537109375
        },
        {
            "corpus_id": "264451831",
            "title": "1-PAGER: One Pass Answer Generation and Evidence Retrieval",
            "text": "\"Retrieve-and-read\" Question Answering Question answering approaches in NLP are dominated by the \"retrieve-and-read\" paradigm where a retriever first fetches hundreds of relevant documents from a corpus, followed by a language model that reranks and extracts the answer (Harabagiu et al., 2003;Chen et al., 2017;Zhu et al., 2021). Sparse retrievers such as BM25 (Robertson et al., 2009) build a highdimensional lexical index over text corpus. Dense retrievers (Karpukhin et al., 2020) use a dual encoder architecture to embed the query and document and perform an approximate nearest neighbor search. Various modifications to dense retrieval have been proposed over the years includ-ing hard negative training (Xiong et al., 2020), late interaction (Khattab and Zaharia, 2020;Santhanam et al., 2022), few-shot learning (Izacard et al., 2022), joint retriever and reader training (Jiang et al., 2022). \n\nA particular variant of interest is the Iterative Retrieval process where the query is reformulated incrementally (Das et al., 2019;Lee et al., 2022) leading to an interactive search process (Jiang et al., 2023;Adolphs et al., 2021). This query augmentation scheme has similarities with our use of search paths. However, we use the paths to iteratively partition the corpus while prior works have used it for refining the query. \n\nTo perform well, retrieve-and-read systems will typically retrieve 10s to 100s of passages that must be processed by a language model. In constrast, 1P retrieves and extracts an answer in a single pass of language model generation. \n\nClosed Book Question Answering With data and parameter scale, LLMs in a closed-book setting (CBQA) have shown competitive performance (OpenAI, 2023;Anil et al., 2023;Yu et al., 2023) to retrieval pipelines (ODQA), however without producing any attributed passages (Rashkin et al., 2021;Bohnet et al., 2022).",
            "score": 0.49265943388446976,
            "section_title": "Related Work",
            "char_start_offset": 3806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 294,
                    "matchedPaperCorpusId": "1717175"
                },
                {
                    "start": 362,
                    "end": 386,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 460,
                    "end": 484,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 749,
                    "end": 776,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1035,
                    "end": 1052,
                    "matchedPaperCorpusId": "249049410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015899658203125
        },
        {
            "corpus_id": "248524694",
            "title": "Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings",
            "text": "Dense retrieval is becoming one of the standard approaches for document and passage ranking. The dual-encoder architecture is widely adopted for scoring question-passage pairs due to its efficiency and high performance. Typically, dense retrieval models are evaluated on clean and curated datasets. However, when deployed in real-life applications, these models encounter noisy user-generated text. That said, the performance of state-of-the-art dense retrievers can substantially deteriorate when exposed to noisy text. In this work, we study the robustness of dense retrievers against typos in the user question. We observe a significant drop in the performance of the dual-encoder model when encountering typos and explore ways to improve its robustness by combining data augmentation with contrastive learning. Our experiments on two large-scale passage ranking and open-domain question answering datasets show that our proposed approach outperforms competing approaches. Additionally, we perform a thorough analysis on robustness. Finally, we provide insights on how different typos affect the robustness of embeddings differently and how our method alleviates the effect of some typos but not of others.",
            "score": 0.49257659213428356,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050048828125
        },
        {
            "corpus_id": "259991673",
            "title": "Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval",
            "text": "Dense retrieval has become a core component within several downstream NLP and web-search tasks such as question-answering [1,2], semantic similarity [3], conversational search [4], entity retrieval [5], fact-checking [6] and passage retrieval [7,8,9,10,11]. In dense retrieval, dual-encoder models encode semantically correlated queries and documents and represent them as spatially close embeddings, i.e., dense vector representations. Dense retrieval can be efficiently conducted via approximate nearest neighbor (ANN) search at inference [12]. \n\nDual-encoder models encode and store document embeddings within an index, which starts to become bulky, once the documents start to increase within the corpus. It becomes expensive to serve dense retrievers practically. For example, storing 21M (million) passages within a dense index requires about 65 GB of memory [13]. As nearest-neighbor (exact) search in large vector spaces is rather slow, ANN techniques such as HNSW [14] are popularly used, which results in even higher memory requirements such as 150 GB [13]. As a result, for searching on a very large corpus, we require a high-memory and expensive machine to host the bulky index, which hinders the practical application of dense retrieval. \n\nAware of the issue, there have been recent efforts to improve the efficiency of dense retrieval models by compressing the vector representation to lower memory, i.e., space requirements. Traditionally, out-of-the-box unsupervised strategies such as dimension reduction with principal component analysis (PCA) or product quantization (PQ) [15] have been popular for retrieval [16,17]. More recently, supervised learning-to-hash (LTH) techniques have been introduced such as BPR [13] and JPQ [18]. LTH techniques map the original dense embeddings into a Hamming space (binary space) and obtain the low-dimensional binary codes. Unfortunately, all existing vector compression methods are proposed under the single-domain retrieval assumption. However, in practice, many retrieval applications span across various domains with diverse distributions; often with scarce training data [19].",
            "score": 0.4924508600129871,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 125,
                    "end": 127,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "231779510"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "9896397"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 1632,
                    "end": 1635,
                    "matchedPaperCorpusId": "239993003"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 1743,
                    "end": 1747,
                    "matchedPaperCorpusId": "236772078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08209228515625
        },
        {
            "corpus_id": "256598370",
            "title": "ResMem: Learn what you can and memorize the rest",
            "text": "The Decoder-Only model used in our experiments is essentially the normal Encoder-Decoder architecture with Encoder and Cross-Attention removed. We pretrained both the T5-small and T5-base model on C4 [42] dataset with auto-regressive language modeling task for 1,000,000 steps, with dropout rate of 0.1 and batch size of 128. The learning rate for the first 10,000 steps is fixed to 0.01 and the rest steps follow a square root decay schedule. \n\nDuring the inference for retrieval key, query embeddings and residuals, we ensured every token has at least 64 preceding context by adopting a sliding window strategy, where a window of 256 token slides from the beginning to the end on each of the articles, with a stride of 256 \u2212 64 = 192. \n\nFor residuals, we only stored the top 128 residuals measured by the absolute magnitude, as the residual vector is as large as T5 vocabulary size (i.e., 32128), and storing all 32128 residuals for each token is too demanding for storage. However, when weight-combining the residuals, we zero filled the missing residuals so that all the residual vectors have 32128 elements.",
            "score": 0.4920850704296067,
            "section_title": "E Additional details of NLP experiments",
            "char_start_offset": 34385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1112
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046630859375
        },
        {
            "corpus_id": "257985191",
            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
            "text": "It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck. \n\nPLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021;Humeau et al., 2019;Xiong et al., 2020;Zhan et al., 2021Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances. CoT-MAE involves selecting two passages from a document as contextual passages, aiming to learn a better representation with the assistance of contextual information. \n\nA shallow encoder, typically with one or two layers, is used to reconstruct the masked passages with the help of context passage embedding. \n\nThe efficiency of PLM-based sparse retrieval (Bai et al., 2020;Formal et al., 2021b,a;Shen et al., 2022) and dense retrieval (Wu et al., 2022;Wang et al., 2022;Gao and Callan, 2021a) demonstrates that both sparse and dense representations effectively capture the text meaning. On the basis of contextual masked auto-encoder (CoT-MAE) tailored for dense retrieval, a natural thought is to incorporate the sparse representation pre-training into CoT-MAE, i.e., exploring multi-view representations in contextual masked auto-encoder pretraining and fine-tuning. Theoretically, sparse representation focuses on the lexicon while dense representation focuses on sentence semantics, offering different perspectives on the text. These two representations are probably compatible and complement each other. \n\nAside from representation variations, there is also a distinction in the structure of the decoder in the previous methods.",
            "score": 0.49179404796918524,
            "section_title": "Introduction",
            "char_start_offset": 1726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2028
                },
                {
                    "start": 2031,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 310,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 349,
                    "end": 366,
                    "matchedPaperCorpusId": "233289894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "258557604",
            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
            "text": "Pre-trained language model-based (PLM) dense retrievers (DRs) have achieved remarkable success in the task of English-only passage retrieval [12-14, 20, 21, 26, 28, 29, 40, 43, 44]. These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33,45]. \n\nTo leverage the semantic modelling power of DRs, recent research has extended English-only DRs to support cross-lingual settings [1,2,19,22,27,31], i.e, where queries and passages are in different languages. This is achieved using multi-lingual PLMs, such as multilingual BERT [6], in place of the English-only PLMs. This approach is particularly important in this setting where traditional bag-of-words methods are ineffective due to the limited number of matching terms across languages. In contrast, cross-lingual DRs (xDRs) are able to encode queries and passages in different languages into a shared embedding space, enabling efficient ANN search across languages. However, such multi-lingual PLM-based xDRs usually are less effective on the cross-lingual passage retrieval task than DRs in the English-only setting [1]. The hypothesis to explain this result is that, in the English-only setting, a DR only needs to learn relevance matching between queries and passages. In contrast, a xDR not only needs to learn the relevance matching task, but also needs to learn how to align the embeddings of texts with similar semantic meaning but in different language [24,41]. It is this language gap that makes cross lingual retrieval a relatively harder task for xDRs. \n\nBased on this hypothesis, this paper proposes the use of crosslingual query generation (xQG) to bridge the language gap for xDRs. Our approach is illustrated in Figure 1.",
            "score": 0.49163583373089276,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 699,
                    "end": 701,
                    "matchedPaperCorpusId": "236428949"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "245144717"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "220871404"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "256461230"
                },
                {
                    "start": 710,
                    "end": 713,
                    "matchedPaperCorpusId": "250390748"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1388,
                    "end": 1391,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 1732,
                    "end": 1736,
                    "matchedPaperCorpusId": "246210468"
                },
                {
                    "start": 1736,
                    "end": 1739,
                    "matchedPaperCorpusId": "248392078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146240234375
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "To produce queries and keys for retrieval, we use dense representations of the span-level embeddings of entity names in questions and passages. We used the pretrained LUKE The macro average of the top-20 retrieval accuracies (denoted as Recall@20) on the 24 relations of EQ test set. # LM refers to the total number of language models used in each dense retriever with different weight parameters. Retrieval training denotes fine-tuning methods for retrieval of each dense retriever. See text for details of the baseline models. \n\nmodel 3 to encode an entity span to a contextualized representation. The LUKE model is based on the RoBERTa model (Liu et al. 2019) and has an extension called entityaware self-attention. This mechanism allows our system to embed a span-level contextualized representation of the entity name. An entity embedding is output as a single vector from the model, even if it consists of multiple tokens. We also use the embedding of the entire span of a title of a passage as a retrieval key.",
            "score": 0.49158558372647354,
            "section_title": "Contextualized Representations of Named Entities",
            "char_start_offset": 12451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1017
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0208892822265625
        },
        {
            "corpus_id": "249191630",
            "title": "Fast and Light-Weight Answer Text Retrieval in Dialogue Systems",
            "text": "In dense passage retrieval, q and p are separately encoded. All the passages can be encoded and indexed offline. During inference time, only the query needs to be encoded; ANN (approximate nearest neighbor) search libraries such as FAISS (Johnson et al., 2017) are used to efficiently search for the most relevant passage.\n\nIn single-vector retrieval models such as DPR  and BERT Siamese/Dual Encoder (Luan et al., 2021), the query and passages are separately encoded into single vectors, models are trained with the objective of mapping the relevant passage vector close to the query vector, and pushing the irrelevant passage vectors far away from the query vector. During inference time, ANN search is used to retrieve directly for the passage vectors closest to the query vector. Several other systems leverage multi-vector representations and attention-based reranking, including Poly-encoders (Humeau et al., 2020), PreTTR (MacAvaney et al., 2020), etc.. In late interaction models such as Col-BERT (Khattab and Zaharia, 2020;Santhanam et al., 2021), the query and passages are separately encoded to obtain query token vectors and passage token vectors. These models adopt token-decomposed scoring, e.g. the sum of maximum-similarity (SumMaxSim) scores to query vectors are used to model the relevance of passages. During training, models are trained with the objective of maximizing the SumMaxSim scores of relevant passage and minimizing those of irrelevant passages. During inference time, the passage tokens closest to query tokens are fetched, and then the relevant passages are re-ranked based on the SumMaxSim scores.\n\nWe experimented with two of the most popular dense retrieval models, DPR and ColBERT. As effective as they are, they still consume more computing resources and take longer response time than required in our real-world use case of hosting thousands of customized systems. Also, in our use case, dialogue system administrators want to reduce the time to fine-tune neural retrieval models on custom training data.",
            "score": 0.4911653591442213,
            "section_title": "Neural Dense Retrieval Preliminaries",
            "char_start_offset": 9093,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 899,
                    "end": 920,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1005,
                    "end": 1032,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "267028109",
            "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
            "text": "The authors studied various types of architectures with multiple components of a broader category of RAG models: retrieval-augmented neural architectures. The components include retrievers, rankers, and encoder-decoders.When a user has a prompt or a query. The retriever component takes this prompt and searches for relevant documents from a large corpus. Once a set of potentially relevant documents is retrieved, the ranker sorts these documents based on their relevance to the user's prompt. This step ensures that the most pertinent information is prioritized. The encoder processes the user's prompt along with the top-ranked documents retrieved and produces an intermediate representation. This representation captures the essential information from both the user's input and the external knowledge sourced from the database. The decoder then uses this representation to generate a response. The encoder-decoder enables the system to provide responses that are grounded in factual information. \n\nThe authors discussed various retrieval augmentation mechanisms.Dense Passage Retrieval (DPR) [19] is a retrieval strategy that employs a dual-encoder architecture to score dialogue context-document pairs. Given a dialogue context, DPR retrieves relevant documents by computing a relevance score between the context and each document in a corpus. It encodes the queries and the documents separately. The term dense opposes sparse retrieval method, as it maps both the query and the documents into continuous vector spaces. Poly-encoders [16] produce a set of vectors (codes) to attentively process the context token outputs of a transformer encoder. These codes capture different distinct semantic aspects or features of the context. Each code interacts with a candidate's vector to compute a score. These scores from all codes are then combined for a final score for that candidate.The Fusion-in-Decoder (FiD) [17] mechanism fuses independent encoder outputs before decoding the final generation. While DPR and Poly-encoder are retrieval strategies used to find relevant documents or passages given the dialogue context, FiD is for integrating the information from these retrieved documents into the final response generated by the model. FiD then separately encodes the dialogue context and each retrieved document using a transformer-based encoder. The",
            "score": 0.49114714121832936,
            "section_title": "NEURAL PATH HUNTER: REDUCING HALLUCINATION IN DIALOGUE SYSTEMS VIA",
            "char_start_offset": 52956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015716552734375
        },
        {
            "corpus_id": "268723791",
            "title": "Scaling Laws For Dense Retrieval",
            "text": "As discussed previously, in this paper, we construct dense retrieval models from the pre-trained language model checkpoints and perform fine-tuning with the annotated query-document pairs in each dataset.One of the most important parts of dense retrieval model training is the negative sampling strategy.Previous work has shown that mining hard negative samples in the training process can significantly improve the retrieval performance.However, the primary objective of this work is to investigate the scaling effects of dense retrieval models.As a result, we do not focus on sophisticated training strategies.For simplicity, we adopt the most straightforward approaches, namely random negative sampling and in-batch negative techniques, for the training of all dense retrieval models in this paper.These methods are employed to minimize the influence of sampling strategies.\n\nFormally, for each query-passage pair (  ,  +  ), we randomly select a set of unlabeled passages from the corpus as the negative.Then we can optimize the following contrastive ranking loss:\n\nwhere  denotes the training batch size, { \u2212  } is the set of negative passages and  (, ;  ) is the scoring function of query and passage:\n\nHere, \u27e8\u2022\u27e9 denotes inner product and  denotes the parameters of the text encoder.We fine-tune the models for a fixed 10,000 steps and random sample 256 negatives at each step.",
            "score": 0.4907738414913796,
            "section_title": "Training Setting",
            "char_start_offset": 16270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1207
                },
                {
                    "start": 1209,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023193359375
        },
        {
            "corpus_id": "271915981",
            "title": "Mistral-SPLADE: LLMs for better Learned Sparse Retrieval",
            "text": "Learned Sparse Retrievers (LSR) have evolved into an effective retrieval strategy that can bridge the gap between traditional keyword-based sparse retrievers and embedding-based dense retrievers. At its core, learned sparse retrievers try to learn the most important semantic keyword expansions from a query and/or document which can facilitate better retrieval with overlapping keyword expansions. LSR like SPLADE has typically been using encoder only models with MLM (masked language modeling) style objective in conjunction with known ways of retrieval performance improvement such as hard negative mining, distillation, etc. In this work, we propose to use decoder-only model for learning semantic keyword expansion. We posit, decoder only models that have seen much higher magnitudes of data are better equipped to learn keyword expansions needed for improved retrieval. We use Mistral as the backbone to develop our Learned Sparse Retriever similar to SPLADE and train it on a subset of sentence-transformer data which is often used for training text embedding models. Our experiments support the hypothesis that a sparse retrieval model based on decoder only large language model (LLM) surpasses the performance of existing LSR systems, including SPLADE and all its variants. The LLM based model (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse retrieval model on the BEIR text retrieval benchmark.",
            "score": 0.490462783562154,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.008514404296875
        },
        {
            "corpus_id": "277501850",
            "title": "LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback",
            "text": "The rapid evolution of information retrieval systems has been largely influenced by the emergence of neural language models, particularly in the domain of dense retrieval [40,46,20,21,12]. While traditional lexical retrieval methods rely on exact term matching, dense retrievers leverage semantic understanding through learned representations, enabling more effective passage retrieval. The introduction of BERT-based dense retrievers is a great advancement in this field, demonstrating superior performance in capturing semantic relationships between queries and passages compared with lexical models at that time. \n\nRecent developments in Large Language Models (LLMs) have pushed the boundaries of natural language understanding and generation even further, presenting new opportunities for enhancing information retrieval systems [30,35,36]. These models, with their improved semantic comprehension and contextual understanding, have shown remarkable capabilities across various natural language processing tasks. Methods that adapt these LLMs to the dense retrieval task have also recently emerged [2,25,16,36], leading to improved retrieval effectiveness, even without requiring the contrastive training of the dense representations typical of other dense retrieval backbones [48]. However, their potential in combining with PRF, particularly through techniques like Vector-based PRF, remains relatively unexplored. \n\nVector Pseudo Relevance Feedback (VPRF), initially proposed for BERTbased dense retrievers, has demonstrated promising results in improving retrieval performance through iterative refinement of query representations [17,18]. This technique leverages the initial retrieval results to modify the query embeddings in vector space, effectively capturing relevant information within the embeddings that might be missing from the original query. The success of VPRF with BERTbased models raises an important question: Can this technique be effectively generalized to current more powerful LLM-based dense retrievers? \n\nThis paper presents LLM-VPRF, an extension of the VPRF technique [17] adapted for Large Language Model-based dense retrievers. Our work investigates the generalizability of VPRF across different models, focusing particularly on its application to state-of-the-art LLM-based retrievers [2,25,48]. We examine whether the improvements in retrieval performance observed with BERT-based models(i.e.",
            "score": 0.4901173294628486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2331
                },
                {
                    "start": 2332,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1102,
                    "end": 1105,
                    "matchedPaperCorpusId": "269009682"
                },
                {
                    "start": 1108,
                    "end": 1111,
                    "matchedPaperCorpusId": "267523323"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "269449435"
                },
                {
                    "start": 1639,
                    "end": 1643,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 1643,
                    "end": 1646,
                    "matchedPaperCorpusId": "254594093"
                },
                {
                    "start": 2101,
                    "end": 2105,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 2321,
                    "end": 2324,
                    "matchedPaperCorpusId": "269009682"
                },
                {
                    "start": 2327,
                    "end": 2330,
                    "matchedPaperCorpusId": "269449435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1820068359375
        },
        {
            "corpus_id": "248965423",
            "title": "Table Retrieval May Not Necessitate Table-specific Model Design",
            "text": "Given the previous analysis, we hypothesize that general-purpose text-based retrievers without tablespecific designs might not be necessarily worse than special-purpose table-based retrievers, contradictory to what most previous work has assumed (Herzig et al., 2021(Herzig et al., , 2020;;Yin et al., 2020;Wang et al., 2021b). Properly trained text-based retrievers might even outperform table-based retrievers because the strong content matching ability learned on text retrieval datasets can transfer to the table retrieval task. \n\nTo validate these assumptions, we examine two representative retrieval systems: the text-based Dense Passage Retriever (DPR) and the table-based Dense Table Retriever (DTR). We first briefly introduce their input formats and model architectures ( \u00a7 3.1, \u00a7 3.2), then conduct experiments in both zeroshot and fine-tuning settings and compare their table retrieval performance ( \u00a7 3.3).",
            "score": 0.49008605097563723,
            "section_title": "Text Retrieval vs Table Retrieval",
            "char_start_offset": 7026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 919
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 290,
                    "matchedPaperCorpusId": "214802901"
                },
                {
                    "start": 290,
                    "end": 307,
                    "matchedPaperCorpusId": "218674345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0121002197265625
        },
        {
            "corpus_id": "265659444",
            "title": "PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models",
            "text": "Dense text retrieval typically adopts the Embedding-based Retrieval Model (ERM) architecture, also known as bi-encoders [6,29,58]. \n\nFor simplicity, we use the term passage/document interchangeably in the rest of paper. Given a query  \u2208 X and a passage  \u2208 X, the relevance scoring function  ERM (, ) of the ERM is measured by \n\nwhere  (\u2022;  ) : X \u2192 R  is the encoder parameterized with  that maps an input text to a -dimensional vector space and \u27e8\u2022, \u2022\u27e9 : R  \u00d7R  \u2192 R is the similarity function, including inner product and cosine similarity. Without loss of generality, we use inner product as the scoring function for the rest of paper. \n\nLearning. Suppose the training data is presented as a set of relevant query-passage pairs \n\n=1 . The encoder parameters  are often learned by maximizing the log-likelihood loss function [6,29] max  (, ) \u2208 D log   ( |), where the conditional probability is defined by the Softmax function \n\nIn practice, various negative sampling techniques [11,29,34,58] have been developed to approximate the expensive partition function of the conditional Softmax. We direct interested readers to the comprehensive study [14] for more details of learning ERMs. \n\nInference. Given a query embedding  \u2208 R  and a corpus of  passage embeddings P = {  }  =1 where   \u2208 R  ,  = 1, . . . , , ERMs retrieve  most relevant passages from P in real time, which is a Maximum Inner Product Search (MIPS) problem. Exact inference of MIPS problem requires  () time, which is prohibited for large-scale retrieval applications. Thus, practitioners leverage Approximate Nearest Neighbor search (ANN) to approximately solve it in time sub-linear (e.g., O log() ) to the size of corpus . \n\nTo achieve sub-linear time complexity of ANN search, ANN methods require an additional index building stage to preprocess the corpus P into specific data structures, such as hierarchical graphs (e.g., HNSW [39], VAMANA [23], etc) and product quantization (e.g., FAISS [25], ScaNN [15], etc).",
            "score": 0.48995188067895123,
            "section_title": "PRELIMINARY 2.1 Dense Text Retrieval",
            "char_start_offset": 4159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 827,
                    "end": 830,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 982,
                    "end": 985,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1144,
                    "end": 1148,
                    "matchedPaperCorpusId": "232147859"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01678466796875
        },
        {
            "corpus_id": "250279955",
            "title": "MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering",
            "text": "Our system differs from the baseline in three ways. First, in the passage retrieval step, we replace mBERT (Devlin et al., 2019) with mLUKE (Ri et al., 2021). Second, we construct sparse indices from which we will retrieve passages to augment dense retriever-retrieved passages, inspired by Zhang et al. (2021) but uses a different densesparse hybrid approach. Finally, we encode each question and passage independently as opposed to all passages together following the Fusion-in-Decoder (Izacard and Grave, 2020)",
            "score": 0.48990625993712206,
            "section_title": "System Architecture and Pipeline",
            "char_start_offset": 4107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 513
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 128,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "267750652",
            "title": "Dense Passage Retrieval: Is it Retrieving?",
            "text": "Retrieval methods can be broadly categorized into two types: sparse and dense (Zhao et al., 2024). Sparse methods encode queries and passages into sparse vectors, usually based on terms that appear in the queries and passages (Robertson and Zaragoza, 2009;Sparck Jones, 1972). Dense methods employ language models to encode the semantic information in queries and passages into dense vectors (Karpukhin et al., 2020;Huang et al., 2013). Dense methods often share two common properties: (a) the joint training of two or more encoding models -one for embedding a query and the other for embedding a knowledge base, and (b) contrastive training. These commonalities were introduced in the DPR method, inspiring many subsequent methods in the literature. \n\nIn this paper, we analyze the original DPR method using the BERT-base backbone. Through the experiments in each section, we analyze DPR from multiple perspectives to understand what is changing in the backbone model during the training process. We find that: \n\n1. BERT's ability to discriminate between passages remains unchanged (Section 2). \n\n2. The way that knowledge is structured in BERT changes to be more decentralized after DPR training (Section 3). \n\n3. The knowledge that BERT can retrieve over is an extension of the knowledge contained within it (Section 4).",
            "score": 0.48950975365774435,
            "section_title": "Introduction",
            "char_start_offset": 2032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1323
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 97,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 226,
                    "end": 256,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 392,
                    "end": 416,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 416,
                    "end": 435,
                    "matchedPaperCorpusId": "8384258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052825927734375
        },
        {
            "corpus_id": "271730744",
            "title": "Coherence-based Query Performance Measures for Dense Retrieval",
            "text": "More recently, pre-trained language models (PLMs) have introduced neural network architectures that encode the embeddings of queries and documents [15,27,28,56], and have led to increased retrieval effectiveness. Often, a BERT-based model is trained for use as a reranker of the result retrieved by (e.g.) BM25 [41] -such cross-encoders include BERT_CLS [36] and monoT5 [37]. On the other hand, dense retrieval approaches [26,56] are increasingly popular, whereby embedding-based representations of documents are indexed, and those with the similar embeddings to the query are identified through nearest-neighbour search (e.g. ANCE [56], TCT-ColBERT [28]). Compared to reranking setups, dense retrieval is attractive as recall is not limited by the initial BM25 retrieval approach, and improvements in the PLM can improve all aspects of the retrieval effectiveness. Therefore, dense retrieval models inspire us to develop predictors that are effective for predicting their rankings. \n\nIn parallel, neural architectures have also been adopted as methods for predicting query difficulty. These post-retrieval methods are supervised, and use refined neural architectures in order to produce a final performance estimate [2,14,23,58]. For instance, BERT-QPP [2] fine-tunes a BERT [15] model for QPP by estimating the relevance of the top-ranked document retrieved for each query. However, its performance is lower or outperformed by unsupervised predictors when using advanced retrieval methods and the TREC Deep Learning datasets [18]. In our view, the problem lies in the mismatch of representations between predictor and ranking, which is best described in Figure 1. On top, we see the pipeline resulting from a BM25 ranking, and, at the bottom, a ranking from a dense retrieval system [26,56]. While BERT-based QPP techniques can be used to predict the effectiveness of BM25 [2,14,23,58], single-representation dense retrieval models already contain representations that can accurately predict their corresponding ranking, thus eliminating the need to apply Step 3 (e.g.",
            "score": 0.48936033133148593,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1217,
                    "end": 1220,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "250340128"
                },
                {
                    "start": 1223,
                    "end": 1226,
                    "matchedPaperCorpusId": "197466307"
                },
                {
                    "start": 1226,
                    "end": 1229,
                    "matchedPaperCorpusId": "24786698"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1527,
                    "end": 1531,
                    "matchedPaperCorpusId": "257038618"
                },
                {
                    "start": 1785,
                    "end": 1789,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0312042236328125
        },
        {
            "corpus_id": "256461130",
            "title": "Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples",
            "text": "Monolingual Dense Passage Retrieval The past few years have witnessed growing interest in monolingual dense passage retrieval. DPR (Karpukhin et al., 2020) is built on a bi-encoder architecture, which is initialized with BERT (Devlin et al., 2019) and outperforms early dense retrieval methods. \n\nRocketQA (Qu et al., 2021) first mines hard negative samples with a trained retrieval model and then uses the mined negative samples to re-train the model. However, RocketQA requires a pretrained cross-encoder to filter out false negative samples, which is not efficient and must be trained in advance. TAS-B (Hofst\u00e4tter et al., 2021) is also a retrieval model using the bi-encoder architecture. \n\nIt utilizes topic-aware sampling to improve training with in-batch negative samples, and applies a dual-teacher supervision paradigm to achieve better knowledge distillation from both a crossencoder and a ColBERT (Khattab and Zaharia, 2020) teacher model simultaneously. Other studies further apply hard negative sample mining to train dense passage retrieval models. Gao et al. (2021b) and Karpukhin et al. (2020) use BM25 (Robertson et al., 2009) top passages as hard negative samples. ANCE (Xiong et al., 2021) enhances hard negative sampling by dynamically mining hard negative samples in the training phase. However, it requires periodically rebuilding the index and refreshing hard negative samples, which greatly increases computational cost. Zhan et al. (2021) combine static BM25 hard negative samples with dynamic hard negative samples retrieved from the entire corpus by the model at the current training step. \n\nMultilingual and Cross-lingual Dense Passage Retrieval Researchers have been utilizing crosslingual knowledge transfer to enhance monolingual retrieval for low-resource languages since the advent of multilingual pretrained language models. Both MacAvaney et al. (2020b) and Shi et al. (2020) investigate zero-shot transfer using a cross-encoder architecture, in which they first fine-tune mBERT on the source language, then apply the model to the target language directly. However, the crossencoder architecture they use is slow in practice.",
            "score": 0.48935554509848206,
            "section_title": "Related Work",
            "char_start_offset": 6663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 155,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 306,
                    "end": 323,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 606,
                    "end": 631,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 908,
                    "end": 934,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1063,
                    "end": 1081,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 1086,
                    "end": 1109,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1119,
                    "end": 1143,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1445,
                    "end": 1463,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1864,
                    "end": 1888,
                    "matchedPaperCorpusId": "209515542"
                },
                {
                    "start": 1893,
                    "end": 1910,
                    "matchedPaperCorpusId": "226284008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15087890625
        },
        {
            "corpus_id": "267377589",
            "title": "An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models",
            "text": "Recently, neural network models have attracted much attention in both academia and industry and have made impressive improvements in many fields.The information retrieval field benefits from pre-trained language models such as BERT [12] and RoBERTa [29].These neural retrieval models can be divided into two groups as below:\n\nNeural Sparse Retrieval Models improve retrieval performance by changing the term importance in different ways.Two common methods are: a) leveraging neural language generation models (e.g., T5 [39]) to expand original documents with semantic-related words, or b) using pre-trained language models to re-compute the weights.Nogueira et al. [33,34] modified the term frequency by using generative models to generate pseudo-queries of original documents and concatenate these pseudo-queries and original documents to form the new documents.Dai and Callan [10,11] used the pre-trained language model to predict contextualized term weight through a regression task.Mallia et al. [31] first used a generator to expand new semantic-related words of the original text and then used a contextualized language model encoder to re-compute term weights.On the other hand, Bai et al. [4] proposed using an important predictor to re-predict term weights and a gating controller to filter or expand terms at the same time.Formal et al. [13,14] further proposed a more simple but effective method, based on explicit sparsity regularization and a log-saturation effect on term weights.All of these works can significantly outperform BM25.\n\nNeural Dense Retrieval Models use dense vectors to represent queries and documents.Two widely adopted architectures of dense retrieval models are cross-encoders and bi-encoders.Crossencoders [32] concatenate the query and the document as the input of pre-trained language models, then use a function to map the output vector to a scalar score.Cross-encoders achieve superior retrieval effectiveness through the full interaction between all query tokens and all document tokens but the requirement of concatenating the query and each candidate document at the inference time makes cross-encoders impractical in the retrieval stage.",
            "score": 0.489260851431198,
            "section_title": "Neural Retrieval Models",
            "char_start_offset": 8893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 324
                },
                {
                    "start": 326,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1494
                },
                {
                    "start": 1494,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1351,
                    "end": 1354,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00974273681640625
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "In practical, to evaluate the relevance of a passage for a given query, retrieval models usually start from two aspects: 1) Precise term overlapping and 2) semantic similarity across related concept [34]. Usually, the sparse models excel at the first problem, while the dense models can be better at the second. To gain a deep understanding of the characteristics of sparse and dense retrieval models, here, we sample some queries along with their top retrieval results, as shown in Table 7. \n\nFor the query \"\u5b69 \u5b50 \u5634 \u91cc \u64e6 \u7d2b \u836f \u6c34 \u4ea7 \u751f \u526f \u4f5c \u7528 \u4e86 \u600e \u4e48 \u529e\" (What should be done if the kid has side effects from rubbing gentian violet in his mouth?) in the medical domain, we can find that: 1) The headword \"\u7d2b\u836f\u6c34\" (gentian violet) appears in all passages retrieved by the sparse model, although these passages do not completely match the query. In the retrieval results of dense models, the top-1 passage is the annotated golden passage, while the third passage does not contain the headword, and this passage is only semantically related to the query to some degree. Therefore, the sparse and dense models have very distinct characteristics, and can make different contributions to the overall passage retrieval performance. Some previous studies attempt to hybrid the two models to engage the merits of both for better retrieval performance [29]. Our analysis illustrates that similar problems also exist in the Chinese passage retrieval task. We hope that the release of the Multi-CPR dataset can help to conduct more in-depth research on this problem. \n\nFurther, we find that different domains place different emphasis on the sparse and dense models. Queries in the E-commerce and entertainment video domains are relatively short in general. Although the search intent of the query is explicit, the key information is missing for some queries. In this case, the dense model can be helpful in finding semantically relevant passages by generalizing to larger concepts. Before the dense model was widely used, previous studies use query reformulation or synonym expansion to supplement the missing information in the query to improve the performance of the sparse model. For example, the query \"iphone13\" will be reformulated to \"iphone13 \u624b\u673a (iphone13 mobile phone)\" in the E-commerce domain.",
            "score": 0.48877164867835043,
            "section_title": "Case Study",
            "char_start_offset": 27563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2156
                },
                {
                    "start": 2157,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00345611572265625
        },
        {
            "corpus_id": "256461230",
            "title": "Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval",
            "text": "In this section, we give a brief review of dense retrieval and re-ranking. The overviews of all methods are presented in Figure 2. \n\nDual-Encoder. Given a query q and a large corpus C, the retrieval task aims to find the relevant passages for the query from a large corpus. Usually, a dense retrieval model employs two dense encoders (e.g., BERT) E Q (\u2022) and E P (\u2022). They encode queries and passages into dense embeddings, respectively. Then, the model uses a similarity function, often dot-product, to perform retrieval: \n\nwhere q and p denote the query and the passage, respectively. During the inference stage, we apply the passage encoder E P (\u2022) to all the passages and index them using FAISS (Johnson et al., 2021) which is an extremely efficient, open-source library for similarity search. Then given a query q, we derive its embedding by v q = E Q (q) and retrieve the top k passages with embeddings closest to v q . \n\nCross-Encoder Re-ranker. Given a query q and top k retrieved passages C, the re-ranking task aims to reorder the passages as the relevant scores. Due to the limited size of the corpus, the re-ranking task usually employs a cross-encoder to perform interaction between words across queries and passages at the same time. These methods also introduce a special token [SEP] to separate q and p, and then the hidden state of the [CLS] token from the crossencoder is fed into a fully-connected layer to output the relevant score: \n\nwhere \"||\" denotes concatenation with the [SEP] token. During the inference stage, we apply the cross-encoder E C (\u2022) to all <q, p> pair and reorder the passages by the scores. \n\nQuery Generator Re-ranker. Similar to crossencoder re-ranker, query generator re-ranker also aims to reorder the passages as the relevant scores. \n\nFor the query generator re-ranker, we use the loglikelyhood of the query against the passage to measure the relevance: \n\n(3) where q <t denotes the previous tokens before q t .",
            "score": 0.488143993471595,
            "section_title": "Preliminaries",
            "char_start_offset": 9441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 699,
                    "end": 721,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07586669921875
        },
        {
            "corpus_id": "258887561",
            "title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
            "text": "The introduction of the Transformer architecture (Vaswani et al., 2017) has led to a performance boost in language modeling (see, e.g., Brown et al. 2020), but also to a steep increase of computational cost, as the number of parameters and data points is constantly growing. In reaction to this development, there has recently been a surge in work on retrieval-augmented language models (Izacard and Grave, 2021a;Li et al., 2022), which shows that enabling models to retrieve context from large corpora results in lower perplexity and better accuracy in downstream tasks such as question answering, while at the same time using considerably fewer parameters. In this paper, we specifically focus on the Retrieval-Enhanced Transformer architecture (RETRO; Borgeaud et al., 2022). \n\nBy augmenting a language model with a retrieval mechanism, RETRO, like similar architectures, tries to decouple memorization of the training data from the additional generalization that * Correspondence to ehsan.doostmohammadi@liu.se. \n\ncomes with increasing the number of parameters. In RETRO, when a chunk of text (a sequence of tokens) has been generated, a dense representation of this chunk is used to retrieve the most similar neighboring chunks from a large retrieval set, based on their L2 distance. Having the previously generated chunks and their nearest neighbors in the retrieval set, the auto-regressive language model has now access to an extended context when predicting the next chunk. The informativeness of this context depends on the effectiveness of the retrieval method. Borgeaud et al. (2022) note that part of RETRO's performance can be attributed to the token overlap between the generated chunks and the retrieval set. Our starting point in this paper is the observation that the performance gain is actually better explained by such surface-level similarities than by the L2 distance between the dense representations that RETRO uses for retrieval. This is in line with recent work by Norlund et al. (2023), who show that the reduction in loss observed in RETRO \"almost exclusively\" stems from such overlap rather than more sophisticated generalization.",
            "score": 0.48794335898135527,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 71,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 136,
                    "end": 154,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 387,
                    "end": 413,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 755,
                    "end": 777,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1573,
                    "end": 1595,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2467041015625
        },
        {
            "corpus_id": "254685782",
            "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
            "text": "In this section, we introduce the task definition of this work and present the typical fine-tuning process of dense retrieval. \n\nTask Definition. Given a query q, the dense passage retrieval task aims to retrieve the most relevant top-k passages {p i } k i=1 from a large candidate pool P. To achieve this goal, the dual-encoder architecture is widely used. It consists of a query encoder E q and a passage encoder E p , mapping the query q and passage p into k-dimensional dense vectors h q and h p , respectively. Then, the semantic relevance score of q and p will be computed using dot product as \n\n(1) \n\nExisting work mostly adopts pre-trained Transformers (e.g., BERT (Devlin et al., 2019)) as the two encoders, using the representations of the [CLS] token as the dense vectors. In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations. Formally, given a pre-training corpus and a Transformer encoder, we focus on devising several tasks to pre-train the parameters of it. Then, the pre-trained Transformer will be used as the backbone of the query encoder E q and passage encoder E p , and can be fine-tuned on downstream dense retrieval tasks. \n\nFine-tuning Dense Retrievers. In the fine-tuning stage, the learning objective is to pull the representations of a query q and its relevant passages P + together (as positives), while pushing apart irrelevant ones P \u2212 = P \\ P + (as negatives). Therefore, high-quality negatives are critical to the effectiveness of dense retrievers. Existing work commonly leverages the BM25 negatives (Karpukhin et al., 2020) or the top-k ranked negatives mined by a well-trained dense retriever (Qu et al., 2021;Xiong et al., 2021), denoted as D\u2212 . Then, the optimization objective can be formulated as:",
            "score": 0.48773603393362286,
            "section_title": "PRELIMINARY",
            "char_start_offset": 8942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 129,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 694,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1683,
                    "end": 1707,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1778,
                    "end": 1795,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1795,
                    "end": 1814,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4375
        },
        {
            "corpus_id": "254877418",
            "title": "Adam: Dense Retrieval Distillation with Adaptive Dark Examples",
            "text": "Information retrieval (IR) that aims to identify relevant passages for a given query is an important topic for both academic and industrial areas, and has powered many downstream tasks such as opendomain QA (Chen et al., 2017) and knowledgegrounded conversation (Dinan et al., 2018). Typically, IR systems usually follow the retrieve-andre-rank paradigm (Hofst\u00e4tter et al., 2020;Huang et al., 2020;Zou et al., 2021) where a fast retriever -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.  (Zhou et al., 2023) over MS-MARCO. POS and NEG mean the distribution of positive and hard negatives respectively. The hard negatives are provided by RocketQAv2 (Ren et al., 2021c). \n\nfirst retrieved a bundle of relevant passages from a large-scale corpus through pre-built indices and then a more sophisticated ranker comes to re-rank these candidate passages to further obtain more accurate retrieval results. Under this paradigm, recent years have witnessed a growing number of works that utilize pre-trained language models (PLMs) (Qu et al., 2021;Gao and Callan, 2021b) as retrievers and rankers to build IR systems. Among these efforts, there are two commonly adopted architectures: cross-encoder (Devlin et al., 2019a) that measure the relevance of a query-passage pair through jointly modeling their deep interactions; dual-encoder (Karpukhin et al., 2020;Qu et al., 2021) that encodes queries and passages separately into dense representations and calculate the similarity. Although dual-encoders are efficient for billions of indices, they suffer from inferior performance compared with cross-encoders since they can't capture the fine-grained semantic relevance between the query and the passage due to the absence of their deep interactions (Luan et al., 2021a).",
            "score": 0.48771662968485213,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "249712198"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1006,
                    "end": 1023,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1174,
                    "end": 1195,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1311,
                    "end": 1335,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1335,
                    "end": 1351,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01056671142578125
        },
        {
            "corpus_id": "258615756",
            "title": "AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages",
            "text": "We present two baseline retrieval systems: translate-retrieve and cross-lingual baselines. In the translate-retrieve baseline, we first translate the queries using the translation systems described in \u00a74.1. The translated queries are used to retrieve relevant passages using different retrieval systems outlined below. Alternatively, the cross-lingual baseline directly retrieves passages in the pivot language without the need for translation using a multilingual dense retriever. \n\nBM25. BM25 (Robertson and Zaragoza, 2009) is a classic term-frequency-based retrieval model that matches queries to relevant passages using the frequency of word occurrences in both queries and passages. We use the BM25 implementation provided by Pyserini (Lin et al., 2021) with default hyperparameters k1 = 0.9, b = 0.4 for all languages. mDPR. We evaluate the performance of mDPR, a multilingual adaptation of the Dense Passage Retriever (DPR) model (Karpukhin et al., 2020) using multilingual BERT (mBERT). We finetuned mDPR on the MS MARCO passage ranking dataset (Bajaj et al., 2018) for our experiments. Retrieval is performed using the Faiss Flat Index implementation provided by Pyserini. \n\nSparse-Dense Hybrid. We also explore sparsedense hybrid baselines, a combination of sparse (BM25) and hybrid (mDPR) retrievers. We use a linear combination of both systems to generate a reranked list of passages for each question.",
            "score": 0.48678109615445864,
            "section_title": "Passage Retrieval (XOR-Retrieve)",
            "char_start_offset": 17900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1414
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 525,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 740,
                    "end": 758,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 937,
                    "end": 961,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01078033447265625
        },
        {
            "corpus_id": "258947148",
            "title": "Multiview Identifiers Enhanced Generative Retrieval",
            "text": "The retrieval performance on NQ and TriviaQA is summarized in Table 1. By jointly analyzing the results, we gained the following findings.\n\n(1) Among the generative retrieval methods, MINDER achieves the best performance. We found that SEAL which takes natural identifiers surpasses DSI based on numeric identifiers. This is because numeric identifiers lack semantic information and DSI requires the model to memorize the mapping from passages to their numeric IDs. As such, it becomes more challenging for DSI on the NQ and TriviaQA datasets with more than 20 million passages. Despite the superiority of SEAL, MINDER still outperforms it. Specifically, the improvements in terms of hits@5 are 4.5% and 1.6% on NQ and TriviaQA, respectively. This verifies the effectiveness of our proposed multiview identifiers, which could rank passages from different perspectives.\n\n(2) On NQ, MINDER achieves the best performance in terms of hits@100 and the second-best results in terms of hits@5, 20. However, generative retrieval methods, including MINDER, perform worse than dual-encoder approaches on TriviaQA. Generative retrieval methods rely on the identifiers to represent passages, and cannot \"see\" the content of the passage. Although the QG module in Methods Natural Questions TriviaQA @5 @20 @100 @5 @20 @100  Table 1: Retrieval performance on NQ and TriviaQA. We use hits@5, @20, and @100, to evaluate the retrieval performance. Inapplicable results are marked by \"-\". The best results in each group are marked in Bold, while the second-best ones are underlined. \u2020 denotes the best result in generative retrieval.  our work generates pseudo-queries based on a passage's content, the autoregressive language model AM still cannot directly \"see\" the original content of the passage. Besides, autoregressive generation has the error accumulation problem. These are the disadvantages of generative retrieval and why it may not perform as well as dense retrievers in some scenarios.",
            "score": 0.4867466790365479,
            "section_title": "Retrieval Results on QA",
            "char_start_offset": 18728,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "258685571",
            "title": "Efficient and Effective Tree-based and Neural Learning to Rank",
            "text": "with the results showing that DPR outperforms a strong BM25 system by 9%-19% absolute points in terms of top-20 passage retrieval accuracy.\n\nIn the general dense retrieval framework of Figure 8.3, there are two key factors that contribute to the overall efficiency and effectiveness: the ANN algorithm itself that performs the search over millions or billions of vectors, and, as we will discuss, the nature and quality of the learnt vector representations. In the rest of this section, we pay particular attention to the latter and review a subset of methods that encode queries and documents for dense retrieval. 1 Existing encoding models fall into one of two categories based on the granularity of the representations they produce: per-document encoders (single-vector) and per-term encoders (multi-vector). The former class, which includes DPR, learns a single vector representation for a document, whereas the latter produces a contextualized representation for every term in a document.",
            "score": 0.48674608759917537,
            "section_title": "Dense retrieval",
            "char_start_offset": 151722,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0303802490234375
        },
        {
            "corpus_id": "248780080",
            "title": "R3 : Refined Retriever-Reader pipeline for Multidoc2dial",
            "text": "Our experimental setup refines both the retriever and reader components of the existing architecture. \n\nRetrieval We analyze the performance of different dense and sparse retrieval methods in a zeroshot setting on the MultiDoc2Dial dataset. For our dense retriever baselines, we conduct experiments with DPR, ANCE (Xiong et al., 2020) and TAS-B (Hofst\u00e4tter et al., 2021). For sparse retrieval methods, we experiment with SPLADE-max and DistilSPLDAE (Formal et al., 2021). During training, we label the retrieved passages (excluding the golden passage) from BM25 as hard negatives. We also experiment with the finetuned DPR model to mine harder negatives. \n\nReranker Following (Fajcik et al., 2021), we select the top 100 passages from the DistilSPLADE retriever to be reranked using RoBERTA as a cross encoder. We use this reranking only during validation time. The top 10 reranked documents are passed to the reader. \n\nReader We experiment with both T5 and BART models as the reader. We use the T5 based reader model to circumvent the limited tokens used for BART along with the FiD model pretrained on natural questions2 . We further experimented by placing the golden passage at the top-most position (Gold setting) in the retrieved passages before passing it to the reader during training. We also apply curriculum learning (CL) in the reader as per described in Section 4.",
            "score": 0.48657948044604976,
            "section_title": "Setup",
            "char_start_offset": 13127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1377
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 370,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 676,
                    "end": 697,
                    "matchedPaperCorpusId": "237439232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01016998291015625
        },
        {
            "corpus_id": "259089062",
            "title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training",
            "text": "In this section, we briefly describe the bi-encoder structure used in dense retrieval and the SOTA Contriever model, on which we build our model. \n\nBi-Encoder Structure Dense retrievers are always a bi-encoder composed of two separate encoders to transform the query and document into a single vector each. The relevance score is obtained by computing the similarity (e.g., inner-product) between the encoded vectors of queries and documents. The typical way to train a dense retriever is using a contrastive loss that aims to pull relevant passages closer to the query and irrelevant passages farther in the embedding space. For each query, the training data involves one positive passage labeled by annotators and a pool of negative passages, which are usually random passages in the corpus. \n\nContriever It crafts pseudo-positive pairs by randomly cropping two spans of the same document. As negative texts have shown to be a key to the success of retrieval training (Xiong et al., 2021), Contriever also applies the MoCo mechanism (He et al., 2020) to utilize negatives in the previous batches to increase the number of negatives. These two factors make Contriever obtain significant decent performance without any human annotations.",
            "score": 0.4864784018148849,
            "section_title": "Preliminary",
            "char_start_offset": 3540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1237
                }
            ],
            "ref_mentions": [
                {
                    "start": 970,
                    "end": 990,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1035,
                    "end": 1052,
                    "matchedPaperCorpusId": "207930212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "278394304",
            "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes",
            "text": "Dense Passage Retrieval (DPR) is a neural-based retrieval model that learns dense vector representations of the provided text. It encodes the passages and queries into high-dimensional vectors with the help of pre-trained transformer models. FAISS is a library for efficient similarity search and clustering of dense vectors [46]. It provides a fast approximate nearest neighbor search using vector representations via indexing and querying. Putting together, DPR and FAISS enable precise and scalable retrieval of relevant documents powered by semantic similarity. \n\nThe document is split into smaller passages to ensure compatibility with the model input limits. \n\nEach passage is then tokenized and then encoded into a dense vector using the DPR model. For fast and scalable similarity searches, FAISS stores these embeddings of passage into an efficient data structure. This allows it to quickly compare the query embeddings with the passage embeddings and retrieve the most similar one. The formula used for determining relevant passages for a query is mentioned Eq. 5: \n\nwhere  \u20d7 represents the query embeddings, and  \u20d7 represents the passage embeddings. Higher similarity scores indicate stronger alignment between the query and passage.",
            "score": 0.48622670246819405,
            "section_title": "DPR with Facebook AI Similarity Search (DPR-FAISS)",
            "char_start_offset": 16071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1244
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "272832734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06085205078125
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "To better understand how different model training schemes affect retrieval performance (RQ4), we perform an ablation on our multimodal dense retriever and discuss our findings below. \n\nLearning Rate In traditional dense text retrieval, the same learning rate is used to update all the weights in the retriever (Karpukhin et al., 2020;Qu et al., 2021). However, in our multimodal dense retrieval setup, the HuBERT (question encoder) and BERT (passage encoder) models are pre-trained independently to allow the usage of available largescale unsupervised data. Therefore, there can be disparities between the two modalities that can hurt performance. To alleviate this problem we follow an alternative setting where the two encoders have different learning rates. In particular, since the language model contains more information than the speech model, we increase the learning rate of the passage encoder by a factor of 10. Comparing the values in the first block of Table 5, we find that the choice of learning rate is important for effectively training our multimodal dense retriever. \n\nPooling Learning Rate AR@20 AR@100 first p: 2e-5, q: 2e-5 50.77 64.48 first p: 2e-5, q: 2e-4 57.25 70.11 mean p: 2e-5, q: 2e-4 56.59 69.97 max p: 2e-5, q: 2e-4 53.57 67.45 Pooling The next ablation involves different pooling methods for encoding the spoken question into a single vector. Following previous works on dense retrieval, we use the [CLS] token embedding output from BERT to encode the text passage. In contrast, this decision is not that straightforward in the case of the spoken question. The HuBERT speech transformer we use for encoding the spoken questions does not have a next-sentence prediction pre-training task as in BERT. Thus, there is no [CLS] token available. To this extent, we asses dif-ferent pooling strategies for encoding the spoken question, namely, max and mean pooling or taking the first embedding of the sequence as a pooling strategy.",
            "score": 0.48606335349680985,
            "section_title": "Ablation Study on Model Training",
            "char_start_offset": 24121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 334,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 334,
                    "end": 350,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02880859375
        },
        {
            "corpus_id": "251442817",
            "title": "Unsupervised Question Clarity Prediction through Retrieved Item Coherency",
            "text": "In order to build a coherency-network as described in section 3 to determine whether we need to ask clarifying questions, we ought to retrieve passages for a given query. The retrieved items are considered as nodes of the coherency network. We use the MS MARCO V2 corpus 4 which is a comprehensive corpus including more than 138 million passages as the corpus of our retrieval. Further, we employ the well-established BM25 retrieval approach to retrieve passages from the corpus. We selected MS MARCO V2 as the corpus since it has high number of passages and more likely to include relevant answers to each query and BM25 as retrieval methodology since it is well-known approach. Thus, we note that our proposed methodology is not dependent on the corpus nor retrieval methodology. Further, we implement our proposed methods using the NetworkX5 package to build the coherency network. To build the coherency network, we set   (  ,   ) = 1 if and only if a pre-trained language model predicts that   is next sentence of   . As pre-trained models, we used BERT base [17] and also the distilled model MiniLM [47], which is lighter than BERT while keeping the same level of performance. Both of these pre-trained models were initially trained on the Masked Language Modeling (MLM) task as well as the Next Sentence Prediction (NSP) task. While MLM teaches the pre-trained language model to understand the relationship between terms, the NSP task goal is to help the model to understand longer-term dependencies and interpret beyond the words to jointly pre-train text-pair representations [17]. Thus, we defined   (  ,   ) on NSP task. Every pair of passages are fed into the model followed by a [SEP] token. Then, we encode the passages and obtain two probabilities for \"IsNextSentence\" and \"NotNextSentence\" class that indicates whether the   is predicted to be the successor of   . We place an edge from   to   if the model predicts that   is a likely successor of   . Further, we measure the connectivity metrics explained in section 3 on the coherency-network, we built the coherency network with the top- retrieved documents were  \u2208 {10, 20, 30, .., 100}.",
            "score": 0.4860604402325124,
            "section_title": "Experimental Setup",
            "char_start_offset": 15410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0062408447265625
        },
        {
            "corpus_id": "250390748",
            "title": "Ask Me Anything in Your Native Language",
            "text": "We follow common (Qu et al., 2021;Asai et al., 2021b;Ouguz et al., 2021) dual-encoder approach in data representation. The system consists of question encoder E q (\u2022) and passage encoder E p (\u2022) which maps text to d-dimensional real-valued vectors. Before run-time, E p (\u2022) applied to all passages in knowledge source to create search index. To find out relevant passages to certain question system operates a similarity function: \n\n(1) \n\ni.e. similarity between the question and the passage defined by the dot product of their vectors. \n\nIn this work we investigate case when E q (\u2022) = E p (\u2022) and call this approach as Single Encoder. In addition to it, a model with E q (\u2022) \u0338 = E p (\u2022) we call Bi-Encoder to avoid confusion. \n\nThe architecture that utilizes Single encoder approach for retrieval (Sentri) shares one encoder for E q (\u2022) and E p (\u2022) contrary to bi-encoder which based on two separate models. \n\nSince our model is used in a multi-lingual setting, the choice of multilingual models is natural for base model. We use XLM-RoBERTa (Conneau et al., 2020) (large) in our experiments. Training Sentri is trained to give positive passages higher scores than negative passages. More specifically, given a question q i in a language from L together with its positive passage p + i and m negative passages {p \u2212 i,j } m j=1 sampled from W multi , we minimize the loss function: \n\nwhere we aim to optimize the negative loglikelihood of the positive passage against a set of m negative passages. \n\nFor each question, we treat other passages in the training batch that do not answer this particular question as negative passages (in-batch negative trick, Henderson et al. 2017;Karpukhin et al. 2020) In particular, for batch size n each question can be further paired with m = n \u2212 1 + n negatives (i.e., positive and hard negative passages of the rest questions) without sampling additional negatives.",
            "score": 0.48595098155328187,
            "section_title": "Single Encoder Retriever",
            "char_start_offset": 10071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 53,
                    "matchedPaperCorpusId": "236428949"
                },
                {
                    "start": 1044,
                    "end": 1066,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00609588623046875
        },
        {
            "corpus_id": "252739471",
            "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
            "text": "Dense retrieval Retrieving relevant documents for a query (Mitra and Craswell, 2018) is crucial in many NLP applications like open-domain question answering and knowledge-intensive tasks (Petroni et al., 2021). Dense retrievers typically build a search index for all documents by pre-computing the dense representations of documents using an encoder. Off-the-shelf libraries for a maximum inner product search (MIPS) (Johnson et al., 2019;Guo et al., 2020) enable model training and indexing to be developed independently (Lin, 2022). However, both training dense retrievers and building indexes should take into account the final retrieval accuracy.\n\nIn this respect, we aim to close the gap between training and inference of dense retrievers.\n\nPhrase retrieval Phrase retrieval (Seo et al., 2019) directly finds an answer with MIPS from an index of contextualized phrase vectors. This removes the need to run an expensive reader for open-domain QA. As a result, phrase retrieval allows real-time search tens of times faster than retriever-reader approaches as an alternative for open-domain QA. DensePhrases (Lee et al., 2021a) removes the requirement of sparse features and significantly improves the accuracy from previous phrase retrieval methods (Seo et al., 2019;Lee et al., 2020). Lee et al. (2021b) show how retrieving phrases could be translated into retrieving larger units of text like a sentence, passage, or document, making phrase retrieval a general framework for retrieval. Despite these advantages, phrase retrieval requires building a large index from billions of representations. In this work, we focus on improving phrase retrieval with more efficient validation.\n\nValidation of dense retrieval Careful validation is essential for developing machine learning models to find a better configuration (Melis et al., 2018) or avoid falling to a wrong conclusion. However, many works on dense retrieval do not clearly state the validation strategy, and most of them presumably perform validation on the entire corpus. It is doable but quite expensive 1 to perform frequent validation and comprehensive tuning. Hence, it motivates us to devise efficient validation for dense retrieval. Like ours, Hofst\u00e4tter et al. (2021) construct a small validation set by sampling queries and using a baseline model",
            "score": 0.4858181192469139,
            "section_title": "3714",
            "char_start_offset": 5535,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 209,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 417,
                    "end": 439,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 439,
                    "end": 456,
                    "matchedPaperCorpusId": "218614141"
                },
                {
                    "start": 522,
                    "end": 533,
                    "matchedPaperCorpusId": "238259539"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 1110,
                    "end": 1129,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 1252,
                    "end": 1270,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 1270,
                    "end": 1287,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1289,
                    "end": 1307,
                    "matchedPaperCorpusId": "237532725"
                },
                {
                    "start": 1818,
                    "end": 1838,
                    "matchedPaperCorpusId": "33513311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0287628173828125
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "Here we describe our proposed zero-shot dense retrieval method using a pretrained and frozen language model. Figure 1 provides a high-level overview of our system. There are three major changes to the DPR: (1) employing embeddings of named entities in passages as retrieval keys, (2) scoring relevance with cosine similarity over multiple keys of each passage and maxpooling the score to output single score, and (3) elimination of encoder fine-tuning for retrieval. The method follows the following steps: \n\n1. Named entities in questions and passages are extracted via the NER procedure. Our implementation limits the number of named entities in a question to one. 2. The named entities in questions and passages are encoded as a query and keys, respectively. When encoding the named entities, the corresponding question and passages are input to the frozen language model to contextualize the named entities. The title of the passage is also encoded and used as a key of the passage. Therefore, a passage with i entity names will have i + 1 keys. 3. The k highest-scoring passages for a query are retrieved. \n\nThe cosine similarity is used as a scoring function. For multiple keys in each passage, similarity scores to a query are calculated respectively, then single relevance score for each passage is calculated via maximum pooling. Formally, given a query q, a total of j keys keys = (k 1 , k 2 , . . . , k j ) of a passage, and the similarity s i = sim(q, k i ) where sim is cosine similarity, the relevance score for the passage is score(q, keys) = maxpool(s 1 , s 2 , . . . , s j ).",
            "score": 0.48577970942855775,
            "section_title": "Zero-Shot Neural Retrieval Pipeline",
            "char_start_offset": 10720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083160400390625
        },
        {
            "corpus_id": "256460962",
            "title": "Explicit Query Rewriting for Conversational Dense Retrieval",
            "text": "The first group of methods formalize the problem as context-aware query rewriting (Ren et al., 2018;Vakulenko et al., 2021b;Lin et al., 2021c;Vakulenko et al., 2021a;Yu et al., 2020). Most of these methods finetune a pretrained language model (e.g.GPT2 and T5) to generate a new query via an auto-aggressive decoder (Vakulenko et al., 2021b,a;Yu et al., 2020). \n\nThe second group of methods are on the top of dense retrieval models (Lee et al., 2019;Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2020;Yu et al., 2021;Lin et al., 2021b). These methods seek to tackle the query ambiguity by learning a contextualized dense representation from the query context, which can be then used for dense retrieval (Yu et al., 2021;Lin et al., 2021b). Though these dense retrieval methods booster the performances in many information retrieval tasks, we argue that the current dense retrieval methods have two limitations in conversational search: (1) previous conversational dense retrieval models fail to reformulate the original query into a well-formed readable query, which is useful for re-ranking, providing explainability and benefit other conversational scenarios (e.g.query suggestion or query clarification) ; (2) most of the current dense retrieval models are trained on ad-hoc queries (Xiong et al., 2020;Yates et al., 2021). And we speculate that these models lack the ability to highlight important terms in the conversational query context accurately (will discuss in Section 4). The two limitations of current conversational dense retrieval methods undermine the potentiality of a conversational search system. \n\nIn this paper, we propose a Conversational Query Rwriting method for Dense Retrieval (CRDR). It explores enhancing the context modelling ability of a dense retrieval model with term supervision signals learned during query rewriting.",
            "score": 0.4855005575170464,
            "section_title": "Introduction",
            "char_start_offset": 1883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 100,
                    "matchedPaperCorpusId": "4500287"
                },
                {
                    "start": 124,
                    "end": 142,
                    "matchedPaperCorpusId": "239459714"
                },
                {
                    "start": 142,
                    "end": 166,
                    "matchedPaperCorpusId": "216868342"
                },
                {
                    "start": 432,
                    "end": 450,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 450,
                    "end": 473,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 473,
                    "end": 491,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 491,
                    "end": 510,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 526,
                    "end": 544,
                    "matchedPaperCorpusId": "233296511"
                },
                {
                    "start": 729,
                    "end": 747,
                    "matchedPaperCorpusId": "233296511"
                },
                {
                    "start": 1295,
                    "end": 1315,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01024627685546875
        },
        {
            "corpus_id": "257806366",
            "title": "Unsupervised Dense Retrieval for Scientific Articles",
            "text": "Since there are no relevance labels for the passages in our corpus, we apply a recent unsupervised dense retrieval model GPL (Wang et al., 2021) to train our dense retrieval model. We generate 3 queries from each passage using a pre-trained query generator (Nogueira et al., 2019). The passagequery pairs will be the pseudo positive examples. For each generated query, we retrieve similar passages using two existing DR models trained on the MS-MARCO dataset (Reimers and Gurevych, 2019), and take the first 50 of each model as pseudo negatives. Finally, we use a student-teacher training method. The teacher model is a cross-encoder trained on MS-MARCO which shows good performance in zero-shot retrieval tasks (Hofst\u00e4tter et al., 2020). The student model is the bi-encoder DR model to be learned. \n\nThe student-teacher training is used because the pseudo labels are noisy and can not be directly used in the traditional pairwise ranking loss (Burges, 2010) or contrastive loss (Wu et al., 2018). Instead, using a cross-encoder has been demonstrated to generalize well on different datasets (Hofst\u00e4tter et al., 2020) and thus can be used as a teacher model through knowledge distillation. \n\nFor the knowledge distillation we have used MarginMSE loss (Hofst\u00e4tter et al., 2020). It is defined as: \n\nwhere f be is the bi-encoder, which maps the text of query or passage to a vector, f ce is the crossencoder, which maps the text of query and passage to a score, q i is the query, p + i is the positive passage, and p \u2212 i is the negative passage. By minimizing L M arginM SE , the MarginMSE loss avoids the hard treatment of the positives and negatives as in pairwise ranking loss (Burges, 2010) and contrastive loss (Wu et al., 2018).",
            "score": 0.4853765406721305,
            "section_title": "GPL Model Training",
            "char_start_offset": 5248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1732
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 487,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07318115234375
        },
        {
            "corpus_id": "251718746",
            "title": "Scattered or Connected? An Optimized Parameter-efficient Tuning Approach for Information Retrieval",
            "text": "For the cross-encoder model which is used for the re-ranking stage, the query and the document are concatenated into a single sequence to input to the model. We truncate the sequence to the first 128 tokens and 512 tokens for passage datasets and document datasets, respectively. We use cross-entropy pairwise loss and pair 5 negative examples for each query in a mini-batch. We use the official top-k candidates as the negatives. We use a batch size of 72 and 36 for passage datasets and document datasets, respectively. We train 5 epochs for all methods and choose the best checkpoint. The only difference between full fine-tuning and other baselines is that we set different learning rates. For full fine-tuning, we use a learning rate of 2e-5. For all other parameter-efficient tuning methods, we use a learning rate of 1e-4. \n\nFor the bi-encoder model which is used for dense retrieval, the query and the document are encoded separately. We set the maximum length of the query to 32, the passage to 128, and the document to 512. We use the official top-k candidates for the passage retrieval task and use BM25 top-k candidates retrieved by anserini [36] for document retrieval task [36]. Training dense retrieval models with official top-k candidates on MARCO Doc results in bad performance. We pair 7 negative examples for each query on passage retrieval and 1 negative example on document retrieval. We use a batch size of 64 and 44 for passage datasets and document datasets, respectively. We train 3 epochs, and 6 epochs for passage datasets and document datasets, respectively. For full fine-tuning, we use a learning rate of 2e-5. For all parameter-efficient tuning methods, we use a learning rate of 1e-4. For all experiments, we use the Adam optimizer with a linear warm-up over the first 10% steps.",
            "score": 0.4851952897818721,
            "section_title": "Training and Optimization",
            "char_start_offset": 23531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023773193359375
        },
        {
            "corpus_id": "258588167",
            "title": "Unsupervised Dense Retrieval Training with Web Anchors",
            "text": "Dense Retrieval. Dense retrieval is the technique of using dense vector representations of text to retrieve relevant documents [5,12]. With the development of pretrained language models [6,15], recent works have developed various techniques for dense retrieval, including retrieval-oriented pretraining [11,13,19] and negative selection [26]. While dense retrieval has exhibited remarkable effectiveness in contrast to traditional sparse retrieval approaches [23], its benefits are generally confined to supervised settings that involve an adequate amount of human annotations [24]. Unsupervised dense retrieval. Previous work on unsupervised dense retrieval mainly adopts contrastive learning to model training. ICT [14] matches the surrounding context of a random sentence. SPAR [3] uses random sentences as queries with positive and negative passages ranked by the BM25 score. Co-condenser [11], COCO-LM [19], and contriever [13] regard independent text spans in one document as positive pairs. QExt [18] further improves their work by selecting the text span with the highest relevance computed by an existing pretrained model. A few other research works use neural models to generate queries, such as question-like queries [2] or the topic, title, and summary of the document [18]. However, both works require a large-scale generation system. Leveraging web anchors in retrieval. Web anchors have been widely applied to classic approaches for information retrieval [4,7,8,10,28]. Recently, HARP [17] designs several pretraining objectives leveraging anchor texts, including representative query prediction or query disambiguation modeling. ReInfoSelect [29] learns to select anchor-document pairs that best weakly supervise the neural ranker. However, these methods either focus on classic bag-of-word modeling or apply a cross-encoder architecture that does not fit the setting of dense retrieval.",
            "score": 0.4851240829056302,
            "section_title": "RELATED WORK",
            "char_start_offset": 3342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "3252915"
                },
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "231942621"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 781,
                    "end": 784,
                    "matchedPaperCorpusId": "238744204"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "231942621"
                },
                {
                    "start": 1470,
                    "end": 1473,
                    "matchedPaperCorpusId": "18826495"
                },
                {
                    "start": 1473,
                    "end": 1475,
                    "matchedPaperCorpusId": "2758036"
                },
                {
                    "start": 1475,
                    "end": 1477,
                    "matchedPaperCorpusId": "571776"
                },
                {
                    "start": 1480,
                    "end": 1483,
                    "matchedPaperCorpusId": "15386465"
                },
                {
                    "start": 1500,
                    "end": 1504,
                    "matchedPaperCorpusId": "237266405"
                },
                {
                    "start": 1658,
                    "end": 1662,
                    "matchedPaperCorpusId": "210932328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05718994140625
        },
        {
            "corpus_id": "258557604",
            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
            "text": "Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does not increase query latency at inference and can be combined with any cross-lingual dense retrieval method. Results from experiments on a benchmark cross-lingual information retrieval dataset show that our approach can improve the effectiveness of existing cross-lingual dense retrieval methods. Implementation of our methods, along with all generated query files are made publicly available at https://github.com/ielab/xQG4xDR.",
            "score": 0.48465410723348024,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01358795166015625
        },
        {
            "corpus_id": "258841415",
            "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
            "text": "Neural passage retrieval maps queries and passages into vector representations, modeling query- passage relevance as the distance between the corresponding vectors. Single-vector dense retrieval simply embeds the input text into a single fixedsized vector and computes cosine-similarity or dotproduct between the query and the passage vectors (Karpukhin et al., 2020;Xiao et al., 2022). Dense retrieval is limited in modeling expressivity as its vector representation is usually lowdimensional (e.g. 768D) due to efficiency concerns. Sparse retrieval improves modeling expressivity by mapping the input text into a long sparse vector (usually vocabulary-sized) and computes dotproduct as query-passage relevance (Mallia et al., 2021;Formal et al., 2021). Alternatively, other work like poly-encoder (Humeau et al., 2020) and late-interaction (Santhanam et al., 2022) represents the input text with multiple vectors and aggregates the vector distance between these multiple-vector representations. All of these neural retrievers can only accept short-length texts, e.g. 512 tokens, limiting their application scenarios. \n\nIn this work, we use: (1) DRAGON+ 22 (Lin et al., 2023), a dense retriever which is pretrained with Masked Auto-Encoder on English Wikipedia, BookCorpus (Zhu et al., 2015) and the MS MARCO corpus (Nguyen et al., 2016) and then finetuned with cross-entropy on the MS MARCO with data augmentation; (2) SPLADEv2 23 (Formal et al., 2022), a sparse retriever which is pretrained with coCondensor (Gao and Callan, 2022) on the pre-training corpora of RetroMAE and then 22 The checkpoint from https://huggingface. co/facebook/dragon-plus-query-encoder and https://huggingface.co/facebook/ dragon-plus-context-encoder. 23 The checkpoint from https://huggingface.co/ naver/splade-cocondenser-ensembledistil.",
            "score": 0.48455565944317935,
            "section_title": "B Neural Passage Retrieval",
            "char_start_offset": 30238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1819
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 367,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 712,
                    "end": 733,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 799,
                    "end": 820,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 842,
                    "end": 866,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1158,
                    "end": 1176,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 1274,
                    "end": 1292,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1317,
                    "end": 1338,
                    "matchedPaperCorpusId": "41479182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04217529296875
        },
        {
            "corpus_id": "248496043",
            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
            "text": "There are two lines of research that are related to our work. The first line of research investigates the integration of PRF with dense retrievers. Li et al. [12] proposed a simple PRF method called Vector-PRF which adapted the classic Rocchio PRF method [25] used on bag-of-words representations, to dense retrievers in a zero-shot manner. Vector-PRF has been shown to improve effectiveness, at additional minimal efficiency expense. We adopt this method in our paper. Wang et al. [27] proposed a more complex model that uses a clustering technique to model the PRF signals; this is in turn applied to the ColBERT dense retriever [11]. However, the improvements achieved by this method come at the cost of efficiency. Yu et al. [29], on the other hand, proposed the ANCE-PRF model that requires the training of a new query encoder based on 1 Also known as hybrid models. \n\nthe original ANCE [28] query encoder. ANCE-PRF achieved significant improvements over ANCE. However, due to the input limit of the BERT-style model (512 tokens [7]), ANCE-PRF is limited in the amount of feedback it can consider: experimentally, = 5 is the maximum PRF depth for MS MARCO. \n\nThe second line of research regards the interpolation of sparse retrieval models and deep language models to further boost effectiveness, especially in terms of recall. Wang et al. [26] investigated the interpolation of BM25 and dense retrievers. Their findings suggest that dense retrievers are highly effective in encoding strong relevance signals, but they are not as effective when dealing with weak relevance signals. The interpolation of BM25 and dense retrievers is able to make up for each other's weaknesses: this interpolation can significantly improve the effectiveness of dense retrievers. Furthermore, Lin and Ma [14], Lin et al. [18] and Arabzadeh et al. [1] also investigated different approaches to combine learned sparse retrieval results with dense retrieval results to improve retrieval effectiveness, and significant improvements are recorded from their experiments. Importantly, according to Wang et al. [26], dense retrievers are not so good at dealing with weak relevance signals.",
            "score": 0.48419070486492904,
            "section_title": "RELATED WORK",
            "char_start_offset": 4228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 259,
                    "matchedPaperCorpusId": "61859400"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "237363901"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1345,
                    "end": 1349,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 1807,
                    "end": 1811,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1833,
                    "end": 1836,
                    "matchedPaperCorpusId": "237593050"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0174407958984375
        },
        {
            "corpus_id": "274965713",
            "title": "MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering",
            "text": "The goal of RAG is to address the limitations in the parametric knowledge of LLMs by incorporating external knowledge. \n\nPassage retrieval and reranking. Retrieval methods are typically categorized into sparse retrieval and dense retrieval. Sparse retrieval methods like BM25 (MacAvaney et al., 2020) rely on lexical matching. In contrast, dense retrieval models (Karpukhin et al., 2020;Zhao et al., 2021;Thakur et al., 2021;Izacard et al., 2021) encode the question q and passage p into low-dimension vectors separately. The semantic similarity is computed using a scoring function (e.g., dot product) as: \n\nHowever, these bi-encoder models lack the ability to capture fine-grained interactions between the query and passage. A common optional3 approach is to have another cross-encoder model to rerank top passages. Cross-encoder models (Khattab and Zaharia, 2020;Wang et al., 2020;Gemma et al., 2024) jointly encode the query q and the passage p together by concatenating them as input into a single model as: \n\n(2) \n\nAnswer generation. Recent reader systems are mainly powered by LLMs with strong reasoning capabilities. With recent advancements in longcontext LLMs (Dubey et al., 2024;Lee et al., 2024b), top documents are typically concatenated with the query as reader input: \n\nTo unlock the reasoning capabilities of LLMs, Chain-of-Thought (CoT) prompting (Wei et al., 2022) introduces intermediate reasoning steps for improved performance. Self-RAG (Asai et al., 2024) critiques the retrieved passages and its own generations. Recent agentic RAG systems dynamically acquire and integrate external knowledge (using either retriever models or search APIs) during the reasoning process (Li et al., 2025;Song et al., 2025;Jin et al., 2025).",
            "score": 0.4839536714379672,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 5014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 300,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "223957053"
                },
                {
                    "start": 1364,
                    "end": 1382,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1458,
                    "end": 1477,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08050537109375
        },
        {
            "corpus_id": "277857392",
            "title": "Building Russian Benchmark for Evaluation of Information Retrieval Models",
            "text": "Neural baseline models used in our work are subdivided into pre-trained dense retrievers (bi-encoders) and rerankers. Bi-encoders generate embeddings for queries and documents and calculate their cosine similarity. Rerankers take a query and a document as an input and calculate the probability of the document to be relevant to the query. Rerankers are applied to the best documents found by lexical or dense retrievers and usually improve the performance of combined retrieval. Dense retrievers include the following pre-trained bi-encoders: \n\n\u2022 LaBSE bi-encoder [12]. LaBSE was pre-trained with a translation ranking task. This allows to find sentence paraphrases in a single language or different languages. \u2022 Multilingual E5 in three sizes: large , base and small [25]. The multilingual E5 model was trained on a large multilingual corpus using a weakly supervised contrastive pretraining method with InfoNCE contrastive loss. Then it was fine-tuned on high-quality labeled multilingual datasets for retrieval tasks. \n\n\u2022 BGE-M3 model [6]. The BGE-M3 model was pre-trained on a large multilingual and cross-lingual unsupervised data, and subsequently fine-tuned on multilingual retrieval datasets using a custom loss function based on the InfoNCE loss function.",
            "score": 0.4839351911017815,
            "section_title": "Neural baseline models",
            "char_start_offset": 17484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "220347683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012725830078125
        },
        {
            "corpus_id": "276287272",
            "title": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval",
            "text": "Our experimental setup includes a description of the datasets, model specifications, and the baseline methods used for comparison in our framework. \n\nDatasets. We evaluate QA-Expand on two benchmark collections: \n\n(1) BEIR Benchmark (Thakur et al., 2021) and ( 2) TREC Passage Datasets (Craswell et al., 2020). Specifically, for BEIR Benchmark, we select four frequently used datasets: webis-touche2020, scifact, trec-covidbeir, and dbpedia-entity. For TREC Datasets, we employ the Deep Learning Passage Tracks from 2019 and 2020, which consist of large-scale passage collections to ensure that our approach performs well in challenging retrieval scenarios. 3 LM and Retrieval Models. For generating the question-answer pairs in QA-Expand, we utilize Qwen2.5-7B-Instruct Model4 (Team, 2024), which is a high-performance language model and, for the retrieval task, we employ multilingual-e5base5 (Wang et al., 2024) to encode both queries and documents into dense representations. Additionally, we incorporate BM25 (Robertson et al., 2009) as a sparse retrieval baseline, specifically using BM25s6 (L\u00f9, 2024), a pure-Python implementation that leverages Scipy (Virtanen et al., 2020) sparse matrices for fast, efficient scoring. \n\nBaselines and Our Approach. We compare QA-Expand with standard retrieval baselines and query expansion methods. Retrieval baselines include BM25 for sparse retrieval and multilingual-e5-base for dense retrieval using cosine similarity. We also evaluate query expansion methods such as Q2D (Wang et al., 2023), which generates pseudo-documents via few-shot prompting, and Q2C (Jagerman et al., 2023), which uses chainof-thought guided reformulation.",
            "score": 0.4836330418570986,
            "section_title": "Setup",
            "char_start_offset": 5044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1678
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1038,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1159,
                    "end": 1182,
                    "matchedPaperCorpusId": "198229805"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01116180419921875
        },
        {
            "corpus_id": "248392078",
            "title": "C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval",
            "text": "Since the introduction of pretrained transformer-based language models, neural retrieval models have been taking advantage of these models for more effective query-document matching. Early work in monolingual retrieval involved building cross-encoder models [9,29,33] that leveraged the full interaction between the queries and documents to produce the relevance scores. Subsequently, similar models [19,22,38,42,44] were adapted to the CLIR setting. While effective, such models can only rerank documents since they process both queries and documents during inference and thus yield a longer running time compared to traditional sparse retrieval techniques such as BM25 [36]. DPR-style dense retrieval models overcome this limitation by scoring the documents based on the similarity of their representations, which allows the language models to encode documents beforehand. \n\nHowever, the representations produced by off-the-shelf language models are undertrained [16] and thus directly scoring documents with such representations yields suboptimal retrieval results [27]. Additional task-specific fine-tuning with relevance labels produces much better representations on either the sequence level, such as ANCE [41] and DPR [23], or the token level, such as ColBERT [24]. Especially for sequence level representations, often summarized in the CLS token, contrastive learning [6,34] that trains a model with one positive and multiple negative examples for each query has been shown to be one of the most effective training techniques for dense retrievers [21,35,41]. In-batch negative sampling further reduces the memory requirement by treating positive examples for other queries as negative [16,23]. A similar objective was utilized to pretrain the cross-encoder retrieval model for the CLIR task [43]. \n\nContinuing pretraining the off-the-shelf language model has been investigated in mono-lingual retrival [5,13,16]. Specifically, coCondenser [16] continued pretraining of the language model with a passage-containing classification task (i.e., determining if a pair of passages belong to the same document) through contrastive learning on the representation of the passages for monolingual IR before fine-tuning it as a DPR model.",
            "score": 0.4832475199269942,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 3559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 261,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "218901033"
                },
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "208058719"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "235324815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04486083984375
        },
        {
            "corpus_id": "247594093",
            "title": "Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval",
            "text": "On the other hand, recent bi-encoder dense retrieval models (Karpukhin et al., 2020;Santhanam et al., 2021;Hofst\u00e4tter et al., 2021;Formal et al., 2021;Gao and Callan, 2021) have been shown to be highly effective for in-domain retrieval while remaining efficient for large corpora in practice. Compared to traditional retrieval methods, these models use dual deep encoders, usually built on top of a Transformer encoder architecture (Vaswani et al., 2017;Devlin et al., 2019), to encode query and document passages separately and eventually output contextual embeddings. Similarity scores can be efficiently computed given these embeddings, which limits costly neural inference to indexing time. The effectiveness of these models can be attributed to the encoder's ability to capture contextual connections or even high-level semantics without the necessity for exact lexical matching. This very complementary benefit compared to more rigorous structure search methods motivates us to investigate whether dense retrieval models can improve MIR results when combined with existing structure search methods. We summarize the contributions of this work as follows: \n\n\u2022 We have performed a fair effectiveness comparison of a token-level and a passage-level dense retrieval baseline in the MIR domain. To our knowledge, this is the first time that a DPR model has been evaluated in this domain. \n\n\u2022 We have successfully combined dense retrievers with a structure search system and have been able to achieve new state-of-the-art effectiveness in recent MIR datasets. \n\n\u2022 A comprehensive list of dense retrievers and strong baselines for major MIR datasets are covered and compared. We believe our well-trained models and data pipeline 1 can serve as a stepping stone for future research in this domain, which suffers from a scarcity of resources. \n\n2 Background and Related Work",
            "score": 0.48322287755012,
            "section_title": "Introduction",
            "char_start_offset": 2041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 131,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11358642578125
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "In passage retrieval for speech-based open-domain QA, the ASR-Retriever pipelines that were used so far suffer from propagating ASR errors to the retriever. Such approaches are not trained end-to-end; thus, the quality of the ASR transcriptions bounds the retrieval performance. Furthermore, the requirement for an ASR model limits the applicability of such pipelines to scenarios where annotated speech is available for training the ASR model. To alleviate the above-mentioned problems, we propose an ASR-free multimodal dense retriever that can support speech, and can be trained end-to-end. Specifically, we modify the dual-encoder architecture for dense text retrieval in traditional opendomain QA (Karpukhin et al., 2020) to account for spoken questions. We replace the BERT-based question encoder with HuBERT (Hsu et al., 2021) Figure 1: Overview of our multimodal dense retriever. and leave the BERT-based passage encoder as is. \n\nHuBERT is a self-supervised model for speech representation learning, which leverages a BERT-like masked prediction loss. It utilizes offline clustering to provide target labels for masked language model pertaining. Figure 1 illustrates the architecture of our ASR-free, multimodal dense retriever. Suppose a pair of question q, in speech form, and a passage p, in text form. The speech and language encoders produce the output representations: \n\nHuBERT (q) = (q 1 , ..., q n ), BERT (p) = (p 1 , ..., p m ). \n\n(1) We use the first token embedding output from the speech and language modules to encode questions and passages into a single vector each: \n\nThe relevance between a question and a passage is computed as the dot product of their vectors: \n\nTo this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages.",
            "score": 0.48305506640717927,
            "section_title": "Multimodal Dense Retriever",
            "char_start_offset": 6360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 726,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 815,
                    "end": 833,
                    "matchedPaperCorpusId": "235421619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022674560546875
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. \n\nThe models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters \u03c4 and \u03b4 set to 0.4 across all models and datasets. \n\nBaselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their fine-tuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSI-QG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline performances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.",
            "score": 0.4825709043686182,
            "section_title": "Implementation details",
            "char_start_offset": 18194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "258048596"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1319,
                    "end": 1323,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14111328125
        },
        {
            "corpus_id": "267200117",
            "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
            "text": "Retrieval-augmented Generation (RAG) has been considered as an effective method to overcome several limitations of LLMs, such as hallucinations [21,46], factuality [52], long-term memory [64] and etc [11]. Usually, an external retriever is first used to retrieve relevant textual knowledge from one specific knowledge source (e.g., Wikipedia), then the reader takes the relevant textual knowledge as external context for generating knowledge-grounded response [25]. Most of previous works try to optimize the retriever and reader independently [11]. During the initial phases, people use sparse retriever, such as BM25 [41] and TF-IDF [59], to make relevance decisions and retrieve corresponding evidence. \n\nHowever, sparse approaches fall short in extracting the semantic features inherent in text content [13]. To overcome this issue, researchers have proposed language model-based dense retrieval methods by encoding documents and queries as dense vectors, which effectively represent the semantic features of text content [4,22,26]. As the first dense retriever, DPR [22] uses two pre-trained language models to encode documents and queries separately, allowing for a more nuanced understanding of the content. In addition, Glass et al. [12] propose a retrieve and re-rank framework for leveraging both advantages of sparse retrieval and dense retrieval. There are also several attempts which optimize the retriever and reader simultaneously [1]. For example, Guu et al. [15] augment language model pretraining with a latent knowledge retriever, allowing the model to retrieve and attend over documents used during pre-training, fine-tuning and inference. However, the complex architectures and interactions make them infeasible and inefficient in the era of LLMs. \n\nMore recently, there are a handful of works exploring the performance of LLMs as retriever [30,45,50,73].",
            "score": 0.4825479238814284,
            "section_title": "Retrieval-augmented Generation",
            "char_start_offset": 9479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1876
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 148,
                    "end": 151,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "247411350"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "18303048"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 1026,
                    "end": 1029,
                    "matchedPaperCorpusId": "253080555"
                },
                {
                    "start": 1029,
                    "end": 1032,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035491943359375
        },
        {
            "corpus_id": "263269118",
            "title": "PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems",
            "text": "We implement three types of baseline retrieval models for both ReCDS-PAR and ReCDS-PPR: sparse retriever, dense retriever, and nearest neighbor retriever. \n\nSparse retriever: We implement a BM25 retriever [19] with Elasticsearch 6 . The parameters of the BM25 algorithm are set as default values in Elasticsearch (b = 0.75, k 1 = 1.2). For ReCDS-PAR, we index the title and abstract of a PubMed article as separate fields and the weights given to the two fields when retrieving are empirically set as 3 : 1. \n\nDense retriever: Dense retrievers represent the patients and articles in a low dimensional space using BERT-based encoders and perform retrieval based on maximum inner-product search. Concretely, we denote the encoder as f , and e d = f (d) refers to the low-dimensional embedding generated by the encoder for a given passage d. Then for a query patient q and an article a in our retrieval corpus A, the relevance score between them is defined as the inner product of their embeddings: s dense (q, a) = e q \u2022 e a . The similarity score s dense (q, p) between q and a patient p \u2208 P is defined similarly. \n\nWe first try direct transferring of Sentence-BERT [20] and Contriever [21], two widely-used dense retrievers pre-trained on MS MARCO [22], a general domain retrieval dataset of large scale. Then we train our own dense retrievers by fine-tuning pre-trained encoders on the PMC-Patients dataset. To be specific, for a given query patient q i , a similar patient / relevant article p + i , and a set of dissimilar patients / irrelevant articles p \u2212 i,1 , p \u2212 i,2 , . . . , p \u2212 i,n from the training data, we use the negative log-likelihood of the positive passage as the loss function: \n\nWe train the dense retrievers with in-batch negatives [23], where",
            "score": 0.4820886912216647,
            "section_title": "Baseline models",
            "char_start_offset": 10519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006488800048828125
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "Our contributions are: \n\n-We propose a paragraph aggregation retrieval model (PARM) for dense documentto-document retrieval and demonstrate higher retrieval effectiveness for dense retrieval with PARM compared to retrieval without PARM and to lexical retrieval with PARM. -We propose vector-based aggregation with reciprocal rank fusion weighting (VRRF) for dense retrieval with PARM and find that VRRF leads to the highest recall for PARM compared to other aggregation strategies. -We investigate training DPR for PARM and compare the impact of fewer, more accurate paragraph-level labels to more, potentially noisy document-level labels. -We publish the code at https://github.com/sophiaalthammer/parm 2 Related work Dense passage retrieval. Improving the first stage retrieval with DPR models is a rapidly growing area in neural IR, mostly focusing on the web domain. Karpukhin et al. [19] propose dense passage retrieval for open-domain QA using BERT models as bi-encoder for the query and the passage. With ANCE, Xiong et al. [39] train a DPR model for open-domain QA with sampling negatives from the continuously updated index. Efficiently training DPR models with distillation [17] and balanced topic aware sampling [18] has demonstrated to improve the retrieval effectiveness. As opposed to this prior work, we move from dense passage to dense document-to-document retrieval and propose PARM to use dense retrieval for document-to-document tasks. \n\nDocument retrieval. The passage level influence for retrieval of documents has been analyzed in multiple works [7,22,37,38] and shown to be beneficial, but in these works the focus lies on passage-to-document retrieval. Cohan et al. [9] present documentlevel representation learning strategies for ranking, however the input length remains bounded by 512 tokens and only title and abstract of the document are considered. Abolghasemi et al. [1] present multi-task learning for document-to-document retrieval. Liu et al. [40] propose similar document matching for documents up to a length of 2048 however here the input length is still bounded and the computational cost of training and using the model is increased.",
            "score": 0.4814850305438231,
            "section_title": "Introduction",
            "char_start_offset": 4493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1571,
                    "end": 1574,
                    "matchedPaperCorpusId": "10714285"
                },
                {
                    "start": 1574,
                    "end": 1577,
                    "matchedPaperCorpusId": "215875147"
                },
                {
                    "start": 1577,
                    "end": 1580,
                    "matchedPaperCorpusId": "197928351"
                },
                {
                    "start": 1690,
                    "end": 1693,
                    "matchedPaperCorpusId": "215768677"
                },
                {
                    "start": 1898,
                    "end": 1901,
                    "matchedPaperCorpusId": "246442016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08538818359375
        },
        {
            "corpus_id": "264172563",
            "title": "On Coherence-based Predictors for Dense Query Performance Prediction",
            "text": "Often, a BERT-based model is trained for use as a reranker of the result retrieved by (e.g.) BM25 [38] -such cross-encoders include BERT_CLS [34] and monoT5 [35]. On the other hand, dense retrieval approaches [25,51] are increasingly popular, whereby an embedding-based representation of the documents are indexed, and those with the similar embeddings to the query are identified through nearest-neighbour search (e.g. ANCE [51], TCT-ColBERT [27]). Compared to reranking setups, dense retrieval is attractive as recall is not limited by the initial BM25 retrieval approach, and improvements in the PLM can improve the retrieval effectiveness. Therefore, dense retrieval models inspire us to develop predictors that are effective for predicting their rankings. \n\nIn parallel, neural architectures have also seen some adoption as methods for predicting query difficulty. These post-retrieval methods are supervised, and use refined neural architectures in order to produce a final performance estimate [2,13,22,53]. For instance, BERT-QPP [2] fine-tunes the BERT language model [14] embeddings to the QPP task by estimating the relevance of the top-ranked document retrieved for each query. Still, BERT-based supervised QPP approaches were initially evaluated of in terms of their correlation with the effectiveness of a BM25 ranking. However, it was recently shown that the performance of BERT-QPP is significantly lower or is outperformed by unsupervised predictors when using more advanced retrieval methods and the TREC Deep Learning datatsets [17]. Instead, we believe that less expensive unsupervised predictors that take advantage of the nature of advanced retrieval methods could be more indicative of query performance. \n\nTherefore, in this work, we aim to accurately predict the effectiveness of recent dense retrieval methods. In this regard, we extend current predictors and propose ways to improve their performance under the new models. We reason that the confidence of well-trained and effective ranking methods should be indicative of the likely effectiveness of the system for a given query.",
            "score": 0.48138340800807927,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1004,
                    "end": 1007,
                    "matchedPaperCorpusId": "250340128"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "197466307"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "matchedPaperCorpusId": "24786698"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1547,
                    "end": 1551,
                    "matchedPaperCorpusId": "257038618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026611328125
        },
        {
            "corpus_id": "270870800",
            "title": "Engineering Conversational Search Systems: A Review of Applications, Architectures, and Functional Components",
            "text": "These use sparse vectors encoding term occurrences in queries and documents, which can be used for retrieval directly, to perform pre-filtering of results (Vakulenko et al., 2021b;Zhang et al., 2021), or to represent model features (e.g., for re-ranking) (Cho et al., 2021).Although computationally efficient, the purely lexical approach of these methods limits them in dealing with synonyms, word order, and spelling mistakes.\n\nDense retrieval addresses these issues, which is often implemented as a dual encoder architecture, where one neural model encodes a document into a dense vector and another the (processed) query (Lin et al., 2021).These models are trained by jointly training these two encoders on labeled query -relevant document pairs.There are variations with additional encoding strategies, but the main idea stays the same (Ferreira et al., 2022).\n\nCandidate Re-Ranking.Once the system has a set of possibly relevant candidate results for the current turn or utterance, the next step is to rank this set in order of informativeness.There are many approaches to re-ranking, with the most dominant one being some model that either classifies, scores, or re-orders a given input set (Ferreira et al., 2022).These models are either fine-tuned on explicitly labeled query -relevant item pairs (Zhang et al., 2021;Mele et al., 2021) or use some distance measure between (part of) the embedded query and (part of) the relevant document.These are the main building blocks of most implementations, but they can be combined into more elaborate setups.Kumar and Callan (2020), for instance, suggest multiview re-ranking, where the system creates different embeddings of the input query.These views include information from dialogue history, relevant terms from the retrieved items, and the rewritten query, which get fused into the final ranking.\n\nKnowledge-Based Response Generation.The final step of a turn in the conversational system is to present the response to the user in the form of natural language.As with information retrieval, natural language generation is a dedicated research field.As such, many distinct approaches and methods within CSSs exist.",
            "score": 0.48125320998510057,
            "section_title": "Conversational Search Functions",
            "char_start_offset": 23826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 427
                },
                {
                    "start": 429,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 864
                },
                {
                    "start": 866,
                    "end": 887
                },
                {
                    "start": 887,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1558
                },
                {
                    "start": 1558,
                    "end": 1692
                },
                {
                    "start": 1692,
                    "end": 1852
                },
                {
                    "start": 1854,
                    "end": 1890
                },
                {
                    "start": 1890,
                    "end": 2015
                },
                {
                    "start": 2015,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 180,
                    "matchedPaperCorpusId": "216868342"
                },
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "235351886"
                },
                {
                    "start": 840,
                    "end": 863,
                    "matchedPaperCorpusId": "247470563"
                },
                {
                    "start": 1197,
                    "end": 1220,
                    "matchedPaperCorpusId": "247470563"
                },
                {
                    "start": 1305,
                    "end": 1325,
                    "matchedPaperCorpusId": "235351886"
                },
                {
                    "start": 1325,
                    "end": 1343,
                    "matchedPaperCorpusId": "237683188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004962921142578125
        },
        {
            "corpus_id": "247450700",
            "title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation",
            "text": "the large language model. However, since our interpolation and perturbation methods only manipulate the already   obtained representations of the documents from the encoder E D , we don't have to newly generate document texts and also to forward generated documents into the model, which greatly saves time and memory (see Table 3). We provide a detailed analysis and discussion of efficiency in Appendix B.  (Kwiatkowski et al., 2019); 2) TriviaQA (TQA) is a QA collection scraped from the Web (Joshi et al., 2017).\n\nRetrieval Models 1) BM25 is a sparse termbased retrieval model based on TF-IDF (Robertson et al., 1994). 2) Dense Passage Retriever (DPR) is a dense retrieval model with a dual-encoder of query-document pairs (Karpukhin et al., 2020). 3) DPR with Query Augmentation (DPR w/ QA) augments pairs with query generation for the document, adopting (Liang et al., 2020;Mao et al., 2021a). 4) DPR with Document Augmentation (DPR w/ DA) augments pairs by replacing words in the document (Ma, 2019). 5) DPR with Axiomatic Regularization (DPR w/ AR) regularizes the retrieval model to satisfy certain axioms (Rosset et al., 2019). 6) DAR is ours with interpolation and perturbation of document representations. Implementation Details For the dense retrieval model based on the DPR framework, we refer to the publicly available code from DPR (Karpukhin et al., 2020). We set the training epoch as 25 and batch size as 32 under academic budgets with a single GeForce RTX 3090 GPU having 24GB memory. We use in-batch negative sampling as our negative sampling strategy without hard negative samples. Also, we retrieve 100 passages per question. We use both interpolation and perturbation schemes for our augmentation methods. Specifically, for the interpolation method, we set \u03bb \u2208 [0, 1] in equation 3 to be sampled from the uniform distribution. Also, for the perturbation method, we set the dropping rate as 0.1, and the",
            "score": 0.48112708430063933,
            "section_title": "Method",
            "char_start_offset": 10233,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 515,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 597,
                    "end": 621,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 727,
                    "end": 751,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 880,
                    "end": 898,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 1348,
                    "end": 1372,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0241851806640625
        },
        {
            "corpus_id": "269457256",
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "text": "Word embeddings are a common approach in natural language processing.Similar to sparse vector representations, they use deep learning techniques to project words into a higher-dimensional vector space.Several articles in the RALM architecture utilize this technique, and we have selected representative ones to describe.Karpukhin et al. (2020) proposed the DPR retrieval model, which indexes all passages in a lowdimensional and continuous space.This allows the reader to efficiently retrieve the first k passages associated with the input problem at runtime.A dense encoder is used to map any text passage to a d-dimensional real-valued vector, creating an index for all M passages used for retrieval.Due to its excellent performance as a retriever in RALM architectures, DPR has been widely adopted by researchers such as Lewis et al. (2020), Izacard and Grave (2020b), and Karpukhin et al. (2020).Izacard and Grave (2020a) takes a similar tactic, unlike DPR, in that he uses the same encoding function for questions and paragraphs through shared parameters.In order to further minimise the intervention and reduce the cost of manual annotation, Izacard et al. (2021) proposed another retriever called Contriever, which was trained using unsupervised data.It is based on successive dense embeddings and has a dual-encoder architecture.Average pooling was applied on the output of the previous layer to obtain one vector representation for each query or document.The similarity score between the query and each document was obtained by computing the dot product between their corresponding embeddings.Researchers (Siriwardhana et al., 2023), (Sachan et al., 2021), and (Guu et al., 2020) have used it as a retriever in the RALM architecture due to its ability to utilize unsupervised data.",
            "score": 0.481025450307912,
            "section_title": "Word Embedding",
            "char_start_offset": 13287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1790
                }
            ],
            "ref_mentions": [
                {
                    "start": 824,
                    "end": 843,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1614,
                    "end": 1641,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1670,
                    "end": 1688,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0164031982421875
        },
        {
            "corpus_id": "271909221",
            "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
            "text": "Throughout the paper, we utilize the dual-encoder architecture with shared parameters, which is commonly used for dense retrieval (DR; Ni et al., 2022). Contrastive learning is a method for training DR models by contrasting positive pairs against negatives. Specifically, given a batch of triplets, each of which consists of a query and its relevant and irrelevant documents: \n\nWe minimize the InfoNCE loss for each query q n : \n\n. \n\n(1) \n\nWe use cosine similarity as the scoring function: \n\n, where E \u03b8 is the encoder parametrized by \u03b8. Following Wang et al. (2022), we incorporate prefix identifiers \"Query:\" and \"Passage:\" for queries and passages, respectively. As shown in prior work (Hofst\u00e4tter et al., 2021;Lin et al., 2021), in-batch negatives mining, the second term of the denominator in Eq (1), plays a crucial role in dense retrieval training. In this work, we study different batch sampling approaches to control in-batch negative mining.",
            "score": 0.4808853440413482,
            "section_title": "Contrastive Learning",
            "char_start_offset": 4985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 951
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 151,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 689,
                    "end": 714,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 714,
                    "end": 731,
                    "matchedPaperCorpusId": "235720578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015960693359375
        },
        {
            "corpus_id": "257495804",
            "title": "NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective Reasoning",
            "text": "The two methods (BM25 and DPR) provide a trade-off for our users between speed and accuracy, with BM25 being the fastest and DPR being the most accurate. \n\nDense Passage Retrieval: DPR (Dense Passage Retrieval) is a method that uses dense text embeddings for both query and documents that need to be matched. It is based on a BERT bi-encoder architecture that computes a dot product similarity between a document and a query. Our DPR is based on [21], using question & context encoders trained with the Natural Questions dataset [25,23]. This DPR is then used as a backend for of our neural_match primitive. The neural_match primitive receives a sub-query whose results are used to create a set of (id, document) pairs. It then tries to match the documents against a target query text, using our DPR to return the top_k results that matched (see full example of neural_match in Section 2.1.1). \n\nReader Model: A Reader or reading comprehension model is a neural network that can perform extractive Q&A by returning relevant text intervals of documents. In our work we use the MiniLM [49] model initially trained on the SQuAD 2.0 dataset [37] and further fine-tuned on the NeuroQL training set to improve its performance (as we describe in Section 2.4). This Reader is then used as a backend for of our neural_extract primitive. The neural_extract primitive receives the name of a new variable to bind for extracted answers, a query to create (id, document) pairs and finally a target query text. It then tries to extract relevant text intervals from our documents using the Reader to return the top_k results found. We fine-tuned our reader model for 3 epochs, using a 384-token sequence length with a 128-token document stride. We used a batch size of 16, learning rate of 1e-5 with a 0.2 warmup and included the ability to return no answers when predicting results. \n\nTranslation Model: A translation model is a sequence-to-sequence neural network trained over pairs of input and target sequences.",
            "score": 0.4804778855380641,
            "section_title": "Methods",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 533,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "259224595",
            "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective",
            "text": "Dense retrieval models. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and utilize embedding similarities as estimated relevance scores for effective retrieval [9]. Karpukhin et al. [12] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [32,35], late interaction [14]. Recently, researchers have also investigated pre-training tasks for dense retrieval [8,19]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually need considerable memory consumption and computational overhead. Generative retrieval models. Generative retrieval has recently garnered increasing interest [1,4,20,30], which retrieves documents by directly generating their identifiers based on the given query. It offers an end-to-end solution for document retrieval tasks [3,20] and allows for better exploitation of the capabilities of large generative language models. For example, De Cao et al. [7] proposed an autoregressive entity retrieval model and Tay et al. [28] introduced a differentiable search index (DSI) and represent documents as atomic ids, naive string, or semantic strings. Chen et al. [5] proposed a pre-trained generative retrieval model called Corpus-Brain to encode all information of the corpus within its parameters in a general way. However, the robustness of generative retrieval models has been overlooked by the community. Out-of-distribution in IR. Current studies on OOD robustness in IR have their own limitations. For example, Wu et al. [31] only explored the OOD generalization performance of neural ranking models. Some works have been devoted to alleviating the poor performance of dense retrieval in the scenarios of query variants [6,23,27,36] or zero/few-shot of corpus [17,29,34]. In this work, we focus on the OOD generalizability of generative retrieval models.",
            "score": 0.48005988452653536,
            "section_title": "RELATED WORK",
            "char_start_offset": 3158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 682,
                    "end": 685,
                    "matchedPaperCorpusId": "251718954"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1009,
                    "end": 1011,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "258418300"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1507,
                    "end": 1510,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "236976200"
                },
                {
                    "start": 2071,
                    "end": 2074,
                    "matchedPaperCorpusId": "250638661"
                },
                {
                    "start": 2074,
                    "end": 2077,
                    "matchedPaperCorpusId": "244709375"
                },
                {
                    "start": 2077,
                    "end": 2080,
                    "matchedPaperCorpusId": "248524694"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "237347134"
                },
                {
                    "start": 2115,
                    "end": 2118,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1546630859375
        },
        {
            "corpus_id": "264121665",
            "title": "Multi-Index Retrieve and Rerank with Sequence-to-Sequence Model",
            "text": "The optimal strategy to enhance the results of information retrieval involves the utilization of various dense retrievers, in conjunction with lexical BM25 matching. This approach amplifies the overall recall of the system, ensuring that passages not deemed relevant by one model may be identified as such by another. Another key insight is the importance of employing an effective reranker. The performance of the reranker is often correlated with the size of the model, with larger models typically achieving superior scores compared to their smaller counterparts. However, this advantage is accompanied by an increase in computational cost, which must be taken into account. Furthermore, reranking a larger number of top retrieved passages enhances the likelihood that a relevant passage is included in the set of reranked passages. This strategy, while potentially more computationally intensive, can significantly improve the precision of the retrieval system at the top ranks, which is often a critical requirement in many information retrieval applications.",
            "score": 0.47994901845176935,
            "section_title": "V. CONCLUSION",
            "char_start_offset": 9407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01142120361328125
        },
        {
            "corpus_id": "269921399",
            "title": "DocReLM: Mastering Document Retrieval with Language Model",
            "text": "The retriever is designed to efficiently extract a select set of document from a vast corpus.It requires rapid and effective retriever capabilities.Typically, two models are employed for this purpose: sparse retriever and dense retriever (Guo et al (2022)).The sparse retriever, such as the famous BM25 (Robertson and Zaragoza ( 2009)), utilizes bag-of-words vector to match candidates.Conversely, dense embedding models explicitly encode sentences into dense vectors (Karpukhin et al (2020); Gao and Callan ( 2022)), employing cosine similarity to evaluate the likelihood of a match between query and passage embedding.In DocReLM, we adopt the neural dense retriever and utilize the open-source jina-embedding-v2-base (G\u00fcnther et al (2023)) as the foundational model, which we further train with our custom data.This retriever operates as a encoder model (Devlin et al (2019); Reimers and Gurevych (2019)).Encoder has been applied to various tasks in the scientific domain (Ji et al (2021); Zhou et al (2023); Liang et al (2023); Gong et al (2024)).Leveraging contrastive learning, dense retrievers generate embedding that emphasize key passage details, enabling the linkage of semantically similar sentences even when they use dissimilar terminology.However, pretrained embedding models lack specificity for document retrieval tasks (Wu et al (2023)), often presenting a domain knowledge gap with domain-specific papers (Wang et al (2022)).To address this, we incorporate a large language model to automatically annotate pseudo queries with academic paper corpora, thereby refining the embedding model to align more closely with our task.In constructing this retrieval system using our tailored embedding model, we process the document corpus, segment long documents into passages, and convert them to embedding.These vectors are stored in the vector database Faiss (Johnson et al (2019); Douze et al ( 2024)).",
            "score": 0.47994346308141833,
            "section_title": "Retriever",
            "char_start_offset": 5116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1640
                },
                {
                    "start": 1640,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 255,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 468,
                    "end": 491,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 856,
                    "end": 876,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 878,
                    "end": 905,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 974,
                    "end": 990,
                    "matchedPaperCorpusId": "221823863"
                },
                {
                    "start": 1335,
                    "end": 1351,
                    "matchedPaperCorpusId": "254854282"
                },
                {
                    "start": 1422,
                    "end": 1440,
                    "matchedPaperCorpusId": "245131402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033538818359375
        },
        {
            "corpus_id": "269430346",
            "title": "ExcluIR: Exclusionary Neural Information Retrieval",
            "text": "To evaluate the performance of various retrieval models on exclusionary retrieval, we select three types of retrieval models with different architectures: sparse retrieval, dense retrieval, and generative retrieval.Sparse retrieval methods calculate the relevance score of documents using term matching metrics such as TF-IDF (Robertson and Walker, 1997).\n\n\u2022 BM25 (Robertson et al., 2009) is a classical probabilistic retrieval method based on the normalization of the frequency of the term and the length of the document.\n\n\u2022 DocT5Query (Nogueira et al., 2019) expands documents by generating pseudo queries using a fine-tuned T5 model before building the BM25 index (Raffel et al., 2020).\n\nDense retrieval utilizes pre-trained language models (PLMs) as the backbones to represent queries and documents as dense vectors for computing relevance scores.\n\n\u2022 DPR (Karpukhin et al., 2020) is a dense retrieval model based on dual-encoder architecture, which uses the representation of the [CLS] token of BERT (Kenton and Toutanova, 2019).\n\n\u2022 Sentence-T5 (Ni et al., 2022a) uses a fine-tuned T5 encoder model to encode queries and documents into dense vectors.\n\n\u2022 GTR (Ni et al., 2022b) has the same architecture as Sentence-T5 and has been pretrained on two billion question-answer pairs collected from the Web.\n\n\u2022 ColBERT (Khattab and Zaharia, 2020) is a late interaction model that learns embeddings for each token in queries and documents, and then uses a MaxSim operator to calculate the relevance score.\n\nGenerative retrieval is an end-to-end retrieval paradigm.\n\n\u2022 GENRE (De Cao et al., 2020) retrieves entities by generating their names through a seq-to-seq model, it can be applied to document retrieval by directly generating document titles.The original GENRE is trained based on BART as the backbone, and we reproduce it using T5.\n\n\u2022 SEAL (Bevilacqua et al., 2022) retrieves documents by generating n-grams within them.",
            "score": 0.4790870268574095,
            "section_title": "Methods for comparison",
            "char_start_offset": 10291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1306
                },
                {
                    "start": 1308,
                    "end": 1503
                },
                {
                    "start": 1505,
                    "end": 1562
                },
                {
                    "start": 1564,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1836
                },
                {
                    "start": 1838,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 354,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 364,
                    "end": 387,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 537,
                    "end": 560,
                    "matchedPaperCorpusId": "208612557"
                },
                {
                    "start": 667,
                    "end": 688,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 859,
                    "end": 883,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1049,
                    "end": 1066,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1572,
                    "end": 1593,
                    "matchedPaperCorpusId": "222125277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "256461230",
            "title": "Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval",
            "text": "Information Retrieval (IR) aims to retrieve pieces of evidence for a given query. Traditional methods mainly use sparse retrieval systems such as BM25 (Robertson and Zaragoza, 2009), which depend on keyword matching between queries and passages. With the development of large-scale pretrained language models (PLMs) (Vaswani et al., 2017;Devlin et al., 2019) such as BERT, dense retrieval methods (Lee et al., 2019;Karpukhin et al., 2020) show quite effective performance. These methods usually employed a dual-encoder architecture to encode both queries and passages into dense embeddings and then perform approximate nearest neighbor searching (Johnson et al., 2021). \n\nRecently, leveraging a cross-encoder re-ranker as the teacher model to distill knowledge to a dual-  Figure 1: The performance of cross-encoder and query generator when varying the number of training samples and retrievers. We use BM25 and DPR as retrievers, respectively. For the cross-encoder (BERT-Large), we use retrieved top-100 passages which do not contain the answer as negative and contrastive loss for training. For the query generator (T5-Base), we firstly train it with the query generation task and then fine-tune the model with the same setting as BERT-Large. The reported performance is the top-5 score of re-ranked top 500 passages on the NQ test set. \n\nencoder has shown quite effective to boost the dualencoder performance. Specifically, these methods first train a warm-up dual-encoder and a warm-up cross-encoder. Then, they perform knowledge distillation from the cross-encoder to the dual-encoder by KL-Divergence or specially designed methods. For example, RocketQAv2 (Qu et al., 2021) proposed dynamic distillation, and AR2 (Zhang et al., 2021) proposed adversarial training. \n\nHowever, there are two major problems when scaling the method to the cross-lingual dense retrieval setting.",
            "score": 0.47895437046104783,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 181,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 316,
                    "end": 338,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 338,
                    "end": 358,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 397,
                    "end": 415,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 415,
                    "end": 438,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 646,
                    "end": 668,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1663,
                    "end": 1680,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0784912109375
        },
        {
            "corpus_id": "249926985",
            "title": "Questions Are All You Need to Train a Dense Passage Retriever",
            "text": "Our work is based on training a dense retriever using pre-trained language models (PLMs), which we have covered in previous sections. Here, we instead focus on other related approaches. \n\nA popular method to train the dual-encoder retriever is to optimize contrastive loss using inbatch negatives (Gillick et al., 2019) and hardnegatives (Karpukhin et al., 2020;Xiong et al., 2021). Alternatives to using hard-negatives such as sampling from cached evidence embeddings have also shown to work well in practice (Lindgren et al., 2021). Multi-vector encoders for questions and passages are more accurate than dualencoders, (Luan et al., 2021;Khattab and Zaharia, 2020;Humeau et al., 2020), although at the cost of an increased latency and storage requirements. \n\nPLMs have been shown to improve passage rankings as they can perform cross-attention between the question and the retrieved passages (Lin et al., 2021). Supervised approaches to rerank either finetune PLMs using question-passage pairs (Nogueira et al., 2020) or finetune PLMs to generate question conditioned on the passage (Nogueira dos Santos et al., 2020) while unsupervised re-rankers are based on zero-shot question scoring (Sachan et al., 2022). The re-ranking process is slow due to the cross-attention step and is bottlenecked by the accuracy of first-stage retrievers. To address these limitations, crossattention distillation approaches from the PLM to retriever have been proposed (Qu et al., 2021). Such distillation can be performed either in a single end-to-end training step (Guu et al., 2020;Sachan et al., 2021b) or in a multi-stage process (Khattab et al., 2021;Izacard and Grave, 2021). \n\nAn alternative approach to using PLMs is to generate data that can aid retrieval. The data can be either the title or an answer that provides more information about the question (Mao et al., 2021).",
            "score": 0.4788136615115643,
            "section_title": "Related Work",
            "char_start_offset": 29201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 319,
                    "matchedPaperCorpusId": "202718954"
                },
                {
                    "start": 338,
                    "end": 362,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 510,
                    "end": 533,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 621,
                    "end": 640,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 640,
                    "end": 666,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 666,
                    "end": 686,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 894,
                    "end": 912,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 996,
                    "end": 1019,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1085,
                    "end": 1119,
                    "matchedPaperCorpusId": "222178252"
                },
                {
                    "start": 1190,
                    "end": 1211,
                    "matchedPaperCorpusId": "248218489"
                },
                {
                    "start": 1453,
                    "end": 1470,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1551,
                    "end": 1569,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1569,
                    "end": 1590,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1619,
                    "end": 1641,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 1641,
                    "end": 1665,
                    "matchedPaperCorpusId": "227746078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1246337890625
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "Information retrieval (IR) is the task of finding relevant knowledge or passages corresponding to a given query. Traditional exact lexical-matching approaches, such as TF-IDF (term frequency and inverse document frequency) or BM25 (Robertson et al. 1995;Robertson and Zaragoza 2009), have performed well on some IR tasks. Recently, neural retrieval with contextualized dense sentence embedding has shown to be effective for tasks such as open-domain question answering (e.g., Karpukhin et al. 2020;Xiong et al. 2021). \n\nDense retriever, or dense passage retrieval (DPR), uses a pair of neural language models as a bi-encoder to obtain latent representations of questions and passages (Lee, Chang, and Toutanova 2019). Bi-encoders require to be finetuned with contrastive learning to embed a question and the relevant passages on semantically similar sentence vectors. Recent works on DPR reported in-domain (tested on Accepted to Workshop on Knowledge Augmented Methods for Natural Language Processing, in conjunction with AAAI 2023. the same dataset which was used to fine-tune) retrieval performances exceeding those from sparse retrievers such as BM25 (Karpukhin et al. 2020). \n\nHowever, it has been shown that conventional DPRs struggle with retrieval on BEIR benchmark (Thakur et al. 2021) including various domains and even on EntityQuestions (EQ) (Sciavolino et al. 2021) built within a confined domain (Wikipedia), whereas BM25 shows better performance on both of them. This indicates that the generalizability of DPR is limited not only out of domain but even in domain when a named entity in a question is a dominant clue for retrieval. In order to address this, Sciavolino et al. (2021) showed that DPR trained with PAQ dataset (Lewis et al. 2021) which consists of 65 million of question-answer pairs containing many of named entities in Wikipedia still performs far worse than BM25 in EQ dataset. The results showed that supervised learning with a huge dataset that covers the domain extensively could not lead to good generalization within the domain.",
            "score": 0.4785507394042481,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 254,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 254,
                    "end": 282,
                    "matchedPaperCorpusId": "3641284"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 684,
                    "end": 716,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1155,
                    "end": 1177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1274,
                    "end": 1294,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1354,
                    "end": 1377,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1673,
                    "end": 1697,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1739,
                    "end": 1757,
                    "matchedPaperCorpusId": "231924957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202392578125
        },
        {
            "corpus_id": "267308609",
            "title": "Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control",
            "text": "Learned sparse retrieval (LSR). Learned sparse retrieval is a family of neural retrieval methods that encode queries and documents into sparse lexical vectors that can be indexed and searched efficiently with an inverted index. There are many LSR approaches in the literature on text retrieval [7,40,50]; they are mainly built up from two types of encoder: MLP and MLM [41]. The MLP encoder uses a linear feedforward layer placed on top of the transformers's last contextualized embeddings to predict the importance of input terms (similar to term-frequency in traditional lexical retrieval). The MLP encoder has no term expansion capability. On the other hand, the MLM encoder utilizes the logits of the masked language model (MLM) for weighting terms and selecting expansion terms. Splade [6,7] is a recent state-of-the-art text-oriented LSR approach that employs the MLM encoder in both query and document side, while other methods [3,25,34] use MLP encoders on both sides or only on the query side. Although it seems to be more beneficial to have expansion on both queries and documents, a recent study [41] found that query and document expansion have a cancellation effect on text retrieval (i.e., having expansion on the document side reduces the usefulness of query expansion) and one could obtain near state-of-the-art results without query expansion. \n\nUnlike prior work focused on converting sparse to dense representations for hybrid ad-hoc text retrieval [26,27], our work explores the reverse task of dense to sparse conversion in the multi-modal domain. This direction presents new challenges due to dimension co-activation and semantic deviation issues. Ram et al. [43] interpreted text dense retrieval by zero-shot projection from dense to vocabulary space using a frozen MLM head. Nguyen et al. [39] propose a simple sparse vision-language (VL) biencoder without query expansion and evaluate the performance on the image suggestion task. We aim for an efficient, effective, and semantically faithful drop-in sparse replacement of multi-modal dense retrieval, necessitating training of the projection layer. Cross-modal retrieval.",
            "score": 0.4783706198905816,
            "section_title": "Related Work",
            "char_start_offset": 4466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "257585074"
                },
                {
                    "start": 791,
                    "end": 794,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 794,
                    "end": 796,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "257585074"
                },
                {
                    "start": 1472,
                    "end": 1475,
                    "matchedPaperCorpusId": "249888962"
                },
                {
                    "start": 1681,
                    "end": 1685,
                    "matchedPaperCorpusId": "254877181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0287628173828125
        },
        {
            "corpus_id": "274992052",
            "title": "Efficient Long Context Language Model Retrieval with Compression",
            "text": "Long Context Language Models (LCLMs) have emerged as a new paradigm to perform Information Retrieval (IR), which enables the direct ingestion and retrieval of information by processing an entire corpus in their single context, showcasing the potential to surpass traditional sparse and dense retrieval methods. However, processing a large number of passages within in-context for retrieval is computationally expensive, and handling their representations during inference further exacerbates the processing time; thus, we aim to make LCLM retrieval more efficient and potentially more effective with passage compression. Specifically, we propose a new compression approach tailored for LCLM retrieval, which is trained to maximize the retrieval performance while minimizing the length of the compressed passages. To accomplish this, we generate the synthetic data, where compressed passages are automatically created and labeled as chosen or rejected according to their retrieval success for a given query, and we train the proposed Compression model for Long context Retrieval (CoLoR) with this data via preference optimization while adding the length regularization loss on top of it to enforce brevity. Through extensive experiments on 9 datasets, we show that CoLoR improves the retrieval performance by 6% while compressing the in-context size by a factor of 1.91.",
            "score": 0.47831174708161067,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01049041748046875
        },
        {
            "corpus_id": "256868474",
            "title": "Augmented Language Models: a Survey",
            "text": "Dense and sparse retrievers. There exist two types of retrievers that can be used to augment a LM: dense and sparse. Sparse retrievers work with sparse bag-of-words representations of the documents and the queries (Robertson and Zaragoza, 2009). In contrast, dense neural retrievers use a dense query and dense document vectors obtained from a neural network (Asai et al., 2021). Both types of retrievers assess the relevance of a document to an information-seeking query. This can be done by (i) checking for precise term overlap or (ii) computing the semantic similarity across related concepts. Sparse retrievers excel at the first sub-problem, while dense retrievers can be better at the second (Luan et al., 2021). \n\nConditioning LMs on retrieved documents. Various works augment LMs with a dense retriever by appending the retrieved documents to the current context (Chen et al., 2017;Clark and Gardner, 2017;Lee et al., 2019;Guu et al., 2020;Khandelwal et al., 2020;Lewis et al., 2020;Izacard and Grave, 2020;Zhong et al., 2022;Borgeaud et al., 2022;Izacard et al., 2022). Even though the idea of retrieving documents to perform question answering is not new, retrieval-augmented LMs have recently demonstrated strong performance in other knowledge intensive tasks besides Q&A. These proposals close the performance gap compared to larger LMs that use significantly more parameters. REALM (Guu et al., 2020) was the first method to jointly train end-to-end a retrieval system with an encoder LM. RAG (Lewis et al., 2020) jointly fine-tunes the retriever with a sequence-to-sequence model. Izacard and Grave (2020) introduced a modification of the seq2seq architecture to efficiently process many retrieved documents. Borgeaud et al. (2022) focuses on an auto-regressive LM, called RETRO, and shows that combining a large-scale corpus with pre-trained frozen BERT embeddings for the retriever removes the need to further train the retriever while obtaining comparable performance to GPT3 on different downstream tasks.",
            "score": 0.4781361685139317,
            "section_title": "Retrieval-augmented language models",
            "char_start_offset": 28010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 932,
                    "end": 949,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 949,
                    "end": 973,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 973,
                    "end": 992,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1396,
                    "end": 1414,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1507,
                    "end": 1527,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4658203125
        },
        {
            "corpus_id": "268876110",
            "title": "Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison",
            "text": "This paper has investigated the effect of varying the retrieval strategy for three commonly used retrievalaugmented machine translation architectures, trying to get a better understanding of the interplay of these two components.While auto-regressive encoder-decoder architecture seems quite robust w.r.t.changes in the retrieval strategy, this is less so for the two other architectures, for which optimizing the retrieval policy can yield significant returns.\n\nOur experiments have also highlighted the benefits of coverage-oriented retrieval policies, based on LCS, especially for the non-autoregressive model.Finally, we have validated the use of the \"indomain\" selection policy and proposed to simplify the inference step by eliminating the filtering process, yielding better performance at the expense of an increased latency.\n\nIn our future work, we would like to continue the exploration of the interplay between retrieval and translation, with the aim to jointly optimize these two processes rather than have them designed independently.",
            "score": 0.47775006522040997,
            "section_title": "Conclusion",
            "char_start_offset": 24202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 461
                },
                {
                    "start": 463,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 832
                },
                {
                    "start": 834,
                    "end": 1046
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06842041015625
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "In Table 1, we compare the performance of Self-Retrieval with various baselines on the NQ and TriviaQA datasets. Self-Retrieval 3B outperforms both strong pre-trained dense retrieval models, such as BGE and GritLM 7B, and other generative retrieval methods. Specifically, Self-Retrieval 3B achieves improvements of 5.46 and 5.07 in MRR@5 over the fine-tuned BGE on NQ and TriviaQA datasets, respectively. \n\nOur results indicate that other generative retrieval baselines exhibit suboptimal performance on passage retrieval. Even the largest DSI-XXL model only achieves an MRR@5 of 50.20 on NQ, significantly lagging behind dense retrieval methods such as GritLM, which achieves an MRR@5 of 57.03. In contrast, our Self-Retrieval model demonstrates strong performance in passage retrieval, achieving an MRR@5 of 69.45, significantly outperforming all other generative methods. \n\nWe further compare Self-Retrieval with conventional 2-stage retriever-reranker pipeline. Representative results are shown in Table 1, while the complete experimental results are provided in Appendix D. Notably, even strong retrieval baselines (BGE-FT, GTR-XL, GritLM, and DSI-XL) enhanced with powerful rerankers (such as BGE-Reranker-FT) still fall short of Self-Retrieval's performance, highlighting the advantages of unifying multiple retrieval processes into a single framework rather than treating them as separate components. \n\nThese findings underscore the efficacy of Self-Retrieval in harnessing the memory, generation, and ranking capabilities of LLMs, thereby excelling in passage retrieval tasks where other generative baselines struggle. \n\nMethod R@1 R@10 M@100 Sparse Retrieval BM25 [37] 29",
            "score": 0.477409490056689,
            "section_title": "Passage retrieval",
            "char_start_offset": 19980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1627
                },
                {
                    "start": 1630,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 1674,
                    "end": 1678,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0762939453125
        },
        {
            "corpus_id": "257622671",
            "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
            "text": "In the context of retrieval-augmented models, numerous works rerank retrieved passages by their relevance to the query and process only the highest scoring passages (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022) and vary the number of processed passages depending on model confidence (Kratzwald and Feuerriegel, 2018;Varshney et al., 2022).Concurrent work CoDA (Lei et al., 2023) employs a related conditional computation mechanism, designed for efficient adaptation rather than modeling long documents.\n\nDevice utilization FLOPs do not tell the whole story, as modeling choices can influence the effective speed of operations achieved by accelerators.\n\nFor long text inputs, autoregressive decoder inference is very slow due to memory bandwidth constraints from repeatedly loading the long sequence of keys and values (Shazeer, 2019;de Jong et al., 2022).Shazeer (2019) introduces multi-query attention (MQA), sharing heads for keys and values to reduce memory bandwidth overhead.Pope et al. (2022) studies how to shard large models, especially in the context of MQA, to obtain optimal device utilization and therefore speed.\n\nTraining objectives T5 introduced the span corruption objective (Raffel et al., 2020), a modification of masked language modeling (Devlin et al., 2019).LONGT5 made use of the PEGA-SUS (Zhang et al., 2020) sentence reconstruc-tion objective for improved summarization performance.Tay et al. (2022) proposes UL2, a mixture of span corruption, prefix, and causal language modeling, and shows that it leads to strong performance on both short-output and generative tasks.",
            "score": 0.4767239725603661,
            "section_title": "Sparse attention",
            "char_start_offset": 5404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 509
                },
                {
                    "start": 511,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1132
                },
                {
                    "start": 1134,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 201,
                    "end": 217,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 290,
                    "end": 323,
                    "matchedPaperCorpusId": "52054464"
                },
                {
                    "start": 367,
                    "end": 385,
                    "matchedPaperCorpusId": "258060039"
                },
                {
                    "start": 1198,
                    "end": 1219,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1264,
                    "end": 1285,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1318,
                    "end": 1338,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059326171875
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance. \n\nTo address the limitation above, we propose alternative retrieval oriented masking (ROM) strategy aiming to mask tokens that are required for passage retrieval. Specifically, in the pre-training process of LM, the probability of each token being masked is not random, but is superimposed by the important weight of the token corresponded. Here, the important weight is represented as a float number between 0 and 1. In this way, we can greatly improve the probability of higher-weight tokens being masked out. Therefore, the pre-trained language model will pay more attention to higherweight words thus making it more proper for downstreaming dense passage retrieval applications. \n\nTo verify the effectiveness and robustness of our proposed retrieval oriented masking method, we conduct experiments on two commonly used passage retrieval benchmarks: the MS MARCO passage ranking and Neural Questions (NQ) datasets. Empirically experiment results demonstrate that our method can remarkably improve the passage retrieval performance.",
            "score": 0.47654865857365697,
            "section_title": "Introduction",
            "char_start_offset": 1883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1217
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0970458984375
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "Furthermore, we employ a query generator to generate queries for passages in target languages due to the limited number of unlabeled queries. The query generation methods have been shown to significantly improve the performance of retrieval models in the monolingual setting (Kulshreshtha et al., 2021;Ma et al., 2021a). Considering the grammatical adjustment and accidental translation problems, we first use the mined positive querypassage pairs to train a query generator. Then, we use the trained model to generate more queries in target languages. Considering that there may exist more relevant passages to the generated queries, we use both sparse and dense retrievers to filter the generated samples. Finally, using only unlabeled data from target languages, LeSTM iteratively mines query passage pairs by the lexicon-enhanced retriever and generator, trains a new better retriever and query generator using these mined pairs, mines again for better query passage pairs, and repeats. \n\nIn summary, our contributions are as follows. \n\n\u2022 To the best of our knowledge, our approach is the first attempt to combine sparse and dense retrievers to mine high-quality positive and negative query-passage pairs for the multilingual dense retriever. \n\n\u2022 We propose to use a query generator to expand the unlabeled queries in target languages and an iterative training paradigm is introduced to further enhance the dense retriever and generator. \n\n\u2022 Extensive experiments on two datasets show the effectiveness of our proposed approach. In particular, experiments demonstrate that our method is model-agnostic, they are effective on various pre-trained language models.",
            "score": 0.476478430582429,
            "section_title": "Introduction",
            "char_start_offset": 4006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1665
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 319,
                    "matchedPaperCorpusId": "231704318"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03240966796875
        },
        {
            "corpus_id": "251224409",
            "title": "Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval",
            "text": "Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not \u201cstructurally ready\u201d to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This \u201clack of readiness\u201d results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pre-trained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg\u2605. By concatenating vectors from the [CLS] token and agg\u2605, our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at https://github.com/castorini/dhr.",
            "score": 0.47642816511135805,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0323486328125
        },
        {
            "corpus_id": "260926325",
            "title": "Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval",
            "text": "Dense passage retrieval has gained sustainable improvements with the recent development of pre-training tasks. Some works focus on contrastive pre-training with constructed span relationship (Chang et al. 2020), randomly cropped spans (Gao and Callan 2022) or multiple granularity alignments (Ma et al. 2022). And meanwhile, the others focus on pre-training with auxiliary bottlenecked decoders, like pre-training with a weak generative decoder (Lu et al. 2021), extreme masked ratio (Liu and Shao 2022), and contextual span sampling (Wu et al. 2023a). Our method is similar to (Gao and Callan 2022) and (Wu et al. 2023a), but our core contribution is the methodology of incorporating expanded queries generated by LLMs into such pre-training schemas, which brings better context alignment and stronger zero-shot and fine-tuned performances.",
            "score": 0.47604348141418573,
            "section_title": "Pre-training for Dense Retrieval",
            "char_start_offset": 21702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 841
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 210,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 445,
                    "end": 461,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 484,
                    "end": 503,
                    "matchedPaperCorpusId": "235358369"
                },
                {
                    "start": 534,
                    "end": 550,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 604,
                    "end": 620,
                    "matchedPaperCorpusId": "251594591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19091796875
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "We compare our proposed multimodal method against pipeline approaches that consist of (i) an ASR model for transcribing the spoken question and (ii) a retriever. In detail, similar to Sidiropoulos et al. (2022) we use wav2vec 2.0 (Baevski et al., 2020) pretrained and fine-tuned on 960 hours of annotated speech data (Panayotov et al., 2015) as the ASR model. Concerning the retrievers, we experiment with popular lexical and dense retrievers. We further experiment with dense retrievers explicitly trained to improve robustness against questions with typos since they improve robustness against ASR noise as well (Sidiropoulos et al., 2022). \n\nWe need to underline that contrary to Sidiropoulos et al. ( 2022), which assumed that the clean textual version of the spoken question is provided for training the retrievers, we follow a real-world scenario where only spoken questions are available; thus, we need to transcribe them for training the retrievers. Our experimental results showed that we build stronger baselines by training on the transcriptions of spoken questions. In particular, we experiment with the following retrievers: \n\nBM25 is a traditional retriever based on termmatching. Question and retrieved passages have lexical overlap. \n\nDense Retriever (DR) is used for scoring question-passage pairs and consists of two separate neural networks (dual-encoder), each repre-senting a question and a passage. Given a question, a positive, and a set of negative passages, the learning task trains the two encoders by minimizing a loss function, typically softmax cross-entropy, to encourage positive question-passage pairs to have smaller distances than the negative ones. To train the DR model, we use the training scheme and hyperparameters described in (Karpukhin et al., 2020) with a batch size of 64 which is the largest we can fit in our GPU setup. \n\nDense retriever with data augmentation (DR+Augm.) alternates the training scheme of classic dense retrieval via the addition of data augmentation.",
            "score": 0.47599248281686074,
            "section_title": "Baselines and Implementation",
            "char_start_offset": 9541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 252,
                    "matchedPaperCorpusId": "219966759"
                },
                {
                    "start": 317,
                    "end": 341,
                    "matchedPaperCorpusId": "2191379"
                },
                {
                    "start": 1767,
                    "end": 1791,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00861358642578125
        },
        {
            "corpus_id": "265608008",
            "title": "AHJL at Qur\u2019an QA 2023 Shared Task: Enhancing Passage Retrieval using Sentence Transformer and Translation",
            "text": "This method fetches passages from search engines and calculates their relevance to a query based on \"entailment similarity\", employing cosine directional similarity as a metric. A similar method for passage retrieval was suggested by Bakari and Neji (2022). Initially, passages related to the query are fetched from Google using the question's keywords. These passages are then refined, standardized, and divided. Next, the questions and passages are examined linguistically, including identifying named entities, analyzing syntax, and assessing morphology. In the end, the main ideas of the question and the passage are presented logically. \n\nMore modern techniques use advanced language models like BERT to determine query relevance. Such methods have an advantage over older sparse retrieval techniques because they recognize wordbased and more profound meaning similarities rather than just looking for exact keyword matches (Nogueira dos Santos et al., 2020). Karpukhin et al. (2020) aimed to develop an effective dense embedding model by merging the BERT pre-trained model with a dual-encoder setup. This model transforms text into a specific vector format and then indexes every passage for retrieval. They found that their model surpassed several other models in question-answer tests on various datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017).",
            "score": 0.4759707502262982,
            "section_title": "Related Work",
            "char_start_offset": 4983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 256,
                    "matchedPaperCorpusId": "215941397"
                },
                {
                    "start": 943,
                    "end": 963,
                    "matchedPaperCorpusId": "222178252"
                },
                {
                    "start": 965,
                    "end": 988,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1324,
                    "end": 1348,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1362,
                    "end": 1382,
                    "matchedPaperCorpusId": "26501419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.010528564453125
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "For dense retrieval models with task-specific pre-training, we include the following methods: coCondenser [15] continues to pre-trained on the target corpus with contrastive loss. Other pre-trained methods, such as SimLM [58], Cot-MAE [60] and RetroMAE [32], employ a bottleneck architecture that learns to compress the passage information into a vector through pre-training. We also include the stateof-the-art methods that facilitate dense retrieval with knowledge distillation: TAS-B [19], RocketQAv2 [49] and ERNIE-Search [34] primarily concentrate on distilling knowledge from a cross-encoder to a single vector retriever. On the other hand, SPLADEv2 [14] and ColBERTv2 [52] focus on distilling knowledge from a cross-encoder to a multi-vector retriever. All baselines with special pre-training or distillation can be categorized as non-interaction retrievers, except for SPLADEv2 [14] and ColBERTv2 [52].",
            "score": 0.4758141651505678,
            "section_title": "Baselines",
            "char_start_offset": 27559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 910
                }
            ],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "244799249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "260926325",
            "title": "Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval",
            "text": "Dense passage retrieval (Karpukhin et al. 2020) has broad real-world applications, like web search (Liu et al. 2021;Zou et al. 2023), retrieval-augmented generation (Lewis et al. 2020;Cai et al. 2022) and query answering (Sakata et al. 2019). It utilizes well-trained language-model-based retrievers to extract sentence representations and retrieve relevant passages with given queries. Recent studies have made impressive progress in improving the effectiveness of dense retrievers, such as hard negative mining (Qu et al. 2021), late interaction (Khattab and Zaharia 2020;Santhanam et al. 2022), distillation (Ren et al. 2021;Lu et al. 2022), and ensembling (Gao and Callan 2022;Wu et al. 2023b). Moreover, the development of task-specific pre-training (Gao and Callan 2021;Wu et al. 2023a;Liu and Shao 2022) pushes the limits of retrieval tasks to new boundaries. Specifically, those studies usually employ contrastive learning with span corruption (Gao and Callan 2022;Izacard et al. 2021;Ma et al. 2022), or additional decoder with bottlenecked structures (Gao and Callan 2021;Lu et al. 2021;Liu and Shao 2022;Wu et al. 2023a) for better representation learning. \n\nLarge language models (LLMs), like ChatGPT (Ouyang et al. 2022), PaLM (Chowdhery et al. 2022), LLaMA (Touvron et al. 2023), and tk-Instruct (Wang et al. 2022b), are pre-trained on large-scale web corpus and exhibit excel-* These authors contributed equally. lent abilities in context generation and instruction following. There has been growing interest in incorporating powerful LLMs into retrieval tasks. Existing studies (Gao et al. 2023;Wang, Yang, and Wei 2023;Jagerman et al. 2023;Yu et al. 2023) focus on query expansion with LLMs for enhancing the lexical match of query-passage pairs.",
            "score": 0.4757768092357937,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 47,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 99,
                    "end": 116,
                    "matchedPaperCorpusId": "235358369"
                },
                {
                    "start": 116,
                    "end": 132,
                    "matchedPaperCorpusId": "252995741"
                },
                {
                    "start": 165,
                    "end": 184,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 184,
                    "end": 199,
                    "matchedPaperCorpusId": "250340214"
                },
                {
                    "start": 221,
                    "end": 241,
                    "matchedPaperCorpusId": "147703958"
                },
                {
                    "start": 513,
                    "end": 529,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 548,
                    "end": 574,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 574,
                    "end": 596,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 611,
                    "end": 628,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 755,
                    "end": 776,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 776,
                    "end": 792,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 792,
                    "end": 810,
                    "matchedPaperCorpusId": "235358369"
                },
                {
                    "start": 1061,
                    "end": 1082,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1082,
                    "end": 1097,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1097,
                    "end": 1115,
                    "matchedPaperCorpusId": "235358369"
                },
                {
                    "start": 1115,
                    "end": 1131,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 1213,
                    "end": 1233,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1271,
                    "end": 1292,
                    "matchedPaperCorpusId": "250311114"
                },
                {
                    "start": 1310,
                    "end": 1329,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.188232421875
        },
        {
            "corpus_id": "250390748",
            "title": "Ask Me Anything in Your Native Language",
            "text": "Several works (Qu et al., 2021;Izacard and Grave, 2020a) refer to iterative learning as a source of model quality improvement. We use this idea in the form described below, which we call self-training. We note that at extremely low-resource languages such as Finnish and Telugu Sentri shows consistent performance, similar to results on moderate-resource languages such as Russian. \n\nnegative (p \u2212\u2212 ) examples. Afterwards, we train new iteration Sentri j+1 model using the passages marked up previously. In contrast to (Asai et al., 2021b) we train iteratively only a retrieval part of the whole system. At stage 0 when there is no trained model available, we use well-known BM25 model (Sanderson, 2010) as a retriever in our experiments. The important feature of this model is that it does not need any kind of training, thus it could be used to retrieve documents from collections in languages with little or no training data. \n\nFor Sentri system we report the results for the second stage of self-training below. For Bi-Encoder we report results for stages 1 and 2 adding the specifying index.",
            "score": 0.475634099648659,
            "section_title": "Self-Training",
            "char_start_offset": 13003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1096
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "236428949"
                },
                {
                    "start": 686,
                    "end": 703,
                    "matchedPaperCorpusId": "31674042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00323486328125
        },
        {
            "corpus_id": "273501946",
            "title": "Improve Dense Passage Retrieval with Entailment Tuning",
            "text": "In this section, we introduce the preliminaries of current dense retrieval framework. Task Definition. Given a query q and a collection of passages P, the goal of the dense retriever M is to retrieve k passages from P that are most relevant to q. Passages in P are encoded using M and represented in the form of dense vectors. They are pre-calculated and stored in a vector database, organized using a index such as FAISS (Johnson et al., 2019). At inference time, query q is first encoded using M . Then, the relevance score of query q and a passage p is calculated using a similarity function f , and k passages with highest scores are retrieved and returned: \n\nTraining of denser retriever M can be broadly classified into two stages, which are instruced as follows. \n\nPre-training. Based on PLMs, dense retriever are further trained in an unsupervised manner on largescale corpus such as NQ and MSMARCO. This retrieval-oriented pre-train aims to adapt PLMs to dense retrieval by encoding richer information into document embedding, using tasks such as inverse cloze prediction and Maskedsalient spans. This stage is not necessary but can provide better initialzation for fine-tuning. Fine-tuning. Fine-tuning employs supervised training scheme using much smaller annotated retrieval data. The paired dataset D consists of triplets {(q i , p \n\n, where q is the query, p + is a positive passage relevant to q and {p \u2212 1 , . . . , p \u2212 m } are negative passages irrelevant to q. \n\nDense retriever is often trained with a constrastive loss funtion as follows: \n\ne sim(q i ,p + i ) + n j=1 e sim(q i ,p \u2212 i,j ) \uf8f9 \uf8fb \n\n(2) Our entailment tuning method can be easily plugged into current dense retrieval pipeline between pre-train and contrastive fine-tune. It is simple and efficient, which costs only a overhead on the two main stages. Details of entailment tuning is described in Section 5. \n\n4 Rethinking relevance in retrieval-augmented QA",
            "score": 0.4755117460183572,
            "section_title": "Preliminaries",
            "char_start_offset": 10328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 444,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08526611328125
        },
        {
            "corpus_id": "269982279",
            "title": "Control Token with Dense Passage Retrieval",
            "text": "This study addresses the hallucination problem in large language models (LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a technique that involves embedding relevant information in the prompt to obtain accurate answers. However, RAG also faced inherent issues in retrieving correct information. To address this, we employed the Dense Passage Retrieval(DPR) (Karpukhin et al., 2020) model for fetching domain-specific documents related to user queries. Despite this, the DPR model still lacked accuracy in document retrieval. We enhanced the DPR model by incorporating control tokens, achieving significantly superior performance over the standard DPR model, with a 13% improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",
            "score": 0.4755062938431327,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "258676297",
            "title": "Synergistic Interplay between Search and Large Language Models for Information Retrieval",
            "text": "Dense Retrieval Document retrieval has been an important component for several knowledgeintensive tasks (Voorhees et al., 1999;Karpukhin et al., 2020). Traditional techniques such as TF-IDF and BM25 depend on term matching and create sparse vectors (Robertson, 2009;Yang et al., 2017;Chen et al., 2017) to ensure efficient retrieval. After the emergence of pre-trained language models (Devlin et al., 2019;Liu et al., 2019), dense retrieval which encodes both queries and documents into low-dimension vectors and then calculates their relevance scores (Lee et al., 2019;Karpukhin et al., 2020), has recently undergone substantial research. Relevant studies include improving training approach (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021), distillation (Lin et al., 2021;Hofst\u00e4tter et al., 2021) and task-specific pre-training (Izacard et al., 2022;Gao & Callan, 2021;Lu et al., 2021;Gao & Callan, 2022;Xiao et al., 2022) of dense retrieval models which significantly outperform sparse approaches. \n\nZero-shot Dense Retrieval Many prior works consider training dense retrieval models on highresource passage retrieval datasets like Natural Questions (NQ) (Kwiatkowski et al., 2019) (133k training examples) or MS-MARCO (Bajaj et al., 2016) (533k training examples) and then evaluating on queries from new tasks. These systems (Wang et al., 2022;Yu et al., 2022) are utilized in a transfer learning configuration (Thakur et al., 2021). However, on the one hand, it is time-consuming and expensive to collect such a vast training corpus. On the other hand, even MS-MARCO has limitations on commercial use and cannot be used in a wide range of real-world applications.",
            "score": 0.47546468594411734,
            "section_title": "RELATED WORK",
            "char_start_offset": 5760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1679
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 127,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 127,
                    "end": 150,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 249,
                    "end": 266,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 385,
                    "end": 406,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 552,
                    "end": 570,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 570,
                    "end": 593,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 693,
                    "end": 717,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 736,
                    "end": 752,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 767,
                    "end": 785,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 785,
                    "end": 809,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 841,
                    "end": 863,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 863,
                    "end": 882,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 882,
                    "end": 898,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 898,
                    "end": 917,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1340,
                    "end": 1359,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1359,
                    "end": 1375,
                    "matchedPaperCorpusId": "253157773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "252367671",
            "title": "Overview of the SV-Ident 2022 Shared Task on Survey Variable Identification in Social Science Publications",
            "text": "As described in Section 5, the baselines include BM25, SPARTA (henceforth, Sparse * ), and a multilingual sentence-transformer (henceforth, Dense * ). While we provide participants all the test sentences, we only evaluate performance on the subset of instances that contain variable mentions, as Task 1 already validates Variable Detection performance (this setup ignores false positive queries submitted by the participants). Unless explicitly stated, the following discussion mainly focuses on the MAP@10 scores. While the participant's system performs close to Dense * for English, Dense * scores twice as high for German. Sparse * outperforms all systems on English data. This is likely due to the system having been trained on a large English retrieval corpus.14 BM25 * and Sparse * perform worse on German. Lexical models, such as BM25 * , are prone to perform worse for languages that have many rare words, such as German, which allows compound nouns. Furthermore, because Sparse * is only specialized for English, it does not perform well for data in a different language. Overall, Dense * outperforms all systems by at least 0.5 points for English, except for Sparse * , and by at least around 10 points for German. \n\nAt the document-level, scores vary significantly (see Table 12 in the Appendix). Scores across different values of K improve as K increases. For dense documents (i.e., documents with a high ratio of variable mention sentences), scores increase significantly when going from k = 1 to k = 5, such as for the IDs 21357, 57204, and 66324. Furthermore, while some systems perform well on a document, others perform poorly. For example, the document with ID 66324 shows the lowest performance by all systems except for BM25 * , which has a score of 22.01 and is the second-highest document score for BM25 * . For 57561, BM25 * achieves only a score of 1.60, while all other systems score higher than 16. S-T5/GS outperforms all baselines only once and twice when compared to only Dense * . Such exceptions may be caused by a larger overlap between the tokens in the document and the underlying data used to train the models.",
            "score": 0.4754640072605142,
            "section_title": "Results",
            "char_start_offset": 23843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011157989501953125
        },
        {
            "paperId": "146f2bb630cdeca997786175093d1e3971a78783",
            "corpusId": 263887657,
            "title": "ConTextual Mask Auto-Encoder for Dense Passage Retrieval",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 43,
            "citationCount": 18,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2208.07670",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2208.07670?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2208.07670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155226596",
                    "name": "Xing Wu"
                },
                {
                    "authorId": "2068996632",
                    "name": "Guangyuan Ma"
                },
                {
                    "authorId": "2156805995",
                    "name": "Meng Lin"
                },
                {
                    "authorId": "2257134389",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2257404349",
                    "name": "Zhongyuan Wang"
                },
                {
                    "authorId": "2257376973",
                    "name": "Songlin Hu"
                }
            ],
            "abstract": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vec-tors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE em-ploys an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval bench-marks and show considerable improvements over strong baselines, demonstrating the high ef\ufb01ciency of CoT-MAE 1 .",
            "corpus_id": "263887657",
            "text": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vec-tors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE em-ploys an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval bench-marks and show considerable improvements over strong baselines, demonstrating the high ef\ufb01ciency of CoT-MAE 1 .",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1890869140625
        },
        {
            "paperId": "8c9372d5e3366131b0b5958688b41ca3d5b37f15",
            "corpusId": 265068306,
            "title": "Auto-Encoding Questions with Retrieval Augmented Decoding for Unsupervised Passage Retrieval and Zero-Shot Question Generation",
            "venue": "Recent Advances in Natural Language Processing",
            "year": 2023,
            "referenceCount": 26,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.26615/978-954-452-092-2_124",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.ranlp-1.124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3383477",
                    "name": "Stalin Varanasi"
                },
                {
                    "authorId": "2265759455",
                    "name": "Muhammad Umer Tariq Butt"
                },
                {
                    "authorId": "2265759262",
                    "name": "Guenter Neumann"
                }
            ],
            "abstract": "Dense passage retrieval models have become state-of-the-art for information retrieval on many Open-domain Question Answering (ODQA) datasets. However, most of these models rely on supervision obtained from the ODQA datasets, which hinders their performance in a low-resource setting. Recently, retrieval-augmented language models have been proposed to improve both zero-shot and supervised information retrieval. However, these models have pre-training tasks that are agnostic to the target task of passage retrieval. In this work, we propose Retrieval Augmented Auto-encoding of Questions for zero-shot dense information retrieval. Unlike other pre-training methods, our pre-training method is built for target information retrieval, thereby making the pre-training more efficient. Our method consists of a dense IR model for encoding questions and retrieving documents during training and a conditional language model that maximizes the question\u2019s likelihood by marginalizing over retrieved documents. As a by-product, we can use this conditional language model for zero-shot question generation from documents. We show that the IR model obtained through our method improves the current state-of-the-art of zero-shot dense information retrieval, and we improve the results even further by training on a synthetic corpus created by zero-shot question generation.",
            "corpus_id": "265068306",
            "text": "Dense passage retrieval models have become state-of-the-art for information retrieval on many Open-domain Question Answering (ODQA) datasets. However, most of these models rely on supervision obtained from the ODQA datasets, which hinders their performance in a low-resource setting. Recently, retrieval-augmented language models have been proposed to improve both zero-shot and supervised information retrieval. However, these models have pre-training tasks that are agnostic to the target task of passage retrieval. In this work, we propose Retrieval Augmented Auto-encoding of Questions for zero-shot dense information retrieval. Unlike other pre-training methods, our pre-training method is built for target information retrieval, thereby making the pre-training more efficient. Our method consists of a dense IR model for encoding questions and retrieving documents during training and a conditional language model that maximizes the question\u2019s likelihood by marginalizing over retrieved documents. As a by-product, we can use this conditional language model for zero-shot question generation from documents. We show that the IR model obtained through our method improves the current state-of-the-art of zero-shot dense information retrieval, and we improve the results even further by training on a synthetic corpus created by zero-shot question generation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09075927734375
        },
        {
            "paperId": "c340f52a424259c3e63ebb1a31b70f29bd9af69e",
            "corpusId": 261660680,
            "title": "CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market",
            "venue": "International Conference on Database and Expert Systems Applications",
            "year": 2023,
            "referenceCount": 19,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.04389",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.04389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135833834",
                    "name": "Jinyuan Wang"
                },
                {
                    "authorId": "2238902627",
                    "name": "Hai Zhao"
                },
                {
                    "authorId": "2238795570",
                    "name": "Zhong Wang"
                },
                {
                    "authorId": "2238903683",
                    "name": "Zeyang Zhu"
                },
                {
                    "authorId": "2238755749",
                    "name": "Jinhao Xie"
                },
                {
                    "authorId": "2238904673",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2238624599",
                    "name": "Yongjian Fei"
                },
                {
                    "authorId": "2238911695",
                    "name": "Yue Huang"
                },
                {
                    "authorId": "2238634538",
                    "name": "Dawei Cheng"
                }
            ],
            "abstract": "In recent years, great advances in pre-trained language models (PLMs) have sparked considerable research focus and achieved promising performance on the approach of dense passage retrieval, which aims at retrieving relative passages from massive corpus with given questions. However, most of existing datasets mainly benchmark the models with factoid queries of general commonsense, while specialised fields such as finance and economics remain unexplored due to the deficiency of large-scale and high-quality datasets with expert annotations. In this work, we propose a new task, policy retrieval, by introducing the Chinese Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages labeled by experienced experts with relevant articles from 10k+ entries in our collected Chinese policy corpus. Experiments on lexical, embedding and fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet also suggests ample potential for improvement. Our best performing baseline achieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on dev set.",
            "corpus_id": "261660680",
            "text": "In recent years, great advances in pre-trained language models (PLMs) have sparked considerable research focus and achieved promising performance on the approach of dense passage retrieval, which aims at retrieving relative passages from massive corpus with given questions. However, most of existing datasets mainly benchmark the models with factoid queries of general commonsense, while specialised fields such as finance and economics remain unexplored due to the deficiency of large-scale and high-quality datasets with expert annotations. In this work, we propose a new task, policy retrieval, by introducing the Chinese Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages labeled by experienced experts with relevant articles from 10k+ entries in our collected Chinese policy corpus. Experiments on lexical, embedding and fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet also suggests ample potential for improvement. Our best performing baseline achieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on dev set.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0273284912109375
        },
        {
            "paperId": "a1b876e5a02f2f2d39189c6d24da822bb36bfc3e",
            "corpusId": 271334005,
            "title": "Enhancing LLM's Cognition via Structurization",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 75,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.16434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283150812",
                    "name": "Kai Liu"
                },
                {
                    "authorId": "26910528",
                    "name": "Zhihang Fu"
                },
                {
                    "authorId": "2282974473",
                    "name": "Chao Chen"
                },
                {
                    "authorId": "2267067881",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "3115947",
                    "name": "Rongxin Jiang"
                },
                {
                    "authorId": "2114904538",
                    "name": "Fan Zhou"
                },
                {
                    "authorId": "2279072073",
                    "name": "Yao-Shen Chen"
                },
                {
                    "authorId": "2282998795",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "2283451411",
                    "name": "Jieping Ye"
                }
            ],
            "abstract": "When reading long-form text, human cognition is complex and structurized. While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively. To enhance LLM's cognition capability, this paper presents a novel concept of context structurization. Specifically, we transform the plain, unordered contextual sentences into well-ordered and hierarchically structurized elements. By doing so, LLMs can better grasp intricate and extended contexts through precise attention and information-seeking along the organized structures. Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). Empirical results show consistent and significant performance gains afforded by a single-round structurization. In particular, we boost the open-sourced LLaMA2-70B model to achieve comparable performance against GPT-3.5-Turbo as the hallucination evaluator. Besides, we show the feasibility of distilling advanced LLMs' language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach. Code is available at https://github.com/alibaba/struxgpt.",
            "corpus_id": "271334005",
            "text": "When reading long-form text, human cognition is complex and structurized. While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively. To enhance LLM's cognition capability, this paper presents a novel concept of context structurization. Specifically, we transform the plain, unordered contextual sentences into well-ordered and hierarchically structurized elements. By doing so, LLMs can better grasp intricate and extended contexts through precise attention and information-seeking along the organized structures. Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). Empirical results show consistent and significant performance gains afforded by a single-round structurization. In particular, we boost the open-sourced LLaMA2-70B model to achieve comparable performance against GPT-3.5-Turbo as the hallucination evaluator. Besides, we show the feasibility of distilling advanced LLMs' language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach. Code is available at https://github.com/alibaba/struxgpt.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.053192138671875
        }
    ],
    "quotes": {
        "cost": 0.071418,
        "quotes": [
            {
                "idx": 0,
                "key": "[251594591 | Wu et al. | 2022 | Citations: 26]",
                "snippets": "Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al., 2021)Humeau et al. 2019;Xiong et al. 2020;(Zhan et al., 2021)Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al., 2021)Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233231706 | Hofstatter et al. | 2021 | Citations: 402]": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                    "[233289894 | Zhan et al. | 2021 | Citations: 276]": "Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance.",
                    "[243865399 | Lu et al. | 2021 | Citations: 74]": "Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 629,
                        "end": 2121,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "233231706",
                            "233289894",
                            "243865399"
                        ],
                        "quote": "Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al., 2021)Humeau et al. 2019;Xiong et al. 2020;(Zhan et al., 2021)Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al., 2021)Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction"
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[253157959 | Long et al. | 2022 | Citations: 4]",
                "snippets": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.\n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.\n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[257985191 | Wu et al. | 2023 | Citations: 8]",
                "snippets": "PLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021)Humeau et al., 2019;Xiong et al., 2020;(Zhan et al., 2021)Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233231706 | Hofstatter et al. | 2021 | Citations: 402]": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                    "[233289894 | Zhan et al. | 2021 | Citations: 276]": "Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 133,
                        "end": 918,
                        "sentence_offsets": [
                            {
                                "start": 133,
                                "end": 388
                            },
                            {
                                "start": 389,
                                "end": 483
                            },
                            {
                                "start": 484,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 918
                            }
                        ],
                        "ref_mentions": [
                            "233231706",
                            "233289894"
                        ],
                        "quote": "PLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021)Humeau et al., 2019;Xiong et al., 2020;(Zhan et al., 2021)Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[258714822 | Ziems et al. | 2023 | Citations: 42]",
                "snippets": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better estimation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[220302658 | Khattab et al. | 2020 | Citations: 100]": "Abstract Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system attains state-of-the-art extractive OpenQA performance on all three datasets.",
                    "[249395549 | Wang et al. | 2022 | Citations: 146]": "Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1758,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 157
                            },
                            {
                                "start": 158,
                                "end": 385
                            },
                            {
                                "start": 386,
                                "end": 441
                            },
                            {
                                "start": 442,
                                "end": 680
                            },
                            {
                                "start": 681,
                                "end": 911
                            },
                            {
                                "start": 914,
                                "end": 1136
                            },
                            {
                                "start": 1137,
                                "end": 1295
                            },
                            {
                                "start": 1296,
                                "end": 1405
                            },
                            {
                                "start": 1406,
                                "end": 1603
                            },
                            {
                                "start": 1604,
                                "end": 1760
                            }
                        ],
                        "ref_mentions": [
                            "215737187",
                            "215737187",
                            "220302658",
                            "216553223",
                            "249395549"
                        ],
                        "quote": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better estimation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022)."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258833383 | Li et al. | 2023 | Citations: 3]",
                "snippets": "Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[258960666 | Yu et al. | 2023 | Citations: 69]",
                "snippets": "Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[267938301 | Yoon et al. | 2024 | Citations: 16]",
                "snippets": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212725651 | Nogueira et al. | 2020 | Citations: 584]": "This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \u201ctarget tokens\u201d, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model\u2019s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Generative Models for Reranking",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 927,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 674
                            },
                            {
                                "start": 675,
                                "end": 762
                            },
                            {
                                "start": 763,
                                "end": 817
                            },
                            {
                                "start": 818,
                                "end": 927
                            }
                        ],
                        "ref_mentions": [
                            "215737187",
                            "212725651"
                        ],
                        "quote": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[268031876 | Wang et al. | 2024 | Citations: 1]",
                "snippets": "Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020).\n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                    "[59604492 | Yang et al. | 2019 | Citations: 495]": "We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 154,
                        "end": 728,
                        "sentence_offsets": [
                            {
                                "start": 79,
                                "end": 212
                            },
                            {
                                "start": 213,
                                "end": 371
                            },
                            {
                                "start": 372,
                                "end": 593
                            },
                            {
                                "start": 596,
                                "end": 788
                            }
                        ],
                        "ref_mentions": [
                            "3618568",
                            "59604492"
                        ],
                        "quote": "Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020).\n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[268889861 | Abdi et al. | 2024 | Citations: 2]",
                "snippets": "The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,(Shen et al., 2014)(Yih et al., 2011). Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15]36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques (Lin et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222310837 | Lin et al. | 2020 | Citations: 627]": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
                        "pdf_hash": "",
                        "start": 348,
                        "end": 741,
                        "sentence_offsets": [
                            {
                                "start": 348,
                                "end": 462
                            },
                            {
                                "start": 463,
                                "end": 653
                            },
                            {
                                "start": 654,
                                "end": 741
                            }
                        ],
                        "ref_mentions": [
                            "207210855",
                            "1965270",
                            "222310837"
                        ],
                        "quote": "The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,(Shen et al., 2014)(Yih et al., 2011). Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15]36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques (Lin et al., 2020)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[269188036 | Huang et al. | 2024 | Citations: 51]",
                "snippets": "FiD-Light (Hofst\u00e4tter et al., 2022) employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252568176 | Hofstatter et al. | 2022 | Citations: 82]": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."
                },
                "metadata": [
                    {
                        "section_title": "Re-Ranking",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 381,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 381
                            }
                        ],
                        "ref_mentions": [
                            "252568176"
                        ],
                        "quote": "FiD-Light (Hofst\u00e4tter et al., 2022) employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.020196,
        "cot": "After examining the user query \"latest works on auto regressive LM dense passage retrieval their performance compared with bi directional encoders\", I need to cluster the quotes to create a comprehensive summary about autoregressive language models for dense passage retrieval, especially in comparison to bidirectional encoder approaches.\n\nFor this topic, I'll create the following dimensions:\n\n1. Introduction/Background - This section will provide foundational information about dense passage retrieval models in general. This should be in synthesis format to give the reader a coherent understanding of the context. I don't see quotes specifically dedicated to background information, but I'll include this dimension with an empty quotes list.\n\n2. Traditional Dense Retrieval Approaches - This section will focus on traditional dense retrieval methods that use bidirectional encoders (like DPR and dual-encoder architectures). This should be in synthesis format to clearly explain how these approaches work and their characteristics. Quotes 0, 1, 2, 3, 7, and 8 discuss traditional dense retrieval methods.\n\n3. Autoregressive Language Models for Retrieval - This section will focus on the latest works using autoregressive LMs for dense passage retrieval. This should be in synthesis format to provide a coherent explanation of how these models work. Quotes 3, 6, 7, and 9 discuss autoregressive approaches.\n\n4. Performance Comparison - This section will directly address the comparison between autoregressive LM retrieval and bidirectional encoder approaches. This should be in synthesis format to provide a clear analysis. Quotes 3 and 7 explicitly compare the approaches.\n\n5. Recent Innovations and Enhancements - This section will cover recent innovations in the field. This should be in list format to clearly enumerate the different approaches researchers are exploring. Quotes 0, 1, 2, 4, 5, and 6 mention various improvements and innovations.\n\nThese dimensions allow for a comprehensive coverage of the topic while ensuring all quotes are included and arranged in a logical sequence for the final summary.",
        "plan": {
            "Introduction/Background (synthesis)": [],
            "Traditional Dense Retrieval Approaches (synthesis)": [
                0,
                1,
                2,
                8
            ],
            "Autoregressive Language Models for Retrieval (synthesis)": [
                3,
                6,
                7,
                9
            ],
            "Performance Comparison (synthesis)": [
                3,
                7
            ],
            "Recent Innovations and Enhancements (list)": [
                0,
                1,
                2,
                4,
                5
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Dense passage retrieval has evolved from traditional bi-directional encoder-based methods to incorporating autoregressive language models. This shift represents a significant advancement in information retrieval, with various approaches showing different performance characteristics. (LLM Memory)",
                "text": "\nInformation retrieval systems have traditionally relied on bi-directional encoder architectures like BERT to create dense vector representations of text for matching queries with relevant documents. These bi-directional encoders process text in both directions simultaneously, allowing them to capture contextual relationships effectively. In recent years, however, the field has seen growing interest in using autoregressive language models (LMs) for retrieval tasks. Unlike bi-directional models, autoregressive LMs process text sequentially, predicting each token based on previous tokens, which enables them to generate coherent text but presents different challenges for retrieval tasks. The exploration of autoregressive models for retrieval is motivated by their impressive performance in natural language understanding and generation tasks, as well as their potential to unify various NLP tasks within a single framework. This shift has opened new avenues for research in dense passage retrieval, with different approaches exhibiting various tradeoffs in terms of effectiveness, efficiency, and computational requirements. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Traditional Dense Retrieval Approaches",
                "tldr": "Traditional dense retrieval systems typically employ dual-encoder architectures that transform queries and documents into vector representations, with the goal of calculating relevance through similarity metrics like cosine similarity or dot products. These systems have evolved through innovations like improved negative sampling strategies, knowledge distillation, and specialized pre-training objectives to enhance retrieval performance. (7 sources)",
                "text": "\nDense retrieval models represent a significant advancement over traditional term-matching methods like BM25, particularly in addressing the vocabulary mismatch problem. At their core, these systems use pre-trained language models with siamese or dual-encoder architectures to encode queries and documents into low-dimensional vector spaces, enabling effective semantic search <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233231706\" paperTitle=\"(Hofstatter et al., 2021)\" isShortName></Paper>. The relevance between queries and documents is calculated using cosine similarity or dot-product functions, making high-quality text representations crucial for performance <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nThe Dense Passage Retriever (DPR) model was a breakthrough that demonstrated dense retrieval could outperform traditional BM25 methods <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>. Since this breakthrough, researchers have pursued various strategies to enhance dense retrieval performance. One significant line of research focuses on improving the pre-training process specifically for retrieval tasks. Some approaches incorporate auxiliary self-supervised reconstruction tasks that force encoders to generate better text representations <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"243865399\" paperTitle=\"(Lu et al., 2021)\" isShortName></Paper>. Other methods have explored contextual-supervised learning and masked auto-encoding to improve the quality of dense vectors <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\nThe training methodology for dense retrievers has also received considerable attention. Effective negative sampling strategies have proven crucial for performance, with many systems employing hard negative sampling techniques <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"233289894\" paperTitle=\"(Zhan et al., 2021)\" isShortName></Paper>. Research has shown that while hard negative sampling outperforms random sampling, it also carries potential risks when implemented statically. This has led to the development of more sophisticated approaches like the Stable Training Algorithm for dense Retrieval (STAR) and the Algorithm for Directly Optimizing Ranking Performance (ADORE), which improve training stability and optimize ranking performance directly <Paper corpusId=\"233289894\" paperTitle=\"(Zhan et al., 2021)\" isShortName></Paper>.\n\nAdditional enhancement strategies include knowledge distillation from more powerful cross-encoder models, data augmentation techniques, and the development of tailored pre-trained models specifically designed for retrieval tasks <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>. These bi-directional encoder-based approaches have been widely adopted in real-world applications like open-domain question answering, conversational systems, and web search <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222310837\" paperTitle=\"(Lin et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wu et al., 2022)",
                        "snippets": [
                            "Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al., 2021)Humeau et al. 2019;Xiong et al. 2020;(Zhan et al., 2021)Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al., 2021)Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction"
                        ],
                        "paper": {
                            "corpus_id": 251594591,
                            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2155226596",
                                    "name": "Xing Wu"
                                },
                                {
                                    "authorId": "2068996632",
                                    "name": "Guangyuan Ma"
                                },
                                {
                                    "authorId": "2156805995",
                                    "name": "Meng Lin"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2135394423",
                                    "name": "Zhongyuan Wang"
                                },
                                {
                                    "authorId": "40845069",
                                    "name": "Songlin Hu"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 26
                        },
                        "score": 0.548828125
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "PLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021)Humeau et al., 2019;Xiong et al., 2020;(Zhan et al., 2021)Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances."
                        ],
                        "paper": {
                            "corpus_id": 257985191,
                            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2155226596",
                                    "name": "Xing Wu"
                                },
                                {
                                    "authorId": "2068996632",
                                    "name": "Guangyuan Ma"
                                },
                                {
                                    "authorId": "144282672",
                                    "name": "Peng Wang"
                                },
                                {
                                    "authorId": "2156805995",
                                    "name": "Meng Lin"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2642200",
                                    "name": "Fuzheng Zhang"
                                },
                                {
                                    "authorId": "40845069",
                                    "name": "Songlin Hu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.52099609375
                    },
                    {
                        "id": "(Hofstatter et al., 2021)",
                        "snippets": [
                            "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further."
                        ],
                        "paper": {
                            "corpus_id": 233231706,
                            "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling",
                            "authors": [
                                {
                                    "authorId": "97393346",
                                    "name": "Sebastian Hofst\u00e4tter"
                                },
                                {
                                    "authorId": "122045993",
                                    "name": "Sheng-Chieh Lin"
                                },
                                {
                                    "authorId": "2109723027",
                                    "name": "Jheng-Hong Yang"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1699657",
                                    "name": "A. Hanbury"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 402
                        },
                        "score": 0
                    },
                    {
                        "id": "(Long et al., 2022)",
                        "snippets": [
                            "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.\n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 253157959,
                            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "2107949588",
                                    "name": "Yanzhao Zhang"
                                },
                                {
                                    "authorId": "2149131512",
                                    "name": "Guangwei Xu"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.52783203125
                    },
                    {
                        "id": "(Lu et al., 2021)",
                        "snippets": [
                            "Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/."
                        ],
                        "paper": {
                            "corpus_id": 243865399,
                            "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
                            "authors": [
                                {
                                    "authorId": "1830381674",
                                    "name": "Shuqi Lu"
                                },
                                {
                                    "authorId": "1391126980",
                                    "name": "Di He"
                                },
                                {
                                    "authorId": "2139787803",
                                    "name": "Chenyan Xiong"
                                },
                                {
                                    "authorId": "35286545",
                                    "name": "Guolin Ke"
                                },
                                {
                                    "authorId": "2060300532",
                                    "name": "Waleed Malik"
                                },
                                {
                                    "authorId": "1897235",
                                    "name": "Zhicheng Dou"
                                },
                                {
                                    "authorId": "144609235",
                                    "name": "Paul N. Bennett"
                                },
                                {
                                    "authorId": "2110264337",
                                    "name": "Tie-Yan Liu"
                                },
                                {
                                    "authorId": "2734525",
                                    "name": "Arnold Overwijk"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 74
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhan et al., 2021)",
                        "snippets": [
                            "Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance."
                        ],
                        "paper": {
                            "corpus_id": 233289894,
                            "title": "Optimizing Dense Retrieval Model Training with Hard Negatives",
                            "authors": [
                                {
                                    "authorId": "1643961315",
                                    "name": "Jingtao Zhan"
                                },
                                {
                                    "authorId": "1644047628",
                                    "name": "Jiaxin Mao"
                                },
                                {
                                    "authorId": "1783406",
                                    "name": "Yiqun Liu"
                                },
                                {
                                    "authorId": "70414094",
                                    "name": "Jiafeng Guo"
                                },
                                {
                                    "authorId": "39767557",
                                    "name": "M. Zhang"
                                },
                                {
                                    "authorId": "8093158",
                                    "name": "Shaoping Ma"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 276
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2020)",
                        "snippets": [
                            "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly."
                        ],
                        "paper": {
                            "corpus_id": 222310837,
                            "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
                            "authors": [
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "143744603",
                                    "name": "Rodrigo Nogueira"
                                },
                                {
                                    "authorId": "144115896",
                                    "name": "Andrew Yates"
                                }
                            ],
                            "year": 2020,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 627
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Autoregressive Language Models for Retrieval",
                "tldr": "Autoregressive LMs address limitations of dual-encoder approaches by enabling deep token-level interactions between queries and documents, often generating document identifiers directly instead of computing similarity scores between embeddings. These models demonstrate strong generalization abilities and can effectively combine retrieval with reranking through sequence generation capabilities. (9 sources)",
                "text": "\nAutoregressive language models represent a significant shift from traditional dual-encoder approaches to dense passage retrieval. While dual-encoder systems like DPR independently encode queries and documents, they suffer from two key limitations: shallow interactions between query and document representations, and potential information loss when compressing documents into single dense vectors <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. To address these limitations, autoregressive search engines have emerged as a promising alternative approach.\n\nUnlike traditional retrievers that compute similarity between query and document embeddings, autoregressive models directly generate document identifiers which are then mapped to complete documents in a candidate pool <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. This approach has gained significant traction in information retrieval research due to several advantages. First, autoregressive generation models perform deep token-level cross-attention during the generation process, enabling more nuanced query-document interactions compared to the shallow interactions in dual-encoder models <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. Second, these models have demonstrated strong generalization capabilities, outperforming traditional BM25 methods even in zero-shot settings <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nThe retrieval-as-generation paradigm effectively addresses the bottleneck of limited interactions in dual-encoder models <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. In the reranking context, sequence generation approaches have shown particular promise by leveraging autoregressive capabilities to perform tasks such as listwise sorting or generating rationales for relevance decisions <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper>. Models that jointly process query and passage information at inference time have proven effective for zero-shot retrieval scenarios <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212725651\" paperTitle=\"(Nogueira et al., 2020)\" isShortName></Paper>.\n\nNotable implementations of this approach include FiD-Light, which employs a supervised learning method to re-rank passages effectively using source pointers during autoregressive text generation <Paper corpusId=\"269188036\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252568176\" paperTitle=\"(Hofstatter et al., 2022)\" isShortName></Paper>. This model uses a listwise auto-regressive re-ranking mechanism trained to identify and prioritize relevant passages based on the output generated during the text generation process <Paper corpusId=\"269188036\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. The Neural Corpus Indexer (NCI) represents another significant advancement, functioning as a sequence-to-sequence network that directly generates relevant document identifiers for queries, achieving substantial improvements in recall performance compared to traditional methods <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249395549\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ziems et al., 2023)",
                        "snippets": [
                            "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better estimation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 258714822,
                            "title": "Large Language Models are Built-in Autoregressive Search Engines",
                            "authors": [
                                {
                                    "authorId": "2264184691",
                                    "name": "Noah Ziems"
                                },
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                },
                                {
                                    "authorId": "72871419",
                                    "name": "Zhihan Zhang"
                                },
                                {
                                    "authorId": "2152153656",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0.54931640625
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020).\n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                        ],
                        "paper": {
                            "corpus_id": 268031876,
                            "title": "Generative Retrieval with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2185022832",
                                    "name": "Ye Wang"
                                },
                                {
                                    "authorId": "2290204960",
                                    "name": "Xinrun Xu"
                                },
                                {
                                    "authorId": "2143721734",
                                    "name": "Rui Xie"
                                },
                                {
                                    "authorId": "2288018918",
                                    "name": "Wenxin Hu"
                                },
                                {
                                    "authorId": "2052980435",
                                    "name": "Wei Ye"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.55419921875
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yoon et al., 2024)",
                        "snippets": [
                            "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful."
                        ],
                        "paper": {
                            "corpus_id": 267938301,
                            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
                            "authors": [
                                {
                                    "authorId": "2287336807",
                                    "name": "Soyoung Yoon"
                                },
                                {
                                    "authorId": "2287970016",
                                    "name": "Eunbi Choi"
                                },
                                {
                                    "authorId": "2287064006",
                                    "name": "Jiyeon Kim"
                                },
                                {
                                    "authorId": "2181032855",
                                    "name": "Yireun Kim"
                                },
                                {
                                    "authorId": "2286896884",
                                    "name": "Hyeongu Yun"
                                },
                                {
                                    "authorId": "2287694374",
                                    "name": "Seung-won Hwang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0.54296875
                    },
                    {
                        "id": "(Nogueira et al., 2020)",
                        "snippets": [
                            "This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \"target tokens\", and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model\u2019s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai."
                        ],
                        "paper": {
                            "corpus_id": 212725651,
                            "title": "Document Ranking with a Pretrained Sequence-to-Sequence Model",
                            "authors": [
                                {
                                    "authorId": "143744603",
                                    "name": "Rodrigo Nogueira"
                                },
                                {
                                    "authorId": "2197574931",
                                    "name": "Zhiying Jiang"
                                },
                                {
                                    "authorId": "1816753042",
                                    "name": "Ronak Pradeep"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                }
                            ],
                            "year": 2020,
                            "venue": "Findings",
                            "n_citations": 584
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "FiD-Light (Hofst\u00e4tter et al., 2022) employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process."
                        ],
                        "paper": {
                            "corpus_id": 269188036,
                            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2260272949",
                                    "name": "Yizheng Huang"
                                },
                                {
                                    "authorId": "2259653248",
                                    "name": "Jimmy X. Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 51
                        },
                        "score": 0.82275390625
                    },
                    {
                        "id": "(Hofstatter et al., 2022)",
                        "snippets": [
                            "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."
                        ],
                        "paper": {
                            "corpus_id": 252568176,
                            "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
                            "authors": [
                                {
                                    "authorId": "97393346",
                                    "name": "Sebastian Hofst\u00e4tter"
                                },
                                {
                                    "authorId": "2809410",
                                    "name": "Jiecao Chen"
                                },
                                {
                                    "authorId": "2062947723",
                                    "name": "K. Raman"
                                },
                                {
                                    "authorId": "2499986",
                                    "name": "Hamed Zamani"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 82
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method."
                        ],
                        "paper": {
                            "corpus_id": 249395549,
                            "title": "A Neural Corpus Indexer for Document Retrieval",
                            "authors": [
                                {
                                    "authorId": "2115659209",
                                    "name": "Yujing Wang"
                                },
                                {
                                    "authorId": "2149103804",
                                    "name": "Ying Hou"
                                },
                                {
                                    "authorId": "49528487",
                                    "name": "Hong Wang"
                                },
                                {
                                    "authorId": "40793591",
                                    "name": "Ziming Miao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Shibin Wu"
                                },
                                {
                                    "authorId": "2118180377",
                                    "name": "Hao Sun"
                                },
                                {
                                    "authorId": "1819450790",
                                    "name": "Qi Chen"
                                },
                                {
                                    "authorId": "2111274567",
                                    "name": "Yuqing Xia"
                                },
                                {
                                    "authorId": "2168109575",
                                    "name": "Chengmin Chi"
                                },
                                {
                                    "authorId": "2905509",
                                    "name": "Guoshuai Zhao"
                                },
                                {
                                    "authorId": "2145976175",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2110972758",
                                    "name": "Xing Xie"
                                },
                                {
                                    "authorId": "2118180377",
                                    "name": "Hao Sun"
                                },
                                {
                                    "authorId": "2066621592",
                                    "name": "Weiwei Deng"
                                },
                                {
                                    "authorId": null,
                                    "name": "Qi Zhang"
                                },
                                {
                                    "authorId": "2110611349",
                                    "name": "Mao Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 146
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparison",
                "tldr": "When comparing traditional bi-directional encoders with autoregressive language models for retrieval, autoregressive approaches demonstrate significant performance improvements by enabling deeper query-document interactions. These newer models show particular strength in generalization capabilities and zero-shot settings, addressing fundamental limitations of dual-encoder architectures. (6 sources)",
                "text": "\nRecent studies have highlighted the performance advantages of autoregressive language models over traditional bi-directional encoder approaches for retrieval tasks. The fundamental limitations of dual-encoder systems like DPR include shallow interactions between query and document representations and potential information loss when compressing documents into single dense vectors <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220302658\" paperTitle=\"(Khattab et al._1, 2020)\" isShortName></Paper>. These limitations create a performance ceiling that autoregressive approaches have been able to surpass.\n\nAutoregressive search engines have demonstrated strong generalization abilities, outperforming traditional BM25 methods even in zero-shot settings where models have not been specifically trained on the target domain <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. This is particularly significant as it suggests that autoregressive models can maintain effectiveness across diverse domains without requiring domain-specific fine-tuning.\n\nThe Neural Corpus Indexer (NCI) represents a concrete example of performance improvements, achieving +21.4% relative enhancement for Recall@1 on the NQ320k dataset and +16.8% improvement for R-Precision on the TriviaQA dataset compared to the best baseline methods <Paper corpusId=\"249395549\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. These substantial improvements highlight the effectiveness of the end-to-end retrieval paradigm that autoregressive models enable.\n\nThe performance gains can be largely attributed to the deeper token-level cross-attention mechanisms employed by autoregressive models, which allow for more nuanced query-document interactions compared to the shallow interactions in dual-encoder systems <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. By directly generating document identifiers rather than computing similarity between independently derived vector representations, autoregressive approaches effectively address the bottleneck of limited interactions in dual-encoder models <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nWhile autoregressive models demonstrate superior retrieval performance, it's worth noting that they typically require more computational resources during inference compared to dual-encoder approaches. However, the performance improvements, particularly in challenging retrieval scenarios, often justify this additional computational cost <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Ziems et al., 2023)",
                        "snippets": [
                            "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020).\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better estimation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 258714822,
                            "title": "Large Language Models are Built-in Autoregressive Search Engines",
                            "authors": [
                                {
                                    "authorId": "2264184691",
                                    "name": "Noah Ziems"
                                },
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                },
                                {
                                    "authorId": "72871419",
                                    "name": "Zhihan Zhang"
                                },
                                {
                                    "authorId": "2152153656",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0.54931640625
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khattab et al._1, 2020)",
                        "snippets": [
                            "Abstract Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system attains state-of-the-art extractive OpenQA performance on all three datasets."
                        ],
                        "paper": {
                            "corpus_id": 220302658,
                            "title": "Relevance-guided Supervision for OpenQA with ColBERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "144922861",
                                    "name": "Christopher Potts"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Current state-of-the-art document retrieval solutions mainly follow an index-retrieve paradigm, where the index is hard to be directly optimized for the final retrieval target. In this paper, we aim to show that an end-to-end deep neural network unifying training and indexing stages can significantly improve the recall performance of traditional methods. To this end, we propose Neural Corpus Indexer (NCI), a sequence-to-sequence network that generates relevant document identifiers directly for a designated query. To optimize the recall performance of NCI, we invent a prefix-aware weight-adaptive decoder architecture, and leverage tailored techniques including query generation, semantic document identifiers, and consistency-based regularization. Empirical studies demonstrated the superiority of NCI on two commonly used academic benchmarks, achieving +21.4% and +16.8% relative enhancement for Recall@1 on NQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to the best baseline method."
                        ],
                        "paper": {
                            "corpus_id": 249395549,
                            "title": "A Neural Corpus Indexer for Document Retrieval",
                            "authors": [
                                {
                                    "authorId": "2115659209",
                                    "name": "Yujing Wang"
                                },
                                {
                                    "authorId": "2149103804",
                                    "name": "Ying Hou"
                                },
                                {
                                    "authorId": "49528487",
                                    "name": "Hong Wang"
                                },
                                {
                                    "authorId": "40793591",
                                    "name": "Ziming Miao"
                                },
                                {
                                    "authorId": null,
                                    "name": "Shibin Wu"
                                },
                                {
                                    "authorId": "2118180377",
                                    "name": "Hao Sun"
                                },
                                {
                                    "authorId": "1819450790",
                                    "name": "Qi Chen"
                                },
                                {
                                    "authorId": "2111274567",
                                    "name": "Yuqing Xia"
                                },
                                {
                                    "authorId": "2168109575",
                                    "name": "Chengmin Chi"
                                },
                                {
                                    "authorId": "2905509",
                                    "name": "Guoshuai Zhao"
                                },
                                {
                                    "authorId": "2145976175",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2110972758",
                                    "name": "Xing Xie"
                                },
                                {
                                    "authorId": "2118180377",
                                    "name": "Hao Sun"
                                },
                                {
                                    "authorId": "2066621592",
                                    "name": "Weiwei Deng"
                                },
                                {
                                    "authorId": null,
                                    "name": "Qi Zhang"
                                },
                                {
                                    "authorId": "2110611349",
                                    "name": "Mao Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 146
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020).\n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                        ],
                        "paper": {
                            "corpus_id": 268031876,
                            "title": "Generative Retrieval with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2185022832",
                                    "name": "Ye Wang"
                                },
                                {
                                    "authorId": "2290204960",
                                    "name": "Xinrun Xu"
                                },
                                {
                                    "authorId": "2143721734",
                                    "name": "Rui Xie"
                                },
                                {
                                    "authorId": "2288018918",
                                    "name": "Wenxin Hu"
                                },
                                {
                                    "authorId": "2052980435",
                                    "name": "Wei Ye"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.55419921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Innovations and Enhancements",
                "tldr": "Recent innovations in dense passage retrieval include specialized pre-training strategies, balanced training algorithms, and retrieval augmentation frameworks that enable more effective query-document matching and better generalization capabilities. (7 sources)",
                "text": "\n* **Context-aware pre-training**: Researchers have developed specialized pre-training techniques for dense retrieval models that incorporate contextual information rather than focusing solely on single-text internal modeling. These approaches include contextual-supervised learning and contextual masked auto-encoding, which significantly improve dense retrieval performance by creating more robust text representations. <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\n* **Masked auto-encoder architectures**: Masked auto-encoder (MAE) pre-training has emerged as one of the most promising approaches for dense passage retrieval. These architectures enhance the text representation ability of encoders by creating more challenging reconstruction tasks for the decoder, effectively pushing the encoder to develop better semantic understanding capabilities. <Paper corpusId=\"258833383\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>\n\n* **Balanced training algorithms**: To address instability issues in dense retrieval training, researchers have developed algorithms like STAR (Stable Training Algorithm for dense Retrieval) that improve training stability by introducing random negatives alongside hard negatives. Complementary approaches like ADORE (Algorithm for Directly Optimizing Ranking Performance) replace static hard negative sampling with dynamic methods to directly optimize ranking performance. <Paper corpusId=\"251594591\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"233289894\" paperTitle=\"(Zhan et al., 2021)\" isShortName></Paper>\n\n* **Topic-aware sampling**: Efficient training techniques like TAS-Balanced (Topic-Aware query and balanced margin Sampling) have been developed to reduce computational requirements while maintaining performance. This approach clusters queries before training and samples from these clusters, enabling effective model training on consumer-grade hardware while still achieving competitive results. <Paper corpusId=\"257985191\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233231706\" paperTitle=\"(Hofstatter et al., 2021)\" isShortName></Paper>\n\n* **Generic retrieval plug-ins**: Moving beyond tightly coupled systems, researchers have begun exploring generic retrieval plug-in frameworks that can assist target language models without requiring joint fine-tuning. This approach allows retrievers to support various language models that may not be known in advance or cannot be fine-tuned together with the retriever. <Paper corpusId=\"258960666\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>\n\n* **Cross-domain applications**: Dense passage retrieval techniques have been successfully applied across multiple domains, including open-domain question answering, conversational systems, and web search, demonstrating their versatility and effectiveness in diverse information retrieval scenarios. <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Wu et al., 2022)",
                        "snippets": [
                            "Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al., 2021)Humeau et al. 2019;Xiong et al. 2020;(Zhan et al., 2021)Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al., 2021)Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction"
                        ],
                        "paper": {
                            "corpus_id": 251594591,
                            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2155226596",
                                    "name": "Xing Wu"
                                },
                                {
                                    "authorId": "2068996632",
                                    "name": "Guangyuan Ma"
                                },
                                {
                                    "authorId": "2156805995",
                                    "name": "Meng Lin"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2135394423",
                                    "name": "Zhongyuan Wang"
                                },
                                {
                                    "authorId": "40845069",
                                    "name": "Songlin Hu"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 26
                        },
                        "score": 0.548828125
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "PLM-based dense retrieval typically employs a siamese or dual-encoder architecture to convert queries and documents into a low-dimensional vector space (Hofst\u00e4tter et al., 2021)Humeau et al., 2019;Xiong et al., 2020;(Zhan et al., 2021)Zhan et al., , 2020)). Relevance between queries and documents is calculated using cosine similarity or dot products. This low-dimension vector is called the dense vector and is trained to capture the sentence semantics at the passage level. Recent efforts aim to improve dense retrieval performance by adding auxiliary tasks to pre-training, like contextual-supervised learning (Gao and Callan, 2021b;Wu et al., 2022  ( Wu et al., 2022), expands on this by incorporating contextual masked auto-encoding, leading to better dense retrieval performances."
                        ],
                        "paper": {
                            "corpus_id": 257985191,
                            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2155226596",
                                    "name": "Xing Wu"
                                },
                                {
                                    "authorId": "2068996632",
                                    "name": "Guangyuan Ma"
                                },
                                {
                                    "authorId": "144282672",
                                    "name": "Peng Wang"
                                },
                                {
                                    "authorId": "2156805995",
                                    "name": "Meng Lin"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2642200",
                                    "name": "Fuzheng Zhang"
                                },
                                {
                                    "authorId": "40845069",
                                    "name": "Songlin Hu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.52099609375
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability."
                        ],
                        "paper": {
                            "corpus_id": 258833383,
                            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2109967721",
                                    "name": "Zehan Li"
                                },
                                {
                                    "authorId": "2107949588",
                                    "name": "Yanzhao Zhang"
                                },
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.60302734375
                    },
                    {
                        "id": "(Zhan et al., 2021)",
                        "snippets": [
                            "Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance."
                        ],
                        "paper": {
                            "corpus_id": 233289894,
                            "title": "Optimizing Dense Retrieval Model Training with Hard Negatives",
                            "authors": [
                                {
                                    "authorId": "1643961315",
                                    "name": "Jingtao Zhan"
                                },
                                {
                                    "authorId": "1644047628",
                                    "name": "Jiaxin Mao"
                                },
                                {
                                    "authorId": "1783406",
                                    "name": "Yiqun Liu"
                                },
                                {
                                    "authorId": "70414094",
                                    "name": "Jiafeng Guo"
                                },
                                {
                                    "authorId": "39767557",
                                    "name": "M. Zhang"
                                },
                                {
                                    "authorId": "8093158",
                                    "name": "Shaoping Ma"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 276
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hofstatter et al., 2021)",
                        "snippets": [
                            "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further."
                        ],
                        "paper": {
                            "corpus_id": 233231706,
                            "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling",
                            "authors": [
                                {
                                    "authorId": "97393346",
                                    "name": "Sebastian Hofst\u00e4tter"
                                },
                                {
                                    "authorId": "122045993",
                                    "name": "Sheng-Chieh Lin"
                                },
                                {
                                    "authorId": "2109723027",
                                    "name": "Jheng-Hong Yang"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1699657",
                                    "name": "A. Hanbury"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 402
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together."
                        ],
                        "paper": {
                            "corpus_id": 258960666,
                            "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
                            "authors": [
                                {
                                    "authorId": "2275526493",
                                    "name": "Zichun Yu"
                                },
                                {
                                    "authorId": "2139787803",
                                    "name": "Chenyan Xiong"
                                },
                                {
                                    "authorId": "150311558",
                                    "name": "S. Yu"
                                },
                                {
                                    "authorId": "2109232579",
                                    "name": "Zhiyuan Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 69
                        },
                        "score": 0.521484375
                    },
                    {
                        "id": "(Long et al., 2022)",
                        "snippets": [
                            "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.\n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 253157959,
                            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "2107949588",
                                    "name": "Yanzhao Zhang"
                                },
                                {
                                    "authorId": "2149131512",
                                    "name": "Guangwei Xu"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.52783203125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.10956600000000001
    }
}