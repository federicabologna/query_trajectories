{
    "query": "What are the most effective methods for improving language model calibration across different knowledge domains, and how do techniques such as multicalibration and few-shot recalibration address domain-specific miscalibration?",
    "user_id": "lib_user",
    "task_id": "19be7c19-642d-4bd1-9a93-fe52db2b303f",
    "timestamp": "2025-06-24T00:43:41.169081",
    "n_retrieval": 256,
    "n_retrieved": 252,
    "n_candidates": 37,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.315171,
    "decomposed_query": {
        "rewritten_query": "Effective methods for improving language model calibration across different knowledge domains, and how techniques such as multicalibration and few-shot recalibration address domain-specific miscalibration.",
        "keyword_query": "language model calibration knowledge domains multicalibration few-shot recalibration domain-specific miscalibration",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010233,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Few-Shot Recalibration of Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.18286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293910776",
                    "name": "Xiang Lisa Li"
                },
                {
                    "authorId": "3030219",
                    "name": "Urvashi Khandelwal"
                },
                {
                    "authorId": "2091768",
                    "name": "Kelvin Guu"
                }
            ],
            "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "corpus_id": 268723623,
            "sentences": [
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "We have shown that while LMs appear to be wellcalibrated on broad distributions, they remain miscalibrated for meaningful slices of that broader distribution.To recalibrate them for each slice, we propose few-shot recalibration which takes fewshot, unlabeled queries and predicts a slice-specific precision curve.We then use the predicted precision curve for two downstream calibration tasks, finding that our approach consistently outperforms existing recalibration methods under all evaluation settings.Future work should study few-shot recalibration for natural language generation tasks, to steer model generated text to be more or less conservative, as well as apply this approach to a broader set of models, including instruction-tuned and RLHF models, and multimodal settings.",
                    "score": 0.6212202065180632,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 26584,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 158,
                            "end": 313
                        },
                        {
                            "start": 313,
                            "end": 505
                        },
                        {
                            "start": 505,
                            "end": 783
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                },
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
                    "score": 0.6452777774061442,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97412109375
                },
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct).This precision curve can be used to achieve many downstream goals.For instance, we can identify the con-Figure 1: An example of the illusion of LM calibration.For a combination of five domains, the model is wellcalibrated with a calibration error of 0.02 (the first plot).However, the same model is miscalibrated on the the five individual domains, each with a higher calibration error. 2   fidence threshold that achieves a minimum level of precision, to control the LM's error rate for this slice.We can also transform the precision curve into the corresponding calibration curve and reduce calibration error on this slice ( \u00a73.1).\n\nTo train our few-shot recalibration model, we employ a synthetic data generation strategy: given a corpus of labeled examples that have been partitioned into different domains, we can synthetically construct many different slices by taking different weighted mixtures of these domains.For example, 80% abstract algebra and 20% virology from MMLU ( \u00a73.2).Since we're working with labeled data, we can directly compute an LM's ground-truth precision curve on that slice.Then, we train our recalibration model to predict this ground-truth precision curve, when only given access to a random sample of unlabeled examples from that slice ( \u00a73.3).At inference time, our trained recalibrator generalizes to previously unseen slices, and only requires unlabeled data.",
                    "score": 0.5574314447582998,
                    "section_title": "Introduction",
                    "char_start_offset": 1618,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 229,
                            "end": 530
                        },
                        {
                            "start": 530,
                            "end": 596
                        },
                        {
                            "start": 596,
                            "end": 689
                        },
                        {
                            "start": 689,
                            "end": 802
                        },
                        {
                            "start": 802,
                            "end": 1029
                        },
                        {
                            "start": 1029,
                            "end": 1163
                        },
                        {
                            "start": 1165,
                            "end": 1450
                        },
                        {
                            "start": 1450,
                            "end": 1519
                        },
                        {
                            "start": 1519,
                            "end": 1633
                        },
                        {
                            "start": 1633,
                            "end": 1806
                        },
                        {
                            "start": 1806,
                            "end": 1924
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                },
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Researchers often study LM calibration for aggregate query distributions (p), which are often composed of mixtures of meaningful finer-grained distributions: p(x) = d\u2208D \u03b1 d p d (x), where D denotes a set of domains, and each p d denotes the distribution of domain d, with relative frequency \u03b1 d .For instance, OpenAI (2023) and Kadavath et al. (2022) have reported LM calibration on MMLU, which consists of 57 individual domains like abstract algebra, high school chemistry etc.However, in practice, users querying an LM at a given point rarely sample from a broad aggregate distribution.They are more likely to sample from meaningful slices, like queries from abstract algebra alone.Yu et al. (2022);Hebert-Johnson et al. (2018) have shown that individual domains often suffer from miscalibration problem even if the aggregate distribution appears well-calibrated.\n\nTo demonstrate the same phenomenon for language models, we measure calibration of LLaMA-65B on combined MMLU (p), and also on each domain separately.As expected, the model is wellcalibrated on p.However, the LM is significantly miscalibrated for most domains.This is shown in (Figure 2) where the aggregate ECE is lower than that of most domains.It appears that the miscalibration problem is hidden for the broader distribution because overconfidence in some domains cancels out underconfidence in others.Figure 1 shows a qualitative example to illustrate the same miscalibration issue.These results show that LMs are not well-calibrated for meaningful slices of a broad distribution, leading us to address the problem via few-shot, slice-specific recalibration.\n\n3 Slice-Specific Few-Shot Recalibration Since individual fine-grained slices may be miscalibrated, we propose to recalibrate each slice.Intuitively, given a few samples from a slice, we can infer the rough identity of that slice, and then appropriately adjust the LM's confidences based on the LM's familiarity with the slice.",
                    "score": 0.6124887431855275,
                    "section_title": "Miscalibration on Slices of Distributions",
                    "char_start_offset": 6884,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 296
                        },
                        {
                            "start": 296,
                            "end": 478
                        },
                        {
                            "start": 478,
                            "end": 588
                        },
                        {
                            "start": 588,
                            "end": 684
                        },
                        {
                            "start": 684,
                            "end": 865
                        },
                        {
                            "start": 867,
                            "end": 1016
                        },
                        {
                            "start": 1016,
                            "end": 1062
                        },
                        {
                            "start": 1062,
                            "end": 1126
                        },
                        {
                            "start": 1126,
                            "end": 1213
                        },
                        {
                            "start": 1213,
                            "end": 1372
                        },
                        {
                            "start": 1372,
                            "end": 1453
                        },
                        {
                            "start": 1453,
                            "end": 1629
                        },
                        {
                            "start": 1631,
                            "end": 1767
                        },
                        {
                            "start": 1767,
                            "end": 1957
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 701,
                            "end": 729,
                            "matchedPaperCorpusId": "51880858"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                },
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Knowing when to trust a model's predictions is typically mapped to the concept of calibration, where the model's confidence estimate for a prediction reflects how likely it is to be correct.Language models (LMs) have recently been shown to be wellcalibrated in a number of settings (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023;Ope-nAI, 2023).However, while models can be wellcalibrated for aggregate distributions (e.g.mixtures of a number of domains), they can be significantly miscalibrated for narrower domains within that distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).\n\nFor instance, Figure 1 shows an LM that is wellcalibrated on the combined distribution of five domains, achieving near perfect calibration curve with low expected calibration error (ECE).However, curves for the individual domains appear significantly miscalibrated in comparison, with the least calibrated domain virology having a 250% higher calibration error.This miscalibration problem is hidden for the combined distribution because overconfidence in some domains cancels out underconfidence in others.\n\nThis illustrates a key problem: LMs are not well-calibrated for meaningful slices of broader distributions.This is particularly relevant in practice where users querying an LM rarely sample from a broad combination of distributions at any given time, and are more likely to sample from slices like abstract algebra or virology.Our goal is to recalibrate LMs for each of these fine-grained slices of a distribution, thereby allowing users to reliably determine when predictions can be trusted.\n\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.",
                    "score": 0.5502629764809013,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 190,
                            "end": 356
                        },
                        {
                            "start": 356,
                            "end": 433
                        },
                        {
                            "start": 433,
                            "end": 599
                        },
                        {
                            "start": 601,
                            "end": 788
                        },
                        {
                            "start": 788,
                            "end": 962
                        },
                        {
                            "start": 962,
                            "end": 1107
                        },
                        {
                            "start": 1109,
                            "end": 1216
                        },
                        {
                            "start": 1216,
                            "end": 1436
                        },
                        {
                            "start": 1436,
                            "end": 1601
                        },
                        {
                            "start": 1603,
                            "end": 1832
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 323,
                            "matchedPaperCorpusId": "247613322"
                        },
                        {
                            "start": 323,
                            "end": 341,
                            "matchedPaperCorpusId": "257039062"
                        },
                        {
                            "start": 570,
                            "end": 598,
                            "matchedPaperCorpusId": "51880858"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                },
                {
                    "corpus_id": "268723623",
                    "title": "Few-Shot Recalibration of Language Models",
                    "text": "Calibration is a key tool for knowing when language model predictions can be trusted and when they should abstain or defer to experts.However, calibration on an individual domain can be much worse than the aggregate data distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).In this paper, we show that large language models suffer from the same calibration failure.While LMs appear to be well-calibrated on average, they are significantly miscalibrated in finer-grained settings.\n\nWe study LM calibration for multiclass classification: let x be an input query drawn from some query distribution p(x) and y \u2208 {1, \u2022 \u2022 \u2022 , K} be the output class.Let p LM (y | x) denote the model probability, which is also the model's confidence.Let \u0177 = arg max y p LM (y | x) be the model's prediction, and y * be the ground truth label.",
                    "score": 0.4310919816026011,
                    "section_title": "The Illusion of LM Calibration",
                    "char_start_offset": 4586,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 134,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 371
                        },
                        {
                            "start": 371,
                            "end": 485
                        },
                        {
                            "start": 487,
                            "end": 649
                        },
                        {
                            "start": 649,
                            "end": 733
                        },
                        {
                            "start": 733,
                            "end": 825
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 251,
                            "end": 279,
                            "matchedPaperCorpusId": "51880858"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Few-Shot Recalibration of Language Models\n# Venue: arXiv.org\n# Authors: Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu\n## Abstract\nRecent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n## Introduction\nKnowing when to trust a model's predictions is typically mapped to the concept of calibration, where the model's confidence estimate for a prediction reflects how likely it is to be correct.Language models (LMs) have recently been shown to be wellcalibrated in a number of settings (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023;Ope-nAI, 2023).However, while models can be wellcalibrated for aggregate distributions (e.g.mixtures of a number of domains), they can be significantly miscalibrated for narrower domains within that distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).\n\nFor instance, Figure 1 shows an LM that is wellcalibrated on the combined distribution of five domains, achieving near perfect calibration curve with low expected calibration error (ECE).However, curves for the individual domains appear significantly miscalibrated in comparison, with the least calibrated domain virology having a 250% higher calibration error.This miscalibration problem is hidden for the combined distribution because overconfidence in some domains cancels out underconfidence in others.\n\nThis illustrates a key problem: LMs are not well-calibrated for meaningful slices of broader distributions.This is particularly relevant in practice where users querying an LM rarely sample from a broad combination of distributions at any given time, and are more likely to sample from slices like abstract algebra or virology.Our goal is to recalibrate LMs for each of these fine-grained slices of a distribution, thereby allowing users to reliably determine when predictions can be trusted.\n\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.\n...\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct).This precision curve can be used to achieve many downstream goals.For instance, we can identify the con-Figure 1: An example of the illusion of LM calibration.For a combination of five domains, the model is wellcalibrated with a calibration error of 0.02 (the first plot).However, the same model is miscalibrated on the the five individual domains, each with a higher calibration error. 2   fidence threshold that achieves a minimum level of precision, to control the LM's error rate for this slice.We can also transform the precision curve into the corresponding calibration curve and reduce calibration error on this slice ( \u00a73.1).\n\nTo train our few-shot recalibration model, we employ a synthetic data generation strategy: given a corpus of labeled examples that have been partitioned into different domains, we can synthetically construct many different slices by taking different weighted mixtures of these domains.For example, 80% abstract algebra and 20% virology from MMLU ( \u00a73.2).Since we're working with labeled data, we can directly compute an LM's ground-truth precision curve on that slice.Then, we train our recalibration model to predict this ground-truth precision curve, when only given access to a random sample of unlabeled examples from that slice ( \u00a73.3).At inference time, our trained recalibrator generalizes to previously unseen slices, and only requires unlabeled data.\n\n## The Illusion of LM Calibration\nCalibration is a key tool for knowing when language model predictions can be trusted and when they should abstain or defer to experts.However, calibration on an individual domain can be much worse than the aggregate data distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).In this paper, we show that large language models suffer from the same calibration failure.While LMs appear to be well-calibrated on average, they are significantly miscalibrated in finer-grained settings.\n\nWe study LM calibration for multiclass classification: let x be an input query drawn from some query distribution p(x) and y \u2208 {1, \u2022 \u2022 \u2022 , K} be the output class.Let p LM (y | x) denote the model probability, which is also the model's confidence.Let \u0177 = arg max y p LM (y | x) be the model's prediction, and y * be the ground truth label.\n\n## Miscalibration on Slices of Distributions\nResearchers often study LM calibration for aggregate query distributions (p), which are often composed of mixtures of meaningful finer-grained distributions: p(x) = d\u2208D \u03b1 d p d (x), where D denotes a set of domains, and each p d denotes the distribution of domain d, with relative frequency \u03b1 d .For instance, OpenAI (2023) and Kadavath et al. (2022) have reported LM calibration on MMLU, which consists of 57 individual domains like abstract algebra, high school chemistry etc.However, in practice, users querying an LM at a given point rarely sample from a broad aggregate distribution.They are more likely to sample from meaningful slices, like queries from abstract algebra alone.Yu et al. (2022);Hebert-Johnson et al. (2018) have shown that individual domains often suffer from miscalibration problem even if the aggregate distribution appears well-calibrated.\n\nTo demonstrate the same phenomenon for language models, we measure calibration of LLaMA-65B on combined MMLU (p), and also on each domain separately.As expected, the model is wellcalibrated on p.However, the LM is significantly miscalibrated for most domains.This is shown in (Figure 2) where the aggregate ECE is lower than that of most domains.It appears that the miscalibration problem is hidden for the broader distribution because overconfidence in some domains cancels out underconfidence in others.Figure 1 shows a qualitative example to illustrate the same miscalibration issue.These results show that LMs are not well-calibrated for meaningful slices of a broad distribution, leading us to address the problem via few-shot, slice-specific recalibration.\n\n3 Slice-Specific Few-Shot Recalibration Since individual fine-grained slices may be miscalibrated, we propose to recalibrate each slice.Intuitively, given a few samples from a slice, we can infer the rough identity of that slice, and then appropriately adjust the LM's confidences based on the LM's familiarity with the slice.\n\n## Conclusion and Future Work\nWe have shown that while LMs appear to be wellcalibrated on broad distributions, they remain miscalibrated for meaningful slices of that broader distribution.To recalibrate them for each slice, we propose few-shot recalibration which takes fewshot, unlabeled queries and predicts a slice-specific precision curve.We then use the predicted precision curve for two downstream calibration tasks, finding that our approach consistently outperforms existing recalibration methods under all evaluation settings.Future work should study few-shot recalibration for natural language generation tasks, to steer model generated text to be more or less conservative, as well as apply this approach to a broader set of models, including instruction-tuned and RLHF models, and multimodal settings.",
            "reference_string": "[268723623 | Li et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 69,
            "citation_count": 19,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295667267",
                    "name": "Gianluca Detommaso"
                },
                {
                    "authorId": "2295665717",
                    "name": "Martin Bertran"
                },
                {
                    "authorId": "2295664744",
                    "name": "Riccardo Fogliato"
                },
                {
                    "authorId": "2295665299",
                    "name": "Aaron Roth"
                }
            ],
            "abstract": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "corpus_id": 269004786,
            "sentences": [
                {
                    "corpus_id": "269004786",
                    "title": "Multicalibration for Confidence Scoring in LLMs",
                    "text": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
                    "score": 0.5098246691029601,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Multicalibration for Confidence Scoring in LLMs\n# Venue: International Conference on Machine Learning\n# Authors: Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth\n## Abstract\nThis paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.\n",
            "reference_string": "[269004786 | Detommaso et al. | 2024 | Citations: 19]"
        },
        {
            "title": "Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 49,
            "citation_count": 24,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.250.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.13003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "121899912",
                    "name": "Chengyu Wang"
                },
                {
                    "authorId": "2642333",
                    "name": "Minghui Qiu"
                },
                {
                    "authorId": "2078113",
                    "name": "Jun Huang"
                },
                {
                    "authorId": "143644849",
                    "name": "Xiaofeng He"
                }
            ],
            "abstract": "Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), served as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.",
            "corpus_id": 214714416,
            "sentences": [
                {
                    "corpus_id": "214714416",
                    "title": "Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining",
                    "text": "Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), served as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.",
                    "score": 0.4109032490960611,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Chengyu Wang, Minghui Qiu, Jun Huang, Xiaofeng He\n## Abstract\nPre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), served as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.\n",
            "reference_string": "[214714416 | Wang et al. | 2020 | Citations: 24]"
        },
        {
            "title": "On the Calibration of Large Language Models and Alignment",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 44,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2232117601",
                    "name": "Chiwei Zhu"
                },
                {
                    "authorId": "1754285124",
                    "name": "Benfeng Xu"
                },
                {
                    "authorId": "2261394849",
                    "name": "Quan Wang"
                },
                {
                    "authorId": "2261394984",
                    "name": "Yongdong Zhang"
                },
                {
                    "authorId": "2261422765",
                    "name": "Zhendong Mao"
                }
            ],
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "corpus_id": 265351565,
            "sentences": [
                {
                    "corpus_id": "265351565",
                    "title": "On the Calibration of Large Language Models and Alignment",
                    "text": "\u2022 Larger Parameter Scales : Improve models' calibration. \n\n\u2022 Longer Training Dynamics : Also benefit calibration accuracy. \n\nFor alignment of LLMs: \n\n\u2022 Instruction Tuning : Deteriorates models' calibration. \n\n\u2022 Synthetic Data : Exacerbates the harmful effect of instruction tuning. \n\n\u2022 Parameter-efficient Fine-tuning : Effective regularization for restraining calibration error. \n\n\u2022 RLHF : Help maintaining calibration accuracy. \n\nFor different tasks: \n\n\u2022 In pre-training: Improvement in calibration accuracy is more significant on fact generation task or language understanding tasks than language modeling task. \n\n\u2022 In alignment training: Calibration accuracy evolves consistently across different downstream tasks including fact generation, language understanding or vanilla language modeling. \n\nWe believe these conclusions as well as detailed experiments can take us a step further towards understanding large language models, especially the intrinsic mechanism of their calibration behaviour. Our experimental results also provide us with some possible solutions to improve calibration, including increasing model scales and employing parameter efficient tuning methods. Besides, diversity guided instruction data construction may also be very promising. Hopefully these findings can shed light on future works to construct more factual and trustworthy assistants.",
                    "score": 0.46644915669159726,
                    "section_title": "For pretraining of LLMs:",
                    "char_start_offset": 4386,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 56
                        },
                        {
                            "start": 59,
                            "end": 122
                        },
                        {
                            "start": 125,
                            "end": 147
                        },
                        {
                            "start": 150,
                            "end": 206
                        },
                        {
                            "start": 209,
                            "end": 281
                        },
                        {
                            "start": 284,
                            "end": 379
                        },
                        {
                            "start": 382,
                            "end": 429
                        },
                        {
                            "start": 432,
                            "end": 452
                        },
                        {
                            "start": 455,
                            "end": 614
                        },
                        {
                            "start": 617,
                            "end": 797
                        },
                        {
                            "start": 800,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1371
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: On the Calibration of Large Language Models and Alignment\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, Zhendong Mao\n## Abstract\nAs large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.\n## For pretraining of LLMs:\n\u2022 Larger Parameter Scales : Improve models' calibration. \n\n\u2022 Longer Training Dynamics : Also benefit calibration accuracy. \n\nFor alignment of LLMs: \n\n\u2022 Instruction Tuning : Deteriorates models' calibration. \n\n\u2022 Synthetic Data : Exacerbates the harmful effect of instruction tuning. \n\n\u2022 Parameter-efficient Fine-tuning : Effective regularization for restraining calibration error. \n\n\u2022 RLHF : Help maintaining calibration accuracy. \n\nFor different tasks: \n\n\u2022 In pre-training: Improvement in calibration accuracy is more significant on fact generation task or language understanding tasks than language modeling task. \n\n\u2022 In alignment training: Calibration accuracy evolves consistently across different downstream tasks including fact generation, language understanding or vanilla language modeling. \n\nWe believe these conclusions as well as detailed experiments can take us a step further towards understanding large language models, especially the intrinsic mechanism of their calibration behaviour. Our experimental results also provide us with some possible solutions to improve calibration, including increasing model scales and employing parameter efficient tuning methods. Besides, diversity guided instruction data construction may also be very promising. Hopefully these findings can shed light on future works to construct more factual and trustworthy assistants.",
            "reference_string": "[265351565 | Zhu et al. | 2023 | Citations: 44]"
        },
        {
            "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 79,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327839292",
                    "name": "Jiancong Xiao"
                },
                {
                    "authorId": "2248244711",
                    "name": "Bojian Hou"
                },
                {
                    "authorId": "2359711807",
                    "name": "Zhanliang Wang"
                },
                {
                    "authorId": "2310437466",
                    "name": "Ruochen Jin"
                },
                {
                    "authorId": "2304385765",
                    "name": "Qi Long"
                },
                {
                    "authorId": "2311908363",
                    "name": "Weijie J. Su"
                },
                {
                    "authorId": "2248152254",
                    "name": "Li Shen"
                }
            ],
            "abstract": "One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.",
            "corpus_id": 278327247,
            "sentences": [
                {
                    "corpus_id": "278327247",
                    "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
                    "text": "In this work, we have investigated the critical issue of calibration degradation in LLMs following preference alignment procedures. To address this, we introduced a theoretical framework distinguishing between calibratable and non-calibratable regimes, and developed practical solutions through calibration-aware fine-tuning approaches. Our experimental results across multiple models demonstrate that our methods can significantly improve calibration while maintaining or enhancing model performance. Future work could explore extending these methods to other types of language tasks and investigating the relationship between calibration and other aspects of model reliability.",
                    "score": 0.4185848365485682,
                    "section_title": "Conclusion",
                    "char_start_offset": 24913,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 679
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                },
                {
                    "corpus_id": "278327247",
                    "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
                    "text": "One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.",
                    "score": 0.4268795056642635,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                },
                {
                    "corpus_id": "278327247",
                    "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
                    "text": "Large Language Models (LLMs) (OpenAI, 2023;Anthropic, 2024) have emerged as powerful tools for a wide range of natural language processing tasks (Bubeck et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023;Team et al., 2023). These models, built upon the Transformer architecture (Vaswani et al., 2017), have demonstrated remarkable abilities to process and generate human-like text, making them increasingly integral to various applications. A crucial development in making LLMs more reliable and aligned with human values is preference alignment techniques, particularly Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023). However, an important side effect of preference alignment is its impact on model calibration-the relationship between a model's predicted probabilities and its actual accuracy. While pretrained LLMs typically demonstrate good calibration properties, preference-aligned models become poorly calibrated, as initially observed in GPT-4 with RLHF (OpenAI, 2023). Our investigation reveals that this is a universal issue across different models aligned with various alignment methods. An example is shown in Figure 1 (left), where the calibration performance of a model aligned by DPO illustrates a poor calibration performance. \n\nUnderstanding and addressing this calibration issue is crucial. First, well-calibrated prediction is essential for reliable decision-making in real-world applications, particularly in high-stakes domains such as legal or healthcare analysis (Savage et al., 2025). Second, overconfident models may mislead users about their capabilities and limitations, potentially leading to inappropriate reliance on model outputs. \n\nIn this paper, we conduct a systematic investigation into two fundamental questions: (1) Why does preference alignment affect calibration? and (2) How can we effectively restore calibration while maintaining the benefits of alignment?",
                    "score": 0.422110321241095,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 891
                        },
                        {
                            "start": 892,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1338
                        },
                        {
                            "start": 1341,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1757
                        },
                        {
                            "start": 1760,
                            "end": 1898
                        },
                        {
                            "start": 1899,
                            "end": 1994
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 43,
                            "end": 59,
                            "matchedPaperCorpusId": "268232499"
                        },
                        {
                            "start": 284,
                            "end": 306,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1582,
                            "end": 1603,
                            "matchedPaperCorpusId": "273322711"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach\n# Venue: arXiv.org\n# Authors: Jiancong Xiao, Bojian Hou, Zhanliang Wang, Ruochen Jin, Qi Long, Weijie J. Su, Li Shen\n## Abstract\nOne of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.\n## Introduction\nLarge Language Models (LLMs) (OpenAI, 2023;Anthropic, 2024) have emerged as powerful tools for a wide range of natural language processing tasks (Bubeck et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023;Team et al., 2023). These models, built upon the Transformer architecture (Vaswani et al., 2017), have demonstrated remarkable abilities to process and generate human-like text, making them increasingly integral to various applications. A crucial development in making LLMs more reliable and aligned with human values is preference alignment techniques, particularly Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023). However, an important side effect of preference alignment is its impact on model calibration-the relationship between a model's predicted probabilities and its actual accuracy. While pretrained LLMs typically demonstrate good calibration properties, preference-aligned models become poorly calibrated, as initially observed in GPT-4 with RLHF (OpenAI, 2023). Our investigation reveals that this is a universal issue across different models aligned with various alignment methods. An example is shown in Figure 1 (left), where the calibration performance of a model aligned by DPO illustrates a poor calibration performance. \n\nUnderstanding and addressing this calibration issue is crucial. First, well-calibrated prediction is essential for reliable decision-making in real-world applications, particularly in high-stakes domains such as legal or healthcare analysis (Savage et al., 2025). Second, overconfident models may mislead users about their capabilities and limitations, potentially leading to inappropriate reliance on model outputs. \n\nIn this paper, we conduct a systematic investigation into two fundamental questions: (1) Why does preference alignment affect calibration? and (2) How can we effectively restore calibration while maintaining the benefits of alignment?\n\n## Conclusion\nIn this work, we have investigated the critical issue of calibration degradation in LLMs following preference alignment procedures. To address this, we introduced a theoretical framework distinguishing between calibratable and non-calibratable regimes, and developed practical solutions through calibration-aware fine-tuning approaches. Our experimental results across multiple models demonstrate that our methods can significantly improve calibration while maintaining or enhancing model performance. Future work could explore extending these methods to other types of language tasks and investigating the relationship between calibration and other aspects of model reliability.",
            "reference_string": "[278327247 | Xiao et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 22,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.19249",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218509878",
                    "name": "Guande He"
                },
                {
                    "authorId": "2276707",
                    "name": "Jianfei Chen"
                },
                {
                    "authorId": "2155220672",
                    "name": "Jun Zhu"
                }
            ],
            "abstract": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
            "corpus_id": 258967945,
            "sentences": [
                {
                    "corpus_id": "258967945",
                    "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
                    "text": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
                    "score": 0.4185848365485682,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                },
                {
                    "corpus_id": "258967945",
                    "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
                    "text": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2020). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
                    "score": 0.45283379289724596,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 3190,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 121,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1366
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 289,
                            "end": 311,
                            "matchedPaperCorpusId": "59599816"
                        },
                        {
                            "start": 387,
                            "end": 405,
                            "matchedPaperCorpusId": "202750126"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Guande He, Jianfei Chen, Jun Zhu\n## Abstract\nLarge pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.\n## INTRODUCTION\nBased on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2020). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
            "reference_string": "[258967945 | He et al. | 2023 | Citations: 22]"
        },
        {
            "title": "Compressing Language Models for Specialized Domains",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 79,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244005949",
                    "name": "Miles Williams"
                },
                {
                    "authorId": "51015453",
                    "name": "G. Chrysostomou"
                },
                {
                    "authorId": "2347038434",
                    "name": "Vitor Jeronymo"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ],
            "abstract": "Compression techniques such as pruning and quantization offer a solution for more efficient deployment of language models (LMs), albeit with small performance drops in benchmark performance. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this, yet requires computationally expensive full-parameter fine-tuning. To this end, we propose cross-calibration, a novel training-free approach for improving the domain performance of compressed LMs. Our approach effectively leverages Hessian-based sensitivity to identify weights that are influential for both in-domain and general performance. Through extensive experimentation, we demonstrate that cross-calibration substantially outperforms existing approaches on domain-specific tasks, without compromising general performance. Notably, these gains come without additional computational overhead, displaying remarkable potential towards extracting domain-specialized compressed models from general-purpose LMs.",
            "corpus_id": 276580536,
            "sentences": [
                {
                    "corpus_id": "276580536",
                    "title": "Compressing Language Models for Specialized Domains",
                    "text": "Compression techniques such as pruning and quantization offer a solution for more efficient deployment of language models (LMs), albeit with small performance drops in benchmark performance. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this, yet requires computationally expensive full-parameter fine-tuning. To this end, we propose cross-calibration, a novel training-free approach for improving the domain performance of compressed LMs. Our approach effectively leverages Hessian-based sensitivity to identify weights that are influential for both in-domain and general performance. Through extensive experimentation, we demonstrate that cross-calibration substantially outperforms existing approaches on domain-specific tasks, without compromising general performance. Notably, these gains come without additional computational overhead, displaying remarkable potential towards extracting domain-specialized compressed models from general-purpose LMs.",
                    "score": 0.44350063932220174,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: Compressing Language Models for Specialized Domains\n# Venue: arXiv.org\n# Authors: Miles Williams, G. Chrysostomou, Vitor Jeronymo, Nikolaos Aletras\n## Abstract\nCompression techniques such as pruning and quantization offer a solution for more efficient deployment of language models (LMs), albeit with small performance drops in benchmark performance. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this, yet requires computationally expensive full-parameter fine-tuning. To this end, we propose cross-calibration, a novel training-free approach for improving the domain performance of compressed LMs. Our approach effectively leverages Hessian-based sensitivity to identify weights that are influential for both in-domain and general performance. Through extensive experimentation, we demonstrate that cross-calibration substantially outperforms existing approaches on domain-specific tasks, without compromising general performance. Notably, these gains come without additional computational overhead, displaying remarkable potential towards extracting domain-specialized compressed models from general-purpose LMs.\n",
            "reference_string": "[276580536 | Williams et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Thermometer: Towards Universal Calibration for Large Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1865672630",
                    "name": "Maohao Shen"
                },
                {
                    "authorId": "2264958368",
                    "name": "Subhro Das"
                },
                {
                    "authorId": "2403306",
                    "name": "K. Greenewald"
                },
                {
                    "authorId": "1706272",
                    "name": "P. Sattigeri"
                },
                {
                    "authorId": "2215562153",
                    "name": "Greg Wornell"
                },
                {
                    "authorId": "2155615492",
                    "name": "Soumya Ghosh"
                }
            ],
            "abstract": "We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.",
            "corpus_id": 268385471,
            "sentences": [
                {
                    "corpus_id": "268385471",
                    "title": "Thermometer: Towards Universal Calibration for Large Language Models",
                    "text": "We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.",
                    "score": 0.4227106823263234,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: Thermometer: Towards Universal Calibration for Large Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Maohao Shen, Subhro Das, K. Greenewald, P. Sattigeri, Greg Wornell, Soumya Ghosh\n## Abstract\nWe consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.\n",
            "reference_string": "[268385471 | Shen et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2197555499",
                    "name": "Kang He"
                },
                {
                    "authorId": "153065698",
                    "name": "Yinghan Long"
                },
                {
                    "authorId": "2284593075",
                    "name": "Kaushik Roy"
                }
            ],
            "abstract": "Prompt-based learning is susceptible to intrinsic bias present in pre-trained language models (LMs), leading to sub-optimal performance in prompt-based zero/few-shot settings. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to probe intrinsic bias of pre-trained LMs. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).",
            "corpus_id": 267740621,
            "sentences": [
                {
                    "corpus_id": "267740621",
                    "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
                    "text": "The calibration promotes a fair starting point for LMs while preserving language modeling abilities. \n\n\u2022 Extensive experiments on eight classification datasets with four prompt-based learning approaches show that our method significantly improves LMs' zero/few-shot performance, and outperforms output-calibration methods.",
                    "score": 0.4064586197702927,
                    "section_title": "Introduction",
                    "char_start_offset": 5700,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 103,
                            "end": 322
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95751953125
                }
            ],
            "relevance_judgement": 0.95751953125,
            "relevance_judgment_input_expanded": "# Title: Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Kang He, Yinghan Long, Kaushik Roy\n## Abstract\nPrompt-based learning is susceptible to intrinsic bias present in pre-trained language models (LMs), leading to sub-optimal performance in prompt-based zero/few-shot settings. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to probe intrinsic bias of pre-trained LMs. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).\n## Introduction\nThe calibration promotes a fair starting point for LMs while preserving language modeling abilities. \n\n\u2022 Extensive experiments on eight classification datasets with four prompt-based learning approaches show that our method significantly improves LMs' zero/few-shot performance, and outperforms output-calibration methods.",
            "reference_string": "[267740621 | He et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 83,
            "citation_count": 96,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.04714",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.04714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "120727449",
                    "name": "Yuxin Xiao"
                },
                {
                    "authorId": "28130078",
                    "name": "Paul Pu Liang"
                },
                {
                    "authorId": "32326200",
                    "name": "Umang Bhatt"
                },
                {
                    "authorId": "2934259",
                    "name": "W. Neiswanger"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-philippe Morency"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.",
            "corpus_id": 247613322,
            "sentences": [
                {
                    "corpus_id": "247613322",
                    "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
                    "text": "In this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration. Our empirical evaluations consist of three distinct NLP classification tasks and two different domain settings. Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage. Compared to existing work, we also observe the following novel phenomena that are unique to PLMbased pipelines:\n\n\u2022 The relative calibration quality of PLMs is consistent in general across tasks and domains, with an exception of XLNet, which is the least robust to domain shift. \u2022 Larger PLMs are better calibrated under the indomain setting in Commonsense Reasoning, unlike in the other NLP tasks. \u2022 Uncertainty quantifiers are generally more effective in improving calibration performance under the out-of-domain setting. \u2022 Ensemble is less effective in reducing calibration error when used with PLM-based pipelines, despite their convincing performance with traditional models.",
                    "score": 0.4280091758583544,
                    "section_title": "Conclusion",
                    "char_start_offset": 20217,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yuxin Xiao, Paul Pu Liang, Umang Bhatt, W. Neiswanger, R. Salakhutdinov, Louis-philippe Morency\n## Abstract\nPre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.\n## Conclusion\nIn this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration. Our empirical evaluations consist of three distinct NLP classification tasks and two different domain settings. Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage. Compared to existing work, we also observe the following novel phenomena that are unique to PLMbased pipelines:\n\n\u2022 The relative calibration quality of PLMs is consistent in general across tasks and domains, with an exception of XLNet, which is the least robust to domain shift. \u2022 Larger PLMs are better calibrated under the indomain setting in Commonsense Reasoning, unlike in the other NLP tasks. \u2022 Uncertainty quantifiers are generally more effective in improving calibration performance under the out-of-domain setting. \u2022 Ensemble is less effective in reducing calibration error when used with PLM-based pipelines, despite their convincing performance with traditional models.",
            "reference_string": "[247613322 | Xiao et al. | 2022 | Citations: 96]"
        },
        {
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "venue": "Findings",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.06690",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.06690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1590802650",
                    "name": "Jaeyoung Kim"
                },
                {
                    "authorId": "2059662364",
                    "name": "Dongbin Na"
                },
                {
                    "authorId": "7236231",
                    "name": "Sungchul Choi"
                },
                {
                    "authorId": "26381055",
                    "name": "Sungbin Lim"
                }
            ],
            "abstract": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
            "corpus_id": 256846523,
            "sentences": [
                {
                    "corpus_id": "256846523",
                    "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
                    "text": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
                    "score": 0.3939493135683824,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: Bag of Tricks for In-Distribution Calibration of Pretrained Transformers\n# Venue: Findings\n# Authors: Jaeyoung Kim, Dongbin Na, Sungchul Choi, Sungbin Lim\n## Abstract\nWhile pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.\n",
            "reference_string": "[256846523 | Kim et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Calibrating Large Language Models with Sample Consistency",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i18.34120",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i18.34120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i18.34120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1904906987",
                    "name": "Qing Lyu"
                },
                {
                    "authorId": "2266467511",
                    "name": "Kumar Shridhar"
                },
                {
                    "authorId": "8805254",
                    "name": "Chaitanya Malaviya"
                },
                {
                    "authorId": "2258934061",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "51131518",
                    "name": "Yanai Elazar"
                },
                {
                    "authorId": "2261389843",
                    "name": "Niket Tandon"
                },
                {
                    "authorId": "2294720129",
                    "name": "Marianna Apidianaki"
                },
                {
                    "authorId": "2790926",
                    "name": "Mrinmaya Sachan"
                },
                {
                    "authorId": "2266396865",
                    "name": "Christopher Callison-Burch"
                }
            ],
            "abstract": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we derive model confidence from the distribution of multiple randomly sampled generations, using three measures of consistency. We extensively evaluate eleven open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches in terms of calibration error. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency can potentially enhance model performance. Finally, we offer guidance on choosing suitable consistency metrics for calibration, tailored to model characteristics such as the exposure to instruction-tuning and RLHF.",
            "corpus_id": 277766037,
            "sentences": [
                {
                    "corpus_id": "277766037",
                    "title": "Calibrating Large Language Models with Sample Consistency",
                    "text": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we derive model confidence from the distribution of multiple randomly sampled generations, using three measures of consistency. We extensively evaluate eleven open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches in terms of calibration error. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency can potentially enhance model performance. Finally, we offer guidance on choosing suitable consistency metrics for calibration, tailored to model characteristics such as the exposure to instruction-tuning and RLHF.",
                    "score": 0.3946961894953656,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: Calibrating Large Language Models with Sample Consistency\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Christopher Callison-Burch\n## Abstract\nAccurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we derive model confidence from the distribution of multiple randomly sampled generations, using three measures of consistency. We extensively evaluate eleven open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches in terms of calibration error. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency can potentially enhance model performance. Finally, we offer guidance on choosing suitable consistency metrics for calibration, tailored to model characteristics such as the exposure to instruction-tuning and RLHF.\n",
            "reference_string": "[277766037 | Lyu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Can Explanations Be Useful for Calibrating Black Box Models?",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 25,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.429.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.07586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50183897",
                    "name": "Xi Ye"
                },
                {
                    "authorId": "1814094",
                    "name": "Greg Durrett"
                }
            ],
            "abstract": "NLP practitioners often want to take existing trained models and apply them to data from new domains. While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box. We study how to improve a black box model\u2019s performance on a new domain by leveraging explanations of the model\u2019s behavior. Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques, then uses a simple calibrator, in the form of a classifier, to predict whether the base model was correct or not. We experiment with our method on two tasks, extractive question answering and natural language inference, covering adaptation from several pairs of domains with limited target-domain data. The experimental results across all the domain pairs show that explanations are useful for calibrating these models, boosting accuracy when predictions do not have to be returned on every example. We further show that the calibration model transfers to some extent between tasks.",
            "corpus_id": 238856959,
            "sentences": [
                {
                    "corpus_id": "238856959",
                    "title": "Can Explanations Be Useful for Calibrating Black Box Models?",
                    "text": "NLP practitioners often want to take existing trained models and apply them to data from new domains. While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box. We study how to improve a black box model\u2019s performance on a new domain by leveraging explanations of the model\u2019s behavior. Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques, then uses a simple calibrator, in the form of a classifier, to predict whether the base model was correct or not. We experiment with our method on two tasks, extractive question answering and natural language inference, covering adaptation from several pairs of domains with limited target-domain data. The experimental results across all the domain pairs show that explanations are useful for calibrating these models, boosting accuracy when predictions do not have to be returned on every example. We further show that the calibration model transfers to some extent between tasks.",
                    "score": 0.4737024308715537,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: Can Explanations Be Useful for Calibrating Black Box Models?\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Xi Ye, Greg Durrett\n## Abstract\nNLP practitioners often want to take existing trained models and apply them to data from new domains. While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box. We study how to improve a black box model\u2019s performance on a new domain by leveraging explanations of the model\u2019s behavior. Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques, then uses a simple calibrator, in the form of a classifier, to predict whether the base model was correct or not. We experiment with our method on two tasks, extractive question answering and natural language inference, covering adaptation from several pairs of domains with limited target-domain data. The experimental results across all the domain pairs show that explanations are useful for calibrating these models, boosting accuracy when predictions do not have to be returned on every example. We further show that the calibration model transfers to some extent between tasks.\n",
            "reference_string": "[238856959 | Ye et al. | 2021 | Citations: 25]"
        },
        {
            "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
            "venue": "Workshop on Representation Learning for NLP",
            "year": 2021,
            "reference_count": 69,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/2021.repl4nlp-1.29",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.repl4nlp-1.29, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48843025",
                    "name": "Han Guo"
                },
                {
                    "authorId": "2121363271",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.",
            "corpus_id": 236486129,
            "sentences": [
                {
                    "corpus_id": "236486129",
                    "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
                    "text": "Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.",
                    "score": 0.4024982055363106,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9443359375
                },
                {
                    "corpus_id": "236486129",
                    "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
                    "text": "Related works in NLP have considered data from similar tasks but from different datasets as OOD (Ovadia et al., 2019;Hendrycks and Gimpel, 2017). Next, in order to make models more calibrated, we study some of the widelyused recalibration methods, with various degrees of effectiveness and computational cost. For example, ensembling models has been shown to be very effective in out-of-domain settings (Ovadia et al., 2019), but the cost of computation scales with the size of ensembles. On the other hand, distillation (Hinton et al., 2015) is a widely-known method for improving the system's performance by learning from a stronger teacher model. In this work, we empirically examine the connection between distillation and calibration. Notably, we view the objective function of distillation as a regularization term that encourages the student model to match the predictive uncertainty of a stronger, more calibrated teacher model. \n\nWe conduct analysis experiments to show that the teacher's calibration performance could be distilled into the student model, even when the teacher model's accuracy remains similar. With this insight, we show that simple methods based on distillation could achieve competitive performance in out-ofdomain calibration, without introducing extra computation at inference time. Finally, we also conduct ablation experiments to understand the usefulness of components of the method. In summary, our contributions are listed as follows: \n\n\u2022 We present a systematic study on the performance of various recalibration methods on finetuned language models for both in-domain and out-of-domain settings. \n\n\u2022 We empirically examine the connection between distillation and calibration, and conduct experiments showing that distillation can distill calibration performance. \n\n\u2022 We describe two simple recalibration methods, and experimental results demonstrate their competitiveness in the out-of-domain settings; finally, we also ablate method's components and measure the extent to which distillation transfers teachers' calibration improvement.",
                    "score": 0.679100115795202,
                    "section_title": "Introduction",
                    "char_start_offset": 3127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 488
                        },
                        {
                            "start": 489,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 936
                        },
                        {
                            "start": 939,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1470
                        },
                        {
                            "start": 1473,
                            "end": 1632
                        },
                        {
                            "start": 1635,
                            "end": 1799
                        },
                        {
                            "start": 1802,
                            "end": 2073
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 521,
                            "end": 542,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.9443359375,
            "relevance_judgment_input_expanded": "# Title: An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation\n# Venue: Workshop on Representation Learning for NLP\n# Authors: Han Guo, Ramakanth Pasunuru, Mohit Bansal\n## Abstract\nRecent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.\n## Introduction\nRelated works in NLP have considered data from similar tasks but from different datasets as OOD (Ovadia et al., 2019;Hendrycks and Gimpel, 2017). Next, in order to make models more calibrated, we study some of the widelyused recalibration methods, with various degrees of effectiveness and computational cost. For example, ensembling models has been shown to be very effective in out-of-domain settings (Ovadia et al., 2019), but the cost of computation scales with the size of ensembles. On the other hand, distillation (Hinton et al., 2015) is a widely-known method for improving the system's performance by learning from a stronger teacher model. In this work, we empirically examine the connection between distillation and calibration. Notably, we view the objective function of distillation as a regularization term that encourages the student model to match the predictive uncertainty of a stronger, more calibrated teacher model. \n\nWe conduct analysis experiments to show that the teacher's calibration performance could be distilled into the student model, even when the teacher model's accuracy remains similar. With this insight, we show that simple methods based on distillation could achieve competitive performance in out-ofdomain calibration, without introducing extra computation at inference time. Finally, we also conduct ablation experiments to understand the usefulness of components of the method. In summary, our contributions are listed as follows: \n\n\u2022 We present a systematic study on the performance of various recalibration methods on finetuned language models for both in-domain and out-of-domain settings. \n\n\u2022 We empirically examine the connection between distillation and calibration, and conduct experiments showing that distillation can distill calibration performance. \n\n\u2022 We describe two simple recalibration methods, and experimental results demonstrate their competitiveness in the out-of-domain settings; finally, we also ablate method's components and measure the extent to which distillation transfers teachers' calibration improvement.",
            "reference_string": "[236486129 | Guo et al. | 2021 | Citations: 7]"
        },
        {
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 36,
            "citation_count": 36,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.07559",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.07559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118885320",
                    "name": "Seohong Park"
                },
                {
                    "authorId": "2140493460",
                    "name": "Cornelia Caragea"
                }
            ],
            "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "corpus_id": 247450599,
            "sentences": [
                {
                    "corpus_id": "247450599",
                    "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
                    "text": "cy maps can benefit in-domain calibration and by dissimilarity can benefit out-of-domain calibration. To monitor training dynamics, we use the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) which measures how different a true label for a sample is compared to a model's beliefs at each epoch and is calculated as the average difference between the logit values for a sample's assigned class and its highest non-assigned class across training epochs.\n\nMoreover, we combine our mixup with wellknown miscalibration correction methods such as label smoothing and temperature scaling (Guo et al., 2017) to investigate their impact on our proposed mixup. We conduct a comprehensive set of experiments using BERT (Devlin et al., 2019) and RoBERTa  to show the efficacy of our mixup approach by testing on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. We achieve the lowest Expected Calibration Error (ECE) without accuracy drops in comparison with strong baseline methods. Our contributions are as follows:\n\n\u2022 We propose a novel mixup method which is guided by AUM and saliency signals and is targeted at improving model calibration. Specifically, we compare logits to categorize samples into two sets (i.e., a set of easy-to-learn samples and another set of hardto-learn/ambiguous samples), and interpolate samples across these two sets by finding the most similar and most dissimilar samples from the other set while leveraging saliency (to compute sample similarities) for pre-trained language models' calibration on in-domain and out-of-domain data.\n\n\u2022 We combine our method with miscalibration correction techniques (i.e., label smoothing, temperature scaling) to investigate their impact on our proposed mixup.\n\n\u2022 We conduct comprehensive experiments showing that our method achieves the lowest expected calibration errors (ECEs) on both in-domain and out-of-domain samples compared with strong baselines without accuracy drops on multiple NLU tasks, namely, natural language inferences, paraphrase detection, and commonsense reasoning.",
                    "score": 0.5519343145524787,
                    "section_title": "Introduction",
                    "char_start_offset": 3791,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 202,
                            "matchedPaperCorpusId": "210932316"
                        },
                        {
                            "start": 592,
                            "end": 610,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 719,
                            "end": 740,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94091796875
                },
                {
                    "corpus_id": "247450599",
                    "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
                    "text": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
                    "score": 0.502205220107158,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91650390625
                }
            ],
            "relevance_judgement": 0.94091796875,
            "relevance_judgment_input_expanded": "# Title: On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Seohong Park, Cornelia Caragea\n## Abstract\nA well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.\n## Introduction\ncy maps can benefit in-domain calibration and by dissimilarity can benefit out-of-domain calibration. To monitor training dynamics, we use the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) which measures how different a true label for a sample is compared to a model's beliefs at each epoch and is calculated as the average difference between the logit values for a sample's assigned class and its highest non-assigned class across training epochs.\n\nMoreover, we combine our mixup with wellknown miscalibration correction methods such as label smoothing and temperature scaling (Guo et al., 2017) to investigate their impact on our proposed mixup. We conduct a comprehensive set of experiments using BERT (Devlin et al., 2019) and RoBERTa  to show the efficacy of our mixup approach by testing on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. We achieve the lowest Expected Calibration Error (ECE) without accuracy drops in comparison with strong baseline methods. Our contributions are as follows:\n\n\u2022 We propose a novel mixup method which is guided by AUM and saliency signals and is targeted at improving model calibration. Specifically, we compare logits to categorize samples into two sets (i.e., a set of easy-to-learn samples and another set of hardto-learn/ambiguous samples), and interpolate samples across these two sets by finding the most similar and most dissimilar samples from the other set while leveraging saliency (to compute sample similarities) for pre-trained language models' calibration on in-domain and out-of-domain data.\n\n\u2022 We combine our method with miscalibration correction techniques (i.e., label smoothing, temperature scaling) to investigate their impact on our proposed mixup.\n\n\u2022 We conduct comprehensive experiments showing that our method achieves the lowest expected calibration errors (ECEs) on both in-domain and out-of-domain samples compared with strong baselines without accuracy drops on multiple NLU tasks, namely, natural language inferences, paraphrase detection, and commonsense reasoning.",
            "reference_string": "[247450599 | Park et al. | 2022 | Citations: 36]"
        },
        {
            "title": "Teaching Models to Express Their Uncertainty in Words",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2022,
            "reference_count": 36,
            "citation_count": 424,
            "influential_citation_count": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.14334",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48639938",
                    "name": "Stephanie C. Lin"
                },
                {
                    "authorId": "2052366271",
                    "name": "Jacob Hilton"
                },
                {
                    "authorId": "47107786",
                    "name": "Owain Evans"
                }
            ],
            "abstract": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
            "corpus_id": 249191391,
            "sentences": [
                {
                    "corpus_id": "249191391",
                    "title": "Teaching Models to Express Their Uncertainty in Words",
                    "text": "Calibration in new domains. Prior work on calibration focuses primarily on the classification setting, where models output a probability distribution over the set of possible classes (Guo et al., 2017;Mukhoti et al., 2020;Minderer et al., 2021), corresponding to what we call the \"answer logit\". To generalize calibration to a new target domain, methods often require samples from the target or from additional source domains (Gong et al., 2021;Csurka, 2017;Wang et al., 2021). We study how calibration generalizes when a pre-trained model is finetuned on a single source domain and must generalize zero-shot to a new domain. \n\nPre-trained language models. Hendrycks et al. (2020) analyze GPT-3's behavior on a benchmark of tasks that vary in both subject matter and difficulty, showing that GPT-3's calibration (for the answer logit) generalizes fairly poorly in both the zero-shot and few-shot settings. To improve the calibration of pre-trained language models, Desai & Durrett (2020) use label smoothing to reduce overconfidence on out-ofdomain data. Kong et al. (2020) introduce on-and off-manifold regularization to handle in-distribution and out-of-distribution calibration, respectively, but focus on OOD detection rather than generalization. Other work focuses on the closely related problem of teaching models to abstain from answering when a model has high uncertainty about its answer. Kamath et al. (2020) train an auxiliary \"calibrator\" to predict whether the primary model correctly answers any given question using a mix of in-domain and out-of-domain data. \n\nIn cases where the calibrator predicts an error, the model can refuse to answer. Additional studies explore the use of manually crafted prompts that instruct models to defer or qualify their answers when uncertain (Askell et al., 2021b;Lin et al., 2021). These methods typically correct for models being overconfident on out-of-domain examples.",
                    "score": 0.4353487993929209,
                    "section_title": "Related work",
                    "char_start_offset": 23758,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 27
                        },
                        {
                            "start": 28,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 625
                        },
                        {
                            "start": 628,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1250
                        },
                        {
                            "start": 1251,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1573
                        },
                        {
                            "start": 1576,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 1920
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 201,
                            "end": 222,
                            "matchedPaperCorpusId": "211252346"
                        },
                        {
                            "start": 222,
                            "end": 244,
                            "matchedPaperCorpusId": "235435823"
                        },
                        {
                            "start": 1398,
                            "end": 1418,
                            "matchedPaperCorpusId": "219721462"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93994140625
                }
            ],
            "relevance_judgement": 0.93994140625,
            "relevance_judgment_input_expanded": "# Title: Teaching Models to Express Their Uncertainty in Words\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Stephanie C. Lin, Jacob Hilton, Owain Evans\n## Abstract\nWe show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.\n## Related work\nCalibration in new domains. Prior work on calibration focuses primarily on the classification setting, where models output a probability distribution over the set of possible classes (Guo et al., 2017;Mukhoti et al., 2020;Minderer et al., 2021), corresponding to what we call the \"answer logit\". To generalize calibration to a new target domain, methods often require samples from the target or from additional source domains (Gong et al., 2021;Csurka, 2017;Wang et al., 2021). We study how calibration generalizes when a pre-trained model is finetuned on a single source domain and must generalize zero-shot to a new domain. \n\nPre-trained language models. Hendrycks et al. (2020) analyze GPT-3's behavior on a benchmark of tasks that vary in both subject matter and difficulty, showing that GPT-3's calibration (for the answer logit) generalizes fairly poorly in both the zero-shot and few-shot settings. To improve the calibration of pre-trained language models, Desai & Durrett (2020) use label smoothing to reduce overconfidence on out-ofdomain data. Kong et al. (2020) introduce on-and off-manifold regularization to handle in-distribution and out-of-distribution calibration, respectively, but focus on OOD detection rather than generalization. Other work focuses on the closely related problem of teaching models to abstain from answering when a model has high uncertainty about its answer. Kamath et al. (2020) train an auxiliary \"calibrator\" to predict whether the primary model correctly answers any given question using a mix of in-domain and out-of-domain data. \n\nIn cases where the calibrator predicts an error, the model can refuse to answer. Additional studies explore the use of manually crafted prompts that instruct models to defer or qualify their answers when uncertain (Askell et al., 2021b;Lin et al., 2021). These methods typically correct for models being overconfident on out-of-domain examples.",
            "reference_string": "[249191391 | Lin et al. | 2022 | Citations: 424]"
        },
        {
            "title": "Leveraging Large Language Models For Optimized Item Categorization using UNSPSC Taxonomy",
            "venue": "International Journal on Cybernetics &amp; Informatics",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.5121/ijci.2024.130601",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337350686",
                    "name": "Anmolika Singh"
                },
                {
                    "authorId": "2336580311",
                    "name": "Yuhang Diao"
                }
            ],
            "abstract": "Effective item categorization is vital for businesses, enabling the transformation of unstructured datasets into organized categories that streamline inventory management. Despite its importance, item categorization remains highly subjective and lacks a uniform standard across industries and businesses. The United Nations Standard Products and Services Code (UNSPSC) provides a standardized system for cataloguing inventory, yet employing UNSPSC categorizations often demands significant manual effort. This paper investigates the deployment of Large Language Models (LLMs) to automate the classification of inventory data into UNSPSC codes based on Item Descriptions. We evaluate the accuracy and efficiency of LLMs in categorizing diverse datasets, exploring their language processing capabilities and their potential as a tool for standardizing inventory classification. Our findings reveal that LLMs can substantially diminish the manual labor involved in item categorization while maintaining high accuracy, offering a scalable solution for businesses striving to enhance their inventory management practices.",
            "corpus_id": 274939920,
            "sentences": [
                {
                    "corpus_id": "274939920",
                    "title": "Leveraging Large Language Models For Optimized Item Categorization using UNSPSC Taxonomy",
                    "text": "Prompt engineering is a critical technique for effectively utilizing large language models (LLMs). By crafting precise and contextually rich prompts, researchers can guide LLMs to produce accurate and relevant outputs. This process is essential for reducing ambiguity and enhancing the consistency of the model's responses. Brown et al. [2] demonstrated the efficiency of few-shot learning with carefully constructed prompts, highlighting that even a few examples can substantially improve model performance. Similarly, Raffel et al. [13] emphasized the importance of prompt format and structure in their exploration of transfer learning with the T5 model, showing that prompt variations can lead to different levels of success in text-to-text tasks. In the context of item categorization, prompt engineering helps address challenges such as ambiguous item descriptions and the need for domain-specific knowledge. Schick and Schutze [14] explored the use of Cloze-style prompts, where the model fills in blanks within a sentence, for few-shot text classification. This technique demonstrated how prompts that mimic natural language questions elicit more accurate responses from LLMs. This approach is particularly relevant for UNSPSC categorization, where items must be matched to precise codes based on nuanced descriptions. By leveraging structured, contextual, and multi-turn prompts, researchers can enhance the accuracy of UNSPSC code assignment, thereby improving data standardization and procurement processes across various industries.",
                    "score": 0.4043947226413656,
                    "section_title": "Prompt Engineering",
                    "char_start_offset": 6117,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 99,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1063
                        },
                        {
                            "start": 1064,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1543
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 534,
                            "end": 538,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Leveraging Large Language Models For Optimized Item Categorization using UNSPSC Taxonomy\n# Venue: International Journal on Cybernetics &amp; Informatics\n# Authors: Anmolika Singh, Yuhang Diao\n## Abstract\nEffective item categorization is vital for businesses, enabling the transformation of unstructured datasets into organized categories that streamline inventory management. Despite its importance, item categorization remains highly subjective and lacks a uniform standard across industries and businesses. The United Nations Standard Products and Services Code (UNSPSC) provides a standardized system for cataloguing inventory, yet employing UNSPSC categorizations often demands significant manual effort. This paper investigates the deployment of Large Language Models (LLMs) to automate the classification of inventory data into UNSPSC codes based on Item Descriptions. We evaluate the accuracy and efficiency of LLMs in categorizing diverse datasets, exploring their language processing capabilities and their potential as a tool for standardizing inventory classification. Our findings reveal that LLMs can substantially diminish the manual labor involved in item categorization while maintaining high accuracy, offering a scalable solution for businesses striving to enhance their inventory management practices.\n## Prompt Engineering\nPrompt engineering is a critical technique for effectively utilizing large language models (LLMs). By crafting precise and contextually rich prompts, researchers can guide LLMs to produce accurate and relevant outputs. This process is essential for reducing ambiguity and enhancing the consistency of the model's responses. Brown et al. [2] demonstrated the efficiency of few-shot learning with carefully constructed prompts, highlighting that even a few examples can substantially improve model performance. Similarly, Raffel et al. [13] emphasized the importance of prompt format and structure in their exploration of transfer learning with the T5 model, showing that prompt variations can lead to different levels of success in text-to-text tasks. In the context of item categorization, prompt engineering helps address challenges such as ambiguous item descriptions and the need for domain-specific knowledge. Schick and Schutze [14] explored the use of Cloze-style prompts, where the model fills in blanks within a sentence, for few-shot text classification. This technique demonstrated how prompts that mimic natural language questions elicit more accurate responses from LLMs. This approach is particularly relevant for UNSPSC categorization, where items must be matched to precise codes based on nuanced descriptions. By leveraging structured, contextual, and multi-turn prompts, researchers can enhance the accuracy of UNSPSC code assignment, thereby improving data standardization and procurement processes across various industries.",
            "reference_string": "[274939920 | Singh et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Localized large language model TCNNet 9B for Taiwanese networking and cybersecurity",
            "venue": "Scientific Reports",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41598-025-90320-9",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11926162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351324453",
                    "name": "Jiun-Yi Yang"
                },
                {
                    "authorId": "2346608931",
                    "name": "Chia-Chun Wu"
                }
            ],
            "abstract": "This paper introduces TCNNet-9B, a specialized Traditional Chinese language model developed to address the specific requirements of the Taiwanese networking industry. Built upon the open-source Yi-1.5-9B architecture, TCNNet-9B underwent extensive pretraining and instruction finetuning utilizing a meticulously curated dataset derived from multi-source web crawling. The training data encompasses comprehensive networking knowledge, DIY assembly guides, equipment recommendations, and localized cybersecurity regulations. Our rigorous evaluation through custom-designed benchmarks assessed the model\u2019s performance across English, Traditional Chinese, and Simplified Chinese contexts. The comparative analysis demonstrated TCNNet-9B\u2019s superior performance over the baseline model, achieving a 2.35-fold improvement in Q&A task accuracy, a 37.6% increase in domain expertise comprehension, and a 29.5% enhancement in product recommendation relevance. The practical efficacy of TCNNet-9B was further validated through its successful integration into Hi5\u2019s intelligent sales advisor system. This research highlights the significance of domain-specific adaptation and localization in enhancing large language models, providing a valuable practical reference for future developments in non-English contexts and vertical specialized fields.",
            "corpus_id": 277189451,
            "sentences": [
                {
                    "corpus_id": "277189451",
                    "title": "Localized large language model TCNNet 9B for Taiwanese networking and cybersecurity",
                    "text": "Finetuning techniques are essential for adapting large language models (LLMs) to specific tasks and domains, allowing them to achieve higher accuracy and relevance in their outputs. As LLMs have evolved, researchers have developed various finetuning methodologies that leverage existing pre-trained models while enhancing their performance through targeted adjustments. \n\nOne of the prominent techniques in this area is transfer learning, which allows models to retain the knowledge gained from large, diverse datasets while adapting to specialized tasks. This approach has significantly reduced the labeled data required for practical training in specific domains. For example, Kamath et al. (2019) explain domain adaptation as a form of transfer learning in which the task remains the same, but the distribution changes between source and target domains. They illustrate how sentiment analysis models trained on electronic product reviews may misinterpret domain-specific phrases when applied to hotel reviews, highlighting the challenges of domain shift without extensive retraining 26 . \n\nAnother notable advancement is instruction tuning, which involves training models on datasets that include explicit instructions or prompts related to the desired tasks. This technique has enhanced the model's ability to follow user queries and generate contextually appropriate responses. Zhao et al. ( 2023) propose a Self-Guide. This multi-stage mechanism synthesizes task-specific input-output pairs from the language model for self-finetuning. Their approach demonstrates substantial improvements in classification and generation tasks, enabling models to understand user intent better and provide relevant information without requiring external learning signals 27 . \n\nMoreover, few-shot learning has emerged as a powerful strategy for finetuning LLMs with limited labeled data. This method allows models to generalize from a small number of examples, making them more adaptable to new tasks without extensive retraining. Recent studies, such as those by Perez et al. (2023) examine \"true fewshot learning\" where no held-out examples are available for model tuning. They reveal challenges in few-shot model selection that suggest previous studies may have overestimated the effectiveness of this approach 28 . \n\nThese finetuning techniques were integral to the development of TCNNet-9B. The model underwent continued pretraining on a carefully curated dataset, including networking knowledge, DIY assembly guides, and local cybersecurity regulations.",
                    "score": 0.5313697241988765,
                    "section_title": "Advances in finetuning techniques for LLMs",
                    "char_start_offset": 20903,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 369
                        },
                        {
                            "start": 372,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1090
                        },
                        {
                            "start": 1093,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1424
                        },
                        {
                            "start": 1425,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1765
                        },
                        {
                            "start": 1768,
                            "end": 1877
                        },
                        {
                            "start": 1878,
                            "end": 2020
                        },
                        {
                            "start": 2021,
                            "end": 2164
                        },
                        {
                            "start": 2165,
                            "end": 2308
                        },
                        {
                            "start": 2311,
                            "end": 2385
                        },
                        {
                            "start": 2386,
                            "end": 2549
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1086,
                            "end": 1088,
                            "matchedPaperCorpusId": "196177312"
                        },
                        {
                            "start": 2304,
                            "end": 2306,
                            "matchedPaperCorpusId": "235166749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: Localized large language model TCNNet 9B for Taiwanese networking and cybersecurity\n# Venue: Scientific Reports\n# Authors: Jiun-Yi Yang, Chia-Chun Wu\n## Abstract\nThis paper introduces TCNNet-9B, a specialized Traditional Chinese language model developed to address the specific requirements of the Taiwanese networking industry. Built upon the open-source Yi-1.5-9B architecture, TCNNet-9B underwent extensive pretraining and instruction finetuning utilizing a meticulously curated dataset derived from multi-source web crawling. The training data encompasses comprehensive networking knowledge, DIY assembly guides, equipment recommendations, and localized cybersecurity regulations. Our rigorous evaluation through custom-designed benchmarks assessed the model\u2019s performance across English, Traditional Chinese, and Simplified Chinese contexts. The comparative analysis demonstrated TCNNet-9B\u2019s superior performance over the baseline model, achieving a 2.35-fold improvement in Q&A task accuracy, a 37.6% increase in domain expertise comprehension, and a 29.5% enhancement in product recommendation relevance. The practical efficacy of TCNNet-9B was further validated through its successful integration into Hi5\u2019s intelligent sales advisor system. This research highlights the significance of domain-specific adaptation and localization in enhancing large language models, providing a valuable practical reference for future developments in non-English contexts and vertical specialized fields.\n## Advances in finetuning techniques for LLMs\nFinetuning techniques are essential for adapting large language models (LLMs) to specific tasks and domains, allowing them to achieve higher accuracy and relevance in their outputs. As LLMs have evolved, researchers have developed various finetuning methodologies that leverage existing pre-trained models while enhancing their performance through targeted adjustments. \n\nOne of the prominent techniques in this area is transfer learning, which allows models to retain the knowledge gained from large, diverse datasets while adapting to specialized tasks. This approach has significantly reduced the labeled data required for practical training in specific domains. For example, Kamath et al. (2019) explain domain adaptation as a form of transfer learning in which the task remains the same, but the distribution changes between source and target domains. They illustrate how sentiment analysis models trained on electronic product reviews may misinterpret domain-specific phrases when applied to hotel reviews, highlighting the challenges of domain shift without extensive retraining 26 . \n\nAnother notable advancement is instruction tuning, which involves training models on datasets that include explicit instructions or prompts related to the desired tasks. This technique has enhanced the model's ability to follow user queries and generate contextually appropriate responses. Zhao et al. ( 2023) propose a Self-Guide. This multi-stage mechanism synthesizes task-specific input-output pairs from the language model for self-finetuning. Their approach demonstrates substantial improvements in classification and generation tasks, enabling models to understand user intent better and provide relevant information without requiring external learning signals 27 . \n\nMoreover, few-shot learning has emerged as a powerful strategy for finetuning LLMs with limited labeled data. This method allows models to generalize from a small number of examples, making them more adaptable to new tasks without extensive retraining. Recent studies, such as those by Perez et al. (2023) examine \"true fewshot learning\" where no held-out examples are available for model tuning. They reveal challenges in few-shot model selection that suggest previous studies may have overestimated the effectiveness of this approach 28 . \n\nThese finetuning techniques were integral to the development of TCNNet-9B. The model underwent continued pretraining on a carefully curated dataset, including networking knowledge, DIY assembly guides, and local cybersecurity regulations.",
            "reference_string": "[277189451 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 77,
            "citation_count": 436,
            "influential_citation_count": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00407/1962628/tacl_a_00407.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.00955, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "47929135",
                    "name": "Haibo Ding"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ],
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "corpus_id": 235078802,
            "sentences": [
                {
                    "corpus_id": "235078802",
                    "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                    "text": "This is especially true if these models are deployed to safety-critical domains such as healthcare and finance, where mistaken answers can have serious consequences. 1 n this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular knowledge-based query?\" Specifically, we examine this from the point of view of calibration, whether the model's probability estimates are well-aligned with the actual probability of the answer being correct. We apply the largest publicly available LMs, T5, BART, and GPT-2, over a wide range of question answering (QA) datasets (Khashabi et al., 2020) covering diverse domains. We first observe that despite the models' high performance (e.g. T5 eclipses other alternatives such as GPT-3 on some datasets), the models tend to not be well calibrated; their probability estimates over candidates have far-from-perfect correspondence with the actual probability that the answer they provide is correct. Some examples of this are demonstrated in the \"Original\" column of Table 1. \n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context. \n\nExperimental results demonstrate that both finetuning and post-hoc methods can improve calibration performance without sacrificing accuracy. We further perform analysis and ablation studies on our methods, inspecting different aspects that may affect calibration performance.",
                    "score": 0.4773483530574582,
                    "section_title": "Introduction",
                    "char_start_offset": 1723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1767
                        },
                        {
                            "start": 1768,
                            "end": 1946
                        },
                        {
                            "start": 1949,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1697,
                            "end": 1722,
                            "matchedPaperCorpusId": "215548393"
                        },
                        {
                            "start": 1746,
                            "end": 1766,
                            "matchedPaperCorpusId": "219721462"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                },
                {
                    "corpus_id": "235078802",
                    "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
                    "text": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
                    "score": 0.4483924175418692,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Zhengbao Jiang, J. Araki, Haibo Ding, Graham Neubig\n## Abstract\nAbstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.\n## Introduction\nThis is especially true if these models are deployed to safety-critical domains such as healthcare and finance, where mistaken answers can have serious consequences. 1 n this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular knowledge-based query?\" Specifically, we examine this from the point of view of calibration, whether the model's probability estimates are well-aligned with the actual probability of the answer being correct. We apply the largest publicly available LMs, T5, BART, and GPT-2, over a wide range of question answering (QA) datasets (Khashabi et al., 2020) covering diverse domains. We first observe that despite the models' high performance (e.g. T5 eclipses other alternatives such as GPT-3 on some datasets), the models tend to not be well calibrated; their probability estimates over candidates have far-from-perfect correspondence with the actual probability that the answer they provide is correct. Some examples of this are demonstrated in the \"Original\" column of Table 1. \n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context. \n\nExperimental results demonstrate that both finetuning and post-hoc methods can improve calibration performance without sacrificing accuracy. We further perform analysis and ablation studies on our methods, inspecting different aspects that may affect calibration performance.",
            "reference_string": "[235078802 | Jiang et al. | 2020 | Citations: 436]"
        },
        {
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.00151",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.00151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "123331686",
                    "name": "Yangyi Chen"
                },
                {
                    "authorId": "2152195191",
                    "name": "Lifan Yuan"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "corpus_id": 253244504,
            "sentences": [
                {
                    "corpus_id": "253244504",
                    "title": "A Close Look into the Calibration of Pre-trained Language Models",
                    "text": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
                    "score": 0.41450130713776434,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: A Close Look into the Calibration of Pre-trained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, Heng Ji\n## Abstract\nPre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.\n",
            "reference_string": "[253244504 | Chen et al. | 2022 | Citations: 51]"
        },
        {
            "title": "FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named Entity Recognition in a Multilingual Framework",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 63,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.02391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151413126",
                    "name": "Ibrahim Bouabdallaoui"
                },
                {
                    "authorId": "2318113190",
                    "name": "Fatima Guerouate"
                },
                {
                    "authorId": "2125336013",
                    "name": "Samya Bouhaddour"
                },
                {
                    "authorId": "8634054",
                    "name": "Chaimae Saadi"
                },
                {
                    "authorId": "2240913194",
                    "name": "Mohammed Sbihi"
                }
            ],
            "abstract": "We introduce FewTopNER, a novel framework that integrates few-shot named entity recognition (NER) with topic-aware contextual modeling to address the challenges of cross-lingual and low-resource scenarios. FewTopNER leverages a shared multilingual encoder based on XLM-RoBERTa, augmented with language-specific calibration mechanisms, to generate robust contextual embeddings. The architecture comprises a prototype-based entity recognition branch, employing BiLSTM and Conditional Random Fields for sequence labeling, and a topic modeling branch that extracts document-level semantic features through hybrid probabilistic and neural methods. A cross-task bridge facilitates dynamic bidirectional attention and feature fusion between entity and topic representations, thereby enhancing entity disambiguation by incorporating global semantic context. Empirical evaluations on multilingual benchmarks across English, French, Spanish, German, and Italian demonstrate that FewTopNER significantly outperforms existing state-of-the-art few-shot NER models. In particular, the framework achieves improvements of 2.5-4.0 percentage points in F1 score and exhibits enhanced topic coherence, as measured by normalized pointwise mutual information. Ablation studies further confirm the critical contributions of the shared encoder and cross-task integration mechanisms to the overall performance. These results underscore the efficacy of incorporating topic-aware context into few-shot NER and highlight the potential of FewTopNER for robust cross-lingual applications in low-resource settings.",
            "corpus_id": 276106916,
            "sentences": [
                {
                    "corpus_id": "276106916",
                    "title": "FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named Entity Recognition in a Multilingual Framework",
                    "text": "Techniques such as parameter-sharing and fine-tuning have been employed to optimize representations across different domains, improving adaptability while minimizing the need for extensive labeled data [18]. However, conventional transfer learning methods often assume that source and target domains share the same label space (homogeneous transfer) or require a sufficient number of annotated examples to bridge the distributional gap in heterogeneous settings. Recent advancements in adversarial domain adaptation have attempted to address these challenges by learning domain-invariant representations, effectively enhancing generalization across diverse datasets [19]. Despite these improvements, challenges persist in adapting NER models to domains with extremely scarce target data, highlighting the need for more efficient domain adaptation techniques. \n\nFew-Shot Learning for NER. Few-shot learning has become a crucial area of research for Named Entity Recognition (NER), enabling models to generalize to new entity types with minimal annotated data. Unlike traditional sequence labeling methods, few-shot NER presents unique challenges due to the variability of entity occurrences within a sentence and the absence of a predefined entity class set [20]. To address these challenges, meta-learning techniques such as FewNER have been proposed. FewNER introduces a task-adaptive training approach that parti-tions the network into task-independent and task-specific components, allowing for rapid adaptation with minimal data while mitigating overfitting [21]. Additionally, decomposed meta-learning frameworks have been introduced to sequentially optimize span detection and entity classification, improving generalization across domains [22]. Other advances include the integration of knowledge graphs to enhance prototype-based few-shot NER models [23] and contrastive learning to improve entity cluster separation in low-resource settings [24]. Despite these advancements, few-shot NER continues to face challenges in handling complex entity dependencies and adapting to highly diverse domains, necessitating further innovations in adaptive learning and self-supervised techniques. \n\nMeta-Learning Approaches in NER. Meta-learning has gained significant traction in natural language processing (NLP), particularly for Named Entity Recognition (NER), where labeled data is often scarce.",
                    "score": 0.39485124668227134,
                    "section_title": "Related Work",
                    "char_start_offset": 5509,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 858
                        },
                        {
                            "start": 861,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1751
                        },
                        {
                            "start": 1752,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 2192
                        },
                        {
                            "start": 2195,
                            "end": 2227
                        },
                        {
                            "start": 2228,
                            "end": 2396
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 202,
                            "end": 206,
                            "matchedPaperCorpusId": "273254452"
                        },
                        {
                            "start": 666,
                            "end": 670,
                            "matchedPaperCorpusId": "270353779"
                        },
                        {
                            "start": 1257,
                            "end": 1261,
                            "matchedPaperCorpusId": "236486243"
                        },
                        {
                            "start": 1562,
                            "end": 1566,
                            "matchedPaperCorpusId": "260172183"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named Entity Recognition in a Multilingual Framework\n# Venue: arXiv.org\n# Authors: Ibrahim Bouabdallaoui, Fatima Guerouate, Samya Bouhaddour, Chaimae Saadi, Mohammed Sbihi\n## Abstract\nWe introduce FewTopNER, a novel framework that integrates few-shot named entity recognition (NER) with topic-aware contextual modeling to address the challenges of cross-lingual and low-resource scenarios. FewTopNER leverages a shared multilingual encoder based on XLM-RoBERTa, augmented with language-specific calibration mechanisms, to generate robust contextual embeddings. The architecture comprises a prototype-based entity recognition branch, employing BiLSTM and Conditional Random Fields for sequence labeling, and a topic modeling branch that extracts document-level semantic features through hybrid probabilistic and neural methods. A cross-task bridge facilitates dynamic bidirectional attention and feature fusion between entity and topic representations, thereby enhancing entity disambiguation by incorporating global semantic context. Empirical evaluations on multilingual benchmarks across English, French, Spanish, German, and Italian demonstrate that FewTopNER significantly outperforms existing state-of-the-art few-shot NER models. In particular, the framework achieves improvements of 2.5-4.0 percentage points in F1 score and exhibits enhanced topic coherence, as measured by normalized pointwise mutual information. Ablation studies further confirm the critical contributions of the shared encoder and cross-task integration mechanisms to the overall performance. These results underscore the efficacy of incorporating topic-aware context into few-shot NER and highlight the potential of FewTopNER for robust cross-lingual applications in low-resource settings.\n## Related Work\nTechniques such as parameter-sharing and fine-tuning have been employed to optimize representations across different domains, improving adaptability while minimizing the need for extensive labeled data [18]. However, conventional transfer learning methods often assume that source and target domains share the same label space (homogeneous transfer) or require a sufficient number of annotated examples to bridge the distributional gap in heterogeneous settings. Recent advancements in adversarial domain adaptation have attempted to address these challenges by learning domain-invariant representations, effectively enhancing generalization across diverse datasets [19]. Despite these improvements, challenges persist in adapting NER models to domains with extremely scarce target data, highlighting the need for more efficient domain adaptation techniques. \n\nFew-Shot Learning for NER. Few-shot learning has become a crucial area of research for Named Entity Recognition (NER), enabling models to generalize to new entity types with minimal annotated data. Unlike traditional sequence labeling methods, few-shot NER presents unique challenges due to the variability of entity occurrences within a sentence and the absence of a predefined entity class set [20]. To address these challenges, meta-learning techniques such as FewNER have been proposed. FewNER introduces a task-adaptive training approach that parti-tions the network into task-independent and task-specific components, allowing for rapid adaptation with minimal data while mitigating overfitting [21]. Additionally, decomposed meta-learning frameworks have been introduced to sequentially optimize span detection and entity classification, improving generalization across domains [22]. Other advances include the integration of knowledge graphs to enhance prototype-based few-shot NER models [23] and contrastive learning to improve entity cluster separation in low-resource settings [24]. Despite these advancements, few-shot NER continues to face challenges in handling complex entity dependencies and adapting to highly diverse domains, necessitating further innovations in adaptive learning and self-supervised techniques. \n\nMeta-Learning Approaches in NER. Meta-learning has gained significant traction in natural language processing (NLP), particularly for Named Entity Recognition (NER), where labeled data is often scarce.",
            "reference_string": "[276106916 | Bouabdallaoui et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2077592405",
                    "name": "Peter Schafhalter"
                },
                {
                    "authorId": "2325909159",
                    "name": "Shun Liao"
                },
                {
                    "authorId": "2326167292",
                    "name": "Yanqi Zhou"
                },
                {
                    "authorId": "2325875050",
                    "name": "Chih-Kuan Yeh"
                },
                {
                    "authorId": "2342419598",
                    "name": "Arun Kandoor"
                },
                {
                    "authorId": "2926266",
                    "name": "J. Laudon"
                }
            ],
            "abstract": "Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods often struggle to balance domain-specific performance, retention of general knowledge, and efficiency for training and inference. To address these challenges, we propose Modular Domain Experts (MoDE). MoDE is a mixture-of-experts architecture that augments a general PLMs with modular, domain-specialized experts. These experts are trained independently and composed together via a lightweight training process. In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts. Our evaluation demonstrates that MoDE achieves comparable target performances to full parameter fine-tuning while achieving 1.65% better retention performance. Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations.",
            "corpus_id": 273346210,
            "sentences": [
                {
                    "corpus_id": "273346210",
                    "title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts",
                    "text": "Recent advances in large-scale Pre-trained Language Models (PLMs) have showcased impressive generalization capabilities (Brown et al., 2020;Chowdhery et al., 2023;Anil et al., 2023;Team et al., 2023). However, when applied to specialized domains such as medical, legal, or financial sectors, these general-purpose models often require further fine-tuning to maximize performance on target domains (Huang et al., 2023;Li et al., 2023;Singhal et al., 2023). \n\nA straightforward approach to domain adaptation is full-parameter fine-tuning, where the entire model is further trained on domain-specific data (Houlsby et al., 2019;Bapna et al., 2019). While this method provides strong performance on target domains, full-parameter fine-tuning may lead to catastrophic forgetting where the model loses previously learned capabilities by overfitting to the target domain (Goodfellow et al., 2013;Kirkpatrick et al., 2017). Additionally, this method is memory-intensive to serve in multi-domain settings as each domain has a unique set of parameters, incurring a significant parameter loading overhead when switching between domains (Hu et al., 2021). In such cases, the cost of frequent \"context switches\" significantly impacts performance, making full-parameter fine-tuning impractical for scalable, efficient deployment (Dhar et al., 2024). \n\nTo address the issues of forgetting and memory-efficiency, parameter-efficient fine-tuning methods have been proposed, such as adapter modules (Houlsby et al., 2019), LoRA (Hu et al., 2021), and CoDA (Lei et al., 2023). These methods introduce a small number of trainable parameters and keep the original model frozen during training. Through targeted parameter updates, these approaches are both computationally efficient and effective at retaining prior knowledge (Biderman et al., 2024). Despite these benefits, parameter-efficient methods are limited in their expressive potential and struggle to scale effectively across large domains and datasets (Niederfahrenhorst et al., 2023).",
                    "score": 0.4025196988042179,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 455
                        },
                        {
                            "start": 458,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1335
                        },
                        {
                            "start": 1338,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 2024
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 140,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 140,
                            "end": 163,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 433,
                            "end": 454,
                            "matchedPaperCorpusId": "255124952"
                        },
                        {
                            "start": 603,
                            "end": 625,
                            "matchedPaperCorpusId": "59599816"
                        },
                        {
                            "start": 889,
                            "end": 914,
                            "matchedPaperCorpusId": "4704285"
                        },
                        {
                            "start": 1315,
                            "end": 1334,
                            "matchedPaperCorpusId": "269438883"
                        },
                        {
                            "start": 1481,
                            "end": 1503,
                            "matchedPaperCorpusId": "59599816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: Scalable Multi-Domain Adaptation of Language Models using Modular Experts\n# Venue: arXiv.org\n# Authors: Peter Schafhalter, Shun Liao, Yanqi Zhou, Chih-Kuan Yeh, Arun Kandoor, J. Laudon\n## Abstract\nDomain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods often struggle to balance domain-specific performance, retention of general knowledge, and efficiency for training and inference. To address these challenges, we propose Modular Domain Experts (MoDE). MoDE is a mixture-of-experts architecture that augments a general PLMs with modular, domain-specialized experts. These experts are trained independently and composed together via a lightweight training process. In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts. Our evaluation demonstrates that MoDE achieves comparable target performances to full parameter fine-tuning while achieving 1.65% better retention performance. Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations.\n## INTRODUCTION\nRecent advances in large-scale Pre-trained Language Models (PLMs) have showcased impressive generalization capabilities (Brown et al., 2020;Chowdhery et al., 2023;Anil et al., 2023;Team et al., 2023). However, when applied to specialized domains such as medical, legal, or financial sectors, these general-purpose models often require further fine-tuning to maximize performance on target domains (Huang et al., 2023;Li et al., 2023;Singhal et al., 2023). \n\nA straightforward approach to domain adaptation is full-parameter fine-tuning, where the entire model is further trained on domain-specific data (Houlsby et al., 2019;Bapna et al., 2019). While this method provides strong performance on target domains, full-parameter fine-tuning may lead to catastrophic forgetting where the model loses previously learned capabilities by overfitting to the target domain (Goodfellow et al., 2013;Kirkpatrick et al., 2017). Additionally, this method is memory-intensive to serve in multi-domain settings as each domain has a unique set of parameters, incurring a significant parameter loading overhead when switching between domains (Hu et al., 2021). In such cases, the cost of frequent \"context switches\" significantly impacts performance, making full-parameter fine-tuning impractical for scalable, efficient deployment (Dhar et al., 2024). \n\nTo address the issues of forgetting and memory-efficiency, parameter-efficient fine-tuning methods have been proposed, such as adapter modules (Houlsby et al., 2019), LoRA (Hu et al., 2021), and CoDA (Lei et al., 2023). These methods introduce a small number of trainable parameters and keep the original model frozen during training. Through targeted parameter updates, these approaches are both computationally efficient and effective at retaining prior knowledge (Biderman et al., 2024). Despite these benefits, parameter-efficient methods are limited in their expressive potential and struggle to scale effectively across large domains and datasets (Niederfahrenhorst et al., 2023).",
            "reference_string": "[273346210 | Schafhalter et al. | 2024 | Citations: 1]"
        },
        {
            "title": "On the Calibration of Massively Multilingual Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 44,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.12265",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.12265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "34725175",
                    "name": "Sandipan Dandapat"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                }
            ],
            "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "corpus_id": 253098773,
            "sentences": [
                {
                    "corpus_id": "253098773",
                    "title": "On the Calibration of Massively Multilingual Language Models",
                    "text": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
                    "score": 0.5784683193167356,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                },
                {
                    "corpus_id": "253098773",
                    "title": "On the Calibration of Massively Multilingual Language Models",
                    "text": "In this work we showed that MMLMs like mBERT and XLMR are miscalibrated in a zero-shot cross lingual setting, with the calibration errors being even worse on low resource languages and languages that are typologically distant from the pivot language (often English). We then demonstrated the effectiveness of standard calibration techniques for improving calibration across languages both with and without collecting any new languagespecific labelled data. We recommend that researchers and practitioners consider, measure and report the calibration of multilingual models while using them for scientific studies and building systems. In future work, we aim to bridge the gap between zero-shot and few-shot calibration methods by exploring unsupervised calibration methods under domain shift (Pampari and Ermon, 2020;Park et al., 2020) that utilizes unlabelled data in new domains to improve calibration. Investigating the cross lingual calibration of MMLMs for tasks other than sentence classification like Sequence Labelling (Pan et al., 2017;Nivre et al., 2018) and Question Answering (Artetxe et al., 2020) is another natural extension of our work.",
                    "score": 0.5429008234016809,
                    "section_title": "Conclusion",
                    "char_start_offset": 12294,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1152
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 817,
                            "end": 835,
                            "matchedPaperCorpusId": "211677721"
                        },
                        {
                            "start": 1027,
                            "end": 1045,
                            "matchedPaperCorpusId": "29939583"
                        },
                        {
                            "start": 1088,
                            "end": 1109,
                            "matchedPaperCorpusId": "204901567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                },
                {
                    "corpus_id": "253098773",
                    "title": "On the Calibration of Massively Multilingual Language Models",
                    "text": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai and Durrett, 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai and Durrett, 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).",
                    "score": 0.39886228064543816,
                    "section_title": "Calibration Methods",
                    "char_start_offset": 4891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 256
                        },
                        {
                            "start": 259,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1406
                        },
                        {
                            "start": 1407,
                            "end": 1623
                        },
                        {
                            "start": 1626,
                            "end": 1858
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 135,
                            "end": 153,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1081,
                            "end": 1102,
                            "matchedPaperCorpusId": "174802983"
                        },
                        {
                            "start": 1140,
                            "end": 1165,
                            "matchedPaperCorpusId": "212747810"
                        },
                        {
                            "start": 1382,
                            "end": 1405,
                            "matchedPaperCorpusId": "226262344"
                        },
                        {
                            "start": 1512,
                            "end": 1537,
                            "matchedPaperCorpusId": "212747810"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: On the Calibration of Massively Multilingual Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, M. Choudhury\n## Abstract\nMassively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.\n## Calibration Methods\nWe briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai and Durrett, 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai and Durrett, 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).\n\n## Conclusion\nIn this work we showed that MMLMs like mBERT and XLMR are miscalibrated in a zero-shot cross lingual setting, with the calibration errors being even worse on low resource languages and languages that are typologically distant from the pivot language (often English). We then demonstrated the effectiveness of standard calibration techniques for improving calibration across languages both with and without collecting any new languagespecific labelled data. We recommend that researchers and practitioners consider, measure and report the calibration of multilingual models while using them for scientific studies and building systems. In future work, we aim to bridge the gap between zero-shot and few-shot calibration methods by exploring unsupervised calibration methods under domain shift (Pampari and Ermon, 2020;Park et al., 2020) that utilizes unlabelled data in new domains to improve calibration. Investigating the cross lingual calibration of MMLMs for tasks other than sentence classification like Sequence Labelling (Pan et al., 2017;Nivre et al., 2018) and Question Answering (Artetxe et al., 2020) is another natural extension of our work.",
            "reference_string": "[253098773 | Ahuja et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.03991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291031833",
                    "name": "Yuxi Xia"
                },
                {
                    "authorId": "152746845",
                    "name": "Pedro Henrique Luz de Araujo"
                },
                {
                    "authorId": "46214177",
                    "name": "Klim Zaporojets"
                },
                {
                    "authorId": "2266756543",
                    "name": "Benjamin Roth"
                }
            ],
            "abstract": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
            "corpus_id": 275342783,
            "sentences": [
                {
                    "corpus_id": "275342783",
                    "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
                    "text": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
                    "score": 0.5507126322443081,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles\n# Venue: arXiv.org\n# Authors: Yuxi Xia, Pedro Henrique Luz de Araujo, Klim Zaporojets, Benjamin Roth\n## Abstract\nCalibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.\n",
            "reference_string": "[275342783 | Xia et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Adapting a Language Model While Preserving its General Knowledge",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 21,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.08986",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.08986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9623089",
                    "name": "Zixuan Ke"
                },
                {
                    "authorId": "74175857",
                    "name": "Yijia Shao"
                },
                {
                    "authorId": "2257447835",
                    "name": "Haowei Lin"
                },
                {
                    "authorId": "33464127",
                    "name": "Hu Xu"
                },
                {
                    "authorId": "145142456",
                    "name": "Lei Shu"
                },
                {
                    "authorId": "47655556",
                    "name": "Bin Liu"
                }
            ],
            "abstract": "Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach.",
            "corpus_id": 256105391,
            "sentences": [
                {
                    "corpus_id": "256105391",
                    "title": "Adapting a Language Model While Preserving its General Knowledge",
                    "text": "Pre-trained general-purpose language models (LMs) like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-3 (Brown et al., 2020) have become a standard component in almost all NLP applications. Researchers have also found that domain-adaptive pre-training (or DA-training for short) using an unlabeled corpus in a specific domain to adapt an LM can further improve the end-task performance in the domain (Gururangan et al., 2020;Xu et al., 2019a,b;Sun et al., 2019;Alsentzer et al., 2019). Note that domain-adaptive pre-training is also called post-training (Xu et al., 2019a).\n\nExisting DA-training methods simply apply the same pre-training objective, i.e., the mask language model (MLM) loss, to further train an LM using a domain corpus. These methods are sub-optimal because they do not explicitly identify what should be preserved and what should be updated in the LM by the domain corpus.\n\nThis paper argues that a good DA-training method has two needs. On the one hand, the general language knowledge learned in the LM should be preserved as much as possible because the target domain data is typically not large enough to be sufficient to learn the general knowledge well. For example, some words and their contexts may appear infrequently in a particular domain. The knowledge about them cannot be learned accurately based on the domain data alone. When these words and contexts appear in an end-task, the system will have difficulties. Thus, we need to rely on the knowledge about them in the LM. Since existing DA-training updates the LM with little guidance, such useful general knowledge may be corrupted. On the other hand, due to polysemy (same word with different meanings in different domains) and the fact that different domains also have their special word usages and contexts, the LM should be specialized or adapted to the target domain. A good DA-training should balance these two needs to adapt the LM to the target domain with minimal corruption to the good general knowledge in the LM. This paper proposes a novel technique to enable a more informed adaptation to (1) preserve",
                    "score": 0.41644481623616314,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 60,
                            "end": 81,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 121,
                            "end": 141,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 461,
                            "end": 478,
                            "matchedPaperCorpusId": "153312532"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: Adapting a Language Model While Preserving its General Knowledge\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu, Bin Liu\n## Abstract\nDomain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach.\n## Introduction\nPre-trained general-purpose language models (LMs) like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-3 (Brown et al., 2020) have become a standard component in almost all NLP applications. Researchers have also found that domain-adaptive pre-training (or DA-training for short) using an unlabeled corpus in a specific domain to adapt an LM can further improve the end-task performance in the domain (Gururangan et al., 2020;Xu et al., 2019a,b;Sun et al., 2019;Alsentzer et al., 2019). Note that domain-adaptive pre-training is also called post-training (Xu et al., 2019a).\n\nExisting DA-training methods simply apply the same pre-training objective, i.e., the mask language model (MLM) loss, to further train an LM using a domain corpus. These methods are sub-optimal because they do not explicitly identify what should be preserved and what should be updated in the LM by the domain corpus.\n\nThis paper argues that a good DA-training method has two needs. On the one hand, the general language knowledge learned in the LM should be preserved as much as possible because the target domain data is typically not large enough to be sufficient to learn the general knowledge well. For example, some words and their contexts may appear infrequently in a particular domain. The knowledge about them cannot be learned accurately based on the domain data alone. When these words and contexts appear in an end-task, the system will have difficulties. Thus, we need to rely on the knowledge about them in the LM. Since existing DA-training updates the LM with little guidance, such useful general knowledge may be corrupted. On the other hand, due to polysemy (same word with different meanings in different domains) and the fact that different domains also have their special word usages and contexts, the LM should be specialized or adapted to the target domain. A good DA-training should balance these two needs to adapt the LM to the target domain with minimal corruption to the good general knowledge in the LM. This paper proposes a novel technique to enable a more informed adaptation to (1) preserve",
            "reference_string": "[256105391 | Ke et al. | 2023 | Citations: 21]"
        },
        {
            "title": "Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 35,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.12427",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.12427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109477294",
                    "name": "Dongkyu Lee"
                },
                {
                    "authorId": "33992653",
                    "name": "Zhiliang Tian"
                },
                {
                    "authorId": "2118977303",
                    "name": "Ying Zhao"
                },
                {
                    "authorId": "2067676799",
                    "name": "K. Cheung"
                },
                {
                    "authorId": "3587277",
                    "name": "N. Zhang"
                }
            ],
            "abstract": "In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \u201cwhen to distill such knowledge.\u201d The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.",
            "corpus_id": 253097876,
            "sentences": [
                {
                    "corpus_id": "253097876",
                    "title": "Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model",
                    "text": "In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \u201cwhen to distill such knowledge.\u201d The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.",
                    "score": 0.4511929252658158,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Dongkyu Lee, Zhiliang Tian, Ying Zhao, K. Cheung, N. Zhang\n## Abstract\nIn knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \u201cwhen to distill such knowledge.\u201d The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.\n",
            "reference_string": "[253097876 | Lee et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 44,
            "citation_count": 26,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.102.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2865034",
                    "name": "Lingkai Kong"
                },
                {
                    "authorId": "5795999",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "8103389",
                    "name": "Yuchen Zhuang"
                },
                {
                    "authorId": "2053220976",
                    "name": "Jie Lyu"
                },
                {
                    "authorId": "36345161",
                    "name": "T. Zhao"
                },
                {
                    "authorId": "145657504",
                    "name": "Chao Zhang"
                }
            ],
            "abstract": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at this https URL.",
            "corpus_id": 222327644,
            "sentences": [
                {
                    "corpus_id": "222327644",
                    "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing",
                    "text": "semantic and syntactic information from such a large corpus, the language models are designed to have enormous capacity, e.g., T5 has about 11 billion parameters. At the fine-tuning stage, however, only limited labeled data are available in the downstream tasks. With the extremely high capacity, these models can easily overfit training data likelihood and be over-confident in their predictions.\n\nTo fight against miscalibration, a natural option is to apply a calibration method such as temperature scaling  in a post-processing step. However, temperature scaling only learns a single parameter to rescale all the logits, which is not flexible and insufficient. Moreover, it cannot improve out-of-distribution calibration. A second option is to mitigate miscalibration during training using regularization. For example, Pereyra et al. (2017) propose an entropy regularizer to prevent over-confidence, but it can needlessly hurt legitimate high confident predictions. A third option is to use Bayesian neural networks (Blundell et al., 2015;Louizos and Welling, 2017), which treat model parameters as probability distributions to represent model uncertainty explicitly. However, these Bayesian approaches are often prohibitive, as the priors of the model parameters are difficult to specify, and exact inference is intractable, which can also lead to unreliable uncertainty estimates.\n\nWe propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples",
                    "score": 0.4145927069443502,
                    "section_title": "Introduction",
                    "char_start_offset": 3374,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1020,
                            "end": 1043,
                            "matchedPaperCorpusId": "39895556"
                        },
                        {
                            "start": 1043,
                            "end": 1069,
                            "matchedPaperCorpusId": "9280646"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, T. Zhao, Chao Zhang\n## Abstract\nFine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at this https URL.\n## Introduction\nsemantic and syntactic information from such a large corpus, the language models are designed to have enormous capacity, e.g., T5 has about 11 billion parameters. At the fine-tuning stage, however, only limited labeled data are available in the downstream tasks. With the extremely high capacity, these models can easily overfit training data likelihood and be over-confident in their predictions.\n\nTo fight against miscalibration, a natural option is to apply a calibration method such as temperature scaling  in a post-processing step. However, temperature scaling only learns a single parameter to rescale all the logits, which is not flexible and insufficient. Moreover, it cannot improve out-of-distribution calibration. A second option is to mitigate miscalibration during training using regularization. For example, Pereyra et al. (2017) propose an entropy regularizer to prevent over-confidence, but it can needlessly hurt legitimate high confident predictions. A third option is to use Bayesian neural networks (Blundell et al., 2015;Louizos and Welling, 2017), which treat model parameters as probability distributions to represent model uncertainty explicitly. However, these Bayesian approaches are often prohibitive, as the priors of the model parameters are difficult to specify, and exact inference is intractable, which can also lead to unreliable uncertainty estimates.\n\nWe propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples",
            "reference_string": "[222327644 | Kong et al. | 2020 | Citations: 26]"
        },
        {
            "title": "A Study on the Calibration of In-context Learning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2119078297",
                    "name": "Hanlin Zhang"
                },
                {
                    "authorId": "2145062298",
                    "name": "Yi-Fan Zhang"
                },
                {
                    "authorId": "2271883337",
                    "name": "Yaodong Yu"
                },
                {
                    "authorId": "10723295",
                    "name": "Dhruv Madeka"
                },
                {
                    "authorId": "2261495467",
                    "name": "Dean Foster"
                },
                {
                    "authorId": "2273055197",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2273056782",
                    "name": "Hima Lakkaraju"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                }
            ],
            "abstract": "Accurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.",
            "corpus_id": 266051956,
            "sentences": [
                {
                    "corpus_id": "266051956",
                    "title": "A Study on the Calibration of In-context Learning",
                    "text": "Accurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.",
                    "score": 0.6854957139538604,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                }
            ],
            "relevance_judgement": 0.9228515625,
            "relevance_judgment_input_expanded": "# Title: A Study on the Calibration of In-context Learning\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric P. Xing, Hima Lakkaraju, S. Kakade\n## Abstract\nAccurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.\n",
            "reference_string": "[266051956 | Zhang et al. | 2023 | Citations: 16]"
        },
        {
            "title": "Unsupervised Domain Adaptation with Adapter",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.00667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48263731",
                    "name": "Rongsheng Zhang"
                },
                {
                    "authorId": "51456789",
                    "name": "Yinhe Zheng"
                },
                {
                    "authorId": "29422474",
                    "name": "Xiaoxi Mao"
                },
                {
                    "authorId": "1730108",
                    "name": "Minlie Huang"
                }
            ],
            "abstract": "Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities.",
            "corpus_id": 240354626,
            "sentences": [
                {
                    "corpus_id": "240354626",
                    "title": "Unsupervised Domain Adaptation with Adapter",
                    "text": "Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . \n\nOur contributions can be summarized as: \n\n1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters are introduced in a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. \n\n2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. The most similar works comparing to our study are the models for zero-shot cross-lingual transfer tasks [Pfeiffer et al., 2020, Vidoni et al., 2020]. However, these models aim to separate language-specific knowledge using adapters, while our UDA task tries to capture common and transferable features across different domains. \n\n3 Method",
                    "score": 0.39691289511820693,
                    "section_title": "Introduction",
                    "char_start_offset": 1794,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1086
                        },
                        {
                            "start": 1089,
                            "end": 1128
                        },
                        {
                            "start": 1131,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1342
                        },
                        {
                            "start": 1345,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1854
                        },
                        {
                            "start": 1857,
                            "end": 1865
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 93,
                            "end": 115,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 267,
                            "end": 288,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                }
            ],
            "relevance_judgement": 0.9228515625,
            "relevance_judgment_input_expanded": "# Title: Unsupervised Domain Adaptation with Adapter\n# Venue: arXiv.org\n# Authors: Rongsheng Zhang, Yinhe Zheng, Xiaoxi Mao, Minlie Huang\n## Abstract\nUnsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities.\n## Introduction\nSpecifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . \n\nOur contributions can be summarized as: \n\n1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters are introduced in a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. \n\n2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. The most similar works comparing to our study are the models for zero-shot cross-lingual transfer tasks [Pfeiffer et al., 2020, Vidoni et al., 2020]. However, these models aim to separate language-specific knowledge using adapters, while our UDA task tries to capture common and transferable features across different domains. \n\n3 Method",
            "reference_string": "[240354626 | Zhang et al. | 2021 | Citations: 18]"
        },
        {
            "title": "Fine-tuning Large Language Models for Domain-specific Machine Translation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 41,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288255638",
                    "name": "Jiawei Zheng"
                },
                {
                    "authorId": "2288156228",
                    "name": "Hanghai Hong"
                },
                {
                    "authorId": "2311243257",
                    "name": "Xiaoli Wang"
                },
                {
                    "authorId": "2287986743",
                    "name": "Jingsong Su"
                },
                {
                    "authorId": "2286595190",
                    "name": "Yonggui Liang"
                },
                {
                    "authorId": "2286341675",
                    "name": "Shikai Wu"
                }
            ],
            "abstract": "Large language models (LLMs) have shown great potential in domain-specific machine translation (MT). However, one major issue is that LLMs pre-trained on general domain corpus might not generalize well to specific domains due to the lack of domain-specific knowledge. To address this issue, this paper focuses on enhancing the domain-specific MT capability of LLMs, by providing high-quality training datasets and proposing a novel fine-tuning framework denoted by DragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced prompting integrates dictionary information into prompts to improve the translation of domain-specific terminology.; (ii) RAG-based few-shot example selection provides high-quality examples that simulate both the domain and style characteristics; (iii) Fine-tuning with few-shot examples further enhances performance when using in-domain examples. We deploy DragFT on three well-known LLM backbones with 13B training parameters to validate its effectiveness. The results on three domain-specific datasets show that DragFT achieves a significant performance boost and shows superior performance compared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance improvement of DragFT over existing LLMs can be attributed to incorporating relevant knowledge while mitigating noise.",
            "corpus_id": 267897581,
            "sentences": [
                {
                    "corpus_id": "267897581",
                    "title": "Fine-tuning Large Language Models for Domain-specific Machine Translation",
                    "text": "Large language models (LLMs) have shown great potential in domain-specific machine translation (MT). However, one major issue is that LLMs pre-trained on general domain corpus might not generalize well to specific domains due to the lack of domain-specific knowledge. To address this issue, this paper focuses on enhancing the domain-specific MT capability of LLMs, by providing high-quality training datasets and proposing a novel fine-tuning framework denoted by DragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced prompting integrates dictionary information into prompts to improve the translation of domain-specific terminology.; (ii) RAG-based few-shot example selection provides high-quality examples that simulate both the domain and style characteristics; (iii) Fine-tuning with few-shot examples further enhances performance when using in-domain examples. We deploy DragFT on three well-known LLM backbones with 13B training parameters to validate its effectiveness. The results on three domain-specific datasets show that DragFT achieves a significant performance boost and shows superior performance compared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance improvement of DragFT over existing LLMs can be attributed to incorporating relevant knowledge while mitigating noise.",
                    "score": 0.42221915016456035,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92236328125
                }
            ],
            "relevance_judgement": 0.92236328125,
            "relevance_judgment_input_expanded": "# Title: Fine-tuning Large Language Models for Domain-specific Machine Translation\n# Venue: arXiv.org\n# Authors: Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, Shikai Wu\n## Abstract\nLarge language models (LLMs) have shown great potential in domain-specific machine translation (MT). However, one major issue is that LLMs pre-trained on general domain corpus might not generalize well to specific domains due to the lack of domain-specific knowledge. To address this issue, this paper focuses on enhancing the domain-specific MT capability of LLMs, by providing high-quality training datasets and proposing a novel fine-tuning framework denoted by DragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced prompting integrates dictionary information into prompts to improve the translation of domain-specific terminology.; (ii) RAG-based few-shot example selection provides high-quality examples that simulate both the domain and style characteristics; (iii) Fine-tuning with few-shot examples further enhances performance when using in-domain examples. We deploy DragFT on three well-known LLM backbones with 13B training parameters to validate its effectiveness. The results on three domain-specific datasets show that DragFT achieves a significant performance boost and shows superior performance compared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance improvement of DragFT over existing LLMs can be attributed to incorporating relevant knowledge while mitigating noise.\n",
            "reference_string": "[267897581 | Zheng et al. | 2024 | Citations: 41]"
        },
        {
            "title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.14660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336554237",
                    "name": "Zijun Chen"
                },
                {
                    "authorId": "2269720631",
                    "name": "Wenbo Hu"
                },
                {
                    "authorId": "2218509878",
                    "name": "Guande He"
                },
                {
                    "authorId": "2336610933",
                    "name": "Zhijie Deng"
                },
                {
                    "authorId": "2330237920",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2269465734",
                    "name": "Richang Hong"
                }
            ],
            "abstract": "Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: https://github.com/hfutml/Calibration-MLLM.",
            "corpus_id": 274859755,
            "sentences": [
                {
                    "corpus_id": "274859755",
                    "title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models",
                    "text": "Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: https://github.com/hfutml/Calibration-MLLM.",
                    "score": 0.43133980766797964,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models\n# Venue: arXiv.org\n# Authors: Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong\n## Abstract\nMultimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: https://github.com/hfutml/Calibration-MLLM.\n",
            "reference_string": "[274859755 | Chen et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Paloma: A Benchmark for Evaluating Language Model Fit",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 87,
            "citation_count": 27,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.10523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2124977543",
                    "name": "Ian Magnusson"
                },
                {
                    "authorId": "2166136235",
                    "name": "Akshita Bhagia"
                },
                {
                    "authorId": "2275055850",
                    "name": "Valentin Hofmann"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "47286118",
                    "name": "A. Jha"
                },
                {
                    "authorId": "3385516",
                    "name": "Oyvind Tafjord"
                },
                {
                    "authorId": "2264248042",
                    "name": "Dustin Schwenk"
                },
                {
                    "authorId": "2158819969",
                    "name": "Pete Walsh"
                },
                {
                    "authorId": "51131518",
                    "name": "Yanai Elazar"
                },
                {
                    "authorId": "46258841",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "3458736",
                    "name": "Dirk Groeneveld"
                },
                {
                    "authorId": "2260133345",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2264002618",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "46666605",
                    "name": "Kyle Richardson"
                },
                {
                    "authorId": "34176020",
                    "name": "Jesse Dodge"
                }
            ],
            "abstract": "Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains--varying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma), a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.",
            "corpus_id": 266348815,
            "sentences": [
                {
                    "corpus_id": "266348815",
                    "title": "Paloma: A Benchmark for Evaluating Language Model Fit",
                    "text": "PERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESSMENT (PALOMA) is for examining LM fit to domains. We use perplexity (and related metrics; \u00a72.4) to measure fit to the distributions of language represented by different domains. We take relative differences in LM fit as a proxy of model familiarity to the shared knowledge, values, and social context that position the humans producing language in a domain. While we expect contemporary LMs to have a limited fit to the most complex of these latent factors of textual domains, improving fit to all factors is important both to improve perplexity and for actual use of the LM. For example, better perplexity on a particular dialect of English suggests that model will make a better chatbot for people that speak that dialect. \n\nPALOMA comprises several types of artifacts for enabling a science of language modeling: training and evaluation guidelines for experiments on LM fit ( \u00a72.1), evaluation data for assessing fit to specific domains ( \u00a72.2), 6 pretrained baselines following training guidelines ( \u00a72.3), metrics computed by our standardized inference code conforming to our evaluation guidelines ( \u00a72.4), and a submission process for coordinating comparable results across the research community ( \u00a72.5).",
                    "score": 0.42684333084302817,
                    "section_title": "PALOMA",
                    "char_start_offset": 5448,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 769
                        },
                        {
                            "start": 772,
                            "end": 1256
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9189453125
                }
            ],
            "relevance_judgement": 0.9189453125,
            "relevance_judgment_input_expanded": "# Title: Paloma: A Benchmark for Evaluating Language Model Fit\n# Venue: arXiv.org\n# Authors: Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, A. Jha, Oyvind Tafjord, Dustin Schwenk, Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hanna Hajishirzi, Noah A. Smith, Kyle Richardson, Jesse Dodge\n## Abstract\nEvaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains--varying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma), a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.\n## PALOMA\nPERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESSMENT (PALOMA) is for examining LM fit to domains. We use perplexity (and related metrics; \u00a72.4) to measure fit to the distributions of language represented by different domains. We take relative differences in LM fit as a proxy of model familiarity to the shared knowledge, values, and social context that position the humans producing language in a domain. While we expect contemporary LMs to have a limited fit to the most complex of these latent factors of textual domains, improving fit to all factors is important both to improve perplexity and for actual use of the LM. For example, better perplexity on a particular dialect of English suggests that model will make a better chatbot for people that speak that dialect. \n\nPALOMA comprises several types of artifacts for enabling a science of language modeling: training and evaluation guidelines for experiments on LM fit ( \u00a72.1), evaluation data for assessing fit to specific domains ( \u00a72.2), 6 pretrained baselines following training guidelines ( \u00a72.3), metrics computed by our standardized inference code conforming to our evaluation guidelines ( \u00a72.4), and a submission process for coordinating comparable results across the research community ( \u00a72.5).",
            "reference_string": "[266348815 | Magnusson et al. | 2023 | Citations: 27]"
        },
        {
            "title": "Tailoring Domain Adaptation for Machine Translation Quality Estimation",
            "venue": "European Association for Machine Translation Conferences/Workshops",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.08891",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1630385856",
                    "name": "Javad Pourmostafa Roshan Sharami"
                },
                {
                    "authorId": "2129048972",
                    "name": "D. Shterionov"
                },
                {
                    "authorId": "1948646",
                    "name": "F. Blain"
                },
                {
                    "authorId": "32380598",
                    "name": "Eva Vanmassenhove"
                },
                {
                    "authorId": "1381666584",
                    "name": "M. Sisto"
                },
                {
                    "authorId": "2709440",
                    "name": "Chris Emmery"
                },
                {
                    "authorId": "1746390",
                    "name": "P. Spronck"
                }
            ],
            "abstract": "While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizabile, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues \u2014 data scarcity and domain mismatch \u2014 this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.",
            "corpus_id": 258187507,
            "sentences": [
                {
                    "corpus_id": "258187507",
                    "title": "Tailoring Domain Adaptation for Machine Translation Quality Estimation",
                    "text": "This paper addresses two key challenges related to quality estimation (QE) of machine translation (MT): (i) the scarcity of available QE data and (ii) the difficulties in estimating translations across diverse domains. The primary aim of this study is to enhance the performance of QE models by addressing these challenges. To do so, we propose a solution that utilizes domain adaptation (DA) techniques adopted from MT. We adapt the \"mixed fine-tuning + fine-tuning\" approach (Chu et al., 2017) and extend it with data augmentation as an alternative to the traditional oversampling technique. We adopt a three-step training methodology: (i) we fine-tune XLM-R, a language model, with a large generic QE dataset, which enables the model to generalize; (ii) we fine-tune the model with a mix of out-of-domain (OOD) and indomain (ID) data derived from two data augmentation (DAG) approaches; and (iii) we fine-tune the model with a small amount of domain-specific data, which leads to a more specific model. We evaluated models' performance with and without domain tags appended to the sentences. \n\nOur experiments show significant improvements across all language pairs under consideration, indicating that our proposed solution has a beneficial impact in addressing the aforementioned challenges. Our study also demonstrates the effectiveness of both proposed DAG approaches and shows that using domain tags improves the performance of the models. Additionally, we find that our model outperforms the baseline in the context of zeroshot learning and in cross-lingual inference. \n\nMoving forward, there are several directions for future work based on our findings. First, it would be interesting to investigate the performance of our pipeline on low-resource language pairs, where there is limited ID data available. This is particularly relevant given the smaller coverage of QE datasets compared to parallel data in MT. Second, we only used one type of OOD data in our experiments (EN-IT); it would be useful to explore other OOD data over different language pairs for QE. Third, it would be valuable to study the performance of other LLMs than XLM-R.",
                    "score": 0.41376423697877485,
                    "section_title": "Conclusion and future work",
                    "char_start_offset": 26639,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1094
                        },
                        {
                            "start": 1097,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1577
                        },
                        {
                            "start": 1580,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 1920
                        },
                        {
                            "start": 1921,
                            "end": 2073
                        },
                        {
                            "start": 2074,
                            "end": 2152
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 477,
                            "end": 495,
                            "matchedPaperCorpusId": "35273027"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91845703125
                }
            ],
            "relevance_judgement": 0.91845703125,
            "relevance_judgment_input_expanded": "# Title: Tailoring Domain Adaptation for Machine Translation Quality Estimation\n# Venue: European Association for Machine Translation Conferences/Workshops\n# Authors: Javad Pourmostafa Roshan Sharami, D. Shterionov, F. Blain, Eva Vanmassenhove, M. Sisto, Chris Emmery, P. Spronck\n## Abstract\nWhile quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizabile, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues \u2014 data scarcity and domain mismatch \u2014 this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.\n## Conclusion and future work\nThis paper addresses two key challenges related to quality estimation (QE) of machine translation (MT): (i) the scarcity of available QE data and (ii) the difficulties in estimating translations across diverse domains. The primary aim of this study is to enhance the performance of QE models by addressing these challenges. To do so, we propose a solution that utilizes domain adaptation (DA) techniques adopted from MT. We adapt the \"mixed fine-tuning + fine-tuning\" approach (Chu et al., 2017) and extend it with data augmentation as an alternative to the traditional oversampling technique. We adopt a three-step training methodology: (i) we fine-tune XLM-R, a language model, with a large generic QE dataset, which enables the model to generalize; (ii) we fine-tune the model with a mix of out-of-domain (OOD) and indomain (ID) data derived from two data augmentation (DAG) approaches; and (iii) we fine-tune the model with a small amount of domain-specific data, which leads to a more specific model. We evaluated models' performance with and without domain tags appended to the sentences. \n\nOur experiments show significant improvements across all language pairs under consideration, indicating that our proposed solution has a beneficial impact in addressing the aforementioned challenges. Our study also demonstrates the effectiveness of both proposed DAG approaches and shows that using domain tags improves the performance of the models. Additionally, we find that our model outperforms the baseline in the context of zeroshot learning and in cross-lingual inference. \n\nMoving forward, there are several directions for future work based on our findings. First, it would be interesting to investigate the performance of our pipeline on low-resource language pairs, where there is limited ID data available. This is particularly relevant given the smaller coverage of QE datasets compared to parallel data in MT. Second, we only used one type of OOD data in our experiments (EN-IT); it would be useful to explore other OOD data over different language pairs for QE. Third, it would be valuable to study the performance of other LLMs than XLM-R.",
            "reference_string": "[258187507 | Sharami et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325820281",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2324108547",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "2324343444",
                    "name": "Yanmin Ji"
                },
                {
                    "authorId": "2324131573",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2324234213",
                    "name": "Keren Tan"
                },
                {
                    "authorId": "2323976802",
                    "name": "Yiyu Zheng"
                }
            ],
            "abstract": "While in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as indiscriminate miscalibration. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.",
            "corpus_id": 273098373,
            "sentences": [
                {
                    "corpus_id": "273098373",
                    "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
                    "text": "While in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as indiscriminate miscalibration. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.",
                    "score": 0.6525177151133824,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference\n# Venue: arXiv.org\n# Authors: Wei Cheng, Tianlu Wang, Yanmin Ji, Fan Yang, Keren Tan, Yiyu Zheng\n## Abstract\nWhile in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as indiscriminate miscalibration. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.\n",
            "reference_string": "[273098373 | Cheng et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293667534",
                    "name": "Wenjun Li"
                },
                {
                    "authorId": "2284449061",
                    "name": "Dexun Li"
                },
                {
                    "authorId": "2275185317",
                    "name": "Kuicai Dong"
                },
                {
                    "authorId": "2284302963",
                    "name": "Cong Zhang"
                },
                {
                    "authorId": "2298263602",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2281903918",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2293939763",
                    "name": "Yong Liu"
                }
            ],
            "abstract": "Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or real-time data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, weather/map apps), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues:(1) increased delays due to unnecessary tool calls, and (2) potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, representing the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Our experiments show that MeCo accurately detects LLMs' internal cognitive signals and significantly improves tool-use decision-making across multiple base models and benchmarks.",
            "corpus_id": 276422013,
            "sentences": [
                {
                    "corpus_id": "276422013",
                    "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
                    "text": "The model is instructed to think step-by-step, reasoning why it does or does not need external tools to address the user query, and finally concludes its decision with \"Yes\" or \"No.\" Before delving into the analysis, we provide some background on the concept of calibration in the context of Large Language Models (LLMs). Calibration refers to the alignment between a model's predicted probabilities and the actual likelihood of those predictions being correct. A well-calibrated model generates probability scores that accurately reflect the true probability of its predictions. \n\nIn Figure 6, we present the distribution of P(Yes) scores for both correct and incorrect Yes/No decisions. Our key observations are as follows: \n\n1. When the model is given detailed instructions and few-shot examples, it demonstrates poor calibration. \n\nAs illustrated in Figure 6(a), the distributions of P(Yes) scores for correct and incorrect decisions do not show a clear distinction. \n\n2. Conversely, when the model lacks detailed context and must rely on its internal beliefs to make decisions, it exhibits improved calibration. In Figure 6(b), the peak of the distribution for correct scores clearly deviates from that of incorrect scores. \n\n3. After fine-tuning, the model displays significantly better calibration, as shown in Figures 6(c) and (d). \n\nMost correct decisions have P(Yes) scores of either 1 (indicating \"Yes\") or 0 (indicating \"No\"), while the P(Yes) scores for incorrect decisions vary between 0 and 1.",
                    "score": 0.4185848365485682,
                    "section_title": "CoT (Chain of Thought):",
                    "char_start_offset": 31635,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 579
                        },
                        {
                            "start": 582,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 833
                        },
                        {
                            "start": 836,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 1116
                        },
                        {
                            "start": 1117,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1339
                        },
                        {
                            "start": 1342,
                            "end": 1508
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.91357421875,
            "relevance_judgment_input_expanded": "# Title: Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger\n# Venue: arXiv.org\n# Authors: Wenjun Li, Dexun Li, Kuicai Dong, Cong Zhang, Hao Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Liu\n## Abstract\nLarge language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or real-time data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, weather/map apps), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues:(1) increased delays due to unnecessary tool calls, and (2) potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, representing the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Our experiments show that MeCo accurately detects LLMs' internal cognitive signals and significantly improves tool-use decision-making across multiple base models and benchmarks.\n## CoT (Chain of Thought):\nThe model is instructed to think step-by-step, reasoning why it does or does not need external tools to address the user query, and finally concludes its decision with \"Yes\" or \"No.\" Before delving into the analysis, we provide some background on the concept of calibration in the context of Large Language Models (LLMs). Calibration refers to the alignment between a model's predicted probabilities and the actual likelihood of those predictions being correct. A well-calibrated model generates probability scores that accurately reflect the true probability of its predictions. \n\nIn Figure 6, we present the distribution of P(Yes) scores for both correct and incorrect Yes/No decisions. Our key observations are as follows: \n\n1. When the model is given detailed instructions and few-shot examples, it demonstrates poor calibration. \n\nAs illustrated in Figure 6(a), the distributions of P(Yes) scores for correct and incorrect decisions do not show a clear distinction. \n\n2. Conversely, when the model lacks detailed context and must rely on its internal beliefs to make decisions, it exhibits improved calibration. In Figure 6(b), the peak of the distribution for correct scores clearly deviates from that of incorrect scores. \n\n3. After fine-tuning, the model displays significantly better calibration, as shown in Figures 6(c) and (d). \n\nMost correct decisions have P(Yes) scores of either 1 (indicating \"Yes\") or 0 (indicating \"No\"), while the P(Yes) scores for incorrect decisions vary between 0 and 1.",
            "reference_string": "[276422013 | Li et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Cross-Domain Content Generation with Domain-Specific Small Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326298968",
                    "name": "Ankit Maloo Abhinav Garg"
                }
            ],
            "abstract": "Generating domain-specific content using small language models poses challenges, especially when dealing with multiple distinct datasets with minimal overlap. In this study, we explore methods to enable a small language model to produce coherent and relevant outputs for two different domains: stories (Dataset A) and recipes (Dataset B). Our initial experiments show that training individual models on each dataset yields satisfactory results, with each model generating appropriate content within its domain. We find that utilizing custom tokenizers tailored to each dataset significantly enhances generation quality compared to using a generic tokenizer. Attempts to adapt a single model to both domains using Low-Rank Adaptation (LoRA) or standard fine-tuning do not yield substantial results, often failing to produce meaningful outputs. Moreover, full fine-tuning without freezing the model's existing weights leads to catastrophic forgetting, where the model loses previously learned information and only retains knowledge from the new data. To overcome these challenges, we employ a knowledge expansion strategy: training only with additional parameters. This approach enables the model to generate both stories and recipes upon request, effectively handling multiple domains without suffering from catastrophic forgetting. Our findings demonstrate that knowledge expansion with frozen layers is an effective method for small language models to generate domain-specific content across distinct datasets. This work contributes to the development of efficient multi-domain language models and provides insights into managing catastrophic forgetting in small-scale architectures.",
            "corpus_id": 272911430,
            "sentences": [
                {
                    "corpus_id": "272911430",
                    "title": "Cross-Domain Content Generation with Domain-Specific Small Language Models",
                    "text": "While previous work has addressed various aspects of small language models, multi-domain learning, and catastrophic forgetting, there is a gap in exploring the combination of these areas. Specifically, the application of knowledge expansion techniques to small language models for handling multiple distinct domains has not been extensively studied. \n\nOur work contributes to this area by: \n\n\u2022 Demonstrating that custom tokenizers enhance generation quality in small models trained on specific domains. \n\n\u2022 Showing the limitations of standard fine-tuning and parameter-efficient methods like LoRA in preventing catastrophic forgetting in small models. \n\n\u2022 Introducing a model expansion strategy that adds new layers to a frozen base model, enabling multi-domain generation without overwriting prior knowledge. \n\nBy addressing these challenges, we aim to advance the understanding of how small language models can be effectively adapted to handle multiple, distinct domains without incurring significant computational costs or sacrificing performance in any single domain.",
                    "score": 0.4507244415290497,
                    "section_title": "Our Position in the Literature",
                    "char_start_offset": 8745,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 349
                        },
                        {
                            "start": 352,
                            "end": 389
                        },
                        {
                            "start": 392,
                            "end": 502
                        },
                        {
                            "start": 505,
                            "end": 651
                        },
                        {
                            "start": 654,
                            "end": 809
                        },
                        {
                            "start": 812,
                            "end": 1071
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91162109375
                }
            ],
            "relevance_judgement": 0.91162109375,
            "relevance_judgment_input_expanded": "# Title: Cross-Domain Content Generation with Domain-Specific Small Language Models\n# Venue: arXiv.org\n# Authors: Ankit Maloo Abhinav Garg\n## Abstract\nGenerating domain-specific content using small language models poses challenges, especially when dealing with multiple distinct datasets with minimal overlap. In this study, we explore methods to enable a small language model to produce coherent and relevant outputs for two different domains: stories (Dataset A) and recipes (Dataset B). Our initial experiments show that training individual models on each dataset yields satisfactory results, with each model generating appropriate content within its domain. We find that utilizing custom tokenizers tailored to each dataset significantly enhances generation quality compared to using a generic tokenizer. Attempts to adapt a single model to both domains using Low-Rank Adaptation (LoRA) or standard fine-tuning do not yield substantial results, often failing to produce meaningful outputs. Moreover, full fine-tuning without freezing the model's existing weights leads to catastrophic forgetting, where the model loses previously learned information and only retains knowledge from the new data. To overcome these challenges, we employ a knowledge expansion strategy: training only with additional parameters. This approach enables the model to generate both stories and recipes upon request, effectively handling multiple domains without suffering from catastrophic forgetting. Our findings demonstrate that knowledge expansion with frozen layers is an effective method for small language models to generate domain-specific content across distinct datasets. This work contributes to the development of efficient multi-domain language models and provides insights into managing catastrophic forgetting in small-scale architectures.\n## Our Position in the Literature\nWhile previous work has addressed various aspects of small language models, multi-domain learning, and catastrophic forgetting, there is a gap in exploring the combination of these areas. Specifically, the application of knowledge expansion techniques to small language models for handling multiple distinct domains has not been extensively studied. \n\nOur work contributes to this area by: \n\n\u2022 Demonstrating that custom tokenizers enhance generation quality in small models trained on specific domains. \n\n\u2022 Showing the limitations of standard fine-tuning and parameter-efficient methods like LoRA in preventing catastrophic forgetting in small models. \n\n\u2022 Introducing a model expansion strategy that adds new layers to a frozen base model, enabling multi-domain generation without overwriting prior knowledge. \n\nBy addressing these challenges, we aim to advance the understanding of how small language models can be effectively adapted to handle multiple, distinct domains without incurring significant computational costs or sacrificing performance in any single domain.",
            "reference_string": "[272911430 | Garg | 2024 | Citations: 0]"
        },
        {
            "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
            "venue": "ACM / IMS Journal of Data Science",
            "year": 2022,
            "reference_count": 75,
            "citation_count": 56,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.02318",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.02318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165361707",
                    "name": "Ryan Smith"
                },
                {
                    "authorId": "31592365",
                    "name": "Jason Alan Fries"
                },
                {
                    "authorId": "34302368",
                    "name": "Braden Hancock"
                },
                {
                    "authorId": "2870504",
                    "name": "Stephen H. Bach"
                }
            ],
            "abstract": "We propose a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited. Rather than apply the model in a typical zero-shot or few-shot fashion, we treat the model as the basis for labeling functions in a weak supervision framework. To create a classifier, we first prompt the model to answer multiple distinct queries about an example and define how the possible responses should be mapped to votes for labels and abstentions. We then denoise these noisy label sources using the Snorkel system and train an end classifier with the resulting training data. Our experimental evaluation shows that prompting large language models within a weak supervision framework can provide significant gains in accuracy. On the WRENCH weak supervision benchmark, this approach can significantly improve over zero-shot performance, an average 19.5% reduction in errors. We also find that this approach produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules.",
            "corpus_id": 248524894,
            "sentences": [
                {
                    "corpus_id": "248524894",
                    "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
                    "text": "We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration [68], where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], NULL, <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions.",
                    "score": 0.46960371009431867,
                    "section_title": "Calibration",
                    "char_start_offset": 25122,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1474
                        },
                        {
                            "start": 1475,
                            "end": 1569
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1030,
                            "end": 1034,
                            "matchedPaperCorpusId": "231979430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91064453125
                }
            ],
            "relevance_judgement": 0.91064453125,
            "relevance_judgment_input_expanded": "# Title: Language Models in the Loop: Incorporating Prompting into Weak Supervision\n# Venue: ACM / IMS Journal of Data Science\n# Authors: Ryan Smith, Jason Alan Fries, Braden Hancock, Stephen H. Bach\n## Abstract\nWe propose a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited. Rather than apply the model in a typical zero-shot or few-shot fashion, we treat the model as the basis for labeling functions in a weak supervision framework. To create a classifier, we first prompt the model to answer multiple distinct queries about an example and define how the possible responses should be mapped to votes for labels and abstentions. We then denoise these noisy label sources using the Snorkel system and train an end classifier with the resulting training data. Our experimental evaluation shows that prompting large language models within a weak supervision framework can provide significant gains in accuracy. On the WRENCH weak supervision benchmark, this approach can significantly improve over zero-shot performance, an average 19.5% reduction in errors. We also find that this approach produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules.\n## Calibration\nWe find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration [68], where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], NULL, <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions.",
            "reference_string": "[248524894 | Smith et al. | 2022 | Citations: 56]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Accurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.",
            "score": 0.6854957139538604,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "236486129",
            "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
            "text": "Related works in NLP have considered data from similar tasks but from different datasets as OOD (Ovadia et al., 2019;Hendrycks and Gimpel, 2017). Next, in order to make models more calibrated, we study some of the widelyused recalibration methods, with various degrees of effectiveness and computational cost. For example, ensembling models has been shown to be very effective in out-of-domain settings (Ovadia et al., 2019), but the cost of computation scales with the size of ensembles. On the other hand, distillation (Hinton et al., 2015) is a widely-known method for improving the system's performance by learning from a stronger teacher model. In this work, we empirically examine the connection between distillation and calibration. Notably, we view the objective function of distillation as a regularization term that encourages the student model to match the predictive uncertainty of a stronger, more calibrated teacher model. \n\nWe conduct analysis experiments to show that the teacher's calibration performance could be distilled into the student model, even when the teacher model's accuracy remains similar. With this insight, we show that simple methods based on distillation could achieve competitive performance in out-ofdomain calibration, without introducing extra computation at inference time. Finally, we also conduct ablation experiments to understand the usefulness of components of the method. In summary, our contributions are listed as follows: \n\n\u2022 We present a systematic study on the performance of various recalibration methods on finetuned language models for both in-domain and out-of-domain settings. \n\n\u2022 We empirically examine the connection between distillation and calibration, and conduct experiments showing that distillation can distill calibration performance. \n\n\u2022 We describe two simple recalibration methods, and experimental results demonstrate their competitiveness in the out-of-domain settings; finally, we also ablate method's components and measure the extent to which distillation transfers teachers' calibration improvement.",
            "score": 0.679100115795202,
            "section_title": "Introduction",
            "char_start_offset": 3127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 521,
                    "end": 542,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "273098373",
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "text": "While in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as indiscriminate miscalibration. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.",
            "score": 0.6525177151133824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "score": 0.6452777774061442,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "273228151",
            "title": "QA-Calibration of Language Model Confidence Scores",
            "text": "Calibration for Language Models. Reinforcement learning from human feedback objective may prioritize adherence to user instructions in dialogue over producing well-calibrated predictions. (Kadavath et al., 2022). Lin et al. (2022) introduced the concept of verbalized confidence that prompts LMs to express confidence directly, focusing on fine-tuning, instead of zero-shot verbalized confidence. Mielke et al. (2022) uses an external calibrator for a white-box large language model. Other methods use consistency measures to improve LM calibration (Lyu et al., 2024). Our experimental setup closely relates to recent works in LM confidence elicitation (Tian et al., 2023;Xiong et al., 2024). These methods lack novel posthoc calibrators and do not offer the rigorous calibration guarantees that ours provide. Calibration has been shown to impact selective QA performance Kamath et al. ( 2020), but they focus on uncertainty quantification and assumes that the LM allows access to the model likelihood. \n\nGroup Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2017;Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space.",
            "score": 0.6295137258543526,
            "section_title": "RELATED WORK",
            "char_start_offset": 23662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 230,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 397,
                    "end": 417,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "259224389"
                },
                {
                    "start": 1161,
                    "end": 1185,
                    "matchedPaperCorpusId": "12845273"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "matchedPaperCorpusId": "75455"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Our approach draws inspiration from prior research on calibration, combining it with recent ideas from in-context few-shot learning (Wei et al., 2021;Min et al., 2021), especially Lee et al. (2021), which also trains a model over synthetically generated slices.Below, we further discuss relevant prior work on calibration for LMs and abstention.\n\nCalibration for LMs Recent work has found that pretrained language models appear mostly wellcalibrated on broader distributions (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023), and can express their uncertainty in words (Lin et al., 2022;Mielke et al., 2022;Tian et al., 2023;Zhou et al., 2023).However, these models are still miscalibrated in some settings (Wang et al., 2020;Stengel-Eskin and Durme, 2023), and prior work has focused on recalibrating neural networks by temperature scaling (Guo et al., 2017), Platt scaling (Platt, 1999), isotonic regression (Niculescu-Mizil and Caruana, 2005;Zadrozny and Elkan, 2002), or his-togram binning (Kumar et al., 2019;Zadrozny and Elkan, 2001).However, the aforementioned works don't address miscalibration within narrower slices, or slicespecific calibration.This problem has been more carefully studied outside LM research, such as for vision models (Yu et al., 2022) and from a theoretical angle (Hebert-Johnson et al., 2018).\n\nIn this work, we address this problem for large language models.Also different from prior work is our ability to recalibrate using only unlabeled fewshot examples, whereas previous methods have usually needed a non-trivial number of labeled examples to achieve domain-specific calibration.\n\nAbstention When the model is not confident about a prediction, abstention or deferral to an expert are desirable alternatives compared to responding with the incorrect answer.",
            "score": 0.6280492221984314,
            "section_title": "Related Work",
            "char_start_offset": 23792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 345
                },
                {
                    "start": 347,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1335
                },
                {
                    "start": 1337,
                    "end": 1401
                },
                {
                    "start": 1401,
                    "end": 1626
                },
                {
                    "start": 1628,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 579,
                    "end": 597,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 597,
                    "end": 617,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 851,
                    "end": 869,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 885,
                    "end": 898,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 920,
                    "end": 955,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 955,
                    "end": 980,
                    "matchedPaperCorpusId": "3349576"
                },
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1024,
                    "end": 1049,
                    "matchedPaperCorpusId": "9594071"
                },
                {
                    "start": 1305,
                    "end": 1334,
                    "matchedPaperCorpusId": "51880858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "Pfohl et al. (2022) measure subgroup calibration, but do not discuss it at length.Although the focus in Haghtalab et al. (2023) is mainly theoretical, the authors also run experiments for both their proposed multicalibration algorithms and additional algorithms present in the literature.However, they do not compare the performance of such algorithms against a baseline NN or tree-based method trained on an identical amount of data, and also do not investigate how much data should be used for training the base model vs. used for multicalibration post-processing.We believe such a comparison is important given that ERM may have latent multicalibration properties, and is by far the most used learning algorithm in practice.\n\nIn recent work, Detommaso et al. (2024) utilize multicalibration as a tool to improve the overall uncertainty and confidence calibration of language model but, to our knowledge, do not focus on or report fairness towards protected subgroups.Like us, they point out various issues with the standard multicalibration algorithm, which they address with early stopping and adaptive binning.We instead perform a large hyperparameter sweep which effectively implements an early stopping mechanism.We discuss this further in Appendix A.1.Nonetheless, our results for large models are complementary to those of Detommaso et al. (2024): both works demonstrate that (1) standard multicalibration can at times be difficult to get working in practice; and (2) ideas from the theoretical multicalibration literature can have impact at the scale of large models.We provide additional discussion of related works in model calibration and subgroup robustness in Appendix A.",
            "score": 0.6254042024732623,
            "section_title": "Related Works: Theory and Practice",
            "char_start_offset": 11593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 727
                },
                {
                    "start": 729,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1260
                },
                {
                    "start": 1260,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 127,
                    "matchedPaperCorpusId": "261959418"
                },
                {
                    "start": 745,
                    "end": 768,
                    "matchedPaperCorpusId": "269004786"
                },
                {
                    "start": 1332,
                    "end": 1355,
                    "matchedPaperCorpusId": "269004786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "We have shown that while LMs appear to be wellcalibrated on broad distributions, they remain miscalibrated for meaningful slices of that broader distribution.To recalibrate them for each slice, we propose few-shot recalibration which takes fewshot, unlabeled queries and predicts a slice-specific precision curve.We then use the predicted precision curve for two downstream calibration tasks, finding that our approach consistently outperforms existing recalibration methods under all evaluation settings.Future work should study few-shot recalibration for natural language generation tasks, to steer model generated text to be more or less conservative, as well as apply this approach to a broader set of models, including instruction-tuned and RLHF models, and multimodal settings.",
            "score": 0.6212202065180632,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 26584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 783
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Researchers often study LM calibration for aggregate query distributions (p), which are often composed of mixtures of meaningful finer-grained distributions: p(x) = d\u2208D \u03b1 d p d (x), where D denotes a set of domains, and each p d denotes the distribution of domain d, with relative frequency \u03b1 d .For instance, OpenAI (2023) and Kadavath et al. (2022) have reported LM calibration on MMLU, which consists of 57 individual domains like abstract algebra, high school chemistry etc.However, in practice, users querying an LM at a given point rarely sample from a broad aggregate distribution.They are more likely to sample from meaningful slices, like queries from abstract algebra alone.Yu et al. (2022);Hebert-Johnson et al. (2018) have shown that individual domains often suffer from miscalibration problem even if the aggregate distribution appears well-calibrated.\n\nTo demonstrate the same phenomenon for language models, we measure calibration of LLaMA-65B on combined MMLU (p), and also on each domain separately.As expected, the model is wellcalibrated on p.However, the LM is significantly miscalibrated for most domains.This is shown in (Figure 2) where the aggregate ECE is lower than that of most domains.It appears that the miscalibration problem is hidden for the broader distribution because overconfidence in some domains cancels out underconfidence in others.Figure 1 shows a qualitative example to illustrate the same miscalibration issue.These results show that LMs are not well-calibrated for meaningful slices of a broad distribution, leading us to address the problem via few-shot, slice-specific recalibration.\n\n3 Slice-Specific Few-Shot Recalibration Since individual fine-grained slices may be miscalibrated, we propose to recalibrate each slice.Intuitively, given a few samples from a slice, we can infer the rough identity of that slice, and then appropriately adjust the LM's confidences based on the LM's familiarity with the slice.",
            "score": 0.6124887431855275,
            "section_title": "Miscalibration on Slices of Distributions",
            "char_start_offset": 6884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 865
                },
                {
                    "start": 867,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1629
                },
                {
                    "start": 1631,
                    "end": 1767
                },
                {
                    "start": 1767,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 729,
                    "matchedPaperCorpusId": "51880858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "For NLP tasks specifically, Desai and Durrett (2020) showed that classifiers trained using pre-trained transformer-based models (Devlin et al., 2019) are well calibrated both in-domain and out-of-domain settings compared to non-pre-trained model baselines (Chen et al., 2017). Notably, Ponti et al. (2021) highlights, since zero-shot cross-lingual transfer represents shifts in the data distribution the point estimates are likely to be miscalibrated, which forms the core setting of this work. \n\nIn light of this, our work has three main contributions. First, we investigate the calibration of two commonly used MMLMs: mBERT and XLM-R on four NLU tasks under zero-shot setting where the models are fine-tuned in English and calibration errors are computed on unseen languages. We find a clear increase in calibration errors compared to English as can be seen in Figures 1a and 1b, with calibration being significantly worse for Swahili compared to English. \n\nSecond, we look for factors that might affect the zero-shot calibration of MMLMs and find in most cases that the calibration error is strongly correlated with pre-training data size, syntactic similarity, and sub-word overlap between the unseen language and English. This reveals that MMLMs are miscalibrated in the zero-shot setting for low-resource languages and languages that are typologically distant from English. \n\nFinally, we show that model calibration across different languages can be substantially improved by utilizing standard calibration techniques like Temperature Scaling (Guo et al., 2017) and Label Smoothing (Pereyra et al., 2017) without collecting any data in the language (see Figure 1c). Using a few examples in a language (the few-shot setting), we see even more significant drops in the calibration errors as can be seen in Figure 1d. \n\nTo the best of our knowledge, ours is the first work to investigate and improve the calibration of MMLMs. We expect this study to be a significant contribution towards building reliable and linguistically fair multilingual models. To encourage future research in the area we make our code publicly available1 .",
            "score": 0.5983170816317761,
            "section_title": "Introduction",
            "char_start_offset": 1637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 52,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 128,
                    "end": 149,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "34032948"
                },
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "273549360",
            "title": "Calibrating Deep Neural Network using Euclidean Distance",
            "text": "Calibration of transformer-based language models has been evaluated across various natural language processing tasks, including machine translation [21], question answering [18], and selective prediction [29,48]. Recently, with the growing prominence of largescale generative language models (LLMs), studies have begun to examine their calibration properties [17,20,27,46,50] and zeroshot/few-shot learning [6]. To enhance text generation quality and improve models' handling of ambiguity, more studies have focused on addressing miscalibration of token-level probabilities by implementing calibration techniques that enable models to express uncertainty more accurately [21,27,30,51,56].",
            "score": 0.594354222331479,
            "section_title": "Calibration in Transformers and LLMs",
            "char_start_offset": 24845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 688
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "208513249"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "247187611"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "260379149",
            "title": "Calibration in Deep Learning: A Survey of the State-of-the-Art",
            "text": "Recent efforts in model calibration have predominantly centered around classification and regression tasks, leaving a noticeable gap in discussions regarding calibration for sequence generation. While advancements in calibration techniques have greatly benefited these tasks, the applicability and efficacy of such techniques in sequence generation remain under-explored. Building upon the seminal work of (Kumar & Sarawagi, 2019), which highlighted the inadequately calibrated token-level probabilities in neural machine translation tasks, there is a pressing need to extend these insights to a broader range of generative models. The findings of (Kumar & Sarawagi, 2019;Zhao, Khalman, Joshi, Narayan, Saleh, & Liu, 2022) revealed a crucial connection between token-level probability miscalibration and the counter-intuitive drop in BLEU scores with increased beam size, as noted by (Koehn & Knowles, 2017). Furthermore, they demonstrated that improved sequencelevel calibration can be achieved through the re-calibration of token-level probabilities. This underscores the importance of addressing token-level probability miscalibration not only for enhancing performance metrics but also for improving the overall reliability and coherence of generated sequences. \n\nExpanding upon these observations, recent studies such as that by (Zhao et al., 2021) have confirmed the presence of token-level probability miscalibration in Large Language Models (LLMs) utilized for few-shot learning tasks. The identified culprit, the softmax bottleneck as elucidated by (Yang, Dai, Salakhutdinov, & Cohen, 2018), particularly on a large vocabulary, further exacerbates this issue. In the context of sequence generation, early token probability miscalibration can have cascading effects, magnifying errors throughout the entire sequence. \n\nMoving forward, addressing the challenge of effectively calibrating token-level probabilities in various settings represents a promising avenue for research, especially in the era of LLMs. Extending the findings from classification and regression tasks to sequence generation domains requires innovative approaches tailored to the unique characteristics and challenges posed by generative models.",
            "score": 0.592452464684998,
            "section_title": "Calibrating Generative Models",
            "char_start_offset": 52766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 672,
                    "end": 722,
                    "matchedPaperCorpusId": "252683988"
                },
                {
                    "start": 884,
                    "end": 907,
                    "matchedPaperCorpusId": "8822680"
                },
                {
                    "start": 1334,
                    "end": 1353,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1558,
                    "end": 1599,
                    "matchedPaperCorpusId": "26238954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "267035278",
            "title": "Leveraging Biases in Large Language Models: \"bias-kNN\" for Effective Few-Shot Learning",
            "text": "Large language models (LLMs) have emerged as powerful tools showcasing impressive zero-shot and few-shot capabilities [1,2]. Leveraging templates and verbalizers [3] to align an LLM's output probability distribution with task-specific labels allows the model to address downstream classification tasks in zero-shot or few-shot contexts. \n\nHowever, these models are not without their challenges, predominantly stemming from biases. These biases influence both the model's inherent discriminative abilities and its output, skewing probability values for specific categories. Moreover, they can also disrupt conventional decision boundaries, thereby compromising their reliability [4,5]. Research identifies these biases mainly as vanilla label bias, where frequently encountered words during pre-training get prediction preference, and domain label bias, where the bias manifestation \u2020Equal contribution *Corresponding authors: Ning Cheng (chengning211@pingan.com.cn) varies based on content domain [6]. Another noteworthy phenomenon is the surface form competition [5], where semantically similar words vie for identical probability space, leading to distributional conflicts. Addressing biases in LLMs necessitates a diverse strategy. Initially, some methods focus on direct bias measurement and recalibration. Take, for instance, Contextual Calibration [4] which uses neutral test inputs, such as \"N/A\" to recalibrate model outputs. In a similar vein, Domain-Context Calibration [6] leverages random in-domain tokens to gauge the bias probability of individual labels. While potent, these approaches sometimes apply a broad-brush correction, occasionally missing the nuanced biases specific to certain test samples. \n\nIn another category, methods like PROCA [7] strategize around defining an optimal classification boundary. They draw on the model's contextual insights and employ a Gaussian Mixture Model (GMM) to understand the data spread. Similarly, approaches such as KNN-C [8] and kNNprompting [9] harness the model representations, emphasizing its capability for representation over prediction [10], to navigate around biases rather than confront them directly.",
            "score": 0.587853340179296,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 121,
                    "end": 123,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "210838924"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 681,
                    "end": 683,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "258887567"
                },
                {
                    "start": 1064,
                    "end": 1067,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 1354,
                    "end": 1357,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1480,
                    "end": 1483,
                    "matchedPaperCorpusId": "258887567"
                },
                {
                    "start": 1759,
                    "end": 1762,
                    "matchedPaperCorpusId": "248964978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "score": 0.5784683193167356,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "258967945",
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "text": "Uncertainty estimation of fine-tuned models is challenging due to the small amount of available data for fine-tuning, especially under out-of-domain settings (Desai & Durrett, 2020;Guo et al., 2021). While prior works illustrate that simple calibration techniques such as temperature scaling (Guo et al., 2017) and label smoothing (Szegedy et al., 2016) are not sufficient to calibrate the fine-tuned LMs under both in-domain (ID) and out-of-domain (OD) settings (Desai & Durrett, 2020;Park & Caragea, 2022), several approaches with strong regularization have been developed to calibrate the fine-tuned model on NLU tasks, including knowledge distillation from deep ensembles (Guo et al., 2021), stochastic network architectures (Fan et al., 2020;Zhang et al., 2021), and Mixup (Park & Caragea, 2022). However, these existing works mostly utilize general calibration methods for supervised learning, while specific properties of the pre-training & fine-tuning paradigm are still largely neglected. \n\nIn this work, we tackle the calibration of fine-tuned models from the perspective of better leveraging the powerful PLMs. Through a carefully designed empirical study on both pre-trained and fine-tuned models, we first observe that PLMs themselves are actually well-calibrated on the masked language modeling (MLM) task and robust to higher levels of perturbation to the inputs, which suggests the PLMs can model the predictive uncertainty well across different domains. However, the pre-trained features are only used as initialization and are distorted by the fully discriminative fine-tuning. The phenomenon is known as catastrophic forgetting (McCloskey & Cohen, 1989;Kirkpatrick et al., 2017;Howard & Ruder, 2018). We show that such forgetting can make the fine-tuned language models fail to hold proper predictive confidence toward the OD and outlier samples, which leads to miscalibration on the downstream tasks. Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs.",
            "score": 0.5679576399894064,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 181,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 181,
                    "end": 198,
                    "matchedPaperCorpusId": "224803318"
                },
                {
                    "start": 292,
                    "end": 310,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 331,
                    "end": 353,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 463,
                    "end": 486,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 676,
                    "end": 694,
                    "matchedPaperCorpusId": "224803318"
                },
                {
                    "start": 729,
                    "end": 747,
                    "matchedPaperCorpusId": "224814357"
                },
                {
                    "start": 747,
                    "end": 766,
                    "matchedPaperCorpusId": "235377150"
                },
                {
                    "start": 1647,
                    "end": 1672,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1672,
                    "end": 1697,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1697,
                    "end": 1718,
                    "matchedPaperCorpusId": "40100965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "Model Calibration Calibration on NLU tasks has been widely studied in related literature. Nguyen and O'Connor (2015) provided the method of how to analyze the calibration of non-neural NLP models. Guo et al. (2017) examined the calibration of modern deep neural networks and revealed that techniques such as temperature scaling and dropout affect the calibration on binary/multiclass classification tasks. Wang et al. (2020b) investigated the calibration of neural machine translation models and found that inference suffers from serious miscalibration. Jagannatha and Yu (2020) demonstrated that neural networks show high calibration error on structured predictions such as NER, POS, and QA, and proposed to use a binary class forecaster to calibrate the predictor confidence for a defined output entity of interest. Desai and Durrett (2020) explored pre-trained language models' calibration in combination with temperature scaling and label smoothing both on in-domain and out-of-domain datasets. Jung et al. (2020) jointly optimized two objectives (a cross-entropy loss and a calibration loss) and directly penalized the difference between the predicted and the true posterior probabilities dynamically over the training steps. He et al. (2021) obtained better calibration on natural language understanding tasks by augmenting and training the classifier jointly with an energybased model using noise-contrastive estimation.\n\nMixup Mixup (Zhang et al., 2018) is a method for data augmentation in which additional samples are generated during training by convexly combining random pairs and their associated labels, and aims to alleviate overfitting. Verma et al. (2019) showed that manipulating hidden representations rather than manipulating input-level features on mixup results in better regularization effects due to the fact that it encourages the neural network to focus more on representations of the real training examples in a low dimensional subspace. Many works have empirically noticed regularization effects that improve model performance on deep neural networks. For example, Guo et al. (2019a) explored the NLU specific mixup strategy by using sentence and word embeddings on CNNs and LSTMs to add performance gains in supervised text classification. Chen et al. (2020",
            "score": 0.5663019448480754,
            "section_title": "Related Work",
            "char_start_offset": 5903,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 116,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 197,
                    "end": 214,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 406,
                    "end": 425,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 554,
                    "end": 578,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 999,
                    "end": 1017,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 1231,
                    "end": 1247,
                    "matchedPaperCorpusId": "231632895"
                },
                {
                    "start": 1441,
                    "end": 1460,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1653,
                    "end": 1672,
                    "matchedPaperCorpusId": "59604501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7587890625
        },
        {
            "corpus_id": "273098373",
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "text": "Large Language Models (LLMs) have exhibited emergent capabilities such as advanced creative writing, summarization, translation, arithmetic and commonsense reasoning, etc (Wei et al., 2022;Brown et al., 2020). One of the most fascinating aspects of LLMs is their in-context learning capabilities. In particular, this involves adding pairs of demonstration examples to the prompt, and has been shown to significantly enhance the performance of LLMs (Brown et al., 2020). This capability offers users the significant advantage of utilizing LLMs without the need to train or fine-tune their own models. As a result, there is a growing need for research into calibration techniques (Zhou et al., 2023;Zhao et al., 2021;Jiang et al., 2023;2021) to ensure the reliability of model outputs as well and improve performance. \n\nThe concept of calibration in modern deep learning models was first introduced in (Guo et al., 2017) where the authors also proposed metrics (such as Expected Calibration Errors, reliability diagrams, etc) and methods (such as temperature scaling) for characterizing and mitigating miscalibration issues. The miscalibration of LLMs has also been studied in recent works (Xiong et al., 2023;Tian et al., 2023;Liusie et al., 2024;Zhao et al., 2023;Geng et al., 2023;Kamath et al., 2020). In this work, we show that LLMs with zero-shot and few-shot prompting exhibit a unique miscalibration issue on classification tasks, which we refer to as indiscriminate miscalibration. This phenomenon occurs when models assign equal confidence to correct and incorrect predictions. We show that this phenomenon cannot be quantitatively measured by Expected Calibration Errors (ECE). One alternative metric that can help catch this phenomenon is using Macro-average Calibration Error (MacroCE) proposed in Si et al. (2022). However, it may not depict the phenomenon thoroughly as it only computes the means of the distributions. We further propose a metric to help describe the phenomenon in more details.",
            "score": 0.5658902068683587,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 208,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 900,
                    "end": 918,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1226,
                    "end": 1246,
                    "matchedPaperCorpusId": "259937561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct).This precision curve can be used to achieve many downstream goals.For instance, we can identify the con-Figure 1: An example of the illusion of LM calibration.For a combination of five domains, the model is wellcalibrated with a calibration error of 0.02 (the first plot).However, the same model is miscalibrated on the the five individual domains, each with a higher calibration error. 2   fidence threshold that achieves a minimum level of precision, to control the LM's error rate for this slice.We can also transform the precision curve into the corresponding calibration curve and reduce calibration error on this slice ( \u00a73.1).\n\nTo train our few-shot recalibration model, we employ a synthetic data generation strategy: given a corpus of labeled examples that have been partitioned into different domains, we can synthetically construct many different slices by taking different weighted mixtures of these domains.For example, 80% abstract algebra and 20% virology from MMLU ( \u00a73.2).Since we're working with labeled data, we can directly compute an LM's ground-truth precision curve on that slice.Then, we train our recalibration model to predict this ground-truth precision curve, when only given access to a random sample of unlabeled examples from that slice ( \u00a73.3).At inference time, our trained recalibrator generalizes to previously unseen slices, and only requires unlabeled data.",
            "score": 0.5574314447582998,
            "section_title": "Introduction",
            "char_start_offset": 1618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 596
                },
                {
                    "start": 596,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 802
                },
                {
                    "start": 802,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1163
                },
                {
                    "start": 1165,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1519
                },
                {
                    "start": 1519,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "276767564",
            "title": "An Efficient Plugin Method for Metric Optimization of Black-Box Models",
            "text": "Recently, Wu et al. (2024) demonstrated that a stronger version of calibration from the algorithmic fairness literature, multicalibration (H\u00e9bert-Johnson et al., 2018), has deep connections to robustness under distribution shift, and proposed a post-processing algorithm which adapts a predictor under both co-variate and label shift for regression tasks. \n\nIt is worth mentioning that language models have their own set of domain adaptation techniques, such as fine-tuning from supervised (Han et al., 2024) or human feedback (Tian et al., 2023), prompt tuning/engineering (Liu et al., 2023), in-context learning (Dong et al., 2022), etc. Our method is agnostic to the choice of underlying base model; nonetheless, we include fine-tuning as a suitable baseline where applicable.",
            "score": 0.5535995943561313,
            "section_title": "Introduction",
            "char_start_offset": 3470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 779
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 167,
                    "matchedPaperCorpusId": "51880858"
                },
                {
                    "start": 527,
                    "end": 546,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 574,
                    "end": 592,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "cy maps can benefit in-domain calibration and by dissimilarity can benefit out-of-domain calibration. To monitor training dynamics, we use the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) which measures how different a true label for a sample is compared to a model's beliefs at each epoch and is calculated as the average difference between the logit values for a sample's assigned class and its highest non-assigned class across training epochs.\n\nMoreover, we combine our mixup with wellknown miscalibration correction methods such as label smoothing and temperature scaling (Guo et al., 2017) to investigate their impact on our proposed mixup. We conduct a comprehensive set of experiments using BERT (Devlin et al., 2019) and RoBERTa  to show the efficacy of our mixup approach by testing on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. We achieve the lowest Expected Calibration Error (ECE) without accuracy drops in comparison with strong baseline methods. Our contributions are as follows:\n\n\u2022 We propose a novel mixup method which is guided by AUM and saliency signals and is targeted at improving model calibration. Specifically, we compare logits to categorize samples into two sets (i.e., a set of easy-to-learn samples and another set of hardto-learn/ambiguous samples), and interpolate samples across these two sets by finding the most similar and most dissimilar samples from the other set while leveraging saliency (to compute sample similarities) for pre-trained language models' calibration on in-domain and out-of-domain data.\n\n\u2022 We combine our method with miscalibration correction techniques (i.e., label smoothing, temperature scaling) to investigate their impact on our proposed mixup.\n\n\u2022 We conduct comprehensive experiments showing that our method achieves the lowest expected calibration errors (ECEs) on both in-domain and out-of-domain samples compared with strong baselines without accuracy drops on multiple NLU tasks, namely, natural language inferences, paraphrase detection, and commonsense reasoning.",
            "score": 0.5519343145524787,
            "section_title": "Introduction",
            "char_start_offset": 3791,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 202,
                    "matchedPaperCorpusId": "210932316"
                },
                {
                    "start": 592,
                    "end": 610,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "275342783",
            "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles",
            "text": "Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.",
            "score": 0.5507126322443081,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Knowing when to trust a model's predictions is typically mapped to the concept of calibration, where the model's confidence estimate for a prediction reflects how likely it is to be correct.Language models (LMs) have recently been shown to be wellcalibrated in a number of settings (Kadavath et al., 2022;Xiao et al., 2022;Kuhn et al., 2023;Ope-nAI, 2023).However, while models can be wellcalibrated for aggregate distributions (e.g.mixtures of a number of domains), they can be significantly miscalibrated for narrower domains within that distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).\n\nFor instance, Figure 1 shows an LM that is wellcalibrated on the combined distribution of five domains, achieving near perfect calibration curve with low expected calibration error (ECE).However, curves for the individual domains appear significantly miscalibrated in comparison, with the least calibrated domain virology having a 250% higher calibration error.This miscalibration problem is hidden for the combined distribution because overconfidence in some domains cancels out underconfidence in others.\n\nThis illustrates a key problem: LMs are not well-calibrated for meaningful slices of broader distributions.This is particularly relevant in practice where users querying an LM rarely sample from a broad combination of distributions at any given time, and are more likely to sample from slices like abstract algebra or virology.Our goal is to recalibrate LMs for each of these fine-grained slices of a distribution, thereby allowing users to reliably determine when predictions can be trusted.\n\nTo recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice.",
            "score": 0.5502629764809013,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 599
                },
                {
                    "start": 601,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1107
                },
                {
                    "start": 1109,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1601
                },
                {
                    "start": 1603,
                    "end": 1832
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 323,
                    "end": 341,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 570,
                    "end": 598,
                    "matchedPaperCorpusId": "51880858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Language models (LMs) that encompass transformer-based architectures (Brown et al., 2020;Chowdhery et al., 2023;OpenAI, 2023) can generate coherent and contextually relevant texts for various use cases. Despite their impressive performance, these models occasionally produce erroneous or overconfident outputs, leading to concerns about their calibration (Dawid, 1982;DeGroot and Fienberg, 1983) which measures how faithful a model's prediction uncertainty is. Such a problem is pressing when users adapt them using a recent paradigm called in-context learning (Brown et al., 2020) to construct performant predictors, especially for applications in safety-critical domains (Bhatt et al., 2021;Pan et al., 2023). \n\nWe provide an in-depth evaluation and analysis of how well these models are calibrated -that is, the alignment between the model's confidence in its predictions and the actual correctness of those predictions. This token-level calibration assessment enables us to measure the discrepancy between the model's perceived and actual performance to assess its accuracy and reliability through a Bayesian uncertainty lens. \n\nWe find that LM such as LLaMA (Touvron et al., 2023a) is poorly calibrated in performant settings and there exists a calibration-accuracy trade-off (Fig. 1) for low-shot settings (k < 4): as we increase the amount of in-context samples, both prediction accuracy and calibration error increase. Such a trade-off can be improved using more ICL examples (k = 8) and larger models. Crucially, this calibration degradation worsens when fine-tuning occurs using specialized data to improve usability, such as curated instructions (Dubois et al., 2023), dialogues (Zheng et al., 2023), or human preference data (Ziegler et al., 2019). Though previous common practice suggests recalibrating models' logits via temperature scaling (Guo et al., 2017), we show that in contrast to classic regimes, the miscalibration issue in ICL can not be easily addressed using such well-established scaling approaches (Platt et al., 1999).",
            "score": 0.5495700887050222,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 89,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 89,
                    "end": 112,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 355,
                    "end": 368,
                    "matchedPaperCorpusId": "121781338"
                },
                {
                    "start": 368,
                    "end": 395,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 561,
                    "end": 581,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 673,
                    "end": 693,
                    "matchedPaperCorpusId": "226965491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "In this work we showed that MMLMs like mBERT and XLMR are miscalibrated in a zero-shot cross lingual setting, with the calibration errors being even worse on low resource languages and languages that are typologically distant from the pivot language (often English). We then demonstrated the effectiveness of standard calibration techniques for improving calibration across languages both with and without collecting any new languagespecific labelled data. We recommend that researchers and practitioners consider, measure and report the calibration of multilingual models while using them for scientific studies and building systems. In future work, we aim to bridge the gap between zero-shot and few-shot calibration methods by exploring unsupervised calibration methods under domain shift (Pampari and Ermon, 2020;Park et al., 2020) that utilizes unlabelled data in new domains to improve calibration. Investigating the cross lingual calibration of MMLMs for tasks other than sentence classification like Sequence Labelling (Pan et al., 2017;Nivre et al., 2018) and Question Answering (Artetxe et al., 2020) is another natural extension of our work.",
            "score": 0.5429008234016809,
            "section_title": "Conclusion",
            "char_start_offset": 12294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1152
                }
            ],
            "ref_mentions": [
                {
                    "start": 817,
                    "end": 835,
                    "matchedPaperCorpusId": "211677721"
                },
                {
                    "start": 1027,
                    "end": 1045,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 1088,
                    "end": 1109,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "277189451",
            "title": "Localized large language model TCNNet 9B for Taiwanese networking and cybersecurity",
            "text": "Finetuning techniques are essential for adapting large language models (LLMs) to specific tasks and domains, allowing them to achieve higher accuracy and relevance in their outputs. As LLMs have evolved, researchers have developed various finetuning methodologies that leverage existing pre-trained models while enhancing their performance through targeted adjustments. \n\nOne of the prominent techniques in this area is transfer learning, which allows models to retain the knowledge gained from large, diverse datasets while adapting to specialized tasks. This approach has significantly reduced the labeled data required for practical training in specific domains. For example, Kamath et al. (2019) explain domain adaptation as a form of transfer learning in which the task remains the same, but the distribution changes between source and target domains. They illustrate how sentiment analysis models trained on electronic product reviews may misinterpret domain-specific phrases when applied to hotel reviews, highlighting the challenges of domain shift without extensive retraining 26 . \n\nAnother notable advancement is instruction tuning, which involves training models on datasets that include explicit instructions or prompts related to the desired tasks. This technique has enhanced the model's ability to follow user queries and generate contextually appropriate responses. Zhao et al. ( 2023) propose a Self-Guide. This multi-stage mechanism synthesizes task-specific input-output pairs from the language model for self-finetuning. Their approach demonstrates substantial improvements in classification and generation tasks, enabling models to understand user intent better and provide relevant information without requiring external learning signals 27 . \n\nMoreover, few-shot learning has emerged as a powerful strategy for finetuning LLMs with limited labeled data. This method allows models to generalize from a small number of examples, making them more adaptable to new tasks without extensive retraining. Recent studies, such as those by Perez et al. (2023) examine \"true fewshot learning\" where no held-out examples are available for model tuning. They reveal challenges in few-shot model selection that suggest previous studies may have overestimated the effectiveness of this approach 28 . \n\nThese finetuning techniques were integral to the development of TCNNet-9B. The model underwent continued pretraining on a carefully curated dataset, including networking knowledge, DIY assembly guides, and local cybersecurity regulations.",
            "score": 0.5313697241988765,
            "section_title": "Advances in finetuning techniques for LLMs",
            "char_start_offset": 20903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2308
                },
                {
                    "start": 2311,
                    "end": 2385
                },
                {
                    "start": 2386,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1088,
                    "matchedPaperCorpusId": "196177312"
                },
                {
                    "start": 2304,
                    "end": 2306,
                    "matchedPaperCorpusId": "235166749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "LLMs for code are extensively studied [54], [55]. While calibration has a long history in modeling [8], [56], it is not a frequently studied topic in the SE community. Early work moving into modern machine learning studied the calibration of smaller neural models performing classification tasks on text and images; while these early models were poorly calibrated per se, their performance could be improved by simple scaling [22] of their output probabilities. As models became larger, calibration was found to improve [57]. Pre-training was also found to improve calibration [25], [58]; however, these findings have been disputed [47]. \n\nMore recent works evaluated LLM calibration on a wide variety of settings [7], [18], [25], [59]. Desai et al. [25] studied non-code (natural language) tasks such as inference or paraphrasing, with only intrinsic measures using oldergeneration models (BERT and RoBERTA). Jiang et al. [7] studied calibration for natural language question-answering using just intrinsic measures. In contrast, we study calibration for three coding-related tasks, using both artificial and natural code datasets, and both intrinsic and reflective confidence measures, to evaluate calibration in the SE domain. \n\nOther prior work has investigated tokens that might be edited. Vasconcelos et al. [60] discusses code model uncertainty for function-synthesis-style problems, and ran human evaluation of the usefullness of colored highlighting of uncertain tokens. They found highlighting a human-derived groundtruth of which tokens might be edited was helpful, and more useful than raw token probabilities from the model. Johnson et al. [61] developed method of highlighting likely edit tokens via a utility optimization algorithm comparing different file completions. We find exploring more on calibrated uncertainty for local areas be a interesting area for additional work. \n\nLi et al. [49] investigate the calibration of Computer vision (CV) models from an operational perspective i.e., the shift between training input and production inputs, presenting it as a software quality problem that can be addressed using Bayesian approaches.",
            "score": 0.5247925707715254,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 44335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "255546225"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "122906757"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "23794124"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 923,
                    "end": 926,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "253393857"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "257255061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Calibration of language models. Calibration is a safety property to measure the faithfulness of machine learning models' uncertainty, especially for error-prone tasks using LMs. Previous works find that pre-training (Desai and Durrett, 2020) and explanation (Zhang et al., 2020;Gonz\u00e1lez et al., 2021) improves calibration. Models can be very poorly calibrated when we prompt LMs (Jiang et al., 2021), while calibration can also depend on model size (Kadavath et al., 2022). (Braverman et al., 2020) assesses the long-term dependencies in a language model's generations compared to those of the underlying language and finds that entropy drifts as models such as when GPT-2 generates text. The intricacy of explanations on complementary team performance poses additional challenges due to the overreliance on explanations of users regardless of their correctness (Bansal et al., 2021). (Mielke et al., 2022) gives a framework for linguistic calibration, a concept that emphasizes the alignment of a model's expressed confidence or doubt with the actual accuracy of its responses. The process involves annotating generations with <DK>, <LO>, <HI> for confidence levels, then training the confidence-controlled model by appending the control token <DK/LO/HI> at the start of the output, followed by training a calibrator to predict these confidence levels, and finally predicting confidence when generating new examples. (Tian et al., 2023) finds that asking LMs for their probabilities can be better than using conditional probabilities in a traditional way. LHTS (Shih et al., 2023) is a simple amortized inference trick for temperaturescaled sampling from LMs and diffusion models. To aggregate log probabilities across semantically equivalent outputs, Kuhn et al. (2023) utilize bidirectional entailment through a model to identify outputs that are semantically similar, thereby refining the uncertainty estimation process.",
            "score": 0.5227481385614718,
            "section_title": "Related Work",
            "char_start_offset": 3677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "210023849"
                },
                {
                    "start": 278,
                    "end": 300,
                    "matchedPaperCorpusId": "236478213"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 474,
                    "end": 498,
                    "matchedPaperCorpusId": "189762078"
                },
                {
                    "start": 862,
                    "end": 883,
                    "matchedPaperCorpusId": "220128138"
                },
                {
                    "start": 885,
                    "end": 906,
                    "matchedPaperCorpusId": "250073258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "276408950",
            "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
            "text": "Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.",
            "score": 0.5149039307020766,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "260682695",
            "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
            "text": "Although LLMs have exhibited a remarkable capability for self-analysis and self-improvement, there remains a lack of theoretical justifications to uncover the mystery of such ability. Therefore, we argue that the study of underlying theoretical principles can offer a more transparent understanding of self-correction. Subsequently, we propose several potential directions for such explorations. \n\nThe ability of language models to self-correct is closely associated with their capacity to exhibit metacognitive awareness, i.e., their understanding of their own knowledge and uncertainties (Kadavath et al., 2022). Similarly, the notion of calibration in language models, referring to their ability to produce well-calibrated predictions with probabilities aligning closely with observed frequencies of outcomes, is of paramount importance (Lin et al., 2023). Recent research by Kadavath et al. (2022) reveals that pre-trained language models, when presented with properly formatted multiplechoice and true/false questions, demonstrate good calibration. Particularly, language models exhibit well-calibrated responses to self-evaluation questions in few-shot settings. On the other hand, finetuned language models, such as those incorporating RLHF, require temperature adjustments to achieve calibration since the model distribution is tailored to optimize specific behaviors. \n\nWhile language models demonstrate some capacity for self-feedback, achieving superior performance often necessitates incorporating external feedback signals. The integration of feedback signals is closely linked to the alignment of language models, a domain that still lacks comprehensive understanding. For example, in RLHF, the choice of the metric to minimize between the reward model output and the final model output significantly impacts downstream task performance (Go et al., 2023), yet this aspect remains underexplored in many applications. Furthermore, the optimal method for automatically generating prompts to instruct language models effectively, for tasks such as evaluating and refining their outputs, remains an open question. Although Sordoni et al. (2023) have addressed this issue by treating natural language prompts as parameters of the language model and performing discrete optimization, their approach still requires hand-crafted meta prompts to implement the algorithm.",
            "score": 0.5107134512807296,
            "section_title": "Theoretical Justifications",
            "char_start_offset": 55255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "269004786",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "text": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "score": 0.5098246691029601,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "Ground-truth uncertainty estimates. Ground-truth estimate of uncertianty might depend on the specified task: multi-choice questions, NMT, and generative tasks. It can be hard to obtain ground truths as a model can be miscalibrated according to different subsets of the model, or bias can be different for certain categories [199]. Is it necessary to have a human-defined threshold for the uncertainty of LFMs? Classical approaches for LLMs. Whether methods for numerical calibration are effective for linguistic calibration recently gained attention [10]. One common consideration for linguistic calibration is that NLP models can provide quite structured inference results [95]. Dirichlet calibration [89], [186], which is a multi-class extension of Beta calibration [224] that estimates Dirichlet distribution for classes, can be applied to LLMs for de-biasing their confidence toward categories or sensitive groups. Frameworks for prior networks, such as DPN [20], can further be extended to LLMs and NLP tasks to capture distributional uncertainty that could exist between train and test datasets. In [225], the authors proposed a dropout variational inference model for transformer architecture. In addition, adapting few-shot recalibration for RLHF and o1/R1based models [226] and Cloze distillation [227] sounds like promising future work. Calibration in Domain Adaptation (DA). Calibration of DA methods has problems to address [82]. One of the existing works for calibration on dataset shift, the TransCal method optimizes transferable calibration objective to get the parameters for calibration of model confidence for predictions on target domain [82]. Calibration in OOD, domain shift, and distributional shift. Teaching models to express uncertainty with domain shift [21]. Can we apply the pre-trained visual uncertainty method [228] for language tasks? Reliable contribution incentivization and Blockchain. Blockchain LLMs [229] as implemented as reward mechanism for preference optimization [230]. Fine-tuning is an important stage for generating reasonable inferences from LLMs, as training from scratch might take a long time.",
            "score": 0.509667999857688,
            "section_title": "B. Future Research Directions",
            "char_start_offset": 58318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 329,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "258049237"
                },
                {
                    "start": 708,
                    "end": 713,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "3580844"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "220545994"
                },
                {
                    "start": 1658,
                    "end": 1662,
                    "matchedPaperCorpusId": "220545994"
                },
                {
                    "start": 1842,
                    "end": 1847,
                    "matchedPaperCorpusId": "268032818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Additionally, we evaluate the performance of MLLMs on the TyDiQA dataset, which is collected from a different resource and different setup than SQuAD, resulting in a distribution shift for the model fine-tuned on SQuAD. In Table 2 we show the performance of the MLLMs on the TyDiQA test set, and observe the following: \n\n\u2022Fine-tuning on SQuAD helps more than finetuning on TyDiQA (en), even though the task is OOD. This can be attributed to the much larger amount of fine-tuning data in SQuAD compared to TyDiQA-en which makes up for the task distribution shift. \n\n\u2022 Fine-tuning on all TyDiQA languages helps much more than just TyDiQA-en or SQuAD. In this section, we explore the role of various strategies to improve calibration including posthoc methods, regularization methods, and ICL. We aim to address the following questions: \n\n\u2022 Do existing calibration techniques work in our cross-lingual transfer settings? \n\n\u2022 Can data-augmented fine-tuning on translated cross-lingual data improve calibration? \n\n\u2022 What are the comparative impacts of having more monolingual data versus having more diverse, cross-lingual data? \n\n\u2022 Can using in-context examples improve confidence calibration? \n\n5.1 Post-hoc Methods: Temperature Scaling \n\nIn Tables 12 we demonstrate the benefits of using temperature scaling (TS) and few-shot learning (FS) on calibration for extractive models and generative models. TS does not affect accuracy by design, but it provides significant benefits in calibration in most cases. Our experiments explore the impact of different validation sets: 1) the SQuAD validation dataset (10.6k English sentences); 2) Merged MLQA validation dataset (with 7 languages, 3k sentences). We also observe that optimizing the temperature on a relatively small multilingual validation dataset is more powerful than on a larger English-only validation dataset. Notably, even though some of the languages (e.g. SW, KO) do not occur in the merged validation dataset of XQuAD and TyDiQA, the temperature computed on the merged dataset still effectively improves the calibration performance in those languages5 .",
            "score": 0.5076458930238086,
            "section_title": "Results for Out-of-Distribution (OOD) Tasks",
            "char_start_offset": 14942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2112
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "266435613",
            "title": "Large Language Models are Miscalibrated In-Context Learners",
            "text": "Following the standard supervised fine-tuning (SFT) paradigm, in-context learning (ICL) has become an efficient approach propelled by the recent advancements in large language models (LLMs), yielding promising performance across various tasks in few-shot data setups. However, both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration), especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration, as well as their interplay. Through extensive controlled experiments, we find that simultaneous gains for both task performance and calibration are difficult to achieve, and the problem of miscalibration exists across all learning methods in low-resource scenarios. To address this challenging trade-off between performance and calibration, we then investigate the potential of self-ensembling techniques applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies). We justify the feasibility of self-ensembling on SFT in addition to ICL, to make the predictions more calibrated and have comparable or even better performance. Our work sheds light on which learning paradigm to choose and how to enhance both task performance and calibration of LLMs.",
            "score": 0.5053491685752405,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89990234375
        },
        {
            "corpus_id": "267617073",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "text": "\u2022 A cost-effective model usage strategy, illustrating the practicality of long-form calibration in optimizing LLM deployment. \n\nMeasuring Calibration Calibration (Guo et al., 2017, Minderer et al., 2021)has been widely studied in language models, whose probabilities derived from logits are generally found to not be calibrated (Jiang et al., 2020, Kadavath et al., 2022, Chen et al., 2023d). Standard metrics to measure the calibration include Expected Calibration Error (ECE) for confidence-accuracy disparity (Naeini et al., 2015), Brier Score for mean squared prediction-outcome differences, and AUROC for assessing confidence-based correct answer identification (Boyd et al., 2013, Kuhn et al., 2023). Selective Accuracy@Coverage measures accuracy within the model's most confident predictions (Liang et al., 2023, Cole et al., 2023). However, these metrics, rooted in a binary notion of correctness, fall short for long-form tasks where correctness spans a range, suggesting a distribution-based approach is more apt. \n\nImproving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023).",
            "score": 0.5050389877887302,
            "section_title": "Introduction",
            "char_start_offset": 5549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1712
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 179,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 370,
                    "end": 391,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 512,
                    "end": 533,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 667,
                    "end": 685,
                    "matchedPaperCorpusId": "37409855"
                },
                {
                    "start": 799,
                    "end": 818,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1112,
                    "end": 1130,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "248964978",
            "title": "Prototypical Calibration for Few-shot Learning of Language Models",
            "text": "To our analysis, decision boundary is of critical importance to the performance of few-shot demonstrations and the traditional decision boundary leads to the fragility of prompting LMs. We propose prototypical calibration to adaptively learn a more robust decision boundary. Experiments show that the calibrated decision boundary is effective across various prompt templates, class proportions and permutations. We achieve on average a 13% absolute improvement across different sizes of pretrained language models on nine popular text classification tasks. \n\nA limitation of our method is that it is not applicable for tasks whose label space is open-ended since a fixed label space is necessary for estimating prototypical clusters. Furthermore, our method is designed for in-context learning on individual downstream tasks, it fails to calibrate the inherent bias of language models like gender and occupation bias. For future work, we would like to extend our method to the tasks with open-ended answer space, such as generative question-answering and text summarization tasks. \n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR, 2021.",
            "score": 0.5048242770539875,
            "section_title": "Conclusion and Limitation",
            "char_start_offset": 21829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "263310485",
            "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
            "text": ". \n\nPrompting large language models (LLMs) (Chowdhery et al., 2022;Anil et al., 2023) has become an efficient learning paradigm for adapting LLMs to a new task by conditioning on humandesigned instructions. The remarkable in-context learning (ICL) ability of LLMs also leads to efficient few-shot learners that can generalize from few-shot input-label pairs (Brown et al., 2020;Liu et al., 2023). However, the predictions of LLMs are highly sensitive and even biased to the choice of templates (Min et al., 2022b), verbalizers (Holtzman et al., 2021), and demonstrations (Liu et al., 2022a), resulting in barriers for pursuing efficiently adaptable and robust LLM applications. \n\nExtensive research has been devoted to mitigating these biases, which we explicitly refer to the a-priori propensity of LLMs to predict certain classes over others unfairly. Lu et al. (2022) provide an analysis of the impacts of the order of ICL examples to LLMs and have explored the order selection mechanisms for ICL. On the other hand, Zhao et al. (2021) reveal the bias of language models toward certain answers and propose to calibrate the LLM given content-free tokens. More recently, Fei et al. (2023) detect the domain-label bias, and Han et al. (2023) treat the calibration of LLMs, a technique for mitigating the label bias, as learning a robust decision boundary. Though multiple calibration solutions have been provided, the field currently lacks a unified analysis that systematically distinguishes and explains the unique characteristics, merits, and downsides of each approach. a unified analysis of these methods in Sec. 3. As we will show, our proposed method differentiates from these methods as a generalizable solution across challenging classification tasks and modalities.",
            "score": 0.5038276492424125,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 1
                },
                {
                    "start": 4,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1775
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 378,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 378,
                    "end": 395,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 494,
                    "end": 513,
                    "matchedPaperCorpusId": "247155069"
                },
                {
                    "start": 527,
                    "end": 550,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 571,
                    "end": 590,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 854,
                    "end": 870,
                    "matchedPaperCorpusId": "233296494"
                },
                {
                    "start": 1020,
                    "end": 1038,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1172,
                    "end": 1189,
                    "matchedPaperCorpusId": "258967265"
                },
                {
                    "start": 1224,
                    "end": 1241,
                    "matchedPaperCorpusId": "248964978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "236134351",
            "title": "Learning ULMFiT and Self-Distillation with Calibration for Medical Dialogue System",
            "text": "ULMFiT has obtained great success in NLP tasks as it transfers information from the pre-trained model to the target application domain, and LS helps in calibration and better uncertainty. We apply LS to ULMFiT to gain a calibrated ULMFiT (CULMFiT) to further improve the feature representation and extract more distinctive information from language modeling. Given \u03b8 U LM F iT is the pre-trained ULMFiT weight, x as the input of the conversational model, the loss function of ULMFiT with LS can be written as follows: \n\n2.3 Self-Distillation with TS Self-distillation (SD) has been proved to replicate the similar accuracy as the knowledge distillation (KD) with the teacher model training on student model, and temperature scaling helps to prevent miscalibration. We integrate TS on SD to attain a well-calibrated distilled model. For this purpose, we adopt KD loss of KL divergence with calibration as in the paper (Hinton et al., 2015). However, temperature set as a scalar value is a similar technique as network calibration, and the optimal temperature is expected to be a better option. In our work, we measure optimal T and assign it to the KD, aiming at preventing inappropriate calibration and investigating the relation between calibration and SD. Suppose the logits for the teacher model and student model are logit T and logit S , and the optimal temperature is T opt . The loss function with KL divergence L KD can be formulated as: \n\nThe final loss L can be demonstrated as:",
            "score": 0.5034733049922249,
            "section_title": "ULMFiT with Label Smoothing",
            "char_start_offset": 9226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1488
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "250551977",
            "title": "Calibrating Zero-shot Cross-lingual (Un-)structured Predictions",
            "text": "On the other hand, large-scale uncertainty estimation and calibration work have mostly been conducted in the vision domain (Ovadia et al., 2019;Minderer et al., 2021). Large-scaled calibration studies put predominant importance on computer vision. In natural language processing, while model calibration has wide applications w.r.t tasks such as text classification (Jung et al., 2020;Kong et al., 2020), seq2seq generation (Ott et al., 2018;Dong et al., 2018;Wang et al., 2020), question answering (Ye and Durrett, 2022;Kamath et al., 2020) and zeroshot learning (Zhao et al., 2021), benchmarking results are not as comprehensive as in vision. \n\nIn this work, we evaluate how the calibration of large-scale multilingual models is affected by the zero-shot cross-lingual transfer, and whether we might mitigate calibration error with standard techniques relying solely on the source language1 . We conduct our experiments on six standard crosslingual transfer tasks across seven typologically diverse target languages, using English as the annotated source language. Our key findings include: \n\n\u2022 NLP models become less calibrated under cross-lingual transfer. \u2022 Task difficulty, data sparsity, and distance between source and target languages each impact model calibration, as shown in fig. 1. \u2022 Post-training calibration methods using only the source language effectively mitigates miscalibration on target languages. \u2022 Post-training calibration in structured prediction is more challenging and requires a more expressive calibration function family. \n\n2 Background",
            "score": 0.5031813664723488,
            "section_title": "Introduction",
            "char_start_offset": 1799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 144,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 366,
                    "end": 385,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 424,
                    "end": 442,
                    "matchedPaperCorpusId": "4375156"
                },
                {
                    "start": 460,
                    "end": 478,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 499,
                    "end": 521,
                    "matchedPaperCorpusId": "238856959"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 564,
                    "end": 583,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "272423997",
            "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
            "text": "Black-box large language models (LLMs) are increasingly deployed in various environments, making it essential for these models to effectively convey their confidence and uncertainty, especially in high-stakes settings. However, these models often exhibit overconfidence, leading to potential risks and misjudgments. Existing techniques for eliciting and calibrating LLM confidence have primarily focused on general reasoning datasets, yielding only modest improvements. Accurate calibration is crucial for informed decision-making and preventing adverse outcomes but remains challenging due to the complexity and variability of tasks these models perform. In this work, we investigate the miscalibration behavior of black-box LLMs within the healthcare setting. We propose a novel method, \\textit{Atypical Presentations Recalibration}, which leverages atypical presentations to adjust the model's confidence estimates. Our approach significantly improves calibration, reducing calibration errors by approximately 60\\% on three medical question answering datasets and outperforming existing methods such as vanilla verbalized confidence, CoT verbalized confidence and others. Additionally, we provide an in-depth analysis of the role of atypicality within the recalibration framework.",
            "score": 0.5026695073060735,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "score": 0.502205220107158,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "273098373",
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "text": "Confidence calibration on modern neural networks has been discussed in (Guo et al., 2017) where the authors showed that large vision models are poorly calibrated. and proposed the Expected Calibration Errors (ECEs) that have been widely used in the literature. A more recent paper reviews some drawbacks of ECEs and proposed Instance-level Calibration Error (ICE) and Macro-average Calibration Error (MacroCE) (Si et al., 2022). Recent studies have shown great interests of model uncertainty and calibration in language models as well (Xiong et al., 2023;Tian et al., 2023;Liusie et al., 2024;Zhao et al., 2023;Geng et al., 2023;Kamath et al., 2020). As one of the most popular method for prompt engineering with LLM, in-context few-shot prompting (Brown et al., 2020) has the output instability issue, which was revealed by Zhao et al. (2021). They proposed a simple method to estimate and adjust majority label bias, recency bias, and common token biases introduced by in-context learning. Jiang et al. (2021) also showed that LLMs can be overconfident about their answers and calibration purposed fine-tuning or post-hoc calibration method can be used to improve the performance. Other similar methods such as batch calibration (Zhou et al., 2023) has been proposed as well. More recent studies show that few-shot prompting, finetuning or chain-ofthoughts can all suffer miscalibration issues (Zhang et al., 2023). In a low shot setting (e.g. < 4 demonstration examples), model prediction accuracy and calibration error can both increase and the trade-off can be improved with larger models or more shots (e.g. > 8 shots). Moreover, instruction fine-tuned LLMs exhibit the same miscalibration issue (Jiang et al., 2023) as well.",
            "score": 0.5008404455162954,
            "section_title": "CONFIDENCE AND CALIBRATION",
            "char_start_offset": 3646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 89,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 573,
                    "end": 593,
                    "matchedPaperCorpusId": "259937561"
                },
                {
                    "start": 748,
                    "end": 768,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 992,
                    "end": 1011,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "For basic evaluation, we report accuracy (Acc) and average confidence score (Conf) on the testing set. For calibration evaluation, we report ECE using equal-mass binning and 100 bins following Minderer et al. (2021). Besides, we provide an application-driven perspective to evaluate model calibration, aiming to quantify two unsatisfied scenarios due to miscalibration in practice: (1) Correct predictions (positive) are rejected due to low confidence; (2) Wrong predictions (negative) are accepted due to high confidence. Specifically, we consider the average confidence in correct predictions Conf pos and wrong predictions Conf neg respectively. For unified comparison, we report two calibration error (CErr) cases, CErr pos = 1 \u2212 Conf pos and CErr neg = Conf neg . In principle, we expect calibrated models having both low CErr pos and CErr neg , indicating that they reasonably assign high confidence in correction predictions and low confidence in wrong predictions.  (Raffel et al., 2020), since they represent two classic types of PLMs, namely encoder-only and encoder-decoder models. We experiment with four representative tasks in NLP, including sentiment analysis, natural language inference, news classification, and topic classification. For datasets, we choose SST-2 (Socher et al., 2013a), MNLI (Williams et al., 2018a), AG-News (Zhang et al., 2015), and Yahoo (Zhang et al., 2015) respectively. We employ the prompt-based learning paradigm (Liu et al., 2021) since its superior performance compared to the traditional fine-tuning, especially in the fewshot setting. Specifically, we inherit the masked language modeling task in the pre-training stage and use templates to wrap samples into prompts. We fine-tune the whole PLMs to fill in the [mask] position in the prompt. The manual template and verbalizer for each dataset are listed in Appendix A.",
            "score": 0.5000234060481228,
            "section_title": "EVALUATION METRICS",
            "char_start_offset": 9206,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 215,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 974,
                    "end": 995,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1281,
                    "end": 1303,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1310,
                    "end": 1334,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1376,
                    "end": 1396,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "273549835",
            "title": "Task Calibration: Calibrating Large Language Models on Inference Tasks",
            "text": "Large language models (LLMs) (Touvron et al., 2023;Chowdhery et al., 2024;Abdin et al., 2024) have demonstrated strong generalization ability to excel in a wide range of downstream tasks. In particular, prompt-based learning has been an effective paradigm for LLMs, enabling zero-shot or few-shot learning (Brown et al., 2020;Liu et al., 2023). Ideally, an LLM with advanced language understanding capabilities could perform natural language inference (NLI) in a zero-shot setting without relying on annotated examples (McKenna et al., 2023). However, research has also shown that zero-shot capabilities of models on inference tasks are currently constrained by the presence of spurious correlations that often lead to biased prediction (Poliak et al., 2018;Gururangan et al., 2018;Zhao et al., 2021;Holtzman et al., 2021;McKenna et al., 2023). \n\nTo address such challenge, previous work (Zhao et al., 2021;Holtzman et al., 2021;Fei et al., 2023;Han et al., 2023;Zhou et al., 2024) has explored model calibration, which mitigates spurious correlations by reweighing output probabilities based on various bias estimators. However, existing calibration methods fall short of addressing the bias that stems from LLMs' reliance on either the premise or hypothesis (McKenna et al., 2023), which we call preference bias. This limits their capacity to generalize in inference tasks. Figure 1 shows an example from QNLI dataset (Rajpurkar et al., 2016), where the task is to determine whether a given context sentence contains the answer to a given question. We observe that the model prediction is incorrect because it relies excessively on the question itself when making the prediction in this example. \n\nBuilding upon this observation, we propose task calibration (TC), a zero-shot and inference-only calibration method. Our work is inspired by mutual information (Tishby et al., 1999;Peng et al., 2005), which measures how much one random variable tells us about another.",
            "score": 0.4962753783983478,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 326,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 519,
                    "end": 541,
                    "matchedPaperCorpusId": "258865517"
                },
                {
                    "start": 737,
                    "end": 758,
                    "matchedPaperCorpusId": "21382535"
                },
                {
                    "start": 758,
                    "end": 782,
                    "matchedPaperCorpusId": "4537113"
                },
                {
                    "start": 800,
                    "end": 822,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 822,
                    "end": 843,
                    "matchedPaperCorpusId": "258865517"
                },
                {
                    "start": 907,
                    "end": 929,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 929,
                    "end": 946,
                    "matchedPaperCorpusId": "258967265"
                },
                {
                    "start": 946,
                    "end": 963,
                    "matchedPaperCorpusId": "248964978"
                },
                {
                    "start": 1260,
                    "end": 1282,
                    "matchedPaperCorpusId": "258865517"
                },
                {
                    "start": 1420,
                    "end": 1444,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "235253968",
            "title": "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation",
            "text": "We discuss work related to cross-lingual semantic divergences and noise effects in Section 2 and now turn to the literature that connects with the methods used in this paper.\n\nFactored Models Factored models are introduced to inject word-level linguistic annotations (e.g., Part-of-Speech tags, lemmas) in translation. Source-side factors have been used in statistical MT (Haddow and Koehn, 2012) and in NMT (Sennrich et al., 2016b;Hoang et al., 2016). Target-side factors are used by Garc\u00eda-Mart\u00ednez et al. (2017) as an extension to the traditional NMT framework that outputs multiple sequences. Although their main motivation is to enable models to handle larger vocabularies, Wilken and Matusov (2019) propose a list of novel applications of target-side factors beyond their initial purpose, such as wordcase prediction and subword segmentation. Our approach draws inspiration from all the aforementioned works, yet it is unique in its use of both source and target factors to incorporate semantics in NMT.\n\nCalibration Kumar and Sarawagi (2019) find that NMT models are miscalibrated, even when conditioned on gold-standard prefixes. They attribute this behavior to the poor calibration of the EOS token and the uncertainty of attention and design a recalibration model to improve calibration. Ott et al. (2018) argue that miscalibration can be attributed to the \"extrinsic\" uncertainty of the noisy, untranslated references found in the training data. M\u00fcller et al. (2019) investigate the effect of label smoothing on calibration. On a similar spirit, Wang et al. (2020) propose graduated label smoothing to improve calibration at inference time. They also link miscalibration to linguistic properties of the data (e.g., frequency, position, syntactic roles). Our work, in contrast, focuses on the semantic properties of the training data that affect calibration.",
            "score": 0.4954214018145126,
            "section_title": "Related Work",
            "char_start_offset": 22250,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 432,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 1457,
                    "end": 1477,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 1557,
                    "end": 1575,
                    "matchedPaperCorpusId": "218487046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79150390625
        },
        {
            "corpus_id": "235078802",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "text": "Calibration Calibration is a well-studied topic in other tasks such as medical diagnosis (Jiang et al., 2012) and image recognition (Guo et al., 2017;Lee et al., 2018). Previous works in NLP have examined calibration in structured prediction problems such as part-of-speech tagging and named entity recognition (Jagannatha and Yu, 2020), natural language understanding tasks such as natural language inference, paraphrase detection, extractive question answering, and text classification (Desai and Durrett, 2020;Kamath et al., 2020;Kong et al., 2020). In contrast, we focus on calibrating LMs themselves by treating them as natural language generators that predict the next words given a particular input. \n\nLM probing Previous works probe pre-trained LMs with respect to syntactic and semantic properties (Hewitt and Manning, 2019;Tenney et al., 2019), factual knowledge (Petroni et al., 2019;Poerner et al., 2019;Jiang et al., 2020b), commonsense knowledge (Trinh and Le, 2018;Kocijan et al., 2019), and other properties (Talmor et al., 2019a). These works usually focus on what LMs know, while in this paper we also consider the cases when LMs do not know the answer with confidence.",
            "score": 0.49506449754752213,
            "section_title": "Related Work",
            "char_start_offset": 29621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1187
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 109,
                    "matchedPaperCorpusId": "7716019"
                },
                {
                    "start": 132,
                    "end": 150,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 150,
                    "end": 167,
                    "matchedPaperCorpusId": "3464416"
                },
                {
                    "start": 311,
                    "end": 336,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 513,
                    "end": 533,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 807,
                    "end": 833,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 833,
                    "end": 853,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 873,
                    "end": 895,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 980,
                    "end": 1001,
                    "matchedPaperCorpusId": "155091369"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "249191391",
            "title": "Teaching Models to Express Their Uncertainty in Words",
            "text": "These methods typically correct for models being overconfident on out-of-domain examples. In comparison, GPT-3's accuracy on our target domain is much higher than its accuracy on the source domain; its predictions therefore tend to be underconfident. The shift between target and source is also much larger, where we move from a single-answer to a multi-answer setting. \n\nNatural language generation. In the specific case of natural language generation, Jiang et al. (2021) study calibration by framing multiple-choice and extractive QA as generative tasks, where a language model's uncertainty can be extracted from its logits over all tokens in an answer sequence. The authors introduce methods for both fine-tuning and post-hoc calibration of logits. To handle answers that can be worded in more than one way, a round-trip translation model is used to generate paraphrases for each answer, and the model's uncertainty is calculated as its total probability across all such paraphrases. While this approach leads to better calibration, it adds additional overhead and doesn't handle the situation where a question has multiple answers that can't be exhaustively listed. \n\nVerbalized uncertainty. Branwen (2020) demonstrates GPT-3's ability to express verbalized uncertainty on simple trivia questions in the in-domain, few-shot setting, using an instructive prompt.",
            "score": 0.4945680970819387,
            "section_title": "Related work",
            "char_start_offset": 25589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1367
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 473,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "We show the comparison of experimental results (ECE) on BERT and RoBERTa in Table 1. For each task, we train the model on in-domain training set, and evaluate its expected calibration errors (ECEs) on in-domain and out-of-domain test sets. We make the following observations: First, for in-domain data, label smoothing (LS) does not exhibit its effectiveness on pretrained language models' calibration. Specifically, for in-domain data, pre-trained language models with LS (i.e., BERT+LS/RoBERTa+LS) achieve higher expected calibration errors (ECEs) compared with vanilla pre-trained language models (i.e., BERT/RoBERTa) on all tasks. In contrast, out-of-domain gains benefit from LS (except RoBERTa on MNLI). From these results, we conclude that simply incorporating label uncertainty (through label smoothing) is not an effective regularization method since LS does not consistently improve the model calibration (especially for the in-domain setting). While temperature scaling (TS) corrects the miscalibration of vanilla pre-trained language models (see BERT/RoBERTa No TS vs. TS in the table), it fails to correct miscalibrated pre-trained language models with LS (see BERT+LS/RoBERTa+LS No TS vs. TS) in-domain. Interestingly, for some cases of out-of-domain data, pre-trained language models with LS show comparatively low ECEs while TS further reduces ECEs (e.g., BERT(LS) on Twit-terPPDB/HellaSWAG, RoBERTa(LS) on TwitterP-PDB). However, its impact is not enough as it still results in high ECE. This implies that TS is not a notable strategy either to pre-trained language models' calibration. Accordingly, we conclude that stronger regularization techniques are required to calibrate the pre-trained language models. Second, we find that mixup on the hidden feature space (i.e., M-Mixup) generally yields lower ECE than mixup on the input embedding space (i.e., Mixup) on most tasks",
            "score": 0.49442528141867637,
            "section_title": "Results",
            "char_start_offset": 21302,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "276408950",
            "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
            "text": "Large Language Models (LLMs) have transformed natural language understanding, achieving stateof-the-art performance across diverse applications, from conversational AI (Skjuve et al., 2024;Zhang, 2024) to scientific discovery (Kumar, 2024). Their versatility has also facilitated advancements in multimodal learning, where language models integrate with vision-based systems to enhance tasks such as automated content generation and decision support (Zhang et al., 2023;Chhikara et al., 2024). However, as LLMs are increasingly deployed in high-stakes domains such as healthcare, finance, and law, critical concerns arise regarding calibration-the alignment between model confidence and actual correctness (Dhuliawala et al., 2023). When LLMs express unwarranted confidence in incorrect predictions, they risk misleading users, spreading misinformation, and reducing trust and reliability in AI-driven systems (Chen et al., 2023;Zhang et al., 2024). Figure 1 illustrates this issue: when asked \"Who received the IEEE Frank Rosenblatt Award in 2010?\", an LLM incorrectly responds with \"Geoffrey Hinton\", assigning a 93% confidence score, despite correct answer being \"Michio Sugeno\". This overconfidence is especially problematic because users tend to equate high-confidence outputs with reliability (Liu et al., 2024). The right-hand side of Figure 1 further demonstrates the broader miscalibration problem: while a well-calibrated model should align confidence with accuracy, real-world LLMs frequently overestimate their correctness. \n\nAlthough calibration errors in neural networks are well-documented (Guo et al., 2017), the calibration behavior of LLMs remains poorly understood, particularly in response to fine-tuning techniques 1 arXiv:2502.11028v1 [cs.CL] 16 Feb 2025 such as Reinforcement Learning from Human Feedback (RLHF) (Leng et al., 2024;Li et al., 2024).",
            "score": 0.4933398270115166,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 201,
                    "matchedPaperCorpusId": "274023997"
                },
                {
                    "start": 226,
                    "end": 239,
                    "matchedPaperCorpusId": "271961846"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "268066116"
                },
                {
                    "start": 470,
                    "end": 492,
                    "matchedPaperCorpusId": "261243300"
                },
                {
                    "start": 706,
                    "end": 731,
                    "matchedPaperCorpusId": "264405995"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 929,
                    "end": 948,
                    "matchedPaperCorpusId": "274023997"
                },
                {
                    "start": 1299,
                    "end": 1317,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 1605,
                    "end": 1623,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "268385471",
            "title": "Thermometer: Towards Universal Calibration for Large Language Models",
            "text": "An alternate body of work (Zhang et al., 2021;Kadavath et al., 2022;Mielke et al., 2022) approach calibration indirectly by learning an auxiliary model for predicting whether a generation is incorrect.There is also a body of work (Abbas et al., 2024;Han et al., 2023;Jiang et al., 2023b;Zhao et al., 2021;Zhou et al., 2024) that use the term calibration to mean de-biasing the predictions of a language model to biases introduced by the choice and ordering of in-context examples.This is distinct from our notion of statistical calibration.Yet others (Tian et al., 2023;Xiong et al., 2023), similar to our approach, consider RLHF-tuned LLMs and find that they can express better-calibrated uncertainties with carefully crafted prompts.Our work is orthogonal, extending temperature scaling to tasks without labeled data, and can be combined with their approach.",
            "score": 0.490161307119962,
            "section_title": "Related Work",
            "char_start_offset": 8348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 860
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 88,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 230,
                    "end": 250,
                    "matchedPaperCorpusId": "267094982"
                },
                {
                    "start": 250,
                    "end": 267,
                    "matchedPaperCorpusId": "248964978"
                },
                {
                    "start": 267,
                    "end": 287,
                    "matchedPaperCorpusId": "264146928"
                },
                {
                    "start": 287,
                    "end": 305,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "263310485"
                },
                {
                    "start": 551,
                    "end": 570,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "As research on estimating the internal confidence of LLMs has increased, scholars have started to focus on model calibration-i.e., the alignment between predicted probabilities and actual correctness. Desai & Durrett (2020) find that pre-trained transformers such as BERT often exhibit poor calibration out-of-the-box, where their confidence estimates fail to correspond to actual correctness. Jiang et al. (2021) also explore methods to improve model calibration, such as temperature scaling, which adjusts predicted probabilities to better match actual outcomes. However, most calibration methods focus on probabilistic outputs (internal confidence) without addressing how certainty is expressed through language generation. The same miscalibration is found by Si et al. (2022), who demonstrate that models like GPT-3 frequently produce overconfident responses even when incorrect. \n\nHowever, we note that despite these known limits, internal confidence scores remain largely inaccessible to users today. Without these scores, it becomes essential for models to effectively communicate uncertainty through external means-such as linguistic cues-to ensure users correctly interpret the model's output.",
            "score": 0.4865099478699323,
            "section_title": "Challenges in Confidence Alignment",
            "char_start_offset": 11412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1202
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "215548393",
            "title": "Calibrating Structured Output Predictors for Natural Language Processing",
            "text": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
            "score": 0.4847986355503301,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89990234375
        },
        {
            "corpus_id": "256846523",
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "text": "The calibration of machine learning models has been mainly studied for the trustworthy deployment of image recognition applications (Lakshminarayanan et al., 2017;Hongyi Zhang, 2018;Guo et al., 2017). Beyond the computer vision fields, research on the calibration ability of language models in the NLP domain has also recently been attracting attention (Desai and Durrett, 2020;Dan and Roth, 2021). Desai and Durrett (2020) investigate the calibration ability of PLMs, and they demonstrate that RoBERTa produces more calibrated predictions than BERT. They also show that temperature scaling (Hinton et al., 2014) and label smoothing (Szegedy et al., 2016) improve the calibration performance of PLMs for language understanding tasks. Dan and Roth (2021) conduct an empirical study of the effects of model capacity on PLMs and show that smaller pre-trained transformers provide more reliable predictions. Moon et al. (2020) find that PLMs tend to produce over-confident outputs based on in-distribution (ID) keywords rather than contextual relations between words. They demonstrate that keyword-biased predictions can be overconfident even in out-of-distribution samples with ID keywords. Kong et al. (2020) suggest two regularizers using generated pseudo-manifold samples to improve both ID and out-of-distribution calibration for PLMs. They use MixUp (Hongyi Zhang, 2018) as a regularization technique for BERT calibration and show that mixed training samples on the data manifold improve the calibration performance. Similarly, Park and Caragea (2022) propose a variant of MixUp utilizing saliency signals and also analyze the impact of combining additional calibration methods with MixUp. However, they only consider temperature scaling and label smoothing as additional calibration methods. \n\n3 Why Re-assess Calibration Methods? Guo et al. (2017) observe that a larger DNN tends to be more poorly calibrated than a smaller one. As the size of the parameters for modern DNNs continues to increase, the miscalibration issues need to be addressed more than ever.",
            "score": 0.48103936473182585,
            "section_title": "Related Work",
            "char_start_offset": 4145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 199,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 353,
                    "end": 378,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 378,
                    "end": 397,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 399,
                    "end": 423,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 591,
                    "end": 612,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 633,
                    "end": 655,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 734,
                    "end": 753,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 1530,
                    "end": 1553,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 1834,
                    "end": 1851,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Out of Box Zero-Shot Calibration (OOB) We first investigate how well calibrated MMLMs are on the languages unseen during fine-tuning without applying any calibration techniques. As can be seen in the Table 1, the average calibration error on languages other than English (column 4) is almost always significantly worse than the errors on English test data (column 3) for both mBERT and XLMR across the 4 tasks. Along with the expected calibration errors across unseen languages we also report the worst case ECE (in column 5), where we see 2\u00d7 to 5\u00d7 increase in errors compared to English. The worst case calibration is commonly observed  (Littell et al., 2017) to compute the syntactic similarity between the pivot and target language as done in Lin et al. (2019). \n\niii) SWO: Finally we consider the sub-word overlap between the pivot and target language as defined in Srinivasan et al. (2021). To compute SWO, first vocabularies V p and V t are identified for the pivot and target langauge respectively by tokenizing the wikipedias in the two languages and getting rid of the tokens that appear less than 10 times in the corpora. The subword overlap is then computed as : \n\nIn the case of XLMR, for all the tasks except MARC, we observe strong negative correlations between ECE and the three factors mentioned above (Table 2), meaning that lower the amount of pre-training data present for a language or its relatedness with English, the higher is the calibration error. Out of the three, the correlations are more consistently (negatively) high with SYN. We observe similar correlations albeit to a slightly lower extent for mBERT as well (Table 6 in Appendix). \n\nImproving Calibration Now that we have identified miscalibration as an issue in MMLMs and factors influencing the same, we seek to improve their calibration across languages. We utilize the calibration methods described in Section 2.1, and report the average calibration errors across the unseen languages in Table 3 for XNLI and XCOPA datasets2 .",
            "score": 0.4802578597225535,
            "section_title": "Results",
            "char_start_offset": 8118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 660,
                    "matchedPaperCorpusId": "17625727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70166015625
        },
        {
            "corpus_id": "235078802",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "text": "This is especially true if these models are deployed to safety-critical domains such as healthcare and finance, where mistaken answers can have serious consequences. 1 n this paper, we ask the question \"how can we know when language models know, with confidence, the answer to a particular knowledge-based query?\" Specifically, we examine this from the point of view of calibration, whether the model's probability estimates are well-aligned with the actual probability of the answer being correct. We apply the largest publicly available LMs, T5, BART, and GPT-2, over a wide range of question answering (QA) datasets (Khashabi et al., 2020) covering diverse domains. We first observe that despite the models' high performance (e.g. T5 eclipses other alternatives such as GPT-3 on some datasets), the models tend to not be well calibrated; their probability estimates over candidates have far-from-perfect correspondence with the actual probability that the answer they provide is correct. Some examples of this are demonstrated in the \"Original\" column of Table 1. \n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context. \n\nExperimental results demonstrate that both finetuning and post-hoc methods can improve calibration performance without sacrificing accuracy. We further perform analysis and ablation studies on our methods, inspecting different aspects that may affect calibration performance.",
            "score": 0.4773483530574582,
            "section_title": "Introduction",
            "char_start_offset": 1723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1697,
                    "end": 1722,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 1746,
                    "end": 1766,
                    "matchedPaperCorpusId": "219721462"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "We follow the setting for baselines as in [11], with some additional baselines explained as follows: normalized and average sequence likelihood with few-shot learning, few-shot learning and CoT tokens, and recent method of verbalized qualitative uncertainty [10], [21], [83], [200] method to obtain confidence of model. We ask verbally to provide confidence from \"very high\" to \"very low\" and then map this scale to numerical values using the scheme we describe in Appendix A-B. In addition, we apply Platt Scaling [97] to average sequence likelihoods and TS [28] to logit scores of generated sequences for obtaining scaled sequence likelihoods. \n\nAnalyses. In this section, we will discuss the suitability of methods and metrics for LLMs based on our experimental evaluation. In general, we notice that calibration methods can make models provide a more accurate probability of their predictions, hence making them more reliable, as noted in Table V. We bold the best results per dataset and model and underline those that are statistically significant compared to all other results after multiple experimental runs. \n\nFor TriviaQA dataset, we provide reliability diagrams for Qwen2.5 in Figure 4, Llama3.1 in Figure 2, and Mistral v0.3 in Figure 3 . We provide the remaining diagrams in Appendix B. We observe that the Qwen2.5 model exhibits low calibration and accuracy. As this model is a multi-lingual model and domain knowledge is also different, it can affect the accuracy of generation and the language mixing problem [4]. In this context, the calibration of LLMs across different domains, such as in scenarios involving dataset shift [21], remains an interesting and open problem for further investigation. Parametric calibration methods make bin-wise confidences align with bin-wise accuracy, hence making the model more calibrated. TS can be implemented in two ways: (1) scaling the model confidences or log probabilities of candidate sequences [11], [199], [200] or (2) scaling the logits before the softmax layer, which operates on the vocabulary size.",
            "score": 0.47625748979373683,
            "section_title": "V. RQ3: EVALUATION",
            "char_start_offset": 43238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 559,
                    "end": 563,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "237532257",
            "title": "Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets",
            "text": "An advantage of probabilistic models is that, in addition to providing a prediction, they also quantify uncertainty. Knowing how certain a model is about a particular prediction can be crucial when using its output for downstream tasks or when weighing its trustworthiness. Of course, the probability estimate associated with a predicted output is an artifact of the model, and is subject to error-separate from the accuracy or error of the prediction itself.\n\nBy and large, NLP evaluations of multiclass classifiers and structured prediction models consider only the top prediction for an input and how closely it matches the gold standard. Only in some studies is the probability assigned to the prediction taken into account at all (e.g. via a precision-recall curve).\n\nA more comprehensive evaluation would examine whether the model's probabilities are wellcalibrated, i.e., whether they correlate well with empirical accuracy (such that \u2248 \u03b1% of predictions with probability close to \u03b1 are in fact correct). Guo et al. (2017) showed that despite high accuracy, modern neural networks can still suffer from severe miscalibration. Fortunately, calibration error is not completely random, and can be corrected post hoc with a second model fit on development data (or even a separate recalibration set if available) as in several recalibration techniques ( \u00a72).\n\nIn domains where NLP models help inform human decision-making (e.g., medicine), having a well-calibrated model is essential. Even in less critical domains, a well-calibrated model has potential to benefit rare instance discovery, pre-annotation, and self-training. In this paper we consider a structured prediction setting of particular relevance in NLP: tagging tasks with sparse tagsets-output spaces with a handful of high-frequency tags and many more rare tags.\n\nMany linguistic phenomena follow power law distributions and thus feature a long tail of individually rare events, which, as we will show, makes it nontrivial to measure calibration error with existing methods, including marginal calibration error (MCE), which requires sufficient samples of each class to produce a reliable estimate (Kumar et al., 2019). We evaluate two English sentence taggers 1 with closed sets of 100s of tags that disambiguate",
            "score": 0.4758072169092096,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1012,
                    "end": 1029,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "260379149",
            "title": "Calibration in Deep Learning: A Survey of the State-of-the-Art",
            "text": "In recent years, a variety of approaches have emerged for model calibration, spanning post-hoc adjustments, regularization techniques, uncertainty estimation, and hybrid methods. Several related surveys have delved into this area (Silva Filho, Song, Perello-Nieto, Santos-Rodriguez, Kull, & Flach, 2023), or those focusing on the highly relevant topic of uncertainty estimation (Silva Filho et al., 2023;Gawlikowski, Tassi, Ali, Lee, Humt, Feng, Kruspe, Triebel, Jung, Roscher, et al., 2021;Mena, Pujol, & Vitria, 2021), which briefly touch upon model calibration. However, there remains a gap in the literature regarding a comprehensive review of recently proposed calibration techniques. Setting itself apart from previous surveys, our research emphasizes several key distinctions: \n\n\u2022 This survey reviews the state-of-the-art calibration methods and focuses mostly on the ones proposed in recent years. Those methods such as kernel-based methods, differentiable calibration proxy, and meta-learning-based approaches. Those are rarely discussed in previous and existing surveys. \n\n\u2022 This survey tries to explain calibration principles of each method via the discussion of the conceptual relationships among over-parameterization, over-fitting, and over-confidence. We systematically categorize those methods into post-hoc, regularization (explicit, implicit and differentiable calibration proxy), uncertainty estimation, and hybrid methods that combines multiple calibration techniques. \n\n\u2022 This survey also delves into the methodologies for calibrating large pre-trained models, with a specific focus on large language models (LLMs). Notably, the calibration of LLMs for zero-shot inference has garnered escalating interest within AI communities. \n\nThe rest of this survey is structured as follows. \n\n\u2022 Section 2 provides an introduction to the concept of model calibration, elucidating the factors that contribute to miscalibration. We explore how various aspects such as model complexity, data distribution, training and measurement procedures can impact the calibration of machine learning models. \n\n\u2022 Section 3 presents an overview of mainstream calibration metrics commonly used to evaluate the calibration performance of models.",
            "score": 0.4752029346232234,
            "section_title": "Scope and Focus",
            "char_start_offset": 3594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 303,
                    "matchedPaperCorpusId": "256901249"
                },
                {
                    "start": 378,
                    "end": 404,
                    "matchedPaperCorpusId": "256901249"
                },
                {
                    "start": 491,
                    "end": 519,
                    "matchedPaperCorpusId": "241258024"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "67855916",
            "title": "Calibration of Encoder Decoder Models for Neural Machine Translation",
            "text": "Calibration is an important property to improve interpretability and reduce bias in any prediction model. For sequence prediction it is additionally important for sound functioning of beam-search or any approximate inference method. We measured the calibration of six state-of-the-art neural machine translation systems built on attention-based encoder-decoder models using our proposed weighted ECE measure to quantify calibration of an entire multinomial distribution and not just the highest confidence token. \n\nThe token probabilities of all six NMT models were found to be surprisingly miscalibrated even when conditioned on true previous tokens. On digging into the reasons, we found the EOS token to be the worst calibrated. Also, positions with higher attention entropy had worse calibration. \n\nWe designed a parametric model to recalibrate as a function of input coverage, attention uncertainty, and token probability. We achieve significant reduction in ECE and show that translation accuracy improves by as much as 0.4 when the right models are used to fix calibration. Existing temperature scaling recalibration actually worsens accuracy. We show that improved calibration leads to greater correlation between probability and error and this manisfests as reduced BLEU drop with increasing beam-size. We further show that in our calibrated models the predicted BLEU is closer to the actual BLEU. \n\nWe have reduced, but not totally eliminated the miscalibration of modern NMT models. Perhaps the next round of fixes will emerge out of a better training mechanism that achieves calibration at the time of training. The insights we have obtained about the relation between coverage and EOS calibration, and attention uncertainty should also be useful for better training.",
            "score": 0.47452336191399047,
            "section_title": "CONCLUSION AND FUTURE WORK",
            "char_start_offset": 29536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1779
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "236134351",
            "title": "Learning ULMFiT and Self-Distillation with Calibration for Medical Dialogue System",
            "text": "To resolve these issues, Self Distillation (SD) (Yuan et al., 2019) is proposed, where the student model is used as the teacher model as well. Results show that SD can almost replicate the accuracy regardless of a well-trained large model or big dataset such as in the image classification task (Zhang et al., 2019). SD has also been applied to NLP tasks such as language model and neural machine translation (Hahn and Choi, 2019) and obtains promising results. Despite obtaining higher accuracy and better performance, modern deep learning models face drawbacks of miscalibration and overconfidence (M\u00fcller et al., 2019;Naeini et al., 2015;Lakshminarayanan et al., 2016). Recent studies resolve this issue by using techniques like label smoothing (M\u00fcller et al., 2019) and temperature scaling (Naeini et al., 2015), and Dirichlet calibration (Kull et al., 2019). These works show that the well-calibrated model can improve the model performance as well as feature representation. As for the NLP downstream tasks, research has shown that calibration benefits both sentence quality and length in the sentence classification (Jung et al., 2020), and helps to improve the model fine-tuning in text generation (Kong et al., 2020). \n\nAs transfer learning techniques and calibration contributes to NLP tasks, we investigate the correlation of improving calibrated feature representation with ULMFiT and SD. Label smoothing is integrated with ULMFiT to extract significant features from language modeling. To improve KD by recalibrating predicted probability, we incorporate temperature scaling (TS) with knowledge distillation loss. We also observe the correlation of a well-calibrated trained network in whole model fine-tuning. We conduct extensive experiments to validate our observations with two datasets of (1) the consultation back pain and (2) medical dialogue. Results show that a well-calibrated model is highly correlated with ULMFiT and SD, as well as finetuning, in terms of both accuracy and calibration error. \n\nOur contributions can be concluded as following:",
            "score": 0.47383549208908166,
            "section_title": "Introduction",
            "char_start_offset": 2221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2018
                },
                {
                    "start": 2021,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 315,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 600,
                    "end": 621,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 621,
                    "end": 641,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 748,
                    "end": 769,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 794,
                    "end": 815,
                    "matchedPaperCorpusId": "6292807"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "238856959",
            "title": "Can Explanations Be Useful for Calibrating Black Box Models?",
            "text": "NLP practitioners often want to take existing trained models and apply them to data from new domains. While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box. We study how to improve a black box model\u2019s performance on a new domain by leveraging explanations of the model\u2019s behavior. Our approach first extracts a set of features combining human intuition about the task with model attributions generated by black box interpretation techniques, then uses a simple calibrator, in the form of a classifier, to predict whether the base model was correct or not. We experiment with our method on two tasks, extractive question answering and natural language inference, covering adaptation from several pairs of domains with limited target-domain data. The experimental results across all the domain pairs show that explanations are useful for calibrating these models, boosting accuracy when predictions do not have to be returned on every example. We further show that the calibration model transfers to some extent between tasks.",
            "score": 0.4737024308715537,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "252918036",
            "title": "A Closer Look at the Calibration of Differentially Private Learners",
            "text": "We study three different experimental settings. We first consider in-domain evaluations, where we evaluate calibration errors on the same domain that they are trained on. Results show that using pre-trained models does not address miscalibration issues in-domain. We then evaluate the same models above in out-of-domain settings, showing that both miscalibration and effectiveness of our recalibration methods carry over to the out-of-domain setting. Finally, we perform careful ablations to isolate and understand the causes of in-domain miscalibration. In each case, we will show that DP-SGD leads to high miscalibration, and DP recalibration substantially reduces calibration errors.",
            "score": 0.47287574305142793,
            "section_title": "Experimental Results",
            "char_start_offset": 13346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 686
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "248524894",
            "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
            "text": "We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration [68], where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], NULL, <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions.",
            "score": 0.46960371009431867,
            "section_title": "Calibration",
            "char_start_offset": 25122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1569
                }
            ],
            "ref_mentions": [
                {
                    "start": 1030,
                    "end": 1034,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "268681255",
            "title": "General LLMs as Instructors for Domain-Specific LLMs: A Sequential Fusion Method to Integrate Extraction and Editing",
            "text": "The substantial interest in updating Large Language Models (LLMs) without retraining from scratch is accompanied by several challenges. This is particularly true when updating LLMs with datasets that necessitate domain-expert reasoning across extensive texts, despite limited samples. We termed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs (FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and Retrieval Augmented Generation (RAG) are inadequate for addressing this critical issue, particularly evident in our exploration of a specific medical dataset that epitomizes the distinct needs of FDoR-UL. To tackle this challenge, we introduce a Sequential Fusion method to integrate knowledge from complex contexts into LLMs. This method employs a two-stage framework: initially leveraging general LLMs to perform relation extraction for knowledge acquisition from complex texts, followed by updating domain-specific LLMs through Knowledge Editing (KE). Employing our method, domain-specific LLMs achieved a 71.7% accuracy (an average gain of 39.1%) in question-answering tasks. Furthermore, we expanded our evaluation to a novel economics-management dataset we developed, where our method achieved a 75.0% accuracy (an average gain of 45.0%). These findings underscore the effectiveness and flexibility of our approach in FDoR-UL across various domains.",
            "score": 0.46947769684921986,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "267522853",
            "title": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach",
            "text": "Large pre-trained language models like GPT-3.5 1 or GPT-4 2 have gained significant attention in natural language research. However, fine-tuning these models for specific tasks is challenging due to limited computational resources or inaccessible parameters. Consequently, many researchers resort to using web APIs for instructing LLMs, leveraging zero-shot or few-shot in-context learning, enabling the LLMs to tackle tasks they weren't explicitly trained for. Unfortunately, this approach falls short when tackling some low-resource sequence generation tasks in machine translation (MT), and logical form (LF)-to-text translation, as shown in Lai et al. (2023); Haroutunian et al. (2023). In such cases, minimal task-specific data was available during the 1 https://platform.openai.com/docs/models/gpt-3-5-turbo 2 https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo LLMs' pre-training phase. The output quality of LLMs for such tasks is compromised due to the absence of task-specific knowledge. \n\nTo address this challenge, a promising set of solutions suggests integrating task-specific knowledge into language models through post-editing the generated text using a smaller model fine-tuned on task-specific data. Yet, these methods are not without their drawbacks. Our findings indicate that exclusive reliance on a smaller model for editing, e.g. Self-Correct (Welleck et al., 2022), results in suboptimal performance in domain generalization scenarios, likely due to the inherently limited domain knowledge within these smaller models. \n\nAs LLMs (i.e. GPT-3.5 or GPT-4) have shown superior domain generalization ability (Wang et al., 2023;Yang et al., 2023) over the fine-tuned model, we introduce an innovative approach based on the programmer-interpreter framework (Reed and de Freitas, 2016), which benefits from the domain generalization ability from LLMs.",
            "score": 0.46794206665416815,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1877
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "\u2022 Larger Parameter Scales : Improve models' calibration. \n\n\u2022 Longer Training Dynamics : Also benefit calibration accuracy. \n\nFor alignment of LLMs: \n\n\u2022 Instruction Tuning : Deteriorates models' calibration. \n\n\u2022 Synthetic Data : Exacerbates the harmful effect of instruction tuning. \n\n\u2022 Parameter-efficient Fine-tuning : Effective regularization for restraining calibration error. \n\n\u2022 RLHF : Help maintaining calibration accuracy. \n\nFor different tasks: \n\n\u2022 In pre-training: Improvement in calibration accuracy is more significant on fact generation task or language understanding tasks than language modeling task. \n\n\u2022 In alignment training: Calibration accuracy evolves consistently across different downstream tasks including fact generation, language understanding or vanilla language modeling. \n\nWe believe these conclusions as well as detailed experiments can take us a step further towards understanding large language models, especially the intrinsic mechanism of their calibration behaviour. Our experimental results also provide us with some possible solutions to improve calibration, including increasing model scales and employing parameter efficient tuning methods. Besides, diversity guided instruction data construction may also be very promising. Hopefully these findings can shed light on future works to construct more factual and trustworthy assistants.",
            "score": 0.46644915669159726,
            "section_title": "For pretraining of LLMs:",
            "char_start_offset": 4386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 59,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 206
                },
                {
                    "start": 209,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "We conduct a large-scale evaluation of multicalibration methods, comparing three families of methods: (1) standard ERM, (2) ERM followed by a classical recalibration method (e.g.Platt scaling), and (3) ERM followed by an explicit multicalibration algorithm (e.g. that of H\u00e9bert-Johnson et al. (2018)).\n\nWe find that in practice, this comparison is surprisingly subtle: multicalibration algorithms do not always improve multicalibration (relative to the ERM baseline), for example.From the results of our extensive experiments on tabular, vision, and language tasks (involving running multicalibration algorithms more than 45K times), we extract a number of observations clarifying the utility of multicalibration.Most significantly, we find:\n\n1. ERM alone is often a strong baseline, and can often be remarkably multicalibrated without further postprocessing.In particular, on tabular datasets, multicalibration post-processing does not improve upon worst group calibration error of ERM for simple NNs.\n\n2. Multicalibration algorithms are very sensitive to hyperparameter choices, and can require large parameter sweeps to avoid overfitting.These algorithms tend to be most effective in regimes with large amounts of available data, such as image and language datasets.\n\n3. Traditional calibration methods such as Platt scaling or isotonic regression can sometimes give nearly the same performance as multicalibration algorithms, and are hyperparameter-free.Furthermore, compared to multicalibration post-processing, they are extremely computationally efficient and do not take very long to evaluate.\n\nWe also present numerous practical takeaways for users of multicalibration algorithms, which are not apparent from the existing theoretical literature, but are crucial considerations in practice.We believe that our investigations will not only broaden the practical applicability of multicalibration as a concept and algorithm, but also provide valuable information to the theoretical community as to what barriers multicalibration faces in practice.To both of these ends, all code used in our experiments is publicly accessible1 .\n\nOrganization.",
            "score": 0.46451315507747903,
            "section_title": "Our Contributions",
            "char_start_offset": 7047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 178,
                    "end": 301
                },
                {
                    "start": 303,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 713
                },
                {
                    "start": 713,
                    "end": 741
                },
                {
                    "start": 743,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 1002
                },
                {
                    "start": 1004,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1269
                },
                {
                    "start": 1271,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1600
                },
                {
                    "start": 1602,
                    "end": 1797
                },
                {
                    "start": 1797,
                    "end": 2052
                },
                {
                    "start": 2052,
                    "end": 2133
                },
                {
                    "start": 2135,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "277673807",
            "title": "Enhancing counterfactual detection in multilingual contexts using a few shot clue phrase approach",
            "text": "However, certain instances, such as the JP-only configuration, exhibit a decline in performance, perhaps attributable to domain shifts or data imbalance effects. \n\nFinally, our findings suggest that increasing the number of shots beyond three-shot learning yields mixed effects: improving generalization in certain instances but causing inconsistencies in others. When scaling a fewshot learning technique, it is important to have a balanced and diverse data set because the effectiveness of fiveshot training depends on the linguistic combination and the caliber of the available samples. \n\nTested on SemEval2020Task5 (multi-domain) The results of the counterfactual model test on an unseen multi-domain dataset are displayed in Table 6. The evaluation of our modified model for the SemEval dataset is in contrast to that of a state-of-the-art few-shot model. The suggested model performs better than the current model, even in a multi-domain configuration, as is readily apparent. \n\nTable 6 presents a comparison of F1 scores in a configuration of one shot, three shot, and five shots for a multidomain scenario that is comparable to the multilingual analysis conducted in the previous section. In this table are presented different training setups and their respective F1 ratings for the SetFit and M-SetFit methods. It offers a comprehensive understanding of the efficacy of various language combinations in a multidomain environment. In both multilingual and multidomain configurations, the M-SetFit model generally outperforms the SetFit model, as evidenced by its higher F1 scores across all languages and domains. This evidence indicates that the model's performance in few-shot learning tasks can be enhanced by incorporating clue phrases into the training approach. \n\nFurthermore, the hypothesis that the generalization of the model is improved by the incorporation of a fiveshot setup is further supported by the additional improvement in F1 scores. This enhancement is due to the fact that the model is able to more effectively capture domain-specific variations and domain-invariant patterns   The noted enhancement in performance with five-shot training substantiates the assertion that few-shot learning models gain from supplementary data points, until a specific threshold is attained, beyond which returns diminish.",
            "score": 0.4641036074590551,
            "section_title": "Evaluation efficacy of M-SetFit",
            "char_start_offset": 26075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "276317967",
            "title": "Hallucination, Monofacts, and Miscalibration: An Empirical Investigation",
            "text": "Calibration asks whether probabilities assigned by a model match empirical frequencies. Analogous to a weather forecaster whose \"30% rain\" on x days prediction should coincide with rain on roughly 30% of those days, a language model is calibrated if, among statements it rates near q, the proportion that are factual is also q. \n\nLogarithmic binning. Following Kalai & Vempala (2024), we partition the unit interval into logarithmic bins and set \u03f5 = 0.1 unless otherwise indicated. \n\nand place each statement x in the unique bin whose range contains g(x). We have also tested adaptive partitions, yielding indistinguishable results.3 \n\nMiscalibration metric. For any partition B(g) induced by g, form the coarsened distribution p B(g) (x) = p(B)/|B| for x \u2208 B. Miscalibration is the total-variation distance between g and this coarsening. Then, miscalibration is defined as4 : \n\nBecause p(B) = x\u2208B p(x) and g(B) = x\u2208B g(x), Eq. ( 3) is equivalently the sum of within-bin total-variation distances, \n\nwith larger values indicating worse calibration. Throughout this paper miscalibration is measured exclusively on the pair (g, p) under the partition defined by g.",
            "score": 0.46186137106523967,
            "section_title": "Miscalibration Measurement",
            "char_start_offset": 8216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1162
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 383,
                    "matchedPaperCorpusId": "265445593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "270764323",
            "title": "A Teacher Is Worth A Million Instructions",
            "text": "The recent advancements in Large Language Models (LLMs) have significantly propelled the field of natural language understanding and generation.Pre-trained language models (PLMs) leveraging extensive training corpus sourced from web [3,6] have demonstrated impressive capabilities across various natural language processing (NLP) tasks.However, additional training steps are required for PLMs to follow instructions and keep the responses aligned to human preferences.\n\nInstruction tuning (IT) [19,27,31] trains a PLM further for instruction following; utilising the general knowledge imparted in the pre-training phase along with the imparted instruction following capability it trains the model to generalise well on unseen tasks.However, while proficient at following instructions, these models may produce outputs that are potentially toxic or ethically questionable.To enhance alignment with human values, further training is necessary, utilizing techniques such as reinforcement learning with human feedback [13], direct preference optimization (DPO) [16] and monolithic preference optimization without reference model (ORPO) [10] based on pairwise preference data.\n\nInstruction tuning requires meticulous attention to data quality, optimization of instruction sets, and the implementation of effective training methodologies to ensure peak performance.A primary challenge in training these instruction-tuned models in specific domains is the potential reduction in the model's generalization ability, a factor we monitor using public evaluation benchmarks in our research.In this study, we present a method that not only addresses these concerns but also improves public benchmarks while aligning the model within a specific domain, in this instance, ecommerce.Drawing from the successful implementation of Knowledge Distillation (KD) [9] in miniLLMs [8] and tasks such as classification, we propose it as an alternative to the commonly used supervised fine-tuning (SFT) and alignment process in language model training.We propose Domain Alignment from Expert (DAE), a unique post-training domain alignment algorithm designed to strengthen domain-specific knowledge within the LLMs.DAE integrates domain-specific expert models into the training process, enhancing the model's understanding of specialized domains while preserving its ability to generalize across broader contexts.",
            "score": 0.4604014654488379,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 468
                },
                {
                    "start": 470,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1171
                },
                {
                    "start": 1173,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1768
                },
                {
                    "start": 1768,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "271971505",
            "title": "Understanding Knowledge Drift in LLMs through Misinformation",
            "text": "Language model architectures and capabilities. Understanding LLM knowledge drift necessitates grounding in the foundational works on language models. Pioneering efforts such as GPT [19], BERT [6], or T5 [20] remain fundamental to contemporary LLMs. This groundwork facilitated the recent advancements in LLM capabilities, exemplified by Brown et al.'s [2] exploration of few-shot learning and the development of powerful models like GPT-4 [1], or PaLM [4] with its pathway architecture. Furthermore, the emergence of efficient and opensource models like LLaMA [24] and its successors highlight the ongoing progress in LLM accessibility and development. \n\nUncertainty in LLMs. While prior work has explored uncertainty quantification in NLP tasks like calibration of classifiers and text regressors ( [10,5,7,25]), these approaches often rely on techniques directly transferable from other domains (e.g., Monte Carlo dropout, Deep Ensembles). However, as highlighted by [13], generative tasks in NLP present unique challenges due to semantic equivalence. For instance, Jiang et al. [10] demonstrate a weak correlation between answer confidence (log-likelihood) and correctness in generative question answering. \n\nRecent efforts have tackled uncertainty or calibration in Natural Language Generation (NLG) by prompting models to assess their outputs or fine-tuning models to predict uncertainty ( [15,14,12]). While these methods can be effective, they often require additional training data and supervision, leading to challenges in reproducibility, cost, and sensitivity to distribution shifts (e.g., hardware limitations preventing implementation as in [12]). \n\nThe challenges associated with uncertainty estimation in NLG mirror those in automatic NLG evaluation. For example, Ott et al. [16] highlight performance limitations in machine translation due to multiple valid translations for a single source sentence. Similarly, Sai et al. [23] discuss the potential of paraphrase detection for NLG evaluation, which may offer insights applicable to uncertainty estimation tasks. \n\nLLM factual knowledge and calibration. Understanding LLM knowledge drift requires examining research on factual knowledge capabilities and calibration in question answering.",
            "score": 0.4575086899444079,
            "section_title": "Related Work",
            "char_start_offset": 3243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "249679786"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1395,
                    "end": 1399,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "4375156"
                },
                {
                    "start": 1939,
                    "end": 1943,
                    "matchedPaperCorpusId": "221340941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494140625
        },
        {
            "corpus_id": "265158225",
            "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
            "text": "Fine-tuning adapts LLMs to specific domains by training them on relevant datasets, using selected architectures and hyper-parameters to modify the model's weights for improved task performance (Guu et al., 2020;Hu et al., 2021;Lu et al., 2022;Dettmers et al., 2023). KGs can further tune these models to update and expand their internal knowledge for domain-specific tasks like custom named-entity recognition (Agrawal et al., 2023b), and text summarization (Kang et al., 2022a). SKILL (Moiseev et al., 2022) used synthetic sentences converted from WikiData (Seminar et al., 2019) and KELM (Agarwal et al., 2020) used KGs to fine-tune the pre-trained model checkpoints. KGLM (Youn and Tagkopoulos, 2022) employed an entity-relation embedding layer with KG triples for link prediction tasks. Cross-lingual reasoning (Foroutan et al., 2023) improved by fine-tuning MultiLM, mBERT, and mT5 models with logical datasets using a self-attention network. LLMs improve more with additional training using datasets with few-shot CoT reasoning prompts and finetuning (Kim et al., 2023;Huang et al., 2022). \n\nFine-tuning language models like ChatGPT, limited by their last knowledge update in 2021, is more efficient than training from scratch. It handles queries beyond this cutoff using a curated, domainspecific knowledge graph. The extent to which updated knowledge is integrated into the model remains to be determined. Onoe et al.'s (Onoe et al., 2023) evaluation framework indicate that while models can recall facts about new entities, inferring based on these is harder. The effect of updating knowledge on existing entities is still an open research question.",
            "score": 0.45734312850828224,
            "section_title": "Knowledge-Aware Fine-Tuning",
            "char_start_offset": 15798,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1658
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 410,
                    "end": 433,
                    "matchedPaperCorpusId": "260357261"
                },
                {
                    "start": 815,
                    "end": 838,
                    "matchedPaperCorpusId": "264439333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "258676559",
            "title": "What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization",
            "text": "At a high-level, we fine-tune (FT) language models with standard maximum likelihood estimation (MLE) on each summarization corpus, and then calibration-tune (CT) on a combined objective, which adds a calibration loss (CA) to the MLE loss:\n\n\u03bb M LE , \u03bb CA are scalars controlling the relative weight of objective. For L CT , L M LE acts as a regularizer, as in Liu et al. (2022);Zhao et al. (2022). We describe the setup (objective, metrics, and candidate generation methods) for Relevance Calibration ( \u00a74.1) and Faithful Calibration ( \u00a74.2, before jointly discussing statistics on each setup ( \u00a74.3).",
            "score": 0.4570074138394915,
            "section_title": "Calibration Pipeline",
            "char_start_offset": 8054,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "258967945",
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "text": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2020). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
            "score": 0.45283379289724596,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 311,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 387,
                    "end": 405,
                    "matchedPaperCorpusId": "202750126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "67855916",
            "title": "Calibration of Encoder Decoder Models for Neural Machine Translation",
            "text": "We study the calibration of several state of the art neural machine translation(NMT) systems built on attention-based encoder-decoder models. For structured outputs like in NMT, calibration is important not just for reliable confidence with predictions, but also for proper functioning of beam-search inference. We show that most modern NMT models are surprisingly miscalibrated even when conditioned on the true previous tokens. Our investigation leads to two main reasons -- severe miscalibration of EOS (end of sequence marker) and suppression of attention uncertainty. We design recalibration methods based on these signals and demonstrate improved accuracy, better sequence-level calibration, and more intuitive results from beam-search.",
            "score": 0.452069410925957,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "253097876",
            "title": "Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model",
            "text": "In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \u201cwhen to distill such knowledge.\u201d The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.",
            "score": 0.4511929252658158,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "272911430",
            "title": "Cross-Domain Content Generation with Domain-Specific Small Language Models",
            "text": "While previous work has addressed various aspects of small language models, multi-domain learning, and catastrophic forgetting, there is a gap in exploring the combination of these areas. Specifically, the application of knowledge expansion techniques to small language models for handling multiple distinct domains has not been extensively studied. \n\nOur work contributes to this area by: \n\n\u2022 Demonstrating that custom tokenizers enhance generation quality in small models trained on specific domains. \n\n\u2022 Showing the limitations of standard fine-tuning and parameter-efficient methods like LoRA in preventing catastrophic forgetting in small models. \n\n\u2022 Introducing a model expansion strategy that adds new layers to a frozen base model, enabling multi-domain generation without overwriting prior knowledge. \n\nBy addressing these challenges, we aim to advance the understanding of how small language models can be effectively adapted to handle multiple, distinct domains without incurring significant computational costs or sacrificing performance in any single domain.",
            "score": 0.4507244415290497,
            "section_title": "Our Position in the Literature",
            "char_start_offset": 8745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1071
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "We also observe that in the distillation settings with (+(LS + TS)), the increase in calibration, in most cases, coincides with the stagnation of student test accuracy, which in turn shows the inefficacy of such regularization techniques (especially for the in-domain setting). The results indicate that label smoothing with temperature scaling may not always be effective in calibrating the pre-trained language model's predictions as they do not show a consistent behavior on the calibration and performance. Consequently, we conclude that stronger regularization strategies are required to temper the miscalibration of the pre-trained language models. Third, as shown in Tables 1 and 2, no significant improvements in calibration or performance are observed by solely incorporating plain mixup (i.e., Mixup) on the standalone models (i.e., BERT or RoBERTa). Similarly, we observe that if a teacher model is trained using plain mixup, the student model distilled from it is impaired in calibration and its generalization capabilities in most cases. Such an aggravation of miscalibration may be due to the quality of the generated augmented samples in the mixup process that afflicts the models to capture the intricacies of the data. We hypothesize that this adversarial impact leads to a loss in the quality of the supervision signal during training or distillation. In contrast, incorporating (+(LS + TS)) on plain mixup leads to lower ECEs on some cases. Nevertheless, solely incorporating plain mixup without other regularization strategies (in our case (+(LS + TS))) is not effective in calibrating the model's predictions. \n\nFinally, it is worth noting that we obtain encouraging results with our proposed informed mixup. From the Table 2, we see that the errors obtained with our mixup method are much smaller in general compared to the other settings (Figure 4 in Appendix C). Interestingly, we observe that the student models distilled from a teacher trained using our mixup strategy yield the best-calibrated models on both the in-domain and out-of-domain data (see self-distillation + Ours ECE compared with other settings).",
            "score": 0.45010424879210154,
            "section_title": "Results",
            "char_start_offset": 23451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2137
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "One limitation of our results on language and vision datasets -as opposed to the tabular datasets -is that we do not optimize for the best choice of base model hyperparameters on each separate calibration split (e.g.changing learning rate or batch size between calibration fractions 0.2 and 0.4).In practice, we imagine that a practitioner would first fit the best possible ERM hyperparameters without using any holdout calibration data.Subsequently, they would then experiment with modifying the amount of hold out data to feed to multicalibration.We choose our hyperparameters for our base model to reflect this particular tuning strategy.\n\nAnother limitation of our results is that they are restricted to binary classification problems.While multicalibration algorithms do extend to multiclass problems, this extension comes at a severe cost of sample efficiency usually exponential in the number of labels (Zhao et al., 2021).We show that -at least for tabular datasets -current multicalibration algorithms do not significantly improve upon a competitive ERM.If we were to further burden the multicalibration algorithm with the larger sample complexity of an additional label, we do not expect that their performance will improve.Nonetheless, we plan to investigate the multiclass setting in future work, and believe that those findings will be consistent with the results present in this paper.\n\nWe believe that our work illuminates many avenues towards improving the viability of multicalibration algorithms in practice.For example, developing parameter free multicalibration methods (akin to what smECE accomplishes for calibration metrics) is an important direction with direct impacts on the practice of fair machine learning.\n\nSimilarly, post-processing techniques with better empirical sample complexity could significantly help the practice of multicalibration.",
            "score": 0.44974992182423656,
            "section_title": "Limitations and Conclusion",
            "char_start_offset": 32779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 641
                },
                {
                    "start": 643,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1399
                },
                {
                    "start": 1401,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1735
                },
                {
                    "start": 1737,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "235828718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36962890625
        },
        {
            "corpus_id": "266051956",
            "title": "A Study on the Calibration of In-context Learning",
            "text": "Fixing the prompt for each experiment and learning w corresponding to the fixed prompt. In other words, w is learned for calibration for every possible ICL prompt. \n\nIn Appendix Alg. 1, we introduce the recalibration algorithm employing temperature scaling. Additionally, we utilize the scaling-binning calibrator (Kumar et al., 2019), which fits a calibration function w \u2208 W to the recalibration dataset: arg min w (x i ,y i ) \u2113(w \u2022 P \u03b8 (x i ), y i ), where \u2113 is logloss. Subsequently, the input space is partitioned into bins, ensuring an equal number of inputs in each bin (defaulting to 10 bins). Within each bin, the average of the w values is computed and outputted for recalibration. Upon examination of Table 3 and Table 4, it is evident that none of the aforementioned strategies utilizing temperature scaling achieves satisfactory calibration performance. This finding contrasts with the well-established success of scaling confidence scores in the supervised learning setting, where it effectively reduces calibration errors (Guo et al., 2017). The fact that applying a postprocessing calibration method, such as temperature scaling, cannot directly resolve the miscalibration issue suggests that ICL might have different properties compared to predictions from classical supervised learning models. On the other hand, the scaling-binning method demonstrates superior performance in our experiments, which successfully reduces calibration errors below 0.1. The effect of fine-tuning. We show that vicuna, alpaca, and LLaMA2-Chat are all more accurate but less calibrated than their LLaMA counterpart backbones (Fig. 3), the margin is especially large for reasoning tasks and vicuna. Our finding indicates that fine-tuning might significantly degrade calibration, corroborating the evidence reported in GPT-4 (OpenAI, 2023), albeit it can greatly improve the reasoning accuracy. Our results provide evidence that though fine-tuning on carefully curated datasets can greatly improve question-answering performance, especially for hard tasks like reasoning problems, attention may need to be paid when assessing the calibration of those models' predictions.",
            "score": 0.4493132350933941,
            "section_title": "(Fix w)",
            "char_start_offset": 14089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 334,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1036,
                    "end": 1054,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "253510101",
            "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
            "text": "We focus on calibration in pre-trained large language models (LLMs), which has also been addressed in other lines of work, first by Mielke et al. (2022), who examine calibration in large pre-trained dialogue models. They focus on \"linguistic calibration,\" describing a guided generation method for introducing verbalized statements of uncertainty (e.g. \"I believe\", \"I'm not sure, but...\", \"I am certain. This divergence is also pointed out by Kadavath et al. (2022), who, in addition to several experiments on QA benchmarks like MMLU (Hendrycks et al., 2021), include experiments on HumanEval code generation examples (Chen et al., 2021) and their own dataset of Python code generation problems. Our experiments differ from theirs along a few axes. Firstly, although making claims about the calibration of \"language models\" broadly, Kadavath et al. only consider a single pre-trained model (of varying sizes) and a single program synthesis task; we consider several models across four datasets. Furthermore, while we consider both finetuned and few-shot models, Kadavath et al. mea-sure calibration only in a few-shot setting. While we obtain confidence estimates from the token probabilities, Kadavath et al. extract their estimates via an additional prompt that asks the model to label a predicted program or answer as \"True\" or \"False\", where the confidence is taken to be P (T rue). This is a natural formulation for few-shot models, but is less compatible with fine-tuned models, which are far more common in practice. Note that Kadavath et al.'s method incurs roughly twice the cost of program generation, as the generated program must be re-encoded to obtain a confidence estimate.",
            "score": 0.4483924175418692,
            "section_title": "Calibration in LLMs",
            "char_start_offset": 7348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1689
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 559,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "235078802",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "text": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "score": 0.4483924175418692,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "271218084",
            "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models",
            "text": "Calibration is essential for ensuring that large language models (LLMs) are honest and dependable.Studies by Kadavath et al. [12] and Plaut et al. [14] demonstrated that larger models tend to be better calibrated, particularly in multiple-choice contexts, and showed a strong correlation between softmax probabilities and the correctness of answers.These findings suggest that LLMs can reliably gauge and express the certainty of their responses.\n\nContrasting these results, our research reveals that the calibration of LLMs in multiple-choice scenarios is inherently fragile and can be easily manipulated to skew confidence levels.This highlights a significant vulnerability in using these models for assessments, indicating that reliance on simple calibration metrics might not adequately reflect an LLM's true capabilities.Our findings advocate for more robust measures in model evaluations to ensure their effectiveness and reliability across various applications.",
            "score": 0.4483924175418692,
            "section_title": "Calibration in LLMs",
            "char_start_offset": 22752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 446
                },
                {
                    "start": 448,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "264289144",
            "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
            "text": "We test all methods on Llama-2-Chat 70B. Given the validation set D c = {(x i , y i )} M i=1 , we perform a full permutation of these M samples to get M ! prompts in total. For each prompt, we could obtain M prediction pairs, i.e., (p, \u0177). For few-shot TS and KDE, we use all unique prediction pairs. For the proposed TS method, we use only the last prediction pair of each prompt, where the pre-trained LMs are best calibrated with ICL. As shown in Fig. 8, both TS and KDE can not calibrate the LM well for all tasks with few-shot examples. In some tasks (e.g., LogiQA, IMDB), their roles can be complementary, while in others (e.g., OpenbookQA), both are bad than out-of-the-box calibration. Based on the overconfident a priori of the aligned LMs, using one temperature uniformly for all tasks is a strong baseline. However, the optimal temperature under different tasks may vastly differ, making this strategy a sub-optimal solution. Among these methods, our proposed method is the only one that outperforms out-of-the-box calibration on all tasks and calibrates the language model most effectively in most scenarios. This suggests that learning the degree to which the predictive distribution of the aligned LM changes relative to the pre-trained LM by simply using one parameter for each task is an effective post-hoc calibration strategy. Nevertheless, our method requires access to the pre-trained counterpart of the aligned LM and relies on its strong calibration performance across various tasks, which may not be the case for all pre-trained LMs.",
            "score": 0.4483924175418692,
            "section_title": "EXPERIMENTAL RESULTS",
            "char_start_offset": 25027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1556
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "271811166",
            "title": "Cross-Domain Fake News Detection Using a Prompt-Based Approach",
            "text": "As a straightforward way to address domain shift here, we added samples from the test set domain to the training set and trained the model using few-shot learning. As shown in Table 9, the model performance improved significantly, and we suspect that increasing the few-shot samples would lead to even better results. The scores of the considered domains (i.e., domains that showed poor performance in the cross-domain experiments shown in Table 2) can be presented as a grayscale heat map. Future research could explore techniques to enhance domain adaptation for improved performance on more nuanced domains like sports and celebrity news. Additionally, investigating the specific transferable features learned by the model could provide valuable insights into the underlying mechanisms facilitating cross-domain generalization. Moving forward, our future work will aim to expand on verbalizers, building on findings that demonstrate how altering verbalizer wording can impact performance to varying degrees. We intend to explore dynamic approaches to optimize the utilization of verbalizers effectively. Additionally, we plan to address the challenge of transferring knowledge to new data through enhanced adaptation techniques and additional information integration. Our initial results indicate that incorporating domain-specific information into prompts significantly enhances contextual understanding, facilitating knowledge transfer across domains. Moreover, the aspects of stability and reliability are critical for the practical deployment of machine learning models and warrant thorough investigation. This study did not extensively evaluate these factors due to scope-and resource-related constraints. However, future work will aim to address this by conducting multiple trials with various data partitions and incorporating external datasets. Such an approach will provide a more robust assessment of the model's performance, ensuring consistency across different scenarios and data conditions. This will significantly contribute to the model's validation, offering a comprehensive understanding of its stability and reliability in real-world applications.",
            "score": 0.4478154410502597,
            "section_title": "Discussion",
            "char_start_offset": 39310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "271212858",
            "title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses",
            "text": "The emergent capabilities of LLMs, like in-context learning and instruction-following [39,40], position them as potential tools to replace humans for NLG evaluation in various tasks, such as text summarization [36,56], dialogue [57] and language generation [58,34].\n\nExisting approaches can be classified into two categories according to whether LLMs are fine-tuned.\n\n(1) Prompt-based Evaluation, which instructs powerful LLMs to judge given text in terms of carefullydesigned instructions, criteria and demonstrations, based on three primary protocols, namely, intuitive scoring-based evaluation [59], multiple-choice evaluation [38] and pairwise comparing one [60,58].\n\nTo further enhance LLMs' performance as evaluators, few-shot examples [61] and Chain-of-Thought (CoT) [62,63] are usually involved; balanced position calibration and multiple evidence calibration [60,58] are developed to address position bias where LLMs exhibit preferences for text exposed at a specific position regardless of quality; and multiple LLM evaluators are included through role-playing [64], agent-debate [65] and communication [66].ALLURE [67] and AUTOCALI-BRATE [38] are designed to better align with human judgement via iterative calibration on training examples.However, this paradigm highly relies on the LLM's own capabilities, robust to text variation but hard to be completely calibrated with uncommon value systems, as shown in Fig. 1 (a).\n\n(2) Finetuning-based Evaluation Several limitations remain for the previous paradigm, such as high API cost, sub-optimal performance on specific domains and concerns of reproducibility and transparency.Therefore, fine-tuning smaller language models serves as a practical alternative, which is widely-used in alignment research [47,68,69].AUTO-J [41] is fine-tuned with massive realworld scenarios and diverse evaluation protocols to improve generalizability and flexibility.Beyond labels, fine-grained feedback and explanations are also collected for enhancement [42,70,71,72].",
            "score": 0.44691730842020483,
            "section_title": "LLM as Automatic Evaluator",
            "char_start_offset": 6233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 267,
                    "end": 366
                },
                {
                    "start": 368,
                    "end": 670
                },
                {
                    "start": 672,
                    "end": 1118
                },
                {
                    "start": 1118,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1433
                },
                {
                    "start": 1435,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1773
                },
                {
                    "start": 1773,
                    "end": 1909
                },
                {
                    "start": 1909,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "258833685"
                },
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "266551654"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 778,
                    "end": 781,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "257767249"
                },
                {
                    "start": 1766,
                    "end": 1769,
                    "matchedPaperCorpusId": "258179434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "271924409",
            "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
            "text": "With the rapid development of large language models (LLMs), many powerful models have been deployed into our daily lives for practical usage to help us make decisions (Yao et al., 2023;Sha et al., 2023;Zhao et al., 2024). This makes it urgent for us to know to what extent we can trust the outputs of the models. Calibration is one of the most important indicators beyond accuracy, which provides a confidence measure to the model's predictions (Guo et al., 2017;Hsieh et al., 2023). In LLMs, confidence is exactly the probability for each generated token. Therefore, a well-calibrated model should align its prediction confidence with its ground-truth correctness likelihood as shown in Figure 1. As an example, recent hallucination detection methods rely on model prediction confidence as a significant indicator of potential hallucination (Zhang et al., 2023;Varshney et al., 2023). If the model is incapable of giving accurate confidence levels, people may fail to detect hallucinations due to the model's over-confidence, or people may falsely identify hallucinations due to the model's under-confidence. Mis-calibration brings significant challenges for the deployment of LLMs in real-world applications. \n\nCurrently, there are two methods to obtain a language model for practical usage. First, fine-tuning, which fine-tunes pre-trained LLMs on specific datasets by matching each token entry with a target ground truth token. Although fine-tuning can consistently improve performance on downstream tasks (Dodge et al., 2020;Sun et al., 2020;Ziegler et al., 2020), we identify that the model obtained in this way exhibits a nature of \"tuning-induced miscalibration\". Second, distillation-based methods transfer knowledge (e.g., soft labels) from larger LLMs to smaller models (Gu et al., 2023). Although distillation shows better calibration than fine-tuning as it matches each token entry with a probability distribution instead of a hard label, we find it is still biased because of the mis-calibration nature of teacher models.",
            "score": 0.4460611888328656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 185,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "258762841"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "261048772"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 463,
                    "end": 482,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 842,
                    "end": 862,
                    "matchedPaperCorpusId": "265351856"
                },
                {
                    "start": 1510,
                    "end": 1530,
                    "matchedPaperCorpusId": "211132951"
                },
                {
                    "start": 1530,
                    "end": 1547,
                    "matchedPaperCorpusId": "53296520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "273098762",
            "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
            "text": "Large Language Models (LLMs) have significantly revolutionized the natural language processing area, showcasing unparalleled abilities across various tasks (Achiam et al., 2023). Despite their versatility, LLMs exhibit limitations in specialized areas such as mathematics, programming, etc., which * Work done during an internship at Alibaba Cloud. \u2020 Tao Gong is the corresponding author. hinder the potential of wide-ranging applications. \n\nTo address these gaps, existing work (Liu et al., 2023;Wang et al., 2023) has sought to enhance the diverse skills of pre-trained LLMs through customized data strategies. However, they require extensive computational efforts and massive data volumes, challenging the widespread accessibility of LLM research. Furthermore, while Parameter-Efficient Fine-Tuning (PEFT) techniques offer a reduction in training requirements, their effectiveness tends to diminish (Biderman et al., 2024;Wu et al., 2024) compared to traditional fine-tuning methods, especially as the size of the model and the dataset grows. \n\nSubsequently, another line of research emerged, focusing on methods such as model merging (Akiba et al., 2024) and model expansion (Wu et al., 2024;Choi and Gazeley, 2024;Kim et al., 2023). Model merging methods strive to synthesize a multifaceted model that amalgamates insights from various pre-trained domain-specific LLMs, potentially crafting a model adept at addressing a multitude of tasks concurrently. However, the process of training multiple domain-specific LLMs is resource-intensive. On the other hand, model expansion methods, exemplified by Llama Pro, seek to refine pre-trained models for domain-specific applications in the post-pretraining phase by only fine-tuning the expanded layers. Therefore, it can employ significantly fewer trainable parameters than full model fine-tuning. \n\nHowever, present model expansion methods generally treat each part of LLMs equally, although different layers may exhibit varying sensitivity to incorporated knowledge. This lack of differentiation can result in less-than-ideal knowledge injection results. An intuitive idea is to inject knowledge into the most important layers so that the LLM can more sufficiently leverage the new knowledge without the overhead of redundant adjustments across all layers.",
            "score": 0.44568376927410885,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "258426367",
            "title": "Calibration Error Estimation Using Fuzzy Binning",
            "text": "Neural network-based decision-making systems have evolved rapidly in the recent decade. Within the domain of natural language processing, deep learning has shaped the current evolution in language modeling. These neural network-based language models are trained on large text corpora and can be fine-tuned across a wide range of NLP tasks and further improved using synthetic semantic enhancement schemes [1], yielding state-of-the-art performance [2][3][4][5]. Ideally, a neural model should output reliable and confident prediction probabilities. But recent works have shown that neural networks are unreliable and output highly overconfident predictions, resulting in over-estimation of the model's confidence in decisions [6][7][8]. This leads to model miscalibration, i.e. a lack of alignment between a model's decision probabilities and its actual likelihood of correctness. This lack of calibration can severely impact the trustworthiness of a model's decisions. \n\nA widely adopted measure of the degree of miscalibration is Expected Calibration Error (ECE) [9], used to measure neural network reliability [10][11][12]. The highly overconfident output prediction probabilities of neural networks result in a left-skewed probability distribution [13]. Since ECE utilizes a fixedwidth crisp binning scheme, this skew results in higher probability bins largely contributing to the calibration error estimation, while lower probability bins are ignored [13][14][15]. To overcome these limitations, prior works have proposed alternative binning strategies such as equal-frequency binning [14], adaptive binning [15], replacing binning with smoothed kernel density estimation [16], and more. Most calibration error estimation techniques rely on crisp binning, which discards edge probabilities (probabilities that typically lie on the bin edge) that could have contributed to a more accurate calibration error estimation. Although some works have utilized fuzzification of prediction probabilities for downstream NLP tasks [17], the calibration impacts of such fuzzification are yet to be studied.",
            "score": 0.44365426686103193,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "233347145"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "211555673"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "220546269"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "229212600"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "262648748"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "229212600"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "262648748"
                },
                {
                    "start": 1677,
                    "end": 1681,
                    "matchedPaperCorpusId": "212725167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "276580536",
            "title": "Compressing Language Models for Specialized Domains",
            "text": "Compression techniques such as pruning and quantization offer a solution for more efficient deployment of language models (LMs), albeit with small performance drops in benchmark performance. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this, yet requires computationally expensive full-parameter fine-tuning. To this end, we propose cross-calibration, a novel training-free approach for improving the domain performance of compressed LMs. Our approach effectively leverages Hessian-based sensitivity to identify weights that are influential for both in-domain and general performance. Through extensive experimentation, we demonstrate that cross-calibration substantially outperforms existing approaches on domain-specific tasks, without compromising general performance. Notably, these gains come without additional computational overhead, displaying remarkable potential towards extracting domain-specialized compressed models from general-purpose LMs.",
            "score": 0.44350063932220174,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "The rapid advancement of Natural Language Processing (NLP) has led to the widespread adoption of Large Language Models (LLMs) across diverse applications. While these models exhibit remarkable versatility [1,2], their effectiveness in specialized domains remains limited due to insufficient exposure to domain-specific knowledge during training. This issue becomes particularly pronounced in fields such as medicine, law, and engineering, where precise understanding and accurate generation of technical terminology are essential. Even minor misunderstandings in these domains can lead to significant misinterpretations, impacting decision-making and real-world applications. Therefore, developing methods to effectively incorporate domain-specific knowledge into LLMs is vital for enhancing their applicability and reliability in specialized contexts. \n\nOne promising approach to addressing this limitation is model merging, which integrates the strengths of multiple LLMs to enhance domain adaptation. Model merging presents a costeffective alternative to full-scale retraining or fine-tuning, allowing the integration of new knowledge without requiring large amounts of additional data or computational resources. However, the extent to which model merging facilitates domain-specific knowledge integration, particularly in multilingual settings, remains an open question. This limitation is particularly problematic for applications that require precise understanding and generation of technical language. An accurate interpretation of terms and concepts is essential in these fields, as even minor misunderstandings can lead to significant errors or miscommunications. \n\nThis study explores the potential of model merging for cross-lingual knowledge transfer, with a particular focus on integrating domain-specific technical vocabulary. The primary challenge lies in ensuring effective knowledge transfer without interference so that newly acquired domainspecific information enhances the model's proficiency while preserving its general linguistic capabilities. Another key issue is whether merging enables the model to retain and accurately utilize domain-specific terminology across different languages, maintaining both contextual meaning and usability in a multilingual setting. To investigate this, we conduct a comprehensive experiment, merging a general-purpose Japanese-specific model with an English medical domain-specific model and assessing various merging strategies. Through quantitative analysis, we evaluate the effectiveness of different approaches in transferring domain-specific terminology knowledge and improving the model's ability to understand technical language, particularly medical jargon.",
            "score": 0.44312337153502057,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2486
                },
                {
                    "start": 2487,
                    "end": 2722
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "258866001",
            "title": "Selectively Answering Ambiguous Questions",
            "text": "Pretrained language models are typically fine-tuned before being used in applications. However, finetuning and other alignment techniques may disturb the model's calibration by emphasizing high performance on specific sets of tasks rather than the full pretraining distribution. 3 Table 4 shows that for Flan-PaLM (Chung et al., 2022), instruction tuning dramatically improves exact match accuracy while simultaneously worsening performance on confidence-based ranking (ROC-AUC) and calibration (ECE), when using model likelihood as the confidence score. However, this miscalibration can be mitigated by using sample-based confidence scores, which dramatically improves the calibration on ambiguous questions (AmbigQA). For the selective-prediction metric C@80, the instructiontuned model with sampling is far ahead of any other method investigated. Note that we investigate Natural Questions and AmbigQA here as they are not in the instruction tuning training data. \n\nWe examine the effect of model scale on calibration, finding that in general, accuracy declines substantially in closed book question answering, but calibration stays roughly constant. See Appendix D for full results. \n\nAs sampling-based approaches require a linear increase in compute for the number of samples, we also examine how calibration scales with the number. In particular, we test three, five, and eight samples and compare that to the original results containing ten samples. Results can be found in Appendix E. Unsurprisingly, more samples seems to improve calibration, though it seems on the unambiguous Natural Questions slice, sampling diversity with a small number of samples works relatively well for the cost.",
            "score": 0.44304147013394923,
            "section_title": "Instuction Tuning and Scaling",
            "char_start_offset": 22910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1697
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "273098527",
            "title": "Dynamic Gradient Alignment for Online Data Mixing",
            "text": "We demonstrate the efficacy of distribution reweighting on the MMLU benchmark [Hendrycks et al., 2021]. MMLU consists of 57 tasks from various knowledge fields, which serves as a testbed of multi-domain language modeling; by measuring the downstream accuracy, we can assess whether the improvements obtained in language modeling transfer to reasoning abilities. \n\nWe construct two specific datasets with different amounts of accessible samples: (1) MMLU a: we take half of the examples from each task used as D spe . We denote the other half of datapoints as MMLU b, which is used for evaluation; (2) MMLU dev: we randomly select 5 samples from each task, simulating the few-shot learning scenario. MMLU a has 7.1k samples while MMLU dev only has 285 samples, which yields sparse importance sampling histograms. For evaluation, we assess the language modeling performance by computing perplexity on  MMLU b. We also measure the accuracy for multiple choice question answering on MMLU b with llm-eval [Gao et al., 2024]. \n\nWe use generic domain splits with k = 64, 4096, 262k domains. We rely on 22 auxiliary subdomains from The Pile [Gao et al., 2020] as our basis sets. For each auxiliary set, we take 15M tokens and compute their importance-sampling histograms as p 1 , . . . , p N \u2208 \u2206 k . To search for the optimal balance between diversity and specificity, we extend the basis sets with the importance sampling histogram from the specific set itself (i.e. MMLU a or MMLU dev), yielding N = 23 distributions. For this experiment, we use 750M models. \n\nDGA with distribution reweighting greatly improves language modeling. We report the loss on MMLU b and average validation loss across 22 domains in the Pile in Figure 2. Both importance sampling and DGA significantly outperform the uniform baseline. Compared to importance sampling, we observe that DGA with distribution reweighting leads to a better Pareto-front, indicating a better balance between specialized (MMLU) and general knowledge (The Pile).",
            "score": 0.44220790946701355,
            "section_title": "Distribution Reweighting: Scaling-up Data Mixing on Extremely Fine-grained Data Domains",
            "char_start_offset": 24063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "269004786",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "text": "In this section, we argue that the promise made by calibration in Definition 2.1 is too weak for the kinds of language model applications we have in mind because the performance of a model is extremely heterogeneous across different kinds of tasks that it can be used for.As an example, consider two prompt/completion pairs x 1 and x 2 , respectively asking and answering a question about (1) the capital cities of US states, and (2) citations to the academic literature for theorems in functional analysis.We would expect that the probability of correctness differs substantially across these examples -and yet calibration is a marginal guarantee that can average over both cases.It could, for example, lead to confidence assessments that are systematically over-confident about academic citations and systematically under-confident about state capitals.This is not in conflict with even perfect calibration.It would be better to promise that our confidence scores were calibrated conditionally on (as fine-grained information as possible about) the prompt used.These kinds of conditional calibration guarantees are what multicalibration aims for.\n\nWe now formalize the concept of groups.A group function g : X \u2192 {0, 1} can be thought of as an indicator function for a group defined as a set of prompt/completion pairs: g(x) = 1 if x is a member of the \"group\" and g(x) = 0 otherwise.The \"group\" induced by an indicator function g is therefore {x \u2208 X : g(x) = 1}.A set of groups G is correspondingly identified by a set of group indicator functions.Crucially, the groups in a collection G can be intersectingi.e.there can be multiple groups that contain the same example x.This corresponds to a prompt/completion pair having multiple non-mutually-exclusive attributes: for example, x might simultaneously be \"a question requiring high-school level knowledge\" and \"a question about mathematics\".",
            "score": 0.4404943534554469,
            "section_title": "Towards Multicalibration",
            "char_start_offset": 10984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1148
                },
                {
                    "start": 1150,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1613
                },
                {
                    "start": 1613,
                    "end": 1674
                },
                {
                    "start": 1674,
                    "end": 1895
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "To address these limitations and enhance the capabilities of LLMs for specific domains, existing methods can be broadly classified into three main approaches: in-context learning, training models from scratch on domain-specific and general data, and fine-tuning existing models using supervised datasets. In-context learning, where LLMs generate results based on a few demonstration examples [24], can be costly, slow in inference, and limited by the model's context window [2]. Moreover, these models can be sensitive to the quality and variability of the provided examples [21]. \n\nTraining models from scratch, while effective, demands significant computational resources and vast domain-specific and general dataset. Fine-tuning existing models is promising alternative but faces challenges such as the need for high-quality datasets and the risk of catastrophic forgetting, where the model's ability to perform general tasks degrades after domain-specific fine-tuning. Techniques like model merging can help mitigate these effects. \n\nPrevious studies have demonstrated that instruction fine-tuning smaller open-source language models can significantly enhance their performance on domain-specific tasks across various fields, such as law and medicine. In many cases, these models have outperformed the zero-shot performance of proprietary LLMs and other state-of-the-art models [19,23,33,61,65]. Within the finance domain, one study fine-tuned the Llama2 model for sentiment analysis, outperforming the FinBERT [62]. Another study fine-tuned Llama2-7B and Llama2-13B models on a variety of financial tasks, including sentiment analysis, relation extraction, question answering, and stock market prediction [54]. However, much of the existing research has focused primarily on fine-tuning models from the Llama family. To address this gap, we explore fine-tuning other powerful, smaller LLMs, specifically Mistral-7B, Phi-3, and Llama2-8B, across four representative financial text classification tasks: sentiment analysis, news headline classification, relation extraction, and hawkish-dovish classification. \n\nOne of the main challenges with fine-tuning LLMs for domain-specific tasks is the degradation of their zero-shot performance on unseen tasks [63].",
            "score": 0.4394658472886717,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "263835169"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "248367393"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "263829356"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "268042106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "270620336",
            "title": "Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox",
            "text": "In recent years, large language models (LLMs) have made groundbreaking advancements, demonstrating remarkable results and outstanding generalization ability across various tasks (56; 72; 1; 54).For example, given a few prompt examples or questions, LLMs can produce insightful answers within the unseen domain (47; 5).However, while LLMs exhibit remarkable capabilities, their substantial size makes real-world implementation cost-prohibitive.To address this challenge, model quantization has emerged as a prevailing technique for reducing the memory footprint of LLMs (14; 31; 10; 6; 66; 62; 51).Specifically, quantization reduces the model size by replacing highprecision floating-point numbers with lower-precision integers (e.g., from FP16 to INT4) (40; 17; 75).Currently, to avoid the substantial retraining costs of LLMs, the quantization methods for large models primarily employ post-training quantization (PTQ) (14; 31; 10; 6), which leverages calibration data to optimize the error caused by the quantization.Given the prevalent view that LLM capabilities stem from their extensive parameter count (24), a critical question emerges: As shown in Fig. 1, the process of model quantization encompasses three distinct stages: pretraining, quantization, and inference, utilizing pre-training data, calibration data, and test data, respectively.Existing quantization researches typically use a standard calibration set, which is usually a subset of the pre-training data (Scenario 1, S1), and evaluate on several fixed datasets (44; 7; 53; 50; 71).However, because using task-specific data for model calibration is a more reasonable choice in practical applications, the relationship between the distribution of calibration data and test data and its impact on the generalization ability of quantized models is a more worthy research topic that has not been deeply explored (Scenario 2, S2).In this work, to answer the abovementioned question and bridge the gap between academic research and practical implementation, we provide a platform to evaluate the generalization ability of quantized LLMs, covering benchmarks, analyses, and a modular-designed toolbox.",
            "score": 0.4367065352409316,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "270371795",
            "title": "When is Multicalibration Post-Processing Necessary?",
            "text": "Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion -- originating in algorithmic fairness -- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration post-processing can help inherently uncalibrated models and large vision and language models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts. We also release a python package implementing multicalibration algorithms, available via `pip install multicalibration'.",
            "score": 0.4363842082705053,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "249191391",
            "title": "Teaching Models to Express Their Uncertainty in Words",
            "text": "Calibration in new domains. Prior work on calibration focuses primarily on the classification setting, where models output a probability distribution over the set of possible classes (Guo et al., 2017;Mukhoti et al., 2020;Minderer et al., 2021), corresponding to what we call the \"answer logit\". To generalize calibration to a new target domain, methods often require samples from the target or from additional source domains (Gong et al., 2021;Csurka, 2017;Wang et al., 2021). We study how calibration generalizes when a pre-trained model is finetuned on a single source domain and must generalize zero-shot to a new domain. \n\nPre-trained language models. Hendrycks et al. (2020) analyze GPT-3's behavior on a benchmark of tasks that vary in both subject matter and difficulty, showing that GPT-3's calibration (for the answer logit) generalizes fairly poorly in both the zero-shot and few-shot settings. To improve the calibration of pre-trained language models, Desai & Durrett (2020) use label smoothing to reduce overconfidence on out-ofdomain data. Kong et al. (2020) introduce on-and off-manifold regularization to handle in-distribution and out-of-distribution calibration, respectively, but focus on OOD detection rather than generalization. Other work focuses on the closely related problem of teaching models to abstain from answering when a model has high uncertainty about its answer. Kamath et al. (2020) train an auxiliary \"calibrator\" to predict whether the primary model correctly answers any given question using a mix of in-domain and out-of-domain data. \n\nIn cases where the calibrator predicts an error, the model can refuse to answer. Additional studies explore the use of manually crafted prompts that instruct models to defer or qualify their answers when uncertain (Askell et al., 2021b;Lin et al., 2021). These methods typically correct for models being overconfident on out-of-domain examples.",
            "score": 0.4353487993929209,
            "section_title": "Related work",
            "char_start_offset": 23758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 222,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 222,
                    "end": 244,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 1398,
                    "end": 1418,
                    "matchedPaperCorpusId": "219721462"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "271270595",
            "title": "Robust Calibration of Large Vision-Language Adapters",
            "text": "I) Task 1: Few-shot domain generalization.Table 1 introduces the average fewshot generalization (OOD) results using black-box Adapters, whereas Table 2 presents the same for PL approaches.We refer the reader to Appendix for the detailed results per dataset.First, results consistently show a miscalibration phenomenon when CLIP models are adapted, regardless of the CLIP backbone  used, or the transferability approach.Few-shot Adapters calibration: We find that miscalibration is especially occurring in few-shot black-box Adapters.For example, Clip-Ad or TaskRes in Tab. 1 (a) show ECE increments of +8.3 and +4.0 respectively.This is further magnified when using the popular TIP-Adapter method.Few-shot PL calibration: PL approaches are relatively more robust in this setting (e.g.+3.8 CoOp in Tab. 2 (a)).On the impact of logit range regularization: Results show the potential of logit range scaling among its different proposed variants, improving calibration for all Prompt Learning approaches, and most of the used Adapters.Impact of different strategies to adjust logit range:.The only strategy that does not allow for consistent performance gains is ZS-Norm, which deteriorates performance in some Adapters (see Clip-Ad in Table 1).We believe that the re-parameterization in Eq 4 might not properly prevent logit range de-adjustment before normalization, and thus overfit to the few support samples.In contrast, Penalty constraint directly regularizes such values, showing consistent ECE decreases for both Adapters (e.g.\u221222.0 for TIP-Ad(f) using ViTs, or \u22124.3 for CLIP-Ad using RN50) and PL (e.g.\u22122.9 for CoOp using RN50, or \u22120.94 for CoCoOp using ViTs).Interestingly, as a side effect, we also observed accuracy improvements for domain generalization for several methods.Nevertheless, the best calibration performance is provided by a simple, yet effective post-processing standardization, SaLS.",
            "score": 0.4350667069274914,
            "section_title": "Results",
            "char_start_offset": 24649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 42,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "261243763",
            "title": "Spoken Language Intelligence of Large Language Models for Language Learning",
            "text": "Deploying LLMs To apply LLMs to real-life scenarios, additional safeguards should be implemented. Language models may amplify social biases present in their training data, may generate incorrect information [35]. In reality, similar issues have been observed in various language evaluation scenarios, such as native speaker judgments of non-native speakers' speech, which are notoriously biased. This phenomenon is known as reverse linguistic stereotyping, whereby a person's speaking performance is evaluated based on the stereotypes associated with their social identity [36]. Therefore, deploying LLMs in sensitive areas such as education must be approached with great care [37,38]. While LLMs have been tested on large benchmarks, such as MMLU [11] and BIG-bench [39], further studies are necessary to apply them to the domain of spoken language learning. \n\nContributions This paper investigates the performances, interpretability, and limitations of prompting methods in spoken language question answering. We have collected a composite dataset that includes a set of concepts mainly designed to test the large models' knowledge of spoken language and application questions toward industrial production. We utilized the GPT [8], LLaMA [40,41], FLAN-T5 [42], UL2 [43] and Pythia [44] series for our research, which is conducted in two rounds. We comprehensively review their performance on a large scale and consider two prompting strategies: direct and CoT prompting, under both zero-shot and few-shot learning paradigms. In the second round, we meticulously analyze representative models with advanced optimized methods. The main contributions of this paper are: \n\n\u2022 We have introduced a dataset about spoken language intelligence that serves as a substantial benchmark for speech and language learning scenarios, providing valuable supplementation to existing large benchmarks. \u2022 We conduct a study on various prompt variations (such as zero-shot, few-shot, direct/CoT, domain-specific sampler, and external tools) and analyze their performance in multiplechoice question form. \u2022 We demonstrate that in-domain example sampling techniques can consistently improve performance on domain-specific data. \u2022 We conducted an expert evaluation of a small set of multi-turn conversation generated by GPT-3.5 3 .",
            "score": 0.4341164258306088,
            "section_title": "body",
            "char_start_offset": 6405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "145731270"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "254876189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368896484375
        },
        {
            "corpus_id": "271923983",
            "title": "Interactive DualChecker for Mitigating Hallucinations in Distilling Large Language Models",
            "text": "The advent of Large Language Models (LLMs) has revolutionized artificial intelligence, providing comprehensive end-to-end solutions for numerous machine learning (ML) tasks (Chang et al. 2024;Huang et al. 2024). Traditional ML approaches predominantly rely on supervised learning, which necessitates large annotated datasets to achieve high performance. In contrast, LLMs, trained on trillions of tokens and possessing hundreds of billions of parameters, can function as extensive knowledge bases and excel in various tasks through in-context learning, without requiring additional training (Kojima et al. 2022;Brown et al. 2020). \n\nHowever, LLMs are prone to hallucination issues, manifesting as either factual inaccuracies or inconsistencies in responses, known respectively as factuality and faithfulness hallucinations (Huang et al. 2023). Figure 1 shows the distribution of hallucination types observed in a preliminary experiment involving 200 labeled samples generated by GPT-3.5 Turbo2 using zero-shot prompting from our green innovation dataset. The results indicate a predominant issue of factuality hallucinations at 94.9%, compared to faithfulness hallucinations at only 5.1% in domain adaptation. \n\nExisting methods use external knowledge to supplement missing information for factuality hallucinations (Yao et al. 2023;Ram et al. 2023) and focus on mitigating language model overconfidence to improve faithfulness (Chen et al. 2022;Schuster et al. 2022;Zhao et al. 2023). However, these solutions face significant challenges: a) constructing external knowledge bases is expensive, and input length limits make it difficult to determine how much external knowledge to feed into the model; b) domain adaptation is challenging due to the variability and complexity of human labeling standards; c) few studies address both factuality and faithfulness hallucinations; and d) most methods require costly additional pre-training or fine-tuning to achieve high accuracy. \n\nTo address these limitations, we introduce DualChecker, a novel interactive framework that mitigates hallucinations and improves the performance of both teacher and student models in knowledge distillation.",
            "score": 0.4327754141209009,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1316,
                    "end": 1333,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1333,
                    "end": 1348,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "252715398"
                },
                {
                    "start": 1446,
                    "end": 1467,
                    "matchedPaperCorpusId": "250526382"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.481689453125
        },
        {
            "corpus_id": "268857146",
            "title": "A Review of Multi-Modal Large Language and Vision Models",
            "text": "Prompts are natural language texts which instruct a LLM or a MM-LLM on the task it should perform.This involves learning a language model that calculates the probability of a given text and uses this probability to predict an output [87].Given such prompts, LLMs are able to perform \"in-context learning\" meaning they have the potential for adaption to a wide variety of tasks without needing new training data [88].This mitigates one of the key problems with fine-tuning a LLM, the need for a large set of training data for each new task [39].\n\nA distinction can be made between few-shot, one-shot, and zero-shot prompt engineering.Few-shot refers to when multiple examples are given to a model but no weight adjustments are allowed i.e no adjustment to the parameter weights.One-shot as the name suggests is similar to few-shot except that only one example is provided.Zero-shot prompting is different again in that only the task instruction is given and there is no example provided.\n\nArguments have been made that in such cases, LLMs and MM-LLMs are not learning tasks from few-shot examples but instead performing \"task location in the model's existing space of learned tasks\" [89].Furthermore, zero-shot performance can equal and even surpass the performance of few-shot prompting.Reynolds et al. in [89] use the translation task to show that a small number of samples is insufficient to learn anything about the translation task.They investigate whether any examples are even necessary, to show that the fewshot method is really directing the model to pre-learned information.Sun et al. argue that while zero-shot performs well, there is a gap when using the model on domain-specific tasks [90].To address this, they designed domainspecific prompts from the knowledge base of the LLM and fine-tuned this using a ranking constraint.",
            "score": 0.4314898021234445,
            "section_title": "Prompt Engineering",
            "char_start_offset": 54278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 544
                },
                {
                    "start": 546,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 986
                },
                {
                    "start": 988,
                    "end": 1187
                },
                {
                    "start": 1187,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1838
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "251253368"
                },
                {
                    "start": 539,
                    "end": 543,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1182,
                    "end": 1186,
                    "matchedPaperCorpusId": "231925131"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "231925131"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "261775688"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5625
        },
        {
            "corpus_id": "274859755",
            "title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models",
            "text": "Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: https://github.com/hfutml/Calibration-MLLM.",
            "score": 0.43133980766797964,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "Calibration is a key tool for knowing when language model predictions can be trusted and when they should abstain or defer to experts.However, calibration on an individual domain can be much worse than the aggregate data distribution (Yu et al., 2022;Hebert-Johnson et al., 2018).In this paper, we show that large language models suffer from the same calibration failure.While LMs appear to be well-calibrated on average, they are significantly miscalibrated in finer-grained settings.\n\nWe study LM calibration for multiclass classification: let x be an input query drawn from some query distribution p(x) and y \u2208 {1, \u2022 \u2022 \u2022 , K} be the output class.Let p LM (y | x) denote the model probability, which is also the model's confidence.Let \u0177 = arg max y p LM (y | x) be the model's prediction, and y * be the ground truth label.",
            "score": 0.4310919816026011,
            "section_title": "The Illusion of LM Calibration",
            "char_start_offset": 4586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 485
                },
                {
                    "start": 487,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 825
                }
            ],
            "ref_mentions": [
                {
                    "start": 251,
                    "end": 279,
                    "matchedPaperCorpusId": "51880858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "272987064",
            "title": "Calibrating Language Models with Adaptive Temperature Scaling",
            "text": "In this section, we aim to evaluate our proposed method on multiple benchmarks to demonstrate its effectiveness in improving calibration of LLMs fine-tuned with RLHF. We compare our method to no calibration as well as existing temperature scaling methods. Additionally, we ablate the main components of our method including the loss function, loss weighting, and head architecture. \n\nEvaluation Setting. We evaluate using two 7B parameter post-RLHF models LLama-2-Chat-7b (Touvron et al., 2023) and Qwen-Chat-7b. As the calibration dataset, we use the Alpaca GPT-4 (Peng et al., 2023) instruction tuning dataset, which contains a diverse set of instructions with high quality answers. We then evaluate model calibration on three downstream tasks. \n\nWe perform multiple choice evaluation on the MMLU (Hendrycks et al., 2020) by aggregating statistics across the entire dataset. Specifically we concatenate the confidences and correctness labels from all subjects, then calculate the calibration metrics. We also evaluate on two free response datasets, TriviaQA (Joshi et al., 2017) and TruthfulQA (Lin et al., 2021). \n\nMetrics. In multiple choice inference, we have a set of tokens ids O which represent the valid options for a multiple choice answer, so the confidence scores are p = \u03c3 SM (q lx,j\u2208O ) where \u03c3 SM denotes the softmax function. To calculate confidences over a long sequence of response tokens for an input x, we sample a generation \u0177 of length l \u0177 from the original language model then concatenate to the instruction to form \u1e91 and q following calibration. Then, we calculate an average over transition probabilities on the response tokens. We use the Expected Calibration Error (ECE) (Guo et al., 2017) and Brier score (Brier, 1950) to evaluate calibration. We also report accuracy but each method does not significantly affect accuracy. \n\nBaselines.",
            "score": 0.4310919816026011,
            "section_title": "Experiments",
            "char_start_offset": 8940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 1698,
                    "end": 1716,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1733,
                    "end": 1746,
                    "matchedPaperCorpusId": "122906757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "259859030",
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "text": "We quantify the influence of available training samples in the calibration task (see Figure 2). We observe overall consistent trends in three different tasks across eight datasets. The results show the continued benefits of increasing the dataset size for the calibration task considering the AUROC and \u2206Conf scores. Surprisingly, the performance in OOD datasets can be improved by introducing more calibration training samples in ID datasets. This is different from the common belief that learning more in-domain knowledge may hurt the OOD robustness due to the reliance on spurious correlations (Radford et al., 2021). However, we note that there is an unnatural trend in the natural language inference task when ANLI is adopted as the  Table 2. An exact balanced distribution of the two classes will mostly benefit the calibration task.\n\nOOD evaluation dataset. The reason may be partially attributed to the unique construction process of ANLI based on human-in-the-loop attacks.",
            "score": 0.43108127833953847,
            "section_title": "Number of Training Samples",
            "char_start_offset": 7940,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 597,
                    "end": 619,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "261243763",
            "title": "Spoken Language Intelligence of Large Language Models for Language Learning",
            "text": "In Figure 6, we present the results of direct few-shot and CoT few-shot using different numbers of examples. Based on the results, we can see that: Increasing the number of examples can improve performance to a limited extent; Increasing the examples of the reasoning chain will have a more significant and stable effect on models above 70B (LLaMA2-chat, GPT-4). However, for smaller models, these prompts may already exceed their capabilities, resulting in degradation of performance. \n\nIn-Domain Prompt v.s. Out Of Domain Prompt We used prompts from different domains for two models capable of responding to CoT. In most cases, most examples are not carefully selected or designed. We used domain-specific prompts for different types of questions. In Figure 7, our approach has shown significant advantages compared to more common examples. In the process of increasing examples, the model not only learned how to answer multiple-choice questions, but also gained some insights. \n\nSelf-Consistency Although the solution to these phonological problems does not have as many reasoning paths as mathematical reasoning questions, we found that self-consistency can improve performance on the GPT-3.5 model (as shown in Table 3). However, for LLaMA2-70b-chat, its Figure 6: Number of correctly answered questions (total 301) on SLIQ-LL (Application Questions subset) using k \" 1...9 shots. The develop examples used in the previous experiments were manually written, while the current ones are sampled from within the dataset. The thought chain is generated by GPT-4 with correctly generated answers (the thought chain is not guaranteed to be correct). As no specific conclusions can be drawn, we are not reporting the performance of few-shot direct with k \u0105 9. Augmented Language Models On the internet, individuals actively share their language learning experiences. By effectively using this external knowledge, LLMs can improve credibility and mitigate hallucination issues. We provided two tools for use by GPT-3.5, but did not get better results (Table 4). One piece of good news is that models using the tools can recognize their limitations and refuse to answer questions they are uncertain about, although this ability still seems relatively limited (Table 5).",
            "score": 0.4302853962613806,
            "section_title": "Few-shots & CoT",
            "char_start_offset": 19974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342529296875
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Massively Multilingual Language Models (MMLMs) like mBERT (Devlin et al., 2019), XLMR (Conneau et al., 2020), mT5 (Xue et al., 2021) and mBART (Liu et al., 2020) have been surprisingly effective at zero-shot cross-lingual transfer i.e. when fine-tuned on an NLP task in one language, they often tend to generalize reasonably well in languages unseen during fine-tuning. \n\nThese models have been evaluated for their performance across a range of multilingual tasks (Pan et al., 2017;Nivre et al., 2018;Conneau et al., 2018) and numerous methods like adapters (Pfeiffer et al., 2020), sparse fine-tuning (Ansell et al., 2022) and few-shot learning (Lauscher et al., 2020) have been proposed to further improve performance of crosslingual transfer. Despite these developments, there has been little to no attention paid to the calibration of these models across languages i.e. how reliable the confidence predictions of these models are. As these models find their way more and more into the real word applications with safety implications, like Hate Speech Detection (Davidson et al., 2017;Deshpande et al., 2022) it becomes important to only take extreme actions for high confidence predictions by the model (Sarkar and KhudaBukhsh, 2021). Hence, calibrated confidences are desirable to have when deploying such systems in practice. Guo et al. (2017) showed that modern neural networks used for Image Recognition (He et al., 2016) though perform much better than the ones introduced decades ago (Lecun et al., 1998), but are significantly worse calibrated and often over-estimate their confidence on incorrect predictions. For NLP tasks specifically, Desai and Durrett (2020) showed that classifiers trained using pre-trained transformer-based models (Devlin et al., 2019) are well calibrated both in-domain and out-of-domain settings compared to non-pre-trained model baselines (Chen et al., 2017).",
            "score": 0.42927287226584343,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 79,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 86,
                    "end": 108,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 464,
                    "end": 482,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 558,
                    "end": 581,
                    "matchedPaperCorpusId": "218470133"
                },
                {
                    "start": 602,
                    "end": 623,
                    "matchedPaperCorpusId": "238856900"
                },
                {
                    "start": 646,
                    "end": 669,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1065,
                    "end": 1088,
                    "matchedPaperCorpusId": "1733167"
                },
                {
                    "start": 1207,
                    "end": 1237,
                    "matchedPaperCorpusId": "227118923"
                },
                {
                    "start": 1332,
                    "end": 1349,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1412,
                    "end": 1429,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1494,
                    "end": 1514,
                    "matchedPaperCorpusId": "14542261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "257279774",
            "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
            "text": "In our experiments, we test out several values for Y , specifically, 1, 5, and 10. This programmatic creation of few-shot demonstrations for language models is inspired by the Demonstrate stage of the DSP framework (Khattab et al., 2022). The gold passage for each synthetic query is the passage it was generated from. We have the option of letting Z be large because using Flan-T5 XXL is considerably less expensive than using GPT-3 as we did in Stage 1. In our experiments, we test 1K, 10K, 100K, and 1M as values for Z. We primarily focus on Z = 10K and 100K in Section 4.3. \n\nWe use multiple corpus-adapted prompts to mitigate edge cases in which we create a low-quality prompt based on the chosen synthetic queries and in-domain passages from the target domain T . (See Stage 4 below for a description of how low-quality prompts can optionally be detected and removed.) \n\nAs a quality filter for selecting synthetic queries, we test whether a synthetic query can return its gold passage within the top 20 retrieved results using a zero-shot ColBERTv2 retriever. We only use synthetic queries that pass this filter, which has been shown to improve domain adaptation in neural IR (Dai et al., 2022;Jeronymo et al., 2023). 2021), who found a teacher ensemble effective for knowledge distillation into retrievers. At this stage, we can simply use all Y of these rerankers for the distillation process. As an alternative, we can select the N best rerankers based on their accuracy on the validation set of the target domain. For our main experiments, we use all of these rerankers. This is the most general case, in which we do not assume that a validation set exists for the target domain. (In Appendix A, we evaluate settings where a subset of them is used.) \n\nStage 5: The domain-specific passage rerankers from Stage 4 serve as multi-teachers creating a single ColBERTv2 retriever in a multi-teacher distillation process.",
            "score": 0.4290746562642492,
            "section_title": "Methodology",
            "char_start_offset": 11309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 237,
                    "matchedPaperCorpusId": "255186555"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "247613322",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "text": "In this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration. Our empirical evaluations consist of three distinct NLP classification tasks and two different domain settings. Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage. Compared to existing work, we also observe the following novel phenomena that are unique to PLMbased pipelines:\n\n\u2022 The relative calibration quality of PLMs is consistent in general across tasks and domains, with an exception of XLNet, which is the least robust to domain shift. \u2022 Larger PLMs are better calibrated under the indomain setting in Commonsense Reasoning, unlike in the other NLP tasks. \u2022 Uncertainty quantifiers are generally more effective in improving calibration performance under the out-of-domain setting. \u2022 Ensemble is less effective in reducing calibration error when used with PLM-based pipelines, despite their convincing performance with traditional models.",
            "score": 0.4280091758583544,
            "section_title": "Conclusion",
            "char_start_offset": 20217,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "250451161",
            "title": "Language Models (Mostly) Know What They Know",
            "text": "We would eventually like to train AI systems that are honest, which requires that these systems accurately and faithfully evaluate their level of confidence in their own knowledge and reasoning. So AI systems must be able to recognize what they do and do not know, as a prerequisite. In this work, we study the extent to which Language Models (LMs) possess this ability and how it can be elicited and imparted. \n\nAs a starting point, we examine calibration: do the probabilistic predictions from language models match up with frequencies of occurrence? Language models can produce well-calibrated predictions for token probabilities on-distribution [Guo et al., 2017]. We show that large language models are also well-calibrated on a diverse array of multiple choice questions, as long as the questions are formatted appropriately. In particular, calibration improves with model size and few-shot prompting. \n\nGood calibration opens up the possibility for using models to evaluate the accuracy of their own outputs (\"self-evaluation\"). For example, given any open-ended query, we can sample an answer from the model and then have the model evaluate P(True), the probability that its answer is correct. We may expect selfevaluation to be challenging, because the model may be overconfident that its own samples1 are correct. Our self-evaluation procedure nevertheless distinguishes correct and incorrect samples, as summarized in Figure 1. Furthermore, as model size and capabilities increase, models improve at self-evaluation, which suggests that verification improves faster than generation quality in this context. \n\nWe also show that self-evaluation can be improved if we provide a model with many of its own samples, before asking it to evaluate any single sample. That is, 'brainstorming other possibilities' helps large models to evaluate the validity of a given answer option. \n\nThese techniques address a question about the world, as they ask models to evaluate \"according to accepted truth in the wider world (i.e. according to humans), is a particular answer to a question correct?\" In the case of self-evaluation, the proposed answer was provided by the model, but its validity is nevertheless an external fact. \n\nBut we are also interested in having language models attempt to directly evaluate their own state of knowledge.",
            "score": 0.4277737155759074,
            "section_title": "Introduction",
            "char_start_offset": 653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2223
                },
                {
                    "start": 2226,
                    "end": 2337
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "266435613",
            "title": "Large Language Models are Miscalibrated In-Context Learners",
            "text": "Machine learning and NLP have undergone a significant transformation recently, largely propelled by language models (LMs) (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Ope-nAI, 2023). Among different learning paradigms, Supervised Fine-Tuning (SFT) and In-Context Learning (ICL) have emerged as predominant methodologies (Raffel et al., 2020;Dong et al., 2022), demonstrating commendable efficacy across many tasks. SFT tunes the model's parameter and effectively specializes a (general-purpose) model to specific tasks by learning the knowledge in the training data and optimizing the objective. ICL, for each input, leverages the few-shot examples (i.e., the so-called demonstrations) to generate predictions without tuning model parameters and treating the model as a 'black box'. Considering the different input format between training with SFT and inference with ICL, Min et al. (2022) and Chen et al. (2022) introduce the in-context examples into training phrase, which we call supervised in-context learning (SICL). However, when the demonstrations, as a strong inductive bias, get combined with SFT, it has been shown that LMs become more likely to fall into the problem of overconfidence (Desai and Durrett, 2020;Jiang et al., 2021); the predicted confidence distribution of ICL may be miscalibrated due to the bias in in-context examples (Fei et al., 2023). Through our extensive experiments, we observe that both paradigms, SFT and ICL, suffer from the problem of miscalibration in low-resource scenarios. \n\nThe important challenges of overconfidence and miscalibration, particularly in scenarios marked by limited data availability, underscore the need for a nuanced understanding of these paradigms. These challenges could be more severe in instructiontuned models considering their strong instructionfollowing abilities.",
            "score": 0.4275042432024373,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 163,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 163,
                    "end": 186,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 339,
                    "end": 360,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 891,
                    "end": 908,
                    "matchedPaperCorpusId": "240288835"
                },
                {
                    "start": 913,
                    "end": 931,
                    "matchedPaperCorpusId": "239009828"
                },
                {
                    "start": 1215,
                    "end": 1240,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1240,
                    "end": 1259,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "258967265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "253384402",
            "title": "Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates",
            "text": "As accurate estimates are required for many difficult or sensitive prediction tasks (Platt, 1999), probability calibration is an important uncertainty estimation task for NLP. Unlike other uncertainty estimation task (e.g., out-of-domain detection, selective inference), calibration focuses on aleatoric uncertainty measured by the probability of the prediction and adjusts the overall model confidence level (Hendrycks and Gimpel, 2017;Pereyra et al., 2017;Guo et al., 2017;Qin et al., 2021). For example, Gal and Ghahramani (2016) propose to adopt multiple predictions with different dropout masks and then combine them to get the confidence estimate. Recently, several works focus on the calibration of PLMs models for NLP tasks (Hendrycks et al., 2019;Desai and Durrett, 2020;Jung et al., 2020;He et al., 2021;Park and Caragea, 2022;Bose et al., 2022). Dan and Roth (2021) investigate the calibration properties of different transformer architectures and sizes of BERT. In line with recent work (Ye and Durrett, 2022), our work focuses on how explanations can help calibration in three NLP tasks. However, we do not need to learn a calibrator by using model interpretations with heuristics, and also do not compare due to its intensive computation cost when generating attributions. In contrast, we explore whether model explanations are useful for calibrating black-box models during training.",
            "score": 0.4268795056642635,
            "section_title": "Related Work",
            "char_start_offset": 12379,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 97,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 409,
                    "end": 437,
                    "matchedPaperCorpusId": "13046179"
                },
                {
                    "start": 437,
                    "end": 458,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 458,
                    "end": 475,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 475,
                    "end": 492,
                    "matchedPaperCorpusId": "245010935"
                },
                {
                    "start": 507,
                    "end": 532,
                    "matchedPaperCorpusId": "160705"
                },
                {
                    "start": 732,
                    "end": 756,
                    "matchedPaperCorpusId": "59336190"
                },
                {
                    "start": 756,
                    "end": 780,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 798,
                    "end": 814,
                    "matchedPaperCorpusId": "231632895"
                },
                {
                    "start": 814,
                    "end": 837,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 857,
                    "end": 876,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 999,
                    "end": 1021,
                    "matchedPaperCorpusId": "238856959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "While some of the calibration methods can be straightforwardly applied to LLMs, we outline important aspects and challenges to consider for better calibration. Calibration and scale of model. Recent studies have empirically shown that calibration in LLMs is influenced by model size [17], [84], [205]. Additionally, research has observed that as model size increases, generation uncertainty also tends to rise [206]. These findings highlight potential challenges in integrating standard calibration methods into large models, necessitating a deeper understanding of their implications. How does the relationship between model scale and calibration affect overall performance? Fine-tuning-based calibration. The fine-tuning process also requires calibration, as a loss function objective [99], [160] or fine-tuning strategy, to provide better model inference. Bayes regularization can be used for fine-tuning calibration by applying it to the last layers with a deterministic attention layer or with a belief network [207], [208]. Do we need fine-tuning to calibrate large models? Can we only rely on an inference-only approach? Should we fundamentally update the large model, or is it fine to focus only on the last layer? Calibration in multi-lingual and multi-modal large language models. Multi-lingual language models have domain shift and scale problems, and handling these occasions with dictionaries and the similarity of multi-lingual embeddings gained attention [120], [183]. Large vision-language models (LVLMs) show user input-related bias and uncertainty, and emerging approaches use BC or calibrate post-hoc [33], [178], [209]. Meanwhile, fine-tuning is an important stage for generating reasonable inferences of LLMs, as training from scratch might take a long time. PEFT methods, such as LoRA [197], might create quality degradation for large models. While understanding the trending and important methods for fine-tuning large foundational models (LFMs) is crucial, the fine-tuning techniques of LFMs will be highly related to the contribution one can make via one's local data.",
            "score": 0.4268795056642635,
            "section_title": "E. Applicability in LLMs",
            "char_start_offset": 37762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 300,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 410,
                    "end": 415,
                    "matchedPaperCorpusId": "267095393"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 1016,
                    "end": 1021,
                    "matchedPaperCorpusId": "224814357"
                },
                {
                    "start": 1023,
                    "end": 1028,
                    "matchedPaperCorpusId": "235377150"
                },
                {
                    "start": 1633,
                    "end": 1638,
                    "matchedPaperCorpusId": "263881686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78125
        },
        {
            "corpus_id": "273812005",
            "title": "Shortcut Learning in In-Context Learning: A Survey",
            "text": "If the prediction distribution shifts, then this shift is attributed to the corresponding context. Subsequently, during actual predictions, LLMs' outputs are adjusted based on the biased distribution to mitigate biases in the context. \n\nAs research progresses gradually, Contextual Calibration has spawned various calibration methods tailored for different shortcut mitigation scenarios. Prototypical Calibration employs a Gaussian mixture distribution to estimate prototypical clusters for all categories and then utilizes the likelihood of these prototype clusters to calibrate LLMs' predictions (Han et al., 2022). Domain-context Calibration employs randomly selected in-domain words from the task corpus to estimate the label bias of the language model and utilizes this estimation for bias mitigation (Fei et al., 2023). Similarly, PMI DC re-weighs scores based on the likelihood of hypotheses (answers) given premises (questions) in a specific task domain (Holtzman et al., 2021). \n\nBut these calibration schemes may fail due to their inability to effectively estimate contextual bias with only content-free and in-domain random tokens. To address this, Batch Calibration updates the bias as more batches of input data are processed, allowing the bias to stabilize after multiple mini-batches are considered (Zhou et al., 2024a). Generative Calibration adjusts the margin of the labels estimated via Monte-Carlo sampling over the in-context model to simply calibrate the contextual prediction distribution (Jiang et al., 2023). In-Context Calibration calibrates the test distribution of the demonstration using the prior semantics expected from the samples (Jang et al., 2024). PriDe is applied to the task of multiple choice questions by arranging the option content on a small number of test samples to estimate prior information, and then using the estimated prior information to debias the remaining samples (Zheng et al., 2024). Unlike the aforementioned probability-based calibration methods, NOISYICL calibrates language models by adding Gaussian noise to the parameters of LLMs, thereby reducing the prediction bias and unreliable confidence in ICL (Zhao et al., 2024b).",
            "score": 0.4268795056642635,
            "section_title": "Model-centric Approach",
            "char_start_offset": 26952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 824,
                    "matchedPaperCorpusId": "258865519"
                },
                {
                    "start": 962,
                    "end": 985,
                    "matchedPaperCorpusId": "233296182"
                },
                {
                    "start": 1314,
                    "end": 1334,
                    "matchedPaperCorpusId": "263310485"
                },
                {
                    "start": 1663,
                    "end": 1682,
                    "matchedPaperCorpusId": "268385528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "278327247",
            "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
            "text": "One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "266348815",
            "title": "Paloma: A Benchmark for Evaluating Language Model Fit",
            "text": "PERPLEXITY ANALYSIS FOR LANGUAGE MODEL ASSESSMENT (PALOMA) is for examining LM fit to domains. We use perplexity (and related metrics; \u00a72.4) to measure fit to the distributions of language represented by different domains. We take relative differences in LM fit as a proxy of model familiarity to the shared knowledge, values, and social context that position the humans producing language in a domain. While we expect contemporary LMs to have a limited fit to the most complex of these latent factors of textual domains, improving fit to all factors is important both to improve perplexity and for actual use of the LM. For example, better perplexity on a particular dialect of English suggests that model will make a better chatbot for people that speak that dialect. \n\nPALOMA comprises several types of artifacts for enabling a science of language modeling: training and evaluation guidelines for experiments on LM fit ( \u00a72.1), evaluation data for assessing fit to specific domains ( \u00a72.2), 6 pretrained baselines following training guidelines ( \u00a72.3), metrics computed by our standardized inference code conforming to our evaluation guidelines ( \u00a72.4), and a submission process for coordinating comparable results across the research community ( \u00a72.5).",
            "score": 0.42684333084302817,
            "section_title": "PALOMA",
            "char_start_offset": 5448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 1256
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "267365203",
            "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
            "text": "Previous works on knowing what language models know have focused on calibration, prompting, or training. Calibration-based approaches attempt to extract confidence scores from models to gauge their uncertainty (Sun et al., 2022;Kuhn et al., 2022;Zhou et al., 2023a;Liu et al., 2023a). Previous works have evaluated (Radford et al., 2019;Liang et al., 2023;Tao et al., 2023;He et al., 2023) and improved the calibration of language models (Desai and Durrett, 2020;Kong et al., 2020;Jagannatha and Yu, 2020;Kamath et al., 2020;Jiang et al., 2021;Mielke et al., 2022;Lin et al., 2022;Tian et al., 2023;Zhou et al., 2023b), while the calibration quality could vary for different downstream tasks (Desai and Durrett, 2020;Wang et al., 2020;Stengel-Eskin and Van Durme, 2023;Kalai and Vempala, 2023). In the setting of AbstainQA where LLMs decide whether to abstain or not given a question, calibration-based approaches would rely on a held-out set to set a threshold over calibrated confidence scores: such a threshold and reliance on a held-out set could jeopardize the generalization of calibration-based approaches across knowledge domains and reasoning contexts. \n\nPrompting-based approaches leverage the instruction-following abilities of LLMs and employ instructions to induce self-reflection and gauge whether the generated answer should be trusted. Kadavath et al. (2022) investigates whether a \"noneof-the-above\" option or self-evaluation prompting would induce good estimations of LLMs' internal factuality. Huang et al. (2023a) follows existing works (Kim et al., 2024;Shinn et al., 2023) to evaluate whether LLMs could self-correct their own reasoning with a three-step prompting strategy.",
            "score": 0.4254652684292325,
            "section_title": "Related Work",
            "char_start_offset": 22908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 228,
                    "matchedPaperCorpusId": "254183614"
                },
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 315,
                    "end": 337,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 356,
                    "end": 373,
                    "matchedPaperCorpusId": "261076328"
                },
                {
                    "start": 438,
                    "end": 463,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 481,
                    "end": 505,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 505,
                    "end": 525,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 525,
                    "end": 544,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 544,
                    "end": 564,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 581,
                    "end": 599,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "265150666"
                },
                {
                    "start": 692,
                    "end": 717,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 717,
                    "end": 735,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 735,
                    "end": 769,
                    "matchedPaperCorpusId": "253510101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428466796875
        },
        {
            "corpus_id": "268032879",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "text": "Prompting Large Language Models. Recent research (Brown et al., 2020;Kojima et al., 2023) on large language models shows that in-context learning (ICL) achieves great effectiveness in using models as few-shot or zero-shot reasoners. Different styles of prompting such as Knowledge prompting (Liu et al., 2022), Chain of Thought (CoT) prompting (Wei et al., 2022b), Self-Consistency prompting (Wang et al., 2023), Self-ask (Press et al., 2023), Tree-of-Thought prompting (Yao et al., 2023), andSkill-in-Context (Chen et al., 2023) are then proposed to guide the model to elicit its knowledge for reasoning in different ways. \n\nMost previous work mainly focuses on how such a prompting method influences the model performance on various tasks. In this paper, we compare how confidence calibration is influenced by different prompting methods. \n\nConfidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai and Durrett, 2020;Si et al., 2023). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed. \n\nRecently, Yang et al. (2023) discuss the honesty problem of models as part of the alignment. Qian et al. ( 2023) study the confidence change when there is a conflict between in-context and model internal knowledge. Another line of work links model confidence with human confidence (Zhou et al., 2023;Steyvers et al., 2024;Zhou et al., 2024).",
            "score": 0.42498094627861005,
            "section_title": "Related Work",
            "char_start_offset": 23915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 69,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 309,
                    "matchedPaperCorpusId": "239016123"
                },
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1038,
                    "end": 1063,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1063,
                    "end": 1079,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "249191391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "272881329",
            "title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical Impacts, Constraints, and Emerging Questions",
            "text": "Technical advancements are the primary focus of LLM research, and several key works [1, 11, 31, 34, 36-38, 41, 46, 55, 60], have concentrated on enhancing the performance of large language models across a diverse range of applications. \n\nA primary objective in LLM research is the generalizability of these models to diverse tasks and domains. For example, Radford et al. [41] demonstrate the capacity of LLMs to perform well across various domains and implicitly on downstream tasks without any parameter or architecture modification. Building on the effectiveness of this implicit learning, Sanh et al. [31] create a model that can better generalize to heldout tasks and perform robustly with diverse prompt wording, using explicit supervised multitask training. Brown et al. [1] examine the generalizability of LLMs on new tasks with limited task-specific data in the few-shot setting. Gruver et al. [60] examine LLMs zero-shot abilities in the task of time series forecasting. Howard and Ruder [37] present Universal Language Model Fine-tuning (ULMFiT) -a transfer learning methodto pretrain a language model on a large general-domain corpus, applicable to different NLP tasks without task-specific modifications. The GLUE and SuperGLUE benchmarks [11,27] further this pursuit of generalizability with evaluation and diagnostic datasets spanning diverse tasks and domains. \n\nWith the introduction of the BERT architecture, Devlin et al. [45] presented a significant development in language representation models that can be effectively fine-tuned without substantial task-specific architecture modifications. Expanding upon BERT's capabilities, Yang et al. [52] address its limitations with autoregressive language modeling in XLNet. \n\nMany articles target data and size efficiency in LLMs. For instance, CONNEAU and Lample [26] significantly advance effectiveness in tasks pertaining to low-resource languages with limited available data. Zhao et al. [35] improve data efficiency by resolving the problem of stability in few-shot learning. Roberts et al. [53] leverage LLMs' ability to implicitly store and retrieve knowledge to determine its utility in answering questions without additional training.",
            "score": 0.4249470009675318,
            "section_title": "Improving LLM Performance",
            "char_start_offset": 10624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 778,
                    "end": 781,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "263908782"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1661,
                    "end": 1665,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1956,
                    "end": 1960,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "270062675",
            "title": "Multicalibration for Modeling Censored Survival Data with Universal Adaptability",
            "text": "The essence of multicalibration boosting lies in its ability to transfer weak learners to stronger learners by auditing the residuals, enabling enhancement in predictive accuracy across diverse subpopulations, which in turn improves the accuracy of statistical estimates within the target domain. How multicalibration leads to precise predictions under covariate shift needs to be elucidated in a formal mathematical language. Our main results relate the prediction error (7) obtained using multicalibration to that of ipsw. Specifically, we reveal that under proper conditions, a multicalibrated function m (k) (\u2022; t) leads to the property of universal adaptability. Recall the collection of propensity score odds H defined in (8) and the specification error d(h, w) defined in (6), with them in place, we are ready to state the theorem below that establishes universal adaptability from multicalibration. Theorem 6. Under the same conditions outlined in Theorem 3, suppose m (k) (\u2022; t) : X \u2192 [0, C] is (H, \u03b1 \u2032 )-multicalibrated over the source distribution D S , then for any target distribution D T , and any w \u2208 \u03a3, m (k) (X; t) satisfies \n\nTheorem 6 also has implications beyond universal adaptability. It demonstrates that multicalibration in the source domain not only ensures universal adaptability but retains some degree of multicalibration over the target domain as well. Its statement can be simply understood as providing an adaptive guarantee under different degrees of shift: 1. If m (k) is well calibrated on the source domain, it will only lose a small amount in the multicalibration guarantee on targets close by. A simple instance is that H(\u03a3) = {Id(x)} in a scenario without any distribution shift. 2. Under extreme shifts, shifts that require the full power of the class to account for H(\u03a3), the universal adaptability and calibration guarantee weakens unless \u03a3 is well specified and A is capable of learning H(\u03a3) or H(\u03a3)\u2297C agnostically.",
            "score": 0.4231884057549757,
            "section_title": "Universal adaptability",
            "char_start_offset": 26870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "218487046",
            "title": "On the Inference Calibration of Neural Machine Translation",
            "text": "The study of calibration on classification tasks has a long history, from statistical machine learning (Platt et al., 1999;Niculescu-Mizil and Caruana, 2005) to deep learning (Guo et al., 2017). However, calibration on structured generation tasks such as neural machine translation (NMT) has not been well studied. Recently, M\u00fcller et al. (2019) and Kumar and Sarawagi (2019) studied the calibration of NMT in the training setting, and found that NMT trained with label smoothing (Szegedy et al., 2016) is well-calibrated. We believe that this setting would cover up a central problem of NMT, the exposure bias (Ranzato et al., 2015) -the training-inference discrepancy caused by teacher forcing in the training of auto-regressive models. \n\nIn response to this problem, this work focuses on the calibration of NMT in inference, which can better reflect the generative capacity of NMT models. To this end, we use translation error rate (TER) (Snover et al., 2006) to automatically annotate the correctness of generated tokens, which makes it feasible to evaluate calibration in infer-arXiv:2005.00963v1 [cs.CL] 3 May 2020 ence. Experimental results on several datasets across language pairs show that even trained with label smoothing, NMT models still suffer from miscalibration errors in inference. Figure 1 shows an example. While modern neural networks on classification tasks have been found to be miscalibrated in the direction of over-estimation (i.e., confidence > accuracy) (Guo et al., 2017), NMT models are also under-estimated (i.e., confidence < accuracy) on low-confidence predictions. In addition, we found that miscalibrated predictions correlate well with the translation errors in inference. Specifically, the over-estimated predictions correlate more with over-translation and mis-translation errors, while the under-estimated predictions correlate more with under-translation errors. This demonstrates the necessity of studying inference calibration for NMT.",
            "score": 0.42299303024991075,
            "section_title": "EnDe Dev",
            "char_start_offset": 954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 123,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 123,
                    "end": 157,
                    "matchedPaperCorpusId": "207158152"
                },
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 350,
                    "end": 375,
                    "matchedPaperCorpusId": "67855916"
                },
                {
                    "start": 480,
                    "end": 502,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 941,
                    "end": 962,
                    "matchedPaperCorpusId": "263887736"
                },
                {
                    "start": 1482,
                    "end": 1500,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "267035272",
            "title": "Learning Shortcuts: On the Misleading Promise of NLU in Language Models",
            "text": "This mismatch between model confidence and actual accuracy leads to the phenomenon known as miscalibration, impacting the reliability of models in real-world applications. Ideally, a well-calibrated model assigns high probabilities to correct predictions and low probabilities to incorrect decisions, aligning predicted probabilities with observed event frequencies. \n\nThe rising deployment of neural network architectures in high-risk real-world scenarios has prompted extensive research into their calibration [15,19,31]. Unfortunately, evaluations of neural network reliability indicate that their confidence predictions are often poorly calibrated and overly confident [12,25]. \n\nFine-tuning pre-trained language models exacerbates miscalibration [2,7,16,18]. This is attributed to the excessive parameterization of the models, leading to overfitting on the training data. The attention garnered by pre-trained language models stems from their inclination to exhibit increasing confidence during training, regardless of prediction accuracy [4]. However, these models showcase calibration deterioration in out-of-domain scenarios [7]. Notably, smaller models demonstrate improved calibration on in-domain data, while larger models exhibit better calibration on out-of-domain data [6]. These findings underscore the current inadequacies of pre-trained language models in terms of confidence calibration and reliability in decision-making.",
            "score": 0.4227106823263234,
            "section_title": "Examining the Impacts",
            "char_start_offset": 6734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "215745290"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 677,
                    "end": 680,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "258426367"
                },
                {
                    "start": 754,
                    "end": 756,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 756,
                    "end": 759,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "253244504"
                },
                {
                    "start": 1133,
                    "end": 1136,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "244119588"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69970703125
        },
        {
            "corpus_id": "270199368",
            "title": "Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs",
            "text": "We set out to examine the impact of the calibration set on the performance of various Large Language Models.Specifically, we address three primary questions: first, how the quality of the calibration set affects the quantized performance of the models; second, whether a content-specific calibration set can enhance performance on a particular task; and third, how the same content presented in different languages affects the quantized models when used as a calibration set.\n\nWe evaluate six distinct LLMs: OPT 6.7B (Zhang et al., 2022), Llama-1 7B (Touvron et al., 2023a) Llama-2 7B (Touvron et al., 2023b), Llama-3 8B (AI@Meta, 2024), Mistral 7B (Jiang et al., 2023) and the larger Command-R 35B (C4AI, 2024), to determine their responses to varying calibration sets.\n\nWe test three different one-shot quantization methods: two weight-only quantization methods, GPTQ W4A16 with a group size of 128 (Frantar et al., 2022) and AWQ W4A16 with a group size of 128 (Lin et al., 2023); and SmoothQuant W8A8, a weight-and-activation quantization method (Xiao et al., 2023).Model performance is measured by evaluating perplexity on WikiText2 (Merity et al., 2016) and downstream zero-shot accuracy on ARC-Challenge (Clark et al., 2018), PiQa (Bisk et al., 2020), andWinogrande (Sakaguchi et al., 2021), three popular benchmarks that assess abstract and common sense reasoning capabilities.Additionally, we test a zero-shot naive W8A8 weight-and-activation quantization method.",
            "score": 0.4227106823263234,
            "section_title": "Experimental setup",
            "char_start_offset": 7822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 475
                },
                {
                    "start": 477,
                    "end": 770
                },
                {
                    "start": 772,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 1049,
                    "end": 1068,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1237,
                    "end": 1261,
                    "matchedPaperCorpusId": "208290939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "268385471",
            "title": "Thermometer: Towards Universal Calibration for Large Language Models",
            "text": "We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.",
            "score": 0.4227106823263234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "272703963",
            "title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty",
            "text": "We compute the accuracy (Acc) of a prediction by language model evaluation (LME) whereby a prompted language model compares the prediction with the ground truth answer (see Prompt 4). To measure calibration, we plot calibration charts between confidence scores and accuracy (Figure 4) where the x-axis is binned according to the probability ranges in the linguistic expressions map (Fagen-Ulmschneider, 2019). Further, to summarize the calibration error into a single scalar, we track,  The lower these scores the better. \n\nOnce the models are finetuned to generate linguistic expressions of uncertainty, we test the calibration of their predictions on held-out test sets for each dataset. For this, we first extract the uncertainty expression from the rest of the prediction using a prompted language model (see Prompt 3). We then convert these uncertainty expressions into probability estimates using the same mapping employed for converting probabilities into linguistic expressions in the previous section. We measure calibration using Expected Calibration Error (ECE) and Brier Score and plot calibration charts based on these probability estimates. The complete evaluation procedure is outlined in Figure 3 and Algorithm 2.  Confidence Bin) and y-axis is probability of that prediction being actually correct (shown here as Accuracy). Expected Calibration Error (ECE) and Brier Score are reported at the top of each plot. \n\nThe error bars show the variance of accuracy in each bin.",
            "score": 0.4227106823263234,
            "section_title": "Evaluation",
            "char_start_offset": 12783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1487
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8896484375
        },
        {
            "corpus_id": "268555952",
            "title": "Beyond Performance: Quantifying and Mitigating Label Bias in LLMs",
            "text": "We examine both performance and bias along axes such as scale and number of in-context demonstrations.We also evaluate the impact of label bias mitigation methods, such as calibration and fewshot LoRA fine-tuning (Hu et al., 2022).\n\nOur investigation reveals substantial label bias in the predictions of LLMs across all evaluated settings, indicating that raw LLM output scores often represent simple, heuristic solutions.While increasing model size, providing in-context demonstrations, and instruction-tuning all contribute to reducing bias, ample bias persists, even after applying mitigation methods.Surprisingly, these results also hold for tasks where the labels are all semantically equivalent (e.g., in multi-choice question answering).Further, although the examined calibration methods can reduce bias and improve performance, we also find cases where they negatively impact both bias and overall performance.\n\nMotivated by these findings, we propose a novel calibration method for few-shot prompting that more accurately estimates a model's label bias, using only its predictions on the in-context demonstrations.Compared to existing LLM bias calibration methods, our method improves performance while also removing considerably more bias.\n\nOur findings highlight the necessity of considering and measuring biases in the predictions of LLMs when evaluating their performance.Moreover, adjusting models to their tasks through more accurate and effective estimation of biases holds promise for improving the reliability of LLMs and their applications.",
            "score": 0.42235243140982137,
            "section_title": "Introduction",
            "char_start_offset": 1890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 102,
                    "end": 231
                },
                {
                    "start": 233,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 744
                },
                {
                    "start": 744,
                    "end": 918
                },
                {
                    "start": 920,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1249
                },
                {
                    "start": 1251,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1559
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 230,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "267897581",
            "title": "Fine-tuning Large Language Models for Domain-specific Machine Translation",
            "text": "Large language models (LLMs) have shown great potential in domain-specific machine translation (MT). However, one major issue is that LLMs pre-trained on general domain corpus might not generalize well to specific domains due to the lack of domain-specific knowledge. To address this issue, this paper focuses on enhancing the domain-specific MT capability of LLMs, by providing high-quality training datasets and proposing a novel fine-tuning framework denoted by DragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced prompting integrates dictionary information into prompts to improve the translation of domain-specific terminology.; (ii) RAG-based few-shot example selection provides high-quality examples that simulate both the domain and style characteristics; (iii) Fine-tuning with few-shot examples further enhances performance when using in-domain examples. We deploy DragFT on three well-known LLM backbones with 13B training parameters to validate its effectiveness. The results on three domain-specific datasets show that DragFT achieves a significant performance boost and shows superior performance compared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance improvement of DragFT over existing LLMs can be attributed to incorporating relevant knowledge while mitigating noise.",
            "score": 0.42221915016456035,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "278327247",
            "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
            "text": "Large Language Models (LLMs) (OpenAI, 2023;Anthropic, 2024) have emerged as powerful tools for a wide range of natural language processing tasks (Bubeck et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023;Team et al., 2023). These models, built upon the Transformer architecture (Vaswani et al., 2017), have demonstrated remarkable abilities to process and generate human-like text, making them increasingly integral to various applications. A crucial development in making LLMs more reliable and aligned with human values is preference alignment techniques, particularly Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023). However, an important side effect of preference alignment is its impact on model calibration-the relationship between a model's predicted probabilities and its actual accuracy. While pretrained LLMs typically demonstrate good calibration properties, preference-aligned models become poorly calibrated, as initially observed in GPT-4 with RLHF (OpenAI, 2023). Our investigation reveals that this is a universal issue across different models aligned with various alignment methods. An example is shown in Figure 1 (left), where the calibration performance of a model aligned by DPO illustrates a poor calibration performance. \n\nUnderstanding and addressing this calibration issue is crucial. First, well-calibrated prediction is essential for reliable decision-making in real-world applications, particularly in high-stakes domains such as legal or healthcare analysis (Savage et al., 2025). Second, overconfident models may mislead users about their capabilities and limitations, potentially leading to inappropriate reliance on model outputs. \n\nIn this paper, we conduct a systematic investigation into two fundamental questions: (1) Why does preference alignment affect calibration? and (2) How can we effectively restore calibration while maintaining the benefits of alignment?",
            "score": 0.422110321241095,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 59,
                    "matchedPaperCorpusId": "268232499"
                },
                {
                    "start": 284,
                    "end": 306,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1582,
                    "end": 1603,
                    "matchedPaperCorpusId": "273322711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "We compare our few-shot recalibrator against the following baselines which output precision curves.SAMPLE AVERAGE is the precision curve of the combined distribution over all the domains based on the queries that appear in the training data.This baseline is not distribution-specific: it uses a single curve for all test set distributions.\n\nDOMAIN AVERAGE involves averaging the precision curves for each domain.Similar to sample averaging, this approach is not distributionspecific.\n\nEMPIRICAL uses the precision curve obtained from only the k few-shot labeled queries.Note that this baseline has an unfair advantage over other approaches, including ours, because it assumes access to the labels of the k few-shot queries.\n\nORACLE is the ground-truth precision curve of the corresponding slice's distribution, and serves as a skyline for the best achievable performance for curve prediction approaches.\n\nIn the reducing calibration error setting, we compare our approach to the canonical recalibration method of temperature scaling (Guo et al., 2017).Temperature scaling (TS) uses a held out calibration set to select a temperature, and then applies that temperature to the test data.We compare against two variants of temperature scaling, and they differ in the choice of the calibration set.TS (FEW-SHOT) uses the k few-shot examples with ground-truth labels as the calibration set.We run grid search on values for the temperature in {0.1, 0.2, \u2022 \u2022 \u2022 , 1.9, 2.0, 3.0, 4.0, 5.0} to find one that minimizes ECE for the k examples.TS (ALL DOMAINS) uses the training data, combining all domains, as the calibration set.Similarly, we run grid search on values for the temperature to minimize ECE for the entire training set. 5 Main Results",
            "score": 0.4220775062026268,
            "section_title": "Baselines",
            "char_start_offset": 17820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 339
                },
                {
                    "start": 341,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 483
                },
                {
                    "start": 485,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 723
                },
                {
                    "start": 725,
                    "end": 903
                },
                {
                    "start": 905,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1294
                },
                {
                    "start": 1294,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1618
                },
                {
                    "start": 1618,
                    "end": 1737
                }
            ],
            "ref_mentions": [
                {
                    "start": 1033,
                    "end": 1051,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "278165590",
            "title": "Bi-directional Model Cascading with Proxy Confidence",
            "text": "A model's calibration is its ability to express some level of confidence associated with an output that accurately reflects the likelihood that it is correct [13]. LLM calibration generally has great value to researchers and industry applications, and is the subject of significant research efforts [12]. The acceptance of LLMs in real-world application depends on their trustworthiness and reliability [4], while in cascading context, calibration has direct and measurable benefits on performance. The success of any model cascade depends on the quality of its deferral process, and therefore the calibration of its component models or accuracy of its confidence metric . \n\nThere are many approaches to approximating LLM confidence. A large subset of these methods involves deriving the confidence from the log-probabilities of the model outputs. In the case of LLMs, these log-probabilities, or \"logits\", reflect a distribution over tokens and the confidence can be estimated using some variation of entropy or maximum probability at the individual token or sequence level. Some techniques that have shown recent promise involve examining consistency across multiple stochastic or permuted generations [18], but the additional computation required makes these approaches poorly suited to efficiency optimizations. Other researchers have asked generative models to explicitly report their confidence in their output with mixed results [22,32], but benchmarks in some domains have found self reported confidence to be very poor, especially among smaller models [29]. \n\nDifferent layers in a language model capture different types of information and fulfill different roles in inference [41]. \n\nOne method of identifying and promoting fact-based and truthful outputs from LLMs is to consider the likelihood of different tokens at different layers within the model, emphasizing the effect of layers most likely to inject knowledge [8]. The internal layers of a pretrained model inform output rather than generate output directly and may hold useful information without the same biases as the output layer [19]. Other, similar techniques applying an output head over internal layers have been used in early-layer exit strategies [40], improve interpretability [52], and to better understand how the internal layers and mechanisms function in LLMs [15,42].",
            "score": 0.4213430320846565,
            "section_title": "LLM Calibration",
            "char_start_offset": 8103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2352
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "265157516"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "226965491"
                },
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "276349944"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 1929,
                    "end": 1932,
                    "matchedPaperCorpusId": "261582463"
                },
                {
                    "start": 2103,
                    "end": 2107,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70947265625
        },
        {
            "corpus_id": "273821704",
            "title": "Graph-based Confidence Calibration for Large Language Models",
            "text": "However, manually designed features are limited in their ability to capture the full extent of self-consistency among LLM responses, leading to poor calibration performance. \n\nTo better calibrate the confidence estimation, some methods directly use correctness labels in their calibration procedures. Mielke et al. (2022) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks. However, these studies have not sufficiently exploited response consistency to predict the probabilities of the responses being correct.",
            "score": 0.42054496085903686,
            "section_title": "Related Work",
            "char_start_offset": 7867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 321,
                    "matchedPaperCorpusId": "250073258"
                },
                {
                    "start": 593,
                    "end": 615,
                    "matchedPaperCorpusId": "257557820"
                },
                {
                    "start": 625,
                    "end": 644,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "239024385",
            "title": "Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters",
            "text": "er et al., 2020) trained independently, enabling zero-shot compositions of adapters. Our ultimate goal is, for ease of deployment and storage, a single model that can handle all languages and domains. In this work we analyse how to combine language adapters with domain adapters in multilingual NMT, and study to what extent the domain knowledge can be transferred across languages.\n\nFirst, we show it is hard to decouple language knowledge from domain knowledge when finetuning multilingual MT systems on new domains. In Section 5.2 we demonstrate that adapters learnt on a subset of language pairs fail to generate into languages not in that subset. Such generation into the wrong language is referred to as 'off-target' translation. We additionally find combinations of domain and language adapters not seen at training time lead to bad performance. We examine how adapter placement and other techniques can improve the compositionality of language and domain adapters when dealing with source or target languages that do not have in-domain data (which we refer to throughout this work as \"out-of-domain languages\"). Our key contributions are:\n\n\u2022 We examine domain adaptation capacity in the multi-lingual, multi-domain setting. We find that encoder-only adapters can be just as effective as default adapters added in every layer, and that composing domain adapters with language adapters outperforms language adapters alone, although fine-tuning with domain tags performs better for most domains.\n\n\u2022 We improve the cross-lingual transfer of domain knowledge for adapters. We analyse different language and domain adapter combinations that improve performance and reduce off-target translations. Our best results for translation into out-of-domain languages use decoder-only domain adapters, regularisation with domain adapter dropout, and data augmentation with English-centric backtranslation.",
            "score": 0.4204455647785198,
            "section_title": "Introduction",
            "char_start_offset": 1901,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "249642460",
            "title": "Task Transfer and Domain Adaptation for Zero-Shot Question Answering",
            "text": "We evaluate the performance of our T+DAPT approach with domain-specific NER, achieving positive results in a zero-shot reading comprehension setting in four different domain-specific QA datasets. These results indicate that our T+DAPT approach robustly improves performance of pretraining language models in zero-shot domain QA across several domains, showing that T+DAPT is a promising approach to domain adaptation in lowresource settings for pretrained language models, particularly when directly training on target task data is difficult. \n\nIn future work, we intend to explore various methods to improve the performance of T+DAPT by remedying catastrophic forgetting and maximizing knowledge transfer. For this we hope to emulate the regularization used by Xu et al. (2020) and implement multi-task learning and continual learning methods like AdapterNet (Hazan et al., 2018). In order to improve the transferability of learned features, we will explore different auxiliary tasks such as NLI and sentiment analysis in addition to few-shot learning approaches.",
            "score": 0.41960351413020025,
            "section_title": "Conclusion",
            "char_start_offset": 10166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "250451161",
            "title": "Language Models (Mostly) Know What They Know",
            "text": "The long-term motivation underlying this work is to begin to understand aspects of honesty in AI models. We use honesty as an umbrella term including several overlapping7 ideas: \n\n\u2022 Truthfulness: Does the AI provide factually accurate information, including finding, using, and evaluating source materials correctly? \n\n\u2022 Calibration: Do the AI's probabilistic predictions correspond with frequencies of occurrence? \n\n\u2022 Self-knowledge: Do AI systems know what they know and make accurate predictions about their own behavior and reasoning? \n\n\u2022 Explainability: Do AI systems reveal their 'thinking' completely and faithfully? \n\n\u2022 Non-deceptiveness: Can we ensure AI systems do not learn to lie (e.g. when human preference data might encourage systematic mistakes or provide more reward for pleasant misconceptions)? \n\nHere we have focused on aspects of calibration, self-knowledge, and truthfulness, but we are only scratching the surface of the larger subject. Our core findings are that large language models can be well-calibrated on diverse multiple choice questions, that they perform well at self-evaluation on a range of subjects, and that they can be trained to predict what they do and don't know and exhibit some generalization to new domains and in-context information sources. \n\nWe found that calibration tends to improve with model size/capability and with additional few-shot examples. \n\nSelf-evaluation also improves with model size, which is non-trivial since we expect the quality of model samples to also be improving as we scale up. This means that in effect, we are observing that verification has an advantage over generation. Within the scope of pretrained language models, we find these results encouraging, and expect they can be usefully leveraged to bootstrap more extensive forms of self or intermodel supervision, and to distill calibration back into open-ended natural language responses. \n\nAnother motivation for this work was to use self-knowledge as a test-bed for generalization. We are particularly interested in how honesty generalizes from easy to hard questions and across domains, because we are worried about how AI systems may behave out-of-distribution, or when they have some knowledge or insight that humans lack. We found that language models do exhibit a degree of generalization across domains, though calibration suffers out-of-distribution.",
            "score": 0.41920196275547916,
            "section_title": "Discussion",
            "char_start_offset": 43307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "253097876",
            "title": "Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model",
            "text": "We give an answer to the question from the perspective of model calibration. Model calibration refers to how well a predicted probability of a model reflects the true accuracy. Therefore, a well-calibrated predictive score represents the likelihood of correctness of a prediction (Guo et al., 2017). In this light, such score can be viewed as a gauge to detect a miscalibration of a student in training; when a student makes a prediction with a probability mass that is higher than the expected accuracy of the prediction (overconfidence), a student model is trained with only supervision from a teacher. In the case of underconfidence, a student is trained with only supervision from ground-truth. \n\nSwitching supervision is supported by two widely accepted ideas: 1) the close link between miscalibration and overfitting, and 2) the regularization effect of KD. Guo et al. (2017) empirically find that a model overfits to negative log likelihood (NLL) training, leading to miscalibration, and Mukhoti et al. (2020) further support the claim. Therefore, we utilize the regularization effect held in KD training (Yuan et al., 2020). Aside from the inter-class relations held in knowledge, recent findings suggest that KD is a form of adaptive regularization (Tang et al., 2021;Yuan et al., 2020), where a teacher enforces a student to distribute probability mass on output space more evenly. \n\nTaking all these factors into account, we present a simple, yet novel KD method, called Hard gate Knowledge Distillation (HKD). Given a calibrated teacher model, the teacher gates supervisions between knowledge and observation for each instance/time step, selecting which objective the student should be optimized to. We introduce two levels of hard gates: the token-level and the sentencelevel which are instance-specific hard gates computed on the fly during forward propagation. Our work validates the proposed idea on a task in the Natural Language Generation (NLG) domain, as there is an inseparable relation between the quality of an output and model calibration (Kumar and Sarawagi, 2019). \n\nThe contributions of the proposed method are as follows:",
            "score": 0.41915520190247,
            "section_title": "Introduction",
            "char_start_offset": 2009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2090
                },
                {
                    "start": 2093,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 298,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 864,
                    "end": 881,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 995,
                    "end": 1016,
                    "matchedPaperCorpusId": "211252346"
                },
                {
                    "start": 1112,
                    "end": 1131,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1277,
                    "end": 1295,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "corpus_id": "258967945",
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "text": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
            "score": 0.4185848365485682,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "276422013",
            "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger",
            "text": "The model is instructed to think step-by-step, reasoning why it does or does not need external tools to address the user query, and finally concludes its decision with \"Yes\" or \"No.\" Before delving into the analysis, we provide some background on the concept of calibration in the context of Large Language Models (LLMs). Calibration refers to the alignment between a model's predicted probabilities and the actual likelihood of those predictions being correct. A well-calibrated model generates probability scores that accurately reflect the true probability of its predictions. \n\nIn Figure 6, we present the distribution of P(Yes) scores for both correct and incorrect Yes/No decisions. Our key observations are as follows: \n\n1. When the model is given detailed instructions and few-shot examples, it demonstrates poor calibration. \n\nAs illustrated in Figure 6(a), the distributions of P(Yes) scores for correct and incorrect decisions do not show a clear distinction. \n\n2. Conversely, when the model lacks detailed context and must rely on its internal beliefs to make decisions, it exhibits improved calibration. In Figure 6(b), the peak of the distribution for correct scores clearly deviates from that of incorrect scores. \n\n3. After fine-tuning, the model displays significantly better calibration, as shown in Figures 6(c) and (d). \n\nMost correct decisions have P(Yes) scores of either 1 (indicating \"Yes\") or 0 (indicating \"No\"), while the P(Yes) scores for incorrect decisions vary between 0 and 1.",
            "score": 0.4185848365485682,
            "section_title": "CoT (Chain of Thought):",
            "char_start_offset": 31635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "250451161",
            "title": "Language Models (Mostly) Know What They Know",
            "text": "Calibration for general ML predictions, and interventions to improve calibration, have been studied [Nguyen and O'Connor, 2015, Hendrycks and Gimpel, 2016, Nalisnick et al., 2019, Guo et al., 2017, Hendrycks et al., 2018, Ovadia et al., 2019, Minderer et al., 2021] for some time. Calibration for language models and QA has also been studied [Desai andDurrett, 2020, Jiang et al., 2021], but typically it has been found that to achieve good calibration predictions must be adjusted. Selective prediction, where models abstain from answering certain questions, has been studied as well [Varshney et al., 2022]. Recently, the calibration of a wide range of models was analyzed on the diverse BIG Bench suite of tasks [Srivastava et al., 2022], where it was shown that language model calibration improves with model size. We are indebted to the BIG Bench collaboration for providing a convenient, huge, and diverse evaluation set. The authors of Gopher [Rae et al., 2021] briefly studied calibration on MMLU [Hendrycks et al., 2021] and found promising results, which led us to experiment with a variety of multiple choice formats. Truthfulness [Evans et al., 2021] has been a recent focus of various works, including benchmarks [Lin et al., 2021] and the incorporation of web search and citation [Nakano et al., 2021, Menick et al., 2022] into language models. That said, truthfulness focuses primarily on factual accuracy \"in the world\", rather than on self-knowledge, or eliciting latent knowledge [Christiano et al., 2021]. We use \"honesty\" [Askell et al., 2021] as an umbrella term for ideas including truthfulness, calibration, self-knowledge, explainability, and non-deceptiveness. Language models finetuned to perform non-language tasks [Ahn et al., 2022, Dinh et al., 2022] might provide an interesting test-bed for honesty in the future.",
            "score": 0.4185848365485682,
            "section_title": "Related Work",
            "char_start_offset": 12073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 126,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 154,
                    "end": 178,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 241,
                    "end": 265,
                    "matchedPaperCorpusId": "235435823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "278327247",
            "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
            "text": "In this work, we have investigated the critical issue of calibration degradation in LLMs following preference alignment procedures. To address this, we introduced a theoretical framework distinguishing between calibratable and non-calibratable regimes, and developed practical solutions through calibration-aware fine-tuning approaches. Our experimental results across multiple models demonstrate that our methods can significantly improve calibration while maintaining or enhancing model performance. Future work could explore extending these methods to other types of language tasks and investigating the relationship between calibration and other aspects of model reliability.",
            "score": 0.4185848365485682,
            "section_title": "Conclusion",
            "char_start_offset": 24913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 679
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "In few-shot learning, a few examples are provided in the prompt, which helps the model understand how to solve the given task question. In a few-shot setting, the number of demonstrations provided in the prompt typically ranges between 10 and 100, or it includes as many examples that can fit into the model's context window. Compared to the finetuning approach, in a few-shot setting, the number of task-specific examples required is drastically reduced, making it a viable alternative for tasks with smaller dataset sizes. In case the task has many edge cases or is fuzzily defined, having more examples in the prompt can help the model understand the task and predict the result more accurately. \n\nIt was shown in GPT-3 [29] how the model performance rapidly improved after a few examples, which, along with a task description, were provided as the context through the window. Similarly, it was demonstrated in Jurassic-1 [31] how classification task accuracy improved after adding more examples in the few-shot setting. Because of the type of tokenizer used in Jurassic-1, it could fit in more examples in the prompt, leading to significant performance gain. \n\nHowever, it was demonstrated in some of the papers, such as [90], that the examples used in the few-shot setting, the sequence in which the examples were ordered, and the format of the prompt directly affected the accuracy. Ref. [90] demonstrated how this instability in few-shot learning stems from the language model's bias toward predicting specific answers. For example, the model can be biased towards answers placed towards the end of the prompt, those appearing frequently, or those that are familiar in the pretrained dataset. To address this instability, Ref. [90] first estimated the model's bias towards each answer. It then used calibration parameters that caused the prediction for the input to be uniform across answers. This calibration procedure improved GPT-3 and GPT-2's average accuracy by up to 30.0% on a diverse set of tasks and also reduced variance across different prompt choices.",
            "score": 0.41835505574775195,
            "section_title": "Few-Shot Learning",
            "char_start_offset": 76226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "238259429",
            "title": "Revisiting Self-training for Few-shot Learning of Language Model",
            "text": "As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and few-shot knowledge transfer across tasks.",
            "score": 0.41827621018955635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "273098733",
            "title": "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning",
            "text": "By benchmarking prominent LLMs on four language directions, we find that LLMs, while promising, still exhibit obvious imbalanced performance across different domains (as illustrated in Figure 1). Recent efforts on LLMs such as prompt strategies (Wei et al., 2022;Li et al., 2022;Zhang et al., 2023a) and fine-tuning (Ouyang et al., 2022;Wang et al., 2022) have shown effective performance improvement for downstream tasks. Nevertheless, in line with prior research (Thompson et al., 2019;Lai et al., 2022), our study uncovers that when LLMs are fine-tuned on domain-specific parallel corpora (in-domain), their ability to generalize to unseen domains (out-of-domain) remains constrained, often resulting in issues such as catastrophic forgetting and overfitting (French, 1999;Saunders, 2021). While certain methods like retrieval-based approaches (Agrawal et al., 2023;Ghazvininejad et al., 2023) and multi-perspective thinking (He et al., 2023) offer performance improvement, these techniques come at the cost of reliance on external knowledge resources and additional computational demands during decoding, aggravating the difficulty of practical deployment of LLMs. \n\nIn this study, our objective is to leverage the inherent multi-domain understanding and generative capacity of LLMs to improve multi-domain translation performance and enhance out-of-domain robustness. Our methodology is designed to inspire LLMs to elicit domain-specific insights from the source text, which are then used as a helpful hint prompt to guide the translation process. To accomplish this, we introduce a domain Chain-of-Thought (CoT) fine-tuning technique, which jointly cultivates LLM to recognize domain-specific information and output translation with given domainspecific hints during training. Despite being trained on only tens of thousands of parallel examples from four domains, our CoT fine-tuning method exhibits stronger translation quality and domain generalization than traditional fine-tuning.",
            "score": 0.41806545874636103,
            "section_title": "Introduction",
            "char_start_offset": 1835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1991
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 263,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 316,
                    "end": 337,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 337,
                    "end": 355,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 465,
                    "end": 488,
                    "matchedPaperCorpusId": "150376583"
                },
                {
                    "start": 488,
                    "end": 505,
                    "matchedPaperCorpusId": "245144567"
                },
                {
                    "start": 762,
                    "end": 776,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 776,
                    "end": 791,
                    "matchedPaperCorpusId": "233231665"
                },
                {
                    "start": 847,
                    "end": 869,
                    "matchedPaperCorpusId": "254246450"
                },
                {
                    "start": 869,
                    "end": 896,
                    "matchedPaperCorpusId": "256868348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "256105391",
            "title": "Adapting a Language Model While Preserving its General Knowledge",
            "text": "Pre-trained general-purpose language models (LMs) like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-3 (Brown et al., 2020) have become a standard component in almost all NLP applications. Researchers have also found that domain-adaptive pre-training (or DA-training for short) using an unlabeled corpus in a specific domain to adapt an LM can further improve the end-task performance in the domain (Gururangan et al., 2020;Xu et al., 2019a,b;Sun et al., 2019;Alsentzer et al., 2019). Note that domain-adaptive pre-training is also called post-training (Xu et al., 2019a).\n\nExisting DA-training methods simply apply the same pre-training objective, i.e., the mask language model (MLM) loss, to further train an LM using a domain corpus. These methods are sub-optimal because they do not explicitly identify what should be preserved and what should be updated in the LM by the domain corpus.\n\nThis paper argues that a good DA-training method has two needs. On the one hand, the general language knowledge learned in the LM should be preserved as much as possible because the target domain data is typically not large enough to be sufficient to learn the general knowledge well. For example, some words and their contexts may appear infrequently in a particular domain. The knowledge about them cannot be learned accurately based on the domain data alone. When these words and contexts appear in an end-task, the system will have difficulties. Thus, we need to rely on the knowledge about them in the LM. Since existing DA-training updates the LM with little guidance, such useful general knowledge may be corrupted. On the other hand, due to polysemy (same word with different meanings in different domains) and the fact that different domains also have their special word usages and contexts, the LM should be specialized or adapted to the target domain. A good DA-training should balance these two needs to adapt the LM to the target domain with minimal corruption to the good general knowledge in the LM. This paper proposes a novel technique to enable a more informed adaptation to (1) preserve",
            "score": 0.41644481623616314,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 81,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 121,
                    "end": 141,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 461,
                    "end": 478,
                    "matchedPaperCorpusId": "153312532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "258837444",
            "title": "Embracing Large Language Models for Medical Applications: Opportunities and Challenges",
            "text": "Transfer learning is a powerful approach that allows LLMs to leverage pre-trained models as a starting point for further training and adaptation to medical domains [9]. By applying domain-specific fine-tuning, which involves training pre-trained LLMs on relevant medicine-specific data to perform well on tasks within the medical field, we can ensure up-to-date and relevant medical knowledge [10]. Prioritizing recent and highly cited articles can improve the model's performance in specific medical domains. This approach would allow for the rapid development of specialized LLMs that can address the unique needs of various medical disciplines. Domain adaptation, closely related to domain-specific fine-tuning, is necessary for LLMs to function effectively in different medical domains, specialties, and languages. While domain-specific fine-tuning focuses on adjusting a model to perform well within a specific field, domain adaptation involves adapting a model trained in one domain to work effectively in a different but related domain without requiring extensive retraining [11]. Developing models that can adapt to various contexts ensures their applicability across diverse healthcare settings, benefiting both patients and practitioners. Moreover, the ability to adapt to different languages can help break down language barriers, facilitating improved global access to medical knowledge and expertise. \n\nAlternative methods for adapting LLMs to medical domains, such as few-shot learning and zero-shot learning, can also be relevant in certain scenarios. Few-shot learning aims to train models to perform well on new tasks with very limited labeled data by leveraging knowledge learned from other tasks [12]. Zeroshot learning, on the other hand, focuses on training models to perform tasks without any labeled data for the target task, relying solely on knowledge learned from other tasks [13]. These approaches can be useful when domain-specific training data is scarce or unavailable, allowing LLMs to adapt to new medical domains more efficiently. \n\nSeveral examples of LLMs fine-tuned for medical applications showcase the potential of transfer learning, domain adaptation, and alternative methods in this field. BioBERT, a pre-trained biomedical language representation model based on the BERT architecture, has been fine-tuned on large-scale biomedical corpora, including PubMed abstracts and PMC full-text articles, leading to significant improvements in biomedical NLP tasks such as named entity recognition, relation extraction, and question-answering [14].",
            "score": 0.4159186048150366,
            "section_title": "Transfer learning, domain-specific fine-tuning, and domain adaptation",
            "char_start_offset": 1993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2579
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 1902,
                    "end": 1906,
                    "matchedPaperCorpusId": "4852047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "248780145",
            "title": "Knowledge Distillation Meets Few-Shot Learning: An Approach for Few-Shot Intent Classification Within and Across Domains",
            "text": "Therefore, an adaptation of knowledge distillation to few-shot learning is necessary. To the best of our knowledge, task-specific knowledge distillation in cross-domain few-shot learning has largely remained unexplored with a few exceptions in computer vision (Zhang et al., 2020b;Li et al., 2020) and natural language processing (NLP; Pan et al. 2021;Zhou et al. 2021). \n\nIn this paper, we propose a task-specific approach for distilling small models with generaliza-tion ability to new classes and domains in two fewshot learning scenarios: 1) in-domain target class generalization in single-and multi-domain intent classification; 2) target domain adaptation in multidomain intent classification. To this end, we first pretrain a Transformer-based prototypical teacher network (Snell et al., 2017) on source classes and domains using meta-learning. Then, we design a prototypical student network and pass the transferable knowledge to the student using knowledge distillation. During the distillation process, we consider a prototype loss as a new component in the standard distillation loss function. This loss measures how much each prototype that is produced by the student model resembles the respective prototype produced by the teacher model. Moreover, as opposed to standard batch training in knowledge distillation, we introduce an episodic distillation process. This way, we obtain a small student model that is compatible with few-shot scenarios and generalizes to unseen target classes and domains. \n\nOur contributions are summarized as follows: 1) We propose a new knowledge distillation approach compatible with few-shot learning by introducing an episodic distillation process and using the prototype-based distillation loss. Our novel approach combines advantages of few-shot learning with knowledge distillation. 2) We perform extensive experiments on four public NLU benchmarks and compare the distilled small model with the large model in the few-shot intent classification scenario. Results show a slight performance drop for the small model while having lower memory consumption and a slightly faster inference speed. \n\n3) We show that the small model can effectively generalize and adapt to target domains without the teacher supervision in the few-shot target domain adaptation. This is a more challenging and realistic scenario for small student models.",
            "score": 0.41589168670325305,
            "section_title": "Introduction",
            "char_start_offset": 2149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2379
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 281,
                    "matchedPaperCorpusId": "221713645"
                },
                {
                    "start": 336,
                    "end": 352,
                    "matchedPaperCorpusId": "227247952"
                },
                {
                    "start": 780,
                    "end": 800,
                    "matchedPaperCorpusId": "309759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "273228072",
            "title": "Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs",
            "text": "In this work, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which addresses the overconfidence issues commonly encountered during fine-tuning of large language models. We present a functional perspective on quantifying epistemic uncertainty in LLMs and utilize it for uncertainty-calibrated finetuning. By incorporating functional-level epistemic uncertainty quantification with a mixture-of-experts framework, our proposed uncertainty-calibrated training loss effectively addresses the challenge of overconfidence in fine-tuned LLMs by significantly improving uncertainty calibration while maintaining high accuracy. Our evaluations demonstrate that UQ4CT reduces the Expected Calibration Error by more than 25% without compromising accuracy across a variety of downstream tasks, including common-sense and domain-specific reasoning, under in-distribution and out-of-distribution scenarios. \n\nThe limitation of UQ4CT lies in its dependency on predictive correctness. For general language modeling tasks such as chat completion, there lacks a clear metric on whether the response is correct or not. This limits the application of UQ4CT as naively token matching is a poor indicator of semantic correctness due to the ambiguous nature of language. \n\nFor future work, we plan to continue exploring ways to adapt UQ4CT on open-ended problems that lacks a definitive metric.",
            "score": 0.4157835135133125,
            "section_title": "Discussion & Conclusion",
            "char_start_offset": 25976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1415
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "222327644",
            "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing",
            "text": "semantic and syntactic information from such a large corpus, the language models are designed to have enormous capacity, e.g., T5 has about 11 billion parameters. At the fine-tuning stage, however, only limited labeled data are available in the downstream tasks. With the extremely high capacity, these models can easily overfit training data likelihood and be over-confident in their predictions.\n\nTo fight against miscalibration, a natural option is to apply a calibration method such as temperature scaling  in a post-processing step. However, temperature scaling only learns a single parameter to rescale all the logits, which is not flexible and insufficient. Moreover, it cannot improve out-of-distribution calibration. A second option is to mitigate miscalibration during training using regularization. For example, Pereyra et al. (2017) propose an entropy regularizer to prevent over-confidence, but it can needlessly hurt legitimate high confident predictions. A third option is to use Bayesian neural networks (Blundell et al., 2015;Louizos and Welling, 2017), which treat model parameters as probability distributions to represent model uncertainty explicitly. However, these Bayesian approaches are often prohibitive, as the priors of the model parameters are difficult to specify, and exact inference is intractable, which can also lead to unreliable uncertainty estimates.\n\nWe propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples",
            "score": 0.4145927069443502,
            "section_title": "Introduction",
            "char_start_offset": 3374,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1020,
                    "end": 1043,
                    "matchedPaperCorpusId": "39895556"
                },
                {
                    "start": 1043,
                    "end": 1069,
                    "matchedPaperCorpusId": "9280646"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "265220901",
            "title": "On the Impact of Calibration Data in Post-training Quantization and Pruning",
            "text": "Previous work has demonstrated that increasing the number of calibration examples offers diminishing gains in language modeling performance (Frantar and Alistarh, 2023;Sun et al., 2024). We therefore follow existing work and randomly sample 128 calibration examples, each consisting of 2,048 tokens (Frantar et al., 2023;Frantar and Alistarh, 2023;Sun et al., 2024). This offers a total of 262,144 tokens in each calibration set. We provide a detailed analysis of how the quantity of calibration examples impacts performance in Section 4. \n\nThe use of random sampling avoids selection bias and ensures that each calibration set is representative of the source dataset. Similarly, we sample without replacement, to ensure that each calibration example appears only once. To examine the variability introduced by random sampling, we repeat the sampling process to create ten nonoverlapping calibration sets for each source dataset. This provides a total of 50 distinct calibration sets. \n\nDue to the vast size of C4, we follow Frantar et al. (2023) in sampling data from the first shard only. We use the same strategy for RefinedWeb, although for RedPajama we use the existing 1B token extract.4",
            "score": 0.41450130713776434,
            "section_title": "Data Sampling",
            "char_start_offset": 12499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 185,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 348,
                    "end": 365,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "This observation indicates tha performance and calibration should be concurrently optimized rather than being treated as objectives. Furthermore, we find that additional fine-tuning may result in worse calibra evidenced by Vicuna-13B exhibiting poorer calibration compared to its predecessor, LLaM To summarize, our contributions can be delineated as follows: We apply LITCAB to Llama2-7B (Touvron et al., 2023b) and compare it against several competitive baselines, including post-processing, training-, verbalization-, and consistency-based methods (Kuhn et al., 2023;Kadavath et al., 2022;Xiong et al., 2023). Our experiments demonstrate the effectiveness of LITCAB, which exhibits uniformly superior calibration than baselines across the text generation benchmarks. \n\nDuring calibration evaluation, we note that existing work mainly studies short answer QA (Tian et al., 2023;Xiong et al., 2023), little attention has been given to LM calibration over long-form outputs. To address this gap, we construct and release Calibration evaluaTion Benchmark (CAT) consisting of eight text generation tasks that cover generations encompassing phrases, sentences, and up to paragraphs. We further conduct extensive evaluation over seven open-source LMs, including GPT (Radford et al., 2019;Wang & Komatsuzaki, 2021), LLaMA (Touvron et al., 2023a), Llama2 (Touvron et al., 2023b), and Vicuna (Chiang et al., 2023), with sizes ranging from 1.5B to 30B. \n\nOur findings underscore insights regarding the relationships among calibration, model scale, and task performance, as well as the impact of instruction tuning on calibration. First, larger models within the same family demonstrate better calibration for phrase-level tasks, but not necessarily for sentence-and paragraph-level tasks. In addition, when comparing different LM families, despite superior task performance, larger models from various families tend to be worse calibrated compared to the smaller GPT-2 XL (1.5B) model.",
            "score": 0.41450130713776434,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 570,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1262,
                    "end": 1284,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "270095154",
            "title": "Calibrating Reasoning in Language Models with Internal Consistency",
            "text": "Understanding the Inner Workings of Language Models The rapid progress in LLM development has necessitated simultaneous efforts to interpret the inner workings of advanced models (Bricken et al., 2023;Ferrando et al., 2024). These works aim to provide an internal view of mechanisms to ensure safety and fairness (Burns et al., 2023;Zou et al., 2023;Li et al., 2024) and to further improve model inference (Schuster et al., 2022;Raposo et al., 2024). Some studies also examine CoT reasoning from an internal perspective and propose methods to teach models to perform implicit CoT (Deng et al., 2023). Unlike these approaches, which require additional training for interpretation, our internal consistency measure offers an off-the-shelf solution to calibrate CoT reasoning, providing a practical and efficient tool for enhancing model reliability. \n\nCalibration in Language Models Traditional calibration methods (Platt et al., 1999;Naeini et al., 2015;Guo et al., 2017) train a parameterized classifier on validation data to adjust the final output of a neural network towards expected outcomes. In the context of LLMs, previous works apply trained parameterized models to the logits at the final layer (Zhao et al., 2021;Shen et al., 2024). In comparison, our study focuses on the phenomenon of internal inconsistency in CoT reasoning and demonstrates that internal consistency is a reliable unsupervised calibration measure.",
            "score": 0.41450130713776434,
            "section_title": "Related Work",
            "char_start_offset": 24496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1427
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 366,
                    "matchedPaperCorpusId": "259088877"
                },
                {
                    "start": 406,
                    "end": 429,
                    "matchedPaperCorpusId": "250526382"
                },
                {
                    "start": 913,
                    "end": 933,
                    "matchedPaperCorpusId": "56563878"
                },
                {
                    "start": 933,
                    "end": 953,
                    "matchedPaperCorpusId": "6292807"
                },
                {
                    "start": 953,
                    "end": 970,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1204,
                    "end": 1223,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78125
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Prior studies (Desai and Durrett, 2020;Dan and Roth, 2021;Xu and Zhang, 2023;He et al., 2021) have investigated the calibration of pre-trained LLMs on downstream tasks, primarily with a focus on monolingual English models and tasks, and primarily in the classification setting. Kuleshov and Liang (2015); Jagannatha and Yu (2020) study calibration for structured prediction, but not in the context of LLMs, Recently, Ahuja et al. (2022) studies calibration of multilingual pre-trained LLMs, specifically mBERT and XLM (Devlin et al., 2019;Conneau et al., 2019) on various downstream classification tasks including natural language inference and commonsense reasoning. They show that multilingual models are not well-calibrated in the classification setting, especially for lowresource languages. Jiang et al. (2022) also explores cross-lingual calibration performance for mBERT and XLM, comparing various post-hoc calibration methods on both structured and unstructured prediction tasks. In this work, we extend this line of work to calibration of MLLMs for QA, in both classification and generative settings, and to cross-lingual and distribution shift settings.",
            "score": 0.41450130713776434,
            "section_title": "Calibration of Large Language Models",
            "char_start_offset": 4454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1163
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 39,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 39,
                    "end": 58,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 58,
                    "end": 77,
                    "matchedPaperCorpusId": "258378137"
                },
                {
                    "start": 278,
                    "end": 303,
                    "matchedPaperCorpusId": "2974522"
                },
                {
                    "start": 305,
                    "end": 329,
                    "matchedPaperCorpusId": "215548393"
                },
                {
                    "start": 518,
                    "end": 539,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 796,
                    "end": 815,
                    "matchedPaperCorpusId": "250551977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "253244504",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "text": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "score": 0.41450130713776434,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "250551977",
            "title": "Calibrating Zero-shot Cross-lingual (Un-)structured Predictions",
            "text": "Why calibration in NLP tasks? Uncertainty quantification for neural networks and model calibration has received attention from various machinelearning-related fields, especially when machine learning is applied in the high-stakes decisionmaking (Gal and Ghahramani, 2016;Kendall and Gal, 2017;Lakshminarayanan et al., 2017;Grathwohl et al., 2020;Thulasidasan et al., 2019). In particular, in NLP tasks, uncertainty plays an important role in AI-aided mental health diagnosis (Chandler et al., 2022) and human-in-the-loop active data curation (Yuan et al., 2022). Also, in languagemodel-based reasoning engines, the searching or filtering step often requires a faithful scoring rule (Dalvi et al., 2021;Weir and Van Durme, 2022;Creswell and Shanahan, 2022), which could be a potential application of a calibrated entailment classifier. \n\nCalibration of large scale models Noticeably, Ovadia et al. (2019); Minderer et al. (2021) have produced large-scale benchmarks over a variety of tasks and existing calibration methods with mixed results. While empirically Ovadia et al. (2019) shows that the traditional post-training calibration methods such as temperature scaling do not always transfer under domain shift, results from Minderer et al. (2021) indicates that there is a correlation between in-domain and out-of-domain calibration error for models with large capacities like ViT (Dosovitskiy et al., 2021), and that model calibration decreases more slowly than accuracy. \n\nIn NLP, Desai and Durrett (2020) shows that pretrained transformer models achieve better calibration and that temperature scaling further reduces calibration error in-domain. Mohta and Raffel (2021) demonstrates that the benefit of the pretrained model diminishes as the domain shift increases. Our work extends these analyses to model calibration under zero-shot cross-lingual transfer.",
            "score": 0.41450130713776434,
            "section_title": "Calibration in NLP Tasks",
            "char_start_offset": 3395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 323,
                    "matchedPaperCorpusId": "226262317"
                },
                {
                    "start": 346,
                    "end": 372,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 475,
                    "end": 498,
                    "matchedPaperCorpusId": "247778877"
                },
                {
                    "start": 542,
                    "end": 561,
                    "matchedPaperCorpusId": "247778877"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "174803437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "While modern language models (LMs) exhibit impressive performance across various tasks (Brown et al., 2020;OpenAI, 2023), they suffer from hallucination (Lin et al., 2021;Zhang et al., 2023), where they can provide nonfactual responses with high confidence. The issue of hallucination undermines user trust and significantly limits the applicability of LMs to domains that require a high degree of reliability, such as legal, financial, and educational sectors. While eliminating hallucination in LMs altogether is highly challenging, calibrating LMs (Nguyen & O'Connor, 2015) by aligning their confidence with the actual probability of output correctness can certainly help. Specifically, a well-calibrated model enables users to gauge the model's confidence and make informed decisions about whether to trust its outputs. Moreover, hallucinated facts can be filtered out when the confidence level is below a certain threshold. \n\nPrevious approaches to calibrating neural models mainly fall into two categories: (i) post-processing and (ii) training-based methods, as outlined in Figure 1. Post-processing techniques have the advantage of not changing the model weights by directly manipulating the sequence probabilities. Example techniques from this family are temperature scaling (Liang et al., 2018) and Platt scaling (Niculescu-Mizil & Caruana, 2005). Post-processing calibration directly adjusts the sharpness of the Motivated by reducing the cost of computation, we present LITCAB, a lightweight calibration technique for LLMs. LITCAB takes as input a sequence of hidden states from the LM's final layer and produces a set of logit biases to adjust the generation confidence. Specifically, LITCAB trains a single linear layer using a contrastive max-margin objective, with a goal of maximizing the token probabilities for correct answers and lowering the likelihood for incorrect ones. As LITCAB can adjust the confidence ranking among outputs, a capability that post-processing methods lack, it offers greater flexibility compared to those methods. On the other hand, the trainable parameters count of LITCAB is less than 2% of the original LM parameters, making it significantly more efficient than standard training-based approaches.",
            "score": 0.41450130713776434,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 107,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 551,
                    "end": 576,
                    "matchedPaperCorpusId": "2879445"
                },
                {
                    "start": 1284,
                    "end": 1304,
                    "matchedPaperCorpusId": "3526391"
                },
                {
                    "start": 1323,
                    "end": 1356,
                    "matchedPaperCorpusId": "207158152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "275931893",
            "title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning",
            "text": "LLMs can generalize knowledge across domains and handle tasks without specific training or fine-tuning, using \"zero-shot\" prompts without examples. While this is impressive, zeroshot performance is often less accurate or reliable than finetuned models trained for specific tasks, leading to less precise responses. For example, [11] have performed few-shot learning remarkably well on GPT-3. However, zero-shot performance is much worse than the few-shot performance on tasks such as reading comprehension, question answering, and natural language inference. One potential reason is that, without fewshot exemplars, it is harder for models to perform well on prompts that are not similar to the format of the pretraining data. Designing prompts that guide the model to produce better zero-shot results is a practical approach. This includes using clear and explicit instructions that reduce ambiguity. To address the issue, [38] demonstrates that LLMs are decent zeroshot reasoners by simply adding \"Let's think step by step\". [49] simply replace \"Let's think step by step\" of Zeroshot-CoT with \"Let's first understand the problem and devise a plan to solve the problem. Then, let's carry out the plan and solve the problem step by step\" to further improve model zeroshot learning capability. FLAN (Fine-tuned Language Net) [50] shows that instruction fine-tuning LLMs on a collection of datasets described via instructions-substantially improves zero-shot performance on unseen tasks. [51] propose a similar approach to finetune T5-11B to respond to prompts, and they also report improved performance on zero-shot learning. However, creating a usable instruction-tuning data set is resourceintensive and is not suitable for domain-specific downstream tasks. Therefore, this study focuses on enhancing performance by optimizing prompts rather than fine-tuning using specific off-task data. In all LLMs, we selected ChatGPT, the mostly widely used LLMs so far as the example.",
            "score": 0.4142801391427393,
            "section_title": "C. Zero-shot learning in LLMs",
            "char_start_offset": 12277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "267740621",
            "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "text": "Prompt-based learning is susceptible to intrinsic bias present in pre-trained language models (LMs), leading to sub-optimal performance in prompt-based zero/few-shot settings. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to probe intrinsic bias of pre-trained LMs. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).",
            "score": 0.4140483558218855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "258187507",
            "title": "Tailoring Domain Adaptation for Machine Translation Quality Estimation",
            "text": "This paper addresses two key challenges related to quality estimation (QE) of machine translation (MT): (i) the scarcity of available QE data and (ii) the difficulties in estimating translations across diverse domains. The primary aim of this study is to enhance the performance of QE models by addressing these challenges. To do so, we propose a solution that utilizes domain adaptation (DA) techniques adopted from MT. We adapt the \"mixed fine-tuning + fine-tuning\" approach (Chu et al., 2017) and extend it with data augmentation as an alternative to the traditional oversampling technique. We adopt a three-step training methodology: (i) we fine-tune XLM-R, a language model, with a large generic QE dataset, which enables the model to generalize; (ii) we fine-tune the model with a mix of out-of-domain (OOD) and indomain (ID) data derived from two data augmentation (DAG) approaches; and (iii) we fine-tune the model with a small amount of domain-specific data, which leads to a more specific model. We evaluated models' performance with and without domain tags appended to the sentences. \n\nOur experiments show significant improvements across all language pairs under consideration, indicating that our proposed solution has a beneficial impact in addressing the aforementioned challenges. Our study also demonstrates the effectiveness of both proposed DAG approaches and shows that using domain tags improves the performance of the models. Additionally, we find that our model outperforms the baseline in the context of zeroshot learning and in cross-lingual inference. \n\nMoving forward, there are several directions for future work based on our findings. First, it would be interesting to investigate the performance of our pipeline on low-resource language pairs, where there is limited ID data available. This is particularly relevant given the smaller coverage of QE datasets compared to parallel data in MT. Second, we only used one type of OOD data in our experiments (EN-IT); it would be useful to explore other OOD data over different language pairs for QE. Third, it would be valuable to study the performance of other LLMs than XLM-R.",
            "score": 0.41376423697877485,
            "section_title": "Conclusion and future work",
            "char_start_offset": 26639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 477,
                    "end": 495,
                    "matchedPaperCorpusId": "35273027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "265295180",
            "title": "Igniting Language Intelligence: The Hitchhiker\u2019s Guide from Chain-of-Thought Reasoning to Language Agents",
            "text": "Language agents have found extensive applications in practical fields such as engineering (Li et al., 2023a;Mehta et al., 2023;Qian et al., 2023), natural sciences (Bran et al., 2023;Kang & Kim, 2023;Boiko et al., 2023), and social sciences (Aher et al., 2023;Akata et al., 2023;Ma et al., 2023;Dan et al., 2023). Despite their widespread use, a significant challenge persists: adapting LLMs to specific, especially unseen domains. This challenge is twofold: firstly, determining an efficient method for acquiring domain-specific knowledge, such as employing CoT prompting techniques. The limitations arise from the finite scope of knowledge acquisition during pre-training on textual corpora, lacking substantial interaction with the physical world. Secondly, there is the challenge of effectively adapting LLMs to diverse, unseen domains. Given the substantial variation in action spaces across tasks (e.g., drone control versus web browsing), aligning the model's knowledge with the specific task requirements remains a formidable obstacle. These challenges underscore a critical gap in current research. The need to enhance LLMs' adaptability to novel domains and help LLMs learn from environments is paramount, requiring innovative solutions that address both knowledge acquisition and effective task alignment. \n\nPrompting and fine-tuning are widely used techniques to adapt pre-trained LLMs to new domains. However, it remains an underexplored area of when and how to leverage prompting (e.g., prompting pattern and reasoning format) and fine-tuning (e.g., instruction tuning) techniques to help LLMs generalize to unseen domains. In doing so, researchers can pave the way for more versatile and impactful applications of language agents across a myriad of fields.",
            "score": 0.41357388174514575,
            "section_title": "Generalization to Unseen Domains",
            "char_start_offset": 78434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 200,
                    "matchedPaperCorpusId": "260438479"
                },
                {
                    "start": 241,
                    "end": 260,
                    "matchedPaperCorpusId": "251719353"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "278129791",
            "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
            "text": "Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.",
            "score": 0.4128558633284255,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "264146928",
            "title": "Generative Calibration for In-context Learning",
            "text": "Besides vanilla ICL, we include the following stateof-the-art ICL calibration methods for comparison: 1) Noisy channel (NC) (Min et al., 2021) changes the slot position of the input and output in the template, and then uses the label likelihood p(x|y) = p LM (T (x)|D \u22121 (D \u03c0 t ), T (y)) for prediction, where D \u22121 (D \u03c0 t ) denotes the concatenation of flipped demonstrations. Since p(x|y) \u221d p(x|y)/p(y), this method actually has the same form as ours in Equation ( 9), so we categorize it as a calibration method. 2) Contextual calibration (CC) (Zhao et al., 2021) estimates the label marginal via context-free texts. \n\n3) Domain-context calibration (DC) (Fei et al., 2023) proposes a further requirement for the context-free texts: they must be also context-free in the task domain. They construct such domain context-free texts by randomly sampling and concatenating words from the task dataset. 4) Prototypical calibration (PC) (Han et al., 2022) learns a Gaussian mixture model (GMM) from the output probability vectors. They then consider each cluster corresponds uniquely to a label, where the learned cluster weights are the estimated label marginal. For a fair comparison, we learn the GMM on the set of generative sequences as the same as GC. \n\nWe consider 2, 4, and 8-shot true few-shot learning settings. For evaluating each method, we ran-domly sample the original training set of the dataset to construct the training examples. LLMs scaling less than 30B are evaluated in 5 runs, while those larger than 30B are evaluated in 2 runs using different random seeds. This finally yields 1944 runs for each method, which the results should be solid. The performance is measured by macro-F1. Implementation details are shown in Appendix C. We also present time complexity analysis in Appendix D.",
            "score": 0.4118782311675382,
            "section_title": "Compared Methods",
            "char_start_offset": 18018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366943359375
        },
        {
            "corpus_id": "214714416",
            "title": "Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining",
            "text": "Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), served as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.",
            "score": 0.4109032490960611,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "266435613",
            "title": "Large Language Models are Miscalibrated In-Context Learners",
            "text": "These challenges could be more severe in instructiontuned models considering their strong instructionfollowing abilities. However, most of the previous work (Mosbach et al., 2023;Sun et al., 2023) only focuses on comparing solely the performance of SFT and ICL on out-of-distribution (OOD) data, targeting general-purpose LMs. Here, we instead focus on studying instruction-tuned taskspecialized language models, where the behavior of different paradigms' in-task performance along with their calibration remains an open research question. Therefore, in this work, in addition to the arXiv:2312.13772v3 [cs.CL]  Figure 1: Illustration of the self-ensembled learning methods. We introduce different types of variations to the input and feed them to a single language model. After having the predictions, we run self-ensembling to obtain final predictions and their confidence. \n\ntask performance of the models, we are interested in: RQ1) how would ICL impact the calibration of LMs? Furthermore, considering the possible issue of overconfidence and miscalibration, we pose and study another crucial research question: RQ2) is it possible to ensure both in-task performance and well-calibrated LM behavior at the same time? Satisfying both requirements is critical to the application of the model in real-life setups: an applied system should provide both accurate and calibrated predictions to be responsible. To address the above challenges, we first investigate the performance and calibration of different model-tuning and ICL methods, along with their interplay, on 7 classification datasets in limited data setups. The experiments without ICL demonstrate that the vanilla language model is not necessarily calibrated despite of the high in-task performance it achieves. We find empirically that ICL doesn't help to improve the calibration consistently across 5 of the 7 classification datasets. Our empirical investigations with ICL unveil a phenomenon between in-task performance and calibration, depending on whether the task dataset has been seen by the model before, which also gains increasing attention over the research community in data contamination (Zhu et al., 2023;Deng et al., 2023).",
            "score": 0.4107880585358375,
            "section_title": "Introduction",
            "char_start_offset": 1746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 179,
                    "matchedPaperCorpusId": "258947047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "276409294",
            "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
            "text": "Large Language Models (LLMs) have achieved extraordinary success across various tasks, showcasing remarkable capabilities in reasoning, knowledge representation, and decisionmaking. However, despite their impressive performance in general-purpose applications, many specialized domains, such as healthcare, chemistry, and legal analysis, demand the integration of domain-specific knowledge to achieve high accuracy and reliability. To address this challenge, researchers have explored methods to enhance LLMs through external or embedded domain expertise, a process often referred to as knowledge injection. This approach aims to bridge the gap between general-purpose language understanding and the stringent requirements of domain-specific tasks, enabling LLMs to perform effectively in highly specialized contexts. \n\nBuilding on the foundational capabilities of generalpurpose LLMs, knowledge injection techniques provide an effective means to address their limitations in handling specialized applications. Compared to the generalized approach of standard LLMs, knowledge injection offers two key advantages: 1) incorporating precise, domain-specific knowledge to improve accuracy and reliability in specialized tasks, and 2) allowing LLMs to dynamically adapt to new information or evolving knowledge bases, ensuring up-to-date expertise. These techniques bridge the gap between general-purpose understanding and domain-specific demands by leveraging both structured and unstructured knowledge sources. As a result, knowledge injection methods have been successfully applied in fields such as healthcare, chemistry, and legal analysis, significantly enhancing LLM performance. For example, biomedical LLMs [Bolton et al., 2024;Yan et al., 2023] have demonstrated superior accuracy in tasks like medical diagnostics and regulatory compliance, while domain-specific models for material science [Xie et al., 2024;Antunes et al., 2024;Zhang et al., 2024a] have achieved advances in material property prediction and discovery. These dedicated models underscore the transformative potential of integrating domain knowledge into LLMs, unlocking solutions to complex, field-specific challenges. \n\nDespite these advancements, early efforts in knowledge injection often treated domains independently, leading to a lack of standardization in methodologies and evaluation. As the volume of research continues to grow rapidly, with applications and studies proliferating across disciplines, the need for a comprehensive review becomes evident.",
            "score": 0.41075703111949813,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2365
                },
                {
                    "start": 2366,
                    "end": 2535
                }
            ],
            "ref_mentions": [
                {
                    "start": 1897,
                    "end": 1915,
                    "matchedPaperCorpusId": "259129602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "253510101",
            "title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing",
            "text": "Since actions in these domains -especially physical domains -can have irreversible effects, the importance of ensuring model safety cannot be understated. Thus, semantic parsing provides a clear motivation for having a well-calibrated sequence generation model: At low confidence, we may prefer for the system to defer action or request clarification, while when confidence is high, these actions may unnecessarily annoy a user and make the system unusable. This reasoning presupposes that the model's confidence is well-correlated with its probability of success. \n\nSimultaneously, the constrained and executable nature of semantic parses makes accuracy easier to measure. In many text-based sequence generation tasks like machine translation, summarization, long-form question-answering (QA), and openended dialogue, evaluating the quality and correctness of a generation poses a variety of challenges, often due to the fact that there are many ways of stating roughly the same proposition in language. Because calibration measures the relationship between accuracy and confidence, our ability to measure calibration will only be as good as our ability to measure accuracy. In executable semantic parsing, the model generates a program (rather than text) with a more restricted vocabulary and known syntactic rules. This generally limits the number of reasonable semantically equivalent outputs and makes quantifying accuracy easier. Furthermore, the executable nature of the programs allows us to measure accuracy via denotation (i.e. the result of program execution) rather than form. These factors make it an ideal domain for benchmarking the calibration of sequence generation models. \n\nIn Section 4, we conduct what is to our knowledge the first large-scale investigation of calibration in sequence generation models as applied to semantic parsing tasks. We examine a variety of commonly-used models and measure their calibration across four popular semantic parsing datasets drawn from two different domains: task-oriented dialogue (TOD) and text-to-SQL. We first document the model's calibration profiles, asking how well-calibrated modern semantic parsing systems are; this includes a large pre-trained model queried in a few-shot setting, as well as more traditional fine-tuned models.",
            "score": 0.4106907326237946,
            "section_title": "Introduction",
            "char_start_offset": 2038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "271924409",
            "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation",
            "text": "Although distillation shows better calibration than fine-tuning as it matches each token entry with a probability distribution instead of a hard label, we find it is still biased because of the mis-calibration nature of teacher models. In addition, distillation faces the challenge of determining the optimal amount of knowledge to transfer. Transferring all the teacher's knowledge leads to high computational costs while transferring too little knowledge results in poor accuracy. Therefore, it is crucial to balance between trustworthiness (accuracy and well-calibration) and efficiency for distillationbased methods. \n\nTo address the challenge of obtaining a trustworthy model, we propose eFfIcient tRustworthy disTillation (FIRST), aiming to efficiently utilize a relatively small amount of the teacher's knowledge. Specifically, we first identify the \"concentrated knowledge\" phenomenon, which shows that in the context of LLMs, the probability distribution of generated tokens is not uniform but rather concentrated on a few high-probability tokens. Based on this finding, we propose to use the top-5 tokens as the knowledge to balance the trade-off between storage space and the amount of knowledge transferred, achieving efficient distillation. Afterward, to eliminate the \"tuning-induced mis-calibration\" of the teacher model, we applied a \"trustworthy maximization\" to this portion of knowledge, ensuring that it maximizes the enhancement of the student model's accuracy while also guaranteeing its well-calibration. \n\nWe first validate our method in in-domain scenarios, discovering that the models obtained by FIRST achieve excellent accuracy, even with the use of a relatively small amount of top-5 knowledge and the \"trustworthy maximization\" process can significantly enhance these models' robustness to miscalibration. Furthermore, we test our approach in out-of-domain settings, demonstrating that models obtained by FIRST still exhibit the best trustworthiness and hold generalization ability. This indicates that FIRST enables smaller models to genuinely learn the capability of being trustworthy, rather than being confined to in-domain scenarios. \n\nIn summary, our key contributions include: \n\n(i) We discover that LLMs exhibit \"concentrated knowledge\" and \"tuning-induced miscalibration\" phenomena, providing insights into obtaining trustworthy models.",
            "score": 0.4106541600242647,
            "section_title": "Introduction",
            "char_start_offset": 1815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2168
                },
                {
                    "start": 2171,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "271329113",
            "title": "Evaluating language models as risk scores",
            "text": "We perform a comprehensive evaluation of risk scores output by LLMs along multiple metrics. Results are summarized in Table 1. \n\nMultiple-choice prompting. We observe that a majority of LLMs (all of size 8B or larger) outperform the linear model baseline (LR) in terms of predictive power (AUC). However, LLMs are clearly far from matching the supervised baselines with respect to calibration: all language models achieve very high calibration error (ECE), while baselines achieve near-perfect calibration. Due to this high miscalibration, models struggle to translate scores with high predictive signal into high accuracy. In fact, while most models achieve high AUC, most models struggle to surpass the supervised linear baseline in terms of accuracy. We recall that AUC is agnostic to calibration, while accuracy on the maximum likelihood answer is not. All of the Gemma models have worse than random accuracy, despite having clearly above random AUC (random would be 0.5). Only the instruction-tuned Mistral models (7B, 8x7B, and 8x22B) outperform the linear model in terms of accuracy. Interestingly, while larger models achieve higher AUC, calibration is not reliably improved by model size -differences across model families are more pronounced than across model sizes. Finally, we focus on comparing base models to their instruction-tuned counterparts, marked with '(it)'. A striking trend is visible across the board: instruction-tuning generally worsens calibration (higher ECE) when using multiple-choice prompting. At the same time, we generally see improvements in AUC and accuracy after instruction-tuning. Appendix A.2 presents results on the four additional prediction tasks. The same trend of instruction-tuning leading to worse calibration and higher AUC is broadly replicated. However, performance across different tasks is somewhat inconsistent: LLMs generally have stronger predictive signal than a supervised linear model on the income prediction and travel-time prediction tasks, but consistently underperform the linear baseline on the address change task (ACSMobility). \n\nNumeric prompting. We observe broad improvements in calibration (lower ECE) and Brier score loss across instruction-tuned models when using numeric prompting.",
            "score": 0.4105700924742362,
            "section_title": "Benchmark results",
            "char_start_offset": 25360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 126
                },
                {
                    "start": 129,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 2094
                },
                {
                    "start": 2097,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "276647283",
            "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
            "text": "Because language models are often miscalibrated, they are often confidently wrong (Kaur et al., 2020). This mismatch between accuracy and confidence causes users to trust models more than they should (Caruana, 2019;Deng et al., 2025), even over their own correct judgment (Krause et al., 2023;Stengel-Eskin and Van Durme, 2023;Liu et al., 2024). These issues are particularly severe when models are miscalibrated in ways that humans are not: users expect models to be at least as calibrated as humans, and when models are worse, users are often not prepared to address these errors. \n\nThus, models should be at least as calibrated as humans, making it especially crucial to identify when models commit calibration errors that humans do not. However, existing work on model calibration lacks comparison with human calibration. \n\nWe thus introduce GRACE, a Granular, Humangrounded Benchmark for Model Calibration Evaluation. Each instance allows fine-grained calibration measurement using an incremental question-answering (QA) framework. Expert writers design GRACE questions, each consisting of at least five sentences of clues that gradually become easier. To prevent models from being confused by ambiguities or false presuppositions, we require that clues challenge models but remain clear for humans. This format measures model calibration with human performance as a reference point: models should give correct answers earlier and more confidently than humans, while minimizing confidently incorrect guesses ( \u00a7 3). \n\nGRACE incorporates human responses from live QA competitions we conduct. Unlike prior calibration evaluation methods that only allow modelmodel calibration comparisons, our dataset thus allows direct human-model calibration comparison. GRACE is the first benchmark dataset designed to evaluate model calibration grounded in human needs. \n\nThis unique dataset is the foundation for a new metric (CALSCORE, \u00a7 4). In contrast to other calibration evaluation methods that only calculate aggregate calibration over the entire dataset, GRACE also facilitates per-instance evaluation, which helps in identifying specific contexts where models are much worse than humans at avoiding confidently incorrect answers.",
            "score": 0.41045944639441756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 101,
                    "matchedPaperCorpusId": "210154117"
                },
                {
                    "start": 200,
                    "end": 215,
                    "matchedPaperCorpusId": "198952481"
                },
                {
                    "start": 215,
                    "end": 233,
                    "matchedPaperCorpusId": "275819678"
                },
                {
                    "start": 272,
                    "end": 293,
                    "matchedPaperCorpusId": "263609662"
                },
                {
                    "start": 293,
                    "end": 327,
                    "matchedPaperCorpusId": "253510101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "248779946",
            "title": "Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference",
            "text": "It's well-known that accuracy will drop in domain transfer. But how about JSD and ECE? Is calibration an intrinsic property of the model which is independent of evaluation benchmark? Can knowledge of linguistic ambiguity transferred across domains? Can the property of calibration learned from general domain transfer to the biomedical? To observe how accuracy, JSD and ECE varies in domain transfer, we first evaluate AmbiSNLI, AmbiMNLI and MedNLI test set, under three NLI models: snli, mnli and smnli trained using SNLI, MNLI training set and their combinations resp. Evaluation of snli model on AmbiMNLI is across textual genres, same as mnli model on AmbiSNLI. All models on MedNLI is across-domain, contrasting with in-domain evaluation -snli and smnli on AmbiSNLI, mnli and smnli on AmbiMNLI. \n\nMetrics of JSD and ECE are data-dependent as accuracy. Table 7, 8 show that they become larger in textual genre and domain transfer. In other words, the model is perfectly-calibrated on benchmark A, but it may poorly-calibrated in other benchmarks that are distantly-distributed from its training data. Calibration is not an intrinsic property of the model, but varies according to data. \n\nThe knowledge of linguistic ambiguity learned from general-purpose domain can be transferred to the medical. In middle of",
            "score": 0.41045944639441756,
            "section_title": "Domain Transfer",
            "char_start_offset": 27177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83251953125
        },
        {
            "corpus_id": "272368309",
            "title": "Does Alignment Tuning Really Break LLMs' Internal Confidence?",
            "text": "Large Language Models (LLMs) have shown remarkable progress, but their real-world application necessitates reliable calibration. This study conducts a comprehensive analysis of calibration degradation of LLMs across four dimensions: models, calibration metrics, tasks, and confidence extraction methods. Initial analysis showed that the relationship between alignment and calibration is not always a trade-off, but under stricter analysis conditions, we found the alignment process consistently harms calibration. This highlights the need for (1) a careful approach when measuring model confidences and calibration errors and (2) future research into algorithms that can help LLMs to achieve both instruction-following and calibration without sacrificing either.",
            "score": 0.41045944639441756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "268379757",
            "title": "Knowledge Conflicts for LLMs: A Survey",
            "text": "Fine-tuning.Elazar et al. (2021) propose a consistency loss function and train the language model with the combination of the consistency loss and standard MLM loss.Li et al. (2023d) utilize one language model in dual capacities: as a generator to produce responses and as a validator to evaluate the accuracy of these responses.The process involves querying the generator for a response, which is subsequently assessed by the validator for accuracy.Only those pairs of responses deemed consistent are retained.This subset of consistent pairs is then used to fine-tune the model, aiming to increase the generation likelihood of consistent response pairs.Plug-in.Jang and Lukasiewicz (2023) leverage the technique of intermediate training, utilizing word-definition pairs from dictionaries to retrain language models and improve their comprehension of symbolic meanings.Subsequently, they propose an efficient parameter integration approach, which amalgamates these enhanced parameters with those of existing language models.This method aims to rectify the models' inconsistent behavior by bolstering their capacity to understand meanings.Output Ensemble.Mitchell et al. (2022) mitigate the inconsistency of language models by leveraging a two-model architecture, involving the utilization of a base model responsible for generating a set of potential answers, followed by a relation model that evaluates the logical coherence among these answers.The final answer is selected by considering both the base model's and the relation model's beliefs.Zhao et al. (2023b) introduce a method to detect whether a question may cause inconsistency for LLMs.Specifically, they first use LLMs to rephrase the original question and obtain corresponding answers.They then cluster these answers and examine the divergence.The detection is determined based on the divergence level.",
            "score": 0.41045944639441756,
            "section_title": "Improving Consistency",
            "char_start_offset": 37083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 12,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1154
                },
                {
                    "start": 1154,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1747
                },
                {
                    "start": 1747,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 32,
                    "matchedPaperCorpusId": "231740560"
                },
                {
                    "start": 165,
                    "end": 182,
                    "matchedPaperCorpusId": "263609159"
                },
                {
                    "start": 1545,
                    "end": 1564,
                    "matchedPaperCorpusId": "264555682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "267412346",
            "title": "Calibration and Correctness of Language Models for Code",
            "text": "Li et al. [49] investigate the calibration of Computer vision (CV) models from an operational perspective i.e., the shift between training input and production inputs, presenting it as a software quality problem that can be addressed using Bayesian approaches. Minderer et al. [45] evaluate the calibration of at the time, state of the art CV models and find improved calibration with more recent models, notably those not using convolutions. Park et al. [46] study the effect of the mixup technique [62] on calibration in a natural language understanding (NLU) setting using older generation models (BERT and RoBERTa). Chen et al. [47] investigate the calibration of pretrained language models on various NLP tasks, also using older generation models (RoBERTa and T5). Bommasani et al. [48] introduce the HELM benchmark, which includes calibration as one of its seven metrics to evaluate language models in a natural language context. Huang et al. [63] explored LM uncertainty with a range of techniques and tasks, including both NLP and function synthesis tasks. They evaluated using correlation measures, rather than focusing on calibration. They explore interesting sample-based and perturbation techniques which could be explored more for calibration on diverse SE tasks. Other work [64] has explored training an ML model that sees code and execution results to estimate correctness probabilities for solution reranking. For natural language question answering tasks, work has explored improving calibration by training a model to adjust token logits [53], and training a model from LLM hidden states specifically around the ECE metric [65]. \n\nWhen suitably prompted, Kadavath et al. [18] found that LLMs can output well-calibrated scores on whether their own answers are correct or not, viz., larger models \"know what they know\". While this work did investigate some function synthesis tasks (HUMANEVAL & an unpublished Python function dataset), they did so using only their private models, and ultimately focused on natural language tasks.",
            "score": 0.41045944639441756,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 46230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "203838412"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "256900680"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "264811211"
                },
                {
                    "start": 1641,
                    "end": 1645,
                    "matchedPaperCorpusId": "270620078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "218487046",
            "title": "On the Inference Calibration of Neural Machine Translation",
            "text": "In this section, we investigate the linguistic properties of miscalibrated tokens in NMT outputs. We explore the following five types of properties: frequency, position, fertility, syntactic roles, and word granularity. \n\nFrequency is generally related to miscalibration; position, fertility, and word granularity are three factors associated with structured prediction; syntactic roles or linguistic roles may vary across language pairs. The results in this section are reported on the held-out set by the retrained model.",
            "score": 0.41045944639441756,
            "section_title": "Linguistic Properties of Miscalibration",
            "char_start_offset": 14642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 219
                },
                {
                    "start": 222,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 523
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "273963883",
            "title": "Model Editing for LLMs4Code: How Far are We?",
            "text": "To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we first build an evaluation benchmark, Code Large Language Models Editing Evaluation (CLMEEval), which consists of two datasets, CoNaLa-Edit (CNLE) and CodeSearchNet-Edit (CSNE), corresponding to the editing of code knowledge in the context of two widely-studied software engineering tasks: a natural language to programming language (NL2PL) code generation task and a programming language to natural language (PL2NL) code summarization task. Drawing from this benchmark, we employ model editing techniques to rectify the inaccuracies produced by LLMs4Code, mirroring real-world scenarios where updates to code knowledge within a model are necessary. This could involve situations like changes in required APIs for completing specific coding tasks or shifts in the primary functionality of a method due to code changes. Following the common practice in the model editing domain [25], our study evaluates the approaches from four dimensions: Effectiveness, the success rate on editing instances; Generalization, the success rate on tests that are semantically identical to the editing instances; Specificity, the success rate on tests unrelated to the editing instances; and Fluency, the fluency of the contents generated by the model [25]. We select six state-of-the-art model editing approaches from the three categories mentioned above and three widely-used LLMs4Code, i.e., CodeLlama (7B) [26], CodeQwen-1.5 (7B) [27], and StableCode (3B) [28] as our study subjects. Through an extensive evaluation, our study makes the following important findings: F1: The External Memorization-based technique, GRACE, can consistently achieve the optimal effectiveness and specificity across different datasets and LLMs4Code. Nonetheless, all the existing model editing techniques perform poorly in terms of generalization. F2: Most model editing techniques perform comparatively poorly on LLMs4Code, being far less proficient compared with editing general LLMs.",
            "score": 0.4101860727674653,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1399,
                    "end": 1403,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "271769387",
            "title": "Benchmarking LLaMA-3 on Arabic Language Generation Tasks",
            "text": "We observe that LLaMA-3-70B's performance does not necessarily increase as with the increased number of shots. This is because the improvement with few-shot learning is model-and task-dependent, and often sensitive to the order of the shots  et al., 2023). Therefore, we recommend that these models not be used in applications without careful prior consideration of potential misuse and bias. \n\ntween languages like Russian and Arabic on model performance. Additionally, integrating retrievalaugmented generation with models like Llama-2 (Touvron et al., 2023b) has shown substantial improvements in complex, domain-specific QA settings (Alawwad et al., 2024). Domain-specificity also plays a crucial role in enhancing QA systems, as evidenced by Yagnik et al. (2024), who evaluate the performance of both general and medical-specific distilled models in medical QA. Their work highlights the significant benefits of domain-specific fine-tuning in improving model accuracy. Complementing these domainspecific approaches, Wang et al. (2023a) introduce the QA-Eval task and the EVOUNA dataset, designed to advance the evaluation of Open-QA systems. Their research underscores the ongoing challenges by comparing AI-generated answers with standard responses and highlights potential improvements in automatic evaluation methods. In the broader scope of open-domain question answering, Zheng et al. critically assess ChatGPT's performance, revealing its capabilities in handling complex user queries where no context is provided. Their analysis demonstrates the model's impressive results and proposes methods to enhance the faithfulness of its answers, addressing key areas where ChatGPT may falter.",
            "score": 0.40956850396695055,
            "section_title": "Prompt",
            "char_start_offset": 18890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 1021,
                    "end": 1040,
                    "matchedPaperCorpusId": "259951284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.335693359375
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Confidence calibration is one such reliability metric that measures whether the model's prediction probability estimates are aligned with the actual probability of the answer being correct. Confidence calibration has been studied in Computer Vision (Guo et al., 2017;Minderer et al., 2021) and Natural Language Processing (Desai and Durrett, 2020;Dan and Roth, 2021). However, most of the prior works are limited to classification settings, which is inapplicable to the generality of the QA task. Recently, Zhang et al. (2021); Jiang et al. (2021) have shown that state-of-the-art English QA models are surprisingly poorly calibrated. However, there remains a gap in understanding of the calibration properties of multilingual QA models. In this paper, we address this gap by a comprehensive study on the Calibration of Multilingual Question Answering Large Language Models. The main research questions we investigate in this paper are: \n\n1) How well are MLLMs calibrated in the crosslingual transfer scenario? \n\n2) How can we improve MLLMs' confidence calibration on multilingual QA datasets? The contributions of our work are as follows: \n\n\u2022 We provide the first comprehensive benchmarking of confidence calibration of multilingual QA models (architectures including extractive models: mBERT, XLM-R and generative models: mT5, mBART, and LLaMa2) over both low-and high-resource languages, in-distribution and out-ofdistribution settings. \n\n\u2022 We observe that the calibration performance on English is not transferable to other languages, across various datasets and architectures. Distance between the target languages and English, and the distribution of different languages at the pretraining stage, are all highly correlated with calibration performance, across the various model types. \n\n\u2022 An investigation of various calibration strategies including post-hoc methods and regularization methods, aimed at enhancing cross-lingual calibration. Temperature scaling (optimized over a cross-lingual validation dataset) shows the most significant improvement even if the target language is absent in the validation data.",
            "score": 0.40880054239121943,
            "section_title": "Introduction",
            "char_start_offset": 1894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 267,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 322,
                    "end": 347,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 347,
                    "end": 366,
                    "matchedPaperCorpusId": "244119588"
                },
                {
                    "start": 528,
                    "end": 547,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "248779946",
            "title": "Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference",
            "text": "The predictive probability of a perfectly-calibrated model can reflect the true correctness likelihood, i.e. empirical accuracy is equal to the prediction confidence. Empirical accuracy is obtained from observations across the human judges. That is, predictive confidence (uncertainty) can represent the human judgment distribution when model is calibrated. To this end, not only label-smoothing, but other recalibration approaches such as temperature scaling can reach the same goal. Our experiments confirmed our hypothesis, when model is calibrated, it can obtain competitive ChaosNLI divergence scores and bring accuracy boost. \n\nOur contributions are two folds: (1) We propose the hypothesis: a well-calibrated network can naturally capture linguistic ambiguity, regardless of using special resource. It reasonably explains the success of training with ambiguity labels, and converts question of \"how to capture human disagreement distribution?\" to a more general one \"how to train a calibrated model?\" Our experiments confirm that commonly-used re-calibration methods are as effective as targeting at ambiguous annotations. (2) Knowledge of linguistic ambiguity learned from the general domain benefits biomedical domain as well, which suggests ambiguity signals can be transferred across domains. But calibration is not an intrinsic property of a model, it's data-dependent.",
            "score": 0.40844465729867296,
            "section_title": "Premise",
            "char_start_offset": 3049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "222141853",
            "title": "Frustratingly Simple Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning",
            "text": "The results for one-shot NER and five-shot NER are summarized in Table 2 and Table 3 respectively. 8 https://huggingface.co/ As shown, our NNShot and STRUCTSHOT perform significantly better than all previous methods across all evaluation settings. By modeling label dependencies with a simple Viterbi decoder, STRUCT-SHOT boosts the performance of NNShot by 2.4% and 4% F1 scores on five-shot tag set extension and domain transfer tasks on average respectively. These performance gains are greater than the ones obtained by joint CRF training with the prototypical network (PrototypicalNet+P&D), suggesting that independently modeling transition and emission scores is a cheap but effective way to capture label dependencies. STRUCTSHOT achieves new SOTA results on the two few-shot NER tasks, outperforming the previous SOTA system (Prototypi-calNet+P&D) by 6% to 9% F1 score on one-shot setting and 11% to 16% F1 score on five-shot setting.\n\nBiLSTM vs. BERT as token embedder The BERT-based systems considerably outperform BiLSTM-based systems on few-shot NER. Language model pre-training is critical for low-resource natural language processing including few-show transfer learning (Cherry et al., 2019). However, task-specific knowledge is usually more important than the general information learned via unsupervised training. For example, the topperforming BiLSTM-based systems can beat Sim-BERT by up to 15% F1 score on some few-shot NER settings. With fine-tuning on the OntoNotes data, NNShot outperforms SimBERT by 20% to 35% F1 scores across different settings, demonstrating the effectiveness of injecting task-specific information into pre-trained language models.\n\nTag set extension vs. Domain transfer The one-shot NER systems generally perform better on domain transfer than on tag set extension, while the five-shot systems work better on the tag set extension task. On the domain transfer task, the source entity classes overlap with some entity classes in the target domain, which benefits NER systems built under the extremely low-resource condition. However, in general, domain transfer is",
            "score": 0.4082805200264452,
            "section_title": "Results",
            "char_start_offset": 18094,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "272987039",
            "title": "Surveying the MLLM Landscape: A Meta-Review of Current Surveys",
            "text": "Feng et al. [67] provides a thorough overview of how Large Language Models (LLMs) are being integrated with external knowledge to enhance their capabilities. The survey categorizes the integration methods into two main approaches: knowledge editing and retrieval augmentation. Knowledge editing involves modifying the input or the model itself to update the outdated or incorrect information, while retrieval augmentation fetches external information during inference without altering the model's core parameters. The authors present a taxonomy covering these methods, benchmarks for evaluation, and applications such as LangChain and ChatDoctor, which leverage these strategies to address domain-specific challenges. Additionally, the paper explores the handling of knowledge conflicts and suggests future research directions for improving LLM performance in complex, real-world tasks through better integration of multi-source knowledge. \n\nShi et al. [68] offer an extensive survey on the continual learning (CL) of Large Language Models (LLMs), addressing the critical challenges and methodologies in this field. presents an extensive overview of the challenges and techniques related to the continual learning (CL) of Large Language Models (LLMs). The authors focus on two primary directions of continuity: vertical continual learning (adapting models from general to specific domains) and horizontal continual learning (adapting models over time across various domains). They discuss the problems of \"catastrophic forgetting,\" where models lose knowledge of previous tasks, and the complexity of continually updating models to maintain performance on both old and new tasks. The survey outlines key CL methods, including continual pre-training, domain-adaptive pre-training, and continual fine-tuning. It also evaluates various CL techniques, such as replaybased, regularization-based, and architecture-based methods, to mitigate forgetting and ensure knowledge retention. \n\nThe authors call for more research into evaluation benchmarks and methodologies to counter forgetting and support knowledge transfer in LLMs, making this an underexplored yet crucial area of machine learning research",
            "score": 0.4079733987584626,
            "section_title": "Contunual Learning",
            "char_start_offset": 30568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "254366308",
            "title": "Towards using Few-Shot Prompt Learning for Automating Model Completion",
            "text": "We proposed a novel approach based on few-shot prompt learning to enable large language models to solve completion tasks in modeling activities. We reformulate model completion as a semantic mapping problem that consists, firstly, in transforming modeling formalism elements into meaningful patterns of sequences of tokens to create prompts with learning shots. Then, we exploit the ability of LLMs to complete partial sequences following the specified patterns to recover elements that can be used for the completion. Those elements are transformed into constructs conforming to the modeling language syntax and suggested to the modelers. Although many research contributions were proposed to solve model completion problems, we do believe that none of them can be effectively used in a real setting, because of the resources needed, i.e., large training datasets, and the limited performance they offer. We do believe, however, that our approach can be effective when modeling both static and dynamic diagrams for two main reasons. Firstly, it does not require to pre-train or fine tune language models on specific tasks or domain. Secondly, the used LLMs are trained on a huge volume of data, which makes it generalizable to many domains and different concept natures and relationships.\n\nAlthough our approach shows promising results, it is still a first attempt and there is room for improvement. Indeed, when defining a prompt, the elements of the already-defined partial diagrams have a great influence on the accuracy of the suggested token. A calibration study is still necessary to determine the boundaries of the provided existing context to have the best suggestions. For example, when we added systematically the package name in the pattern, the results improved considerably, but we cannot determine whether this observation is valid only on the used benchmark. Another consideration that has to be studied is the use of non-natural language elements such as symbols and digits. In our experiments, their existence generated poor results as these elements are rarely present in the data used for training LLM. We believe that a more sophisticated mapping of those elements would considerably improve the results.\n\nUsing LLMs proves to be efficient in modeling formalism that rely heavily on natural language identifiers. However, other modeling languages such as Petri nets are definitely difficult to handle as they involve modeling elements that cannot be captured by LLMs.\n\nIn our approach, we utilize the advanced capabilities of GPT-3 by OpenAI, a state-",
            "score": 0.40760372888992835,
            "section_title": "IV. DISCUSSION",
            "char_start_offset": 17415,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "258331833",
            "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
            "text": "Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output. \n\nRobustness and Calibration. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59]. \n\nThe models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116]. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43]. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51]. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations. However, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed. \n\nFairness and Bias. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17]. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race [59]. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23]. \n\nSpurious Biases.",
            "score": 0.4074932128266292,
            "section_title": "Trustworthiness",
            "char_start_offset": 48313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 501,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "53622943"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "3298854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6044921875
        },
        {
            "corpus_id": "273963769",
            "title": "Epistemic Integrity in Large Language Models",
            "text": "In this work, we introduced the problem of epistemic calibration for LLMs: ensuring that the confidence expressed in a model's communication aligns with its underlying reliability. We argued that this normative ideal is critical for LLMs to serve as robust and responsible information sources. Through a decomposition of the problem into external and internal certainty, we developed a framework for understanding and evaluating epistemic calibration in LLMs. Using our new approach that greatly improves fidelity of assertiveness measurements compared to prior models, our empirical investigation of a state-of-the-art modeleveals significant gaps between the model's internal confidence estimates and the assertiveness of its generated language. This miscalibration poses risks to users, who may be misled by overconfident model outputs. \n\nOur work also highlights the need for further research to fully understand and address the challenges of epistemic calibration. One key direction is developing new training and inference techniques to improve the alignment between LLMs' probability estimates and their linguistic expression of confidence. Another is studying the downstream impacts of epistemic miscalibration on user trust, decision making, and information ecosystems, through a combination of user studies and large-scale simulations. We believe that the epistemic calibration framework introduced in this paper provides a valuable foundation for these future efforts. We discuss further applications, to RLHF, silicon sampling, and debate, in Appendix I. Ultimately, achieving epistemic calibration in language models is not just a technical challenge, but a societal imperative. As these models become ever more integrated into our information-seeking and decision-making practices, ensuring that they express confidence in a calibrated and responsible way is essential for mitigating the risks of misinformation, confusion, and unwarranted trust. Figure 4: Our method's predicted assertiveness score is reasonably well aligned with the human scores, while the model's (internal) certainty is not.",
            "score": 0.4072289797977599,
            "section_title": "DISCUSSION AND CONCLUSION",
            "char_start_offset": 27192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "265456905",
            "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
            "text": "Such consistent improvements across multiple metrics and settings confirm that RAG serves as a significant enhancement to both pretrained and instruction-tuned models. \n\nPrompt-based tool learning helps significantly on the time-sensitive task. As discerned from Table 2, the smaller models, Llama2-7B and Llama2-7B-chat, without the integration of an external tool, exhibit negligible accuracy in handling stock queries. Conversely, the application of the prompt-based tool learning leads to a transformative elevation in performance, with Llama2-7B+tool and Llama2-7B-chat+tool achieving remarkable accuracies of 100.00% with only one training example. This enhancement in reliability and precision is critical, especially within the rapidly evolving landscape of financial domains. It's notable that the sophisticated models, GPT3.5-turbo and GPT4, refrain from addressing stock price queries in a zero-shot setting. However, when augmented with prompt-based tool tool learning, these models are adeptly optimized to provide accurate answers. The integration of zero-shot and few-shot tool learning thus emerges as an effective strategy bridging the knowledge gap and enhancing the reliability of language models, especially in the dynamic domain of finance. \n\nFew-shot learning better improves the ability to follow the question-answering format than factuality. As shown in Table 1, we observe that the pretrained Llama2-7B and Llama2-13B models improve significantly in their question-answering capabilities under few-shot learning, compared to their zero-shot performance. However, this improvement does not markedly surpass the performance of their zero-shot, instruction-tuned counterparts, Llama2-7B-chat and Llama2-13B-chat. Furthermore, for the chat variants, few-shot learning's impact is more limited, yielding only modest improvements. This trend suggests that while few-shot learning aids in adapting to question formats, its role in enhancing factual precision is less substantial. \n\nDoLa has limitations in enhancing models with knowledge gaps in training data. The DoLa decoding method is designed to enhance the factual accuracy of LMs by contrasting the outputs from different layers of the model.",
            "score": 0.40699023934702516,
            "section_title": "Quantifying Hallucinations",
            "char_start_offset": 16582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "247594392",
            "title": "Cluster & Tune: Boost Cold Start Performance in Text Classification",
            "text": "(Lv et al., 2020;Wang et al., 2019a;Pruksachatkun et al., 2020). All these works aim to improve the performance upon transfer, making it more suitable for any new domain. In contrast, we focus on improvement given the domain.\n\nWith a transferred model, one can further improve performance with domain-specific information. For example, utilizing metadata (Melamud et al., 2019), training on weakly-supervised data (Raisi and Huang, 2018;Meng et al., 2020) or multitasking on related tasks concurrently (Liu et al., 2019a). Given no domain-specific information, it was suggested to further pretrain on unlabeled data from the domain (Whang et al., 2019;Sung et al., 2019;Rietzler et al., 2020;Lee et al., 2020;Gururangan et al., 2020). This, however, is sometimes unhelpful or even hurts results (Pan, 2019).\n\nTransferring a model and retraining with paucity of labels is often termed few-shot learning. Few shot learning is used for many language-related tasks such as named entity recognition (Wang et al., 2020b), relation classification (Hui et al., 2020), and parsing (Schuster et al., 2019). There have also been suggestions other than fine-tuning the model. Koch (2015) suggests ranking examples' similarity with Siamese networks. Vinyals et al. (2016) rely on memory and attention to find neighboring examples and Snell et al. (2017) search for prototypes to compare to. Ravi and Larochelle (2017) don't define in advance how to compare the examples. Instead, they meta-learn how to train the few shot learner. These works addressed the image classification domain, but they supply general methods which are used, improved and adapted on language domains (Geng et al., 2019;Yu et al., 2018).\n\nIn conclusion, separate successful practices foreshadow our findings: Clustering drives pretraining on images; supervised classification aids pre-training; and training on unlabeled domain examples is helpful with MLM.",
            "score": 0.40683532516593146,
            "section_title": "Related Work",
            "char_start_offset": 23940,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 377,
                    "matchedPaperCorpusId": "202766917"
                },
                {
                    "start": 414,
                    "end": 437,
                    "matchedPaperCorpusId": "46945333"
                },
                {
                    "start": 652,
                    "end": 670,
                    "matchedPaperCorpusId": "202763579"
                },
                {
                    "start": 795,
                    "end": 806,
                    "matchedPaperCorpusId": "204785115"
                },
                {
                    "start": 1040,
                    "end": 1058,
                    "matchedPaperCorpusId": "219971238"
                },
                {
                    "start": 1072,
                    "end": 1095,
                    "matchedPaperCorpusId": "67856005"
                },
                {
                    "start": 1237,
                    "end": 1258,
                    "matchedPaperCorpusId": "8909022"
                },
                {
                    "start": 1321,
                    "end": 1340,
                    "matchedPaperCorpusId": "309759"
                },
                {
                    "start": 1378,
                    "end": 1404,
                    "matchedPaperCorpusId": "67413369"
                },
                {
                    "start": 1662,
                    "end": 1681,
                    "matchedPaperCorpusId": "202776447"
                },
                {
                    "start": 1681,
                    "end": 1697,
                    "matchedPaperCorpusId": "29162291"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "215744839",
            "title": "Meta-Learning in Neural Networks: A Survey",
            "text": "Language Modelling. Few-shot language modelling has been a popular way to showcase the versatility of metalearners, with early methods like matching networks showing impressive performances on one-shot tasks such as filling in missing words [86]. Many more tasks have since been tackled, including neural program induction [261] and synthesis [262], English to SQL program synthesis [263], text-based relationship graph extractor [264], machine translation [265], and quickly adapting to new personas in dialogue tasks [266]. Speech Recognition Deep learning is now established as the dominant paradigm for state of the art automatic speech recognition (ASR). Meta-learning is beginning to be applied to address the many few-shot adaptation problems that arise within ASR including learning how to train for low-resource languages [267], cross-accent adaptation [268] and optimising models for individual speakers [269].",
            "score": 0.40675708098089947,
            "section_title": "Language and Speech",
            "char_start_offset": 95387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 920
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 328,
                    "matchedPaperCorpusId": "19504559"
                },
                {
                    "start": 457,
                    "end": 462,
                    "matchedPaperCorpusId": "52100101"
                },
                {
                    "start": 831,
                    "end": 836,
                    "matchedPaperCorpusId": "204904570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "264811211",
            "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
            "text": "A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding<2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs.",
            "score": 0.4064586197702927,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "269921283",
            "title": "Data Contamination Calibration for Black-box LLMs",
            "text": "Baseline Methods: We selected six popular methods to evaluate our approach: four calibrationbased and two calibration-free.Calibrationbased methods include: the Neighborhood attack (Neighbor) (Mattern et al., 2023), which assesses loss differences between original samples and their neighbors generated by masked language models; and perplexity-based calibration (Carlini et al., 2021) techniques utilizing Zlib entropy (Zlib) (Gailly and Adler, 2004), lowercased sample perplexity (Lower), and comparisons with reference models trained on the same dataset (Ref).Calibration-free methods comprise the Min-K% method (Shi et al., 2023), predicting pre-trained samples through low-probability outlier words; and the Loss Attack (Yeom et al., 2018), substituting loss with Perplexity (PPL) in LLMs.\n\nDatasets and Metric.We utilize the StackMI-Asub benchmark (Section 3.2) and the WikiMIA dataset proposed by (Shi et al., 2023).WikiMIA (Appendix B) leverages Wikipedia timestamps and model release dates to identify member and nonmember data sets, applicable for LLMs trained up to 2023.Both datasets are transformed into two formats as the guidelines in Section 3.2: the original format (ori) and the synonym rewritten format (syn).\n\nFor evaluation, we follow (Mattern et al., 2023;Carlini et al., 2022;Watson et al., 2021) and plot the ROC curve analysis method.To facilitate numerical comparison, we primarily use the AUC score (Area Under the ROC Curve).The AUC score (Appendix F), independent of any specific threshold, accurately gauges the method's ability to differentiate between members and non-members.It also eliminates bias from threshold selection.\n\nModels.We conduct experiments against 10 commonly used LLMs.",
            "score": 0.4064586197702927,
            "section_title": "Experiments Settings",
            "char_start_offset": 20257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 794
                },
                {
                    "start": 796,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1228
                },
                {
                    "start": 1230,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1657
                },
                {
                    "start": 1659,
                    "end": 1666
                },
                {
                    "start": 1666,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 385,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 615,
                    "end": 633,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 725,
                    "end": 744,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 904,
                    "end": 922,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 1278,
                    "end": 1299,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1299,
                    "end": 1319,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "238856959",
            "title": "Can Explanations Be Useful for Calibrating Black Box Models?",
            "text": "Throughout this work, we have assumed a black box model that cannot be fine-tuned on a new domain. In this section, we compare calibration-based approaches with glass-box methods that require access to the model architectures and parameters. We evaluate two glass-box methods in two different settings ( Figure 2) On QA tasks, the limited training data is not sufficient for successfully finetuning a ROBERTA model. Consequently, FINETUNE ROBERTA does not achieve credible performance. Finetuning a base QA model greatly improves the performance, surpassing LIMECAL on SQUAD-ADV and HOT-POTQA. However, we still find that on TRIVIAQA, LIMECAL slightly outperforms ADAPT. This is a surprising result, and shows that explanation-based calibrators can still be beneficial in some scenarios, even if we have full access to the model.\n\nOn NLI tasks that are substantially easier than QA, finetuning either a ROBERTA LM model or a base NLI model can reach an accuracy of roughly 80%. Our explanation-based approach largely lags glass-box methods, likely because the base NLI model utterly fails on QNLI (50.5% accuracy) and MRPC (55.0% accuracy) and does not grant much support for the two tasks. Nonetheless, the results on NLI still support our main hypothesis: explanations can be useful for calibration.",
            "score": 0.4064586197702927,
            "section_title": "Comparison to Finetuned Models",
            "char_start_offset": 21440,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396728515625
        },
        {
            "corpus_id": "267682759",
            "title": "Addressing the Binning Problem in Calibration Assessment through Scalar Annotations",
            "text": "With recently released large-scale language models (LLMs) demonstrating impressive few-shot, zero-shot, and task-agnostic performance (Brown et al., 2020;Kojima et al., 2022;Ouyang et al., 2022), there is a boom of interest in deploying NLP-based systems to aid various human decision making (Chen et al., 2021;Nori et al., 2023). However, the black-box nature of LLMs gives little insight into how the predictions are made by these models (Zhao et al., 2021), risking user trust in model prediction reliability. \n\nA common proposal to address this concern is to explore model calibration (Guo et al., 2017;Kull et al., 2019), which requires a model to approximately predict the true label distribution. This evaluation has been adopted by many recent language model benchmarking efforts (Desai and Durrett, 2020;Hendrycks et al., 2020;Jiang et al., 2022;OpenAI, 2023); these works often consider confidence calibration for classification and adopt Expected Calibration Error (ECE) (Guo et al., 2017) as the main empirical evaluation metric. ECE, along with variants like Adaptive Calibration Error (ACE) (Nixon et al., 2019), involve binning in their calculation, which groups hard categorical labels into bins to approximate label distributions. This is mainly because many popular NLP tasks are annotated predominantly with categorical labels. However, these empirical evaluations are sensitive to the choice of binning schemes (Nixon et al., 2019), and can severely underestimate calibration error (Ovadia et al., 2019;Kumar et al., 2019;Baan et al., 2022). \n\nInstance-level calibration (Zhao et al., 2020) avoids the binning issue and matches model confidence with human annotations at an individual level, as uncertainty from human annotations is a good surrogate for true label distribution (Nie et al., 2020b;Baan et al., 2022).",
            "score": 0.4064586197702927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 194,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 440,
                    "end": 459,
                    "matchedPaperCorpusId": "235828718"
                },
                {
                    "start": 589,
                    "end": 607,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 607,
                    "end": 625,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 788,
                    "end": 813,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "250551977"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1105,
                    "end": 1125,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1431,
                    "end": 1451,
                    "matchedPaperCorpusId": "102486060"
                },
                {
                    "start": 1523,
                    "end": 1542,
                    "matchedPaperCorpusId": "202718866"
                },
                {
                    "start": 1542,
                    "end": 1560,
                    "matchedPaperCorpusId": "253224378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "This, in turn, makes teacher models more robust, helping the student models to be more accurate and produce better-calibrated predictions. \n\nWhile mixup is making significant inroads in a broad range of tasks ranging from computer vision (Zhang et al., 2018;Thulasidasan et al., 2019;Carratino et al., 2020;Wang et al., 2020a) to natural language processing (Guo et al., 2019;Guo, 2020;Chen et al., 2020;Yin et al., 2021;Kong et al., 2020;Liang et al., 2021), there has hitherto been a limited number of works focusing on its effectiveness on model calibration specifically in NLP (Kong et al., 2020;Park and Caragea, 2022). With that caveats, what is not yet studied is using mixup for calibrating the student model predictions on the knowledge distillation setting; that is what this paper focuses on. \n\nIn this paper, we study, for the first time to our knowledge, the impact of the mixup data augmentation technique on the distillation objective and propose a simple yet effective mixup strategy that is informed by training dynamics (Swayamdipta et al., 2020) for calibrating the student models. To this end, we first characterize data instances based on their contributions to the model's learning, which yields distinct regions in the data, presenting easy-to-learn, ambiguous, or hard-to-learn instances. Then, we generate mixup samples by interpolating easy-to-learn with ambiguous samples as a regularization technique to promote generalization to both in-domain (ID) and out-of-domain (OOD) test sets and improve the student model calibration. While ambiguous/hard-to-learn instances are intuitively the most challenging yet informative for learning, easy-to-learn instances are essential for convergence (Swayamdipta et al., 2020). Therefore, interpolating samples from different regions (e.g., easy-to-learn with ambiguous) in the teacher model can potentially result in a better-calibrated student model with improved ID and OOD perfor-mance.",
            "score": 0.4064586197702927,
            "section_title": "Introduction",
            "char_start_offset": 3216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 258,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 258,
                    "end": 284,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 307,
                    "end": 326,
                    "matchedPaperCorpusId": "214727875"
                },
                {
                    "start": 376,
                    "end": 386,
                    "matchedPaperCorpusId": "212928508"
                },
                {
                    "start": 386,
                    "end": 404,
                    "matchedPaperCorpusId": "216553182"
                },
                {
                    "start": 404,
                    "end": 421,
                    "matchedPaperCorpusId": "236477688"
                },
                {
                    "start": 439,
                    "end": 458,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 1038,
                    "end": 1064,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1716,
                    "end": 1742,
                    "matchedPaperCorpusId": "221856637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "267740621",
            "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "text": "The calibration promotes a fair starting point for LMs while preserving language modeling abilities. \n\n\u2022 Extensive experiments on eight classification datasets with four prompt-based learning approaches show that our method significantly improves LMs' zero/few-shot performance, and outperforms output-calibration methods.",
            "score": 0.4064586197702927,
            "section_title": "Introduction",
            "char_start_offset": 5700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 103,
                    "end": 322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "In instruction tuning, the model is finetuned on a collection of datasets where the NLP tasks are described using natural language instructions. Natural language instructions are added to the prompt to let the model know which task to perform for a given input. For instance, to ask the model to perform a sentiment analysis task on a given input, instructions such as 'Classify this review either as negative, positive, or neutral' can be provided in the prompt. Various factors determine the effectiveness of instruction tuning on LLMs, such as the prompt format used, objectives used during finetuning, diversity of tuning tasks, distribution of datasets, etc. Additionally, the zero-shot task generalization of LLMs performs poorly across tasks. To address this, multitask finetuning (MTF) has emerged and become one of the promising techniques to improve the performance of LLMs in zero-shot settings. \n\nCreating instruction datasets for many tasks from scratch is a resource-intensive process. Instead, FLAN [44] expresses existing 62 NLP datasets in the instructional format. This transformed dataset with instructions is then used to finetune the model. For each dataset, 10 unique templates were created to describe the task in instructional format for that dataset. Based on the task type, the datasets were grouped into clusters, and then, to evaluate the performance on each task, the specific task cluster was held out while the remaining clusters were used during instruction tuning. \n\nFLAN demonstrated how instruction tuning substantially improved the zero-shot performance on held-out tasks that were not part of the instruction tuning process and also helped the model generalize well on unseen tasks. FLAN outperformed GPT-3 (zero-and few-shot) on 20 of the 25 datasets used for evaluation. It was observed that the instruction tuning approach is more effective for tasks such as QA, NLI, and translation that can easily be verbalized as instructions. Instruction tuning is less effective for tasks where the instructions are redundant since they can be formulated simply as language modeling tasks, such as commonsense reasoning. FLAN also demonstrated how instruction tuning can hurt smaller models since their capacity is mostly exhausted in learning different instruction tasks.",
            "score": 0.40603639529917657,
            "section_title": "Instruction Tuning",
            "char_start_offset": 65950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "227247952",
            "title": "Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains",
            "text": "datasets to acquire the metaknowledge across domains by incorporating the domain corruption loss. For each domain, the student model learns to solve the task over a domain-specific dataset with guidance from the meta-teacher. To improve the student's distillation ability, the meta-distillation module minimizes the distillation loss from both intermediate layers, output layers and transferable knowledge, combined with an instance-specific domain-expertise weighting technique.\n\nTo verify the effectiveness of Meta-KD, we conduct extensive experiments on two NLP tasks across multiple domains, namely natural language inference (Williams, Nangia, and Bowman 2018) and sentiment analysis (Blitzer, Dredze, and Pereira 2007). Experimental results show the effectiveness and superiority of the Meta-KD framework. Moreover, we find our method is well performed especially when the in-domain dataset is very small while distillation, i.e. the few-shot learning setting, or there is no in-domain dataset, i.e. the zero-shot learning setting.\n\nIn summary, the contributions of this study are:\n\n\u2022 To our knowledge, our work is the first to explore metalearning based algorithms for PLM compression. \u2022 We propose a novel Meta-KD framework to distill knowledge in PLMs across domains, which consists of two major stages: meta-teacher learning and meta-distillation. \u2022 We conduct extensive experiments to demonstrate the superiority of Meta-KD and also explore the capability of this framework in both few-shot and zero-shot learning settings. We will release our source code on Github.",
            "score": 0.4059740108621581,
            "section_title": "Introduction",
            "char_start_offset": 3677,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 630,
                    "end": 665,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 689,
                    "end": 724,
                    "matchedPaperCorpusId": "14688775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "275920759",
            "title": "Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing",
            "text": "The evolution of large language models has been characterized by significant strides in their architecture, training paradigms, and applications [1]. Early transformer-based architectures were expanded through innovations in model size and the introduction of pretraining objectives designed to enhance linguistic coherence and contextual understanding [2]. Language models have demonstrated remarkable performance improvements through pretraining on diverse, large-scale corpora, leveraging unsupervised learning to acquire general-purpose language representations [3]. Techniques such as masked language modeling and autoregressive pretraining significantly enhanced the models' ability to generate coherent and contextually appropriate text across a variety of tasks [4]. Parallel advancements in computational infrastructure enabled training on exponentially larger datasets, achieving greater scalability and robustness in handling complex linguistic patterns [5,6]. Efforts to refine fine-tuning methods further improved task-specific performance, allowing models to excel in specialised applications through smaller, domain-focused datasets [7]. The incorporation of reinforcement learning from human feedback introduced an additional layer of refinement, ensuring alignment with human preferences and task-specific requirements [8]. Additionally, methods such as zero-shot and few-shot prompting showcased the models' ability to perform new tasks without task-specific training, significantly broadening their applicability [9]. Through architectural innovations such as sparse attention mechanisms, computational efficiency and inference speed were improved while preserving high performance [10,11]. The integration of multimodal capabilities also demonstrated the potential for large language models to process and generate information across text, images, and other modalities [12]. Such advancements demonstrate the versatility and scalability of large language models in addressing diverse challenges [13].",
            "score": 0.405523825040499,
            "section_title": "Advancements in Large Language Models",
            "char_start_offset": 3529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "267539494",
            "title": "Do Large Language Models Show Human-like Biases? Exploring Confidence - Competence Gap in AI",
            "text": "Ouyang et al. aligned language models by fine-tuning with a wide range of feedback [8]. Liang et al. presented a holistic evaluation of these models, where they validated 25 findings concerning different situations [9]. Schick et al. presented how language models can teach themselves [10]. Kraus et al. discussed how language models must be accurate and integrate their resources to deliver more precise responses [11]. Yogatama et al. analyzed the state of the art of natural language understanding and evaluated the task-independence of this knowledge [12]. They also assessed test data based on a metric to determine how quickly an existing model can learn new tasks. The study conducted by Acerbi and Stubbersfield examined if LLMs show biases, and they concluded that the presence of biases is widespread in model training data [13]. Our study focuses on designing test categories with different levels depending on the questions' complexity. We tested seven different language models and evaluated their responses. \n\nDrawing inspiration from human cognitive biases, Erik Jones and J. Steinhardt [14] studied the failures of LLMs, focusing on the need to detect inaccurate behaviors. Hongbin Ye et al.'s study on hallucinations in LLMs [15] aligns with our skepticism on LLM-generated outputs, although our work focuses primarily on confidence calibration. The above authors discussed the methods for detecting and improving hallucinations by providing a taxonomy of hallucinations. Furthermore, Ref. [16] investigated empathy in LLMs, highlighting the significance of social skills. Ranaldi and Giulia (2023) [17] focused on the susceptibility of language models to sycophantic responses, particularly when influenced by human prompts across diverse tasks. Their research highlighted that these models tend to be biased towards agreeableness, especially in scenarios involving subjective opinions or when confronted with statements that would typically warrant a response based on factual contradiction. This tendency underscores a lack of robustness in current language model designs. \n\nIn our study, we examine the confidence scores (self-assessment scores) before and after LLMs answer questions, which aligns with Jiaxin Huang et al.'s work [18], wherein the authors demonstrated the self-improving capabilities of LLMs.",
            "score": 0.40500505134984255,
            "section_title": "Related Literature",
            "char_start_offset": 4126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2092
                },
                {
                    "start": 2095,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "264517917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47705078125
        },
        {
            "corpus_id": "251196761",
            "title": "LAD: Language Models as Data for Zero-Shot Dialog",
            "text": "A long-standing goal of dialog research is to develop mechanisms for flexibly adapting dialog systems to new domains and tasks (Rastogi et al., 2020;Mosig et al., 2020). While the advent of large-scale pre-training (Devlin et al., 2018;Liu et al., 2019b;Zhang et al., 2019) has brought about significant progress in few-shot and zero-shot generalization across many different problems in Natural Language Processing (Brown et al., 2020;, zero-shot generalization in task-oriented dialog remains elusive. A likely reason for this discrepancy is that dialog models require significant data because they need to learn task-specific structural constraints, such as the domain ontology and the dialog policy. While large language models (e.g., GPT-3) exhibit strong language understanding and generation abilities (Brown et al., 2020), they have no a priori knowledge of the structural constraints implied by a specific (unseen) problem setting (e.g., relevant intents, dialog policy, etc.). As such, in order to adapt a pre-trained LM for task-oriented dialog, it is necessary to impose structural constraints on the unstructured Figure 1: Prompting must convey the structural constraints through a natural language prompt. In contrast, LAD uses large LMs to induce diversity in a synthetic dataset. As such, LAD conveys structural constraints through both the synthetic data and the inductive biases in the downstream problem-specific models.\n\nrepresentation space of a pre-trained model. Finetuning moderately-sized language models (LMs) (e.g., BERT) with well-motivated inductive biases (Mitchell, 1980) facilitates sample-efficient learning of the structural constraints (Peng et al., 2020;Mehri and Eskenazi, 2021b). However, fine-tuning can be impractical (e.g., in academic settings) with large LMs (e.g., GPT-3) due to the cost, computational power and immutable architectures. To this end, this paper aims to address the following: 'How can we leverage the strong language understanding and generation abilities of large LMs to facilitate zero-shot",
            "score": 0.40493239902472417,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 149,
                    "matchedPaperCorpusId": "202565722"
                },
                {
                    "start": 416,
                    "end": 436,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 809,
                    "end": 829,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1690,
                    "end": 1716,
                    "matchedPaperCorpusId": "235422323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "274939920",
            "title": "Leveraging Large Language Models For Optimized Item Categorization using UNSPSC Taxonomy",
            "text": "Prompt engineering is a critical technique for effectively utilizing large language models (LLMs). By crafting precise and contextually rich prompts, researchers can guide LLMs to produce accurate and relevant outputs. This process is essential for reducing ambiguity and enhancing the consistency of the model's responses. Brown et al. [2] demonstrated the efficiency of few-shot learning with carefully constructed prompts, highlighting that even a few examples can substantially improve model performance. Similarly, Raffel et al. [13] emphasized the importance of prompt format and structure in their exploration of transfer learning with the T5 model, showing that prompt variations can lead to different levels of success in text-to-text tasks. In the context of item categorization, prompt engineering helps address challenges such as ambiguous item descriptions and the need for domain-specific knowledge. Schick and Schutze [14] explored the use of Cloze-style prompts, where the model fills in blanks within a sentence, for few-shot text classification. This technique demonstrated how prompts that mimic natural language questions elicit more accurate responses from LLMs. This approach is particularly relevant for UNSPSC categorization, where items must be matched to precise codes based on nuanced descriptions. By leveraging structured, contextual, and multi-turn prompts, researchers can enhance the accuracy of UNSPSC code assignment, thereby improving data standardization and procurement processes across various industries.",
            "score": 0.4043947226413656,
            "section_title": "Prompt Engineering",
            "char_start_offset": 6117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1543
                }
            ],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "247450599",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "text": "ly noticed regularization effects that improve model performance on deep neural networks. For example, Guo et al. (2019a) explored the NLU specific mixup strategy by using sentence and word embeddings on CNNs and LSTMs to add performance gains in supervised text classification. Chen et al. (2020) proposed mixup for semi-supervised learning in which labeled and unlabeled samples are interpolated with their hidden representations to improve the performance of text classification.  explored mixup for sequence labeling tasks with active learning to improve the performance of supervised sequence labeling tasks. Yin et al. (2021) proposed mixup that interpolates every instance in a mini-batch to boost the performance of NLU tasks on the pre-trained language model RoBERTa . Similar to us, Yoon et al. (2021) explored mixup by incorporating saliency signals to generate augmented samples. Precisely, they use saliency signals to select a span of text from one sample to be replaced with another text span from another sample. However, in contrast, our method first divides data samples into two categories (easyto-learn and hard-to-learn/ambiguous categories) according to their AUM (Pleiss et al., 2020) distribution monitored over training epochs and then uses saliency to find the most similar/dissimilar samples across these two data categories.\n\nRecently, several works started to explore mixup for NLU model calibration. For example, Thulasidasan et al. (2019) investigated the impact of mixup for model calibration of NLU but only explored in-domain settings with simple deep learning architecture such as CNNs. Kong et al. (2020) explored BERT calibration using mixup as a regularization component on in-domain and out-of-domain. However, their mixup method only relied on the feature space distance between samples. In contrast, we explore a novel mixup method in which we categorize the training samples into two sets using AUM (Pleiss et al., 2020) and combine samples across these two sets based on saliency signals, for in-domain and out-of-domain model calibration.",
            "score": 0.40438457554160523,
            "section_title": "Related Work",
            "char_start_offset": 7893,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 297,
                    "matchedPaperCorpusId": "216553182"
                },
                {
                    "start": 614,
                    "end": 631,
                    "matchedPaperCorpusId": "236477688"
                },
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "235436032"
                },
                {
                    "start": 1186,
                    "end": 1207,
                    "matchedPaperCorpusId": "210932316"
                },
                {
                    "start": 1941,
                    "end": 1962,
                    "matchedPaperCorpusId": "210932316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "258187507",
            "title": "Tailoring Domain Adaptation for Machine Translation Quality Estimation",
            "text": "Despite efforts from initiatives like the QE shared task to publicly release QE datasets, such resources remain scarce across language pairs and, by extension, also have a limited coverage across domains (Fomicheva et al., 2020a;Fomicheva et al., 2022). This can pose a challenge for all QE models, especially recent ones that utilize large pre-trained language models (LLMs) (Ranasinghe et al., 2020;Zerva et al., 2022), since fine-tuning pre-trained models with small datasets has been demonstrated to be quite unstable (Zhang et al., 2020;Rubino, 2020). \n\nFurthermore, QE models trained on specific data do not generalize well to other domains that are outside of the training domain (Kocyigit et arXiv:2304.08891v2 [cs.CL] 9 May 2023May al., 2022)). Domain mismatches lead to significant decreases in the performance of QE models (de Souza et al., 2014a;Zouhar et al., 2023). To improve the generalizability of QE models, it is important to establish the right balance between domain-specific and generic training data. To date, only a few attempts have been made to address this challenge (de Souza et al., 2014b;Rubino, 2020;Lee, 2020). Thus, the majority of QE models have difficulty with accurately estimating quality across different domains, whether they are generic or specific (Zouhar et al., 2023). \n\nIn this work, we propose to tackle both the data scarcity and the domain mismatch challenge that LLM-based QE models face. We propose a methodology whereby a small amount of domainspecific data is used to boost the overall QE prediction performance. This approach is inspired by work on domain adaptation (DA) in the field of MT, where a large generic model is initially trained and then fine-tuned with domain-specific data (Chu and Wang, 2018;Pham et al., 2022).",
            "score": 0.40432010195785734,
            "section_title": "Introduction",
            "char_start_offset": 1453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1778
                }
            ],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 401,
                    "matchedPaperCorpusId": "226237546"
                },
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "256461325"
                },
                {
                    "start": 542,
                    "end": 555,
                    "matchedPaperCorpusId": "229366028"
                },
                {
                    "start": 737,
                    "end": 752,
                    "matchedPaperCorpusId": "247476065"
                },
                {
                    "start": 834,
                    "end": 858,
                    "matchedPaperCorpusId": "5412263"
                },
                {
                    "start": 1094,
                    "end": 1118,
                    "matchedPaperCorpusId": "28522152"
                },
                {
                    "start": 1118,
                    "end": 1131,
                    "matchedPaperCorpusId": "229366028"
                },
                {
                    "start": 1131,
                    "end": 1141,
                    "matchedPaperCorpusId": "229365835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "258378137",
            "title": "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models",
            "text": "Since many industrial applications involve safety -critical domains such as healthcare (Li et al., 2019;Blinov et al., 2020;Li et al., 2020;Rasmy et al., 2021;Sarabadani, 2019), anticipating credit card defaults (Sun and Vasarhalyi, 2021) and selfdriving (Khaitan et al., 2021), it's essential for machine learning systems to provide not only accurate but also well-calibrated predictions (Li et al., 2019), which can help to decide whether it can be trusted. \n\nHowever, models achieving high accuracy usually lead to overconfidence and miscalibration (Guo et al., 2017;Thulasidasan et al., 2019;Ovadia et al., 2019). This motivates an interesting and important area that attempts to achieve a better trade-off between accuracy and calibration. In addition to ID calibration, it's more important for machine learning models to produce high uncertainty when OOD data is observed, rather than to produce wrong yet wildly confident predictions. Related works. To overcome the problem of miscalibration, numerous methods have been proposed. The natural way is post-hoc calibration that transforms the output of the original network into calibrated confidence scores while maintaining the network's accuracy (Guo et al., 2017;Rahimi et al., 2020;Jung et al., 2020). The second method to mitigate miscalibration is to add regularizations during training such as label smoothing (Wang et al., 2020), Mixup (Zhang et al., 2018). Desai and Durrett (2020) and Kong et al. (2020) further conveys that the aforementioned methods can be applied to improve the calibration of pre-trained language models on NLU tasks. The third way is to design a specific loss function to minimize the discrepancy between accuracy and confidence. For example, Kong et al. (2020) lately propose the ID and OOD regularizer to leverage the relationship between accuracy and uncertainty, and it obtains a significant improvement over previous methods in ID calibration and OOD detection. Energy-based Models.",
            "score": 0.4036880159412788,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 104,
                    "matchedPaperCorpusId": "202567532"
                },
                {
                    "start": 104,
                    "end": 124,
                    "matchedPaperCorpusId": "220525684"
                },
                {
                    "start": 124,
                    "end": 140,
                    "matchedPaperCorpusId": "212622878"
                },
                {
                    "start": 140,
                    "end": 159,
                    "matchedPaperCorpusId": "218889776"
                },
                {
                    "start": 159,
                    "end": 176,
                    "matchedPaperCorpusId": "203396356"
                },
                {
                    "start": 212,
                    "end": 238,
                    "matchedPaperCorpusId": "56917503"
                },
                {
                    "start": 255,
                    "end": 277,
                    "matchedPaperCorpusId": "224814527"
                },
                {
                    "start": 389,
                    "end": 406,
                    "matchedPaperCorpusId": "202567532"
                },
                {
                    "start": 552,
                    "end": 570,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 570,
                    "end": 596,
                    "matchedPaperCorpusId": "166228660"
                },
                {
                    "start": 596,
                    "end": 616,
                    "matchedPaperCorpusId": "174803437"
                },
                {
                    "start": 1203,
                    "end": 1221,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1221,
                    "end": 1241,
                    "matchedPaperCorpusId": "212725550"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "216867328"
                },
                {
                    "start": 1372,
                    "end": 1391,
                    "matchedPaperCorpusId": "218487046"
                },
                {
                    "start": 1399,
                    "end": 1419,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1421,
                    "end": 1445,
                    "matchedPaperCorpusId": "212747810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "278310422",
            "title": "Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages",
            "text": "Language adaptation is particularly valuable for extending the model's capabilities to underrepresented languages, enabling inclusivity and broader accessibility. Nevertheless, it is particularly challenging because of VLM's English-centric training foundations [14], [15]. The predominant bias towards English-language training data creates significant limitations for models attempting to operate effectively across different linguistic landscapes [16]. This issue is particularly pro-nounced for low-resource languages [17], which suffer from systemic under-representation in both pre-training corpora and annotated datasets. Several state-of-the-art methodologies have been proposed to address these limitations [18]- [20]. Muennighoff et al. [18] investigated the zero-shot generalization capabilities of LLMs, emphasizing their ability to generalize effectively across various languages. However, their analysis focused solely on generalization to languages encountered during pre-training, without exploring the potential for generalization to languages introduced exclusively during finetuning. Chen et al. [19] examined the effects of monolingual and multilingual instruction tuning, showing that in a resourceconstrained environment, multilingual tuning offers significant advantages over monolingual tuning. Similarly, Shaham et al. [20] analyzed monolingual and multilingual instruction tuning, showing that models trained on multilingual datasets achieve superior performance to monolingual models while requiring significantly fewer examples per language. Despite these contributions, a comprehensive analysis of the impact of various instruction tuning approaches on subsequent downstream task fine-tuning remains lacking. \n\nDomain adaptation ensures the model performs optimally in specialized applications, such as legal analysis or medical diagnosis, where domain-specific knowledge is essential. The unique terminologies and clinical contexts specific to healthcare, as well as data scarcity, make it critical. To further improve the performance of VLMs in medical downstream tasks, Supervised Fine-Tuning (SFT) is typically conducted using datasets that are specifically designed for those tasks [21]- [23]. Chen et al. [21] fine-tuned MEDITRON on MedQA, PubMedQA, and MedMCQA datasets to enhance its performance in medical question answering.",
            "score": 0.40367157070797544,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "260605825"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "262053896"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge Distillation (KD) is an effective method to transfer knowledge from one network (a.k.a. teacher) to another (a.k.a. student). In this paper, we study KD on the emotion-related tasks from a new perspective: calibration. We further explore the impact of the mixup data augmentation technique on the distillation objective and propose to use a simple yet effective mixup method informed by training dynamics for calibrating the student models. Underpinned by the regularization impact of the mixup process by providing better training signals to the student models using training dynamics, our proposed mixup strategy gradually enhances the student model\u2019s calibration while effectively improving its performance. We evaluate the calibration of pre-trained language models through knowledge distillation over three tasks of emotion detection, sentiment analysis, and empathy detection. By conducting extensive experiments on different datasets, with both in-domain and out-of-domain test sets, we demonstrate that student models distilled from teacher models trained using our proposed mixup method obtained the lowest Expected Calibration Errors (ECEs) and best performance on both in-domain and out-of-domain test sets.",
            "score": 0.40295814049119416,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "273950544",
            "title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding",
            "text": "Large language models (LLMs) evaluation is gaining increasing attention as these models are typically trained on general-domain datasets while demonstrating notable performance on tasks out of their training domains (Mosbach et al., 2023). The creation of evaluation datasets helps to identify the capabilities of LLMs, pinpoint shortcomings, and establish a measurable path for improvement. Based on Chang et al. (2024), LLM evaluation addresses questions such as what to evaluate (subjects and topics), where to evaluate (selecting appropriate datasets), and how to evaluate (the evaluation process). \n\nTo improve LLMs' capabilities and effectively assess their performance, researchers are creating benchmark datasets using a diverse range of domains and languages. This inclusive methodology allows for a more comprehensive evaluation of LLMs' performance across various domains and languages. Popular benchmark datasets like MMLU (Hendrycks et al., 2020) and MEGA-VERSE (Ahuja et al., 2023) cover a wide range of extensive world knowledge tasks and subjects. \n\nTo create evaluation benchmarks that are multilingual, researchers Koto et al. (2024); Li et al. (2023); Son et al. (2024) introduced benchmark datasets for different languages by translating a subset of the MMLU dataset. Beyond research efforts, translating existing benchmarks into different languages is an effective strategy to evaluate the multilingual capabilities of closed-source LLMs. These benchmarks evaluate multilingual understanding of models by presenting a range of extensive world knowledge tasks in the language of interest. While combining different subjects in a benchmark dataset may seem beneficial, it does not always provide a clear picture of the model's shortcomings. For example, using MMLU in different languages tests language and subject understanding simultaneously (Hendrycks et al., 2020). There should be evaluation benchmarks that disentangle language understanding and specific subject knowledge. \n\nLanguage understanding of LLM can be measured in numerous ways, and it is crucial to introduce benchmarks that evaluate complex text comprehension while considering each language's specific linguistic, cultural, and contextual nuances.",
            "score": 0.4029308128678116,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 935,
                    "end": 959,
                    "matchedPaperCorpusId": "267760288"
                },
                {
                    "start": 1133,
                    "end": 1151,
                    "matchedPaperCorpusId": "267760288"
                },
                {
                    "start": 1863,
                    "end": 1887,
                    "matchedPaperCorpusId": "267760288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "269042793",
            "title": "Improving Retrieval for RAG based Question Answering Models on Financial Documents",
            "text": "In recent years, the emergence of Large Language Models (LLMs) represent a critical turning point in Generative AI and its ability to expedite productivity across a variety domains. However, the capabilities of these models, while impressive, are limited in a number of ways that have hindered certain industries from being able to take full advantage of the potential of this technology. A key disadvantage is the tendency for LLMs to hallucinate information and its lack of knowledge in domain specific areas. The knowledge of LLMs are limited by their training data, and without the use of additional techniques, these models have very poor performance of very domain specific tasks. \n\nIn order to develop a large language model, the first step is the pre-training process where a transformer is trained on a very large corpus of text data. This data is very general and not specific to a certain domain or field, as well as unchanging with time. This is a reason why LLMs like ChatGPT might perform well for general queries but fail on questions on more specific and higher-level topics. Additionally, a model's performance about a certain topic is highly dependent on how often that information appears in the training data, meaning that LLMs struggle with information that does not appear frequently. 1 This is the case for most domain-specific information, such as financial information and expertise, which is why standard LLMs have poor performance with domainspecific questions. \n\nThe concept of knowledge injection refers to the ability to give the model access to information beyond its original training data, and the main way we can improve the performance of large language models on domain-specific tasks. The two primary techniques of knowledge injection are additional training of the model or fine-tuning, or in-context learning, the most popular version of which is Retrieval Augmented Generation (RAG).",
            "score": 0.40270069138824693,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "273346210",
            "title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts",
            "text": "Recent advances in large-scale Pre-trained Language Models (PLMs) have showcased impressive generalization capabilities (Brown et al., 2020;Chowdhery et al., 2023;Anil et al., 2023;Team et al., 2023). However, when applied to specialized domains such as medical, legal, or financial sectors, these general-purpose models often require further fine-tuning to maximize performance on target domains (Huang et al., 2023;Li et al., 2023;Singhal et al., 2023). \n\nA straightforward approach to domain adaptation is full-parameter fine-tuning, where the entire model is further trained on domain-specific data (Houlsby et al., 2019;Bapna et al., 2019). While this method provides strong performance on target domains, full-parameter fine-tuning may lead to catastrophic forgetting where the model loses previously learned capabilities by overfitting to the target domain (Goodfellow et al., 2013;Kirkpatrick et al., 2017). Additionally, this method is memory-intensive to serve in multi-domain settings as each domain has a unique set of parameters, incurring a significant parameter loading overhead when switching between domains (Hu et al., 2021). In such cases, the cost of frequent \"context switches\" significantly impacts performance, making full-parameter fine-tuning impractical for scalable, efficient deployment (Dhar et al., 2024). \n\nTo address the issues of forgetting and memory-efficiency, parameter-efficient fine-tuning methods have been proposed, such as adapter modules (Houlsby et al., 2019), LoRA (Hu et al., 2021), and CoDA (Lei et al., 2023). These methods introduce a small number of trainable parameters and keep the original model frozen during training. Through targeted parameter updates, these approaches are both computationally efficient and effective at retaining prior knowledge (Biderman et al., 2024). Despite these benefits, parameter-efficient methods are limited in their expressive potential and struggle to scale effectively across large domains and datasets (Niederfahrenhorst et al., 2023).",
            "score": 0.4025196988042179,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 140,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 140,
                    "end": 163,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 433,
                    "end": 454,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 603,
                    "end": 625,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 889,
                    "end": 914,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1315,
                    "end": 1334,
                    "matchedPaperCorpusId": "269438883"
                },
                {
                    "start": 1481,
                    "end": 1503,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "265212730",
            "title": "On the Calibration of Multilingual Question Answering LLMs",
            "text": "Recently, there has been growing interest in studying calibration of English QA models (Kamath et al., 2020;Zhang et al., 2021;Jiang et al., 2021;Si et al., 2022). Kamath et al. (2020) trains an extra calibrator of confidence scores to improve the calibration performance and examines the calibration performance on an out-of-domain (OOD) setting. They utilize the scores from the calibrator and uses it as a reranker to select the answers. Zhang et al. (2021) extends this work by adding the features of the context and back-translated context. Jiang et al. (2021) analyzes the calibration performance of generative language models, and find that the generative models on QA are not well-calibrated. \n\nOur work in contrast investigates the calibration of pre-trained multilingual LLMs (both extractive, with an encoder-only architecture, and generative, with an encoder-decoder or decoder-only architecture) on QA, and various techniques to improve calibration such as temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016) and cross-lingual data augmentation. \n\n3 Background In this work, we focus on two broad types of models for QA: extractive (or discriminative) and generative (as shown in Figure 2). For models based on encoder-only language models like mBERT and XLM-R (Conneau et al., 2019;Devlin et al., 2019;Liu et al., 2019), the prediction of the answer span within the given context is framed as a classification task and achieved using two linear layers. These linear layers are placed on top of the hidden representations and are responsible for predicting the start and end indices of the answer span Y , respectively. The logit score of the answer z ans is defined as the sum of the logits of the start and end positions, z start and z end (Si et al., 2022).",
            "score": 0.4024982055363106,
            "section_title": "Calibration of models on Question Answering tasks",
            "char_start_offset": 5671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 108,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 127,
                    "end": 146,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 146,
                    "end": 162,
                    "matchedPaperCorpusId": "253098276"
                },
                {
                    "start": 164,
                    "end": 184,
                    "matchedPaperCorpusId": "219721462"
                },
                {
                    "start": 546,
                    "end": 565,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 990,
                    "end": 1008,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1026,
                    "end": 1048,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1323,
                    "end": 1343,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "In early meteorology, calibration was noted as validity (Miller, 1962) or reliability (Murphy, 1973), indicating the trustworthiness of forecasters. Well calibrated probabilities can provide extra information for users to decide whether to trust the model's prediction, particularly for modern neural networks whose decisions are harder to interpret (Guo et al., 2017). Studies have also pointed out that calibration is helpful to reduce hallucination in language models (Xiao and Wang, 2021;Tian et al., 2020). Previous works have shown that pre-trained language models can generate well-calibrated predictions (Desai and Durrett, 2020;Kadavath et al., 2022). However, these works mainly concentrate on vanilla language models, while the aligned language models receive less focus. A newly proposed work evaluates calibration of some aligned models by prompting them to verbalize confidence in the token space (Tian et al., 2023), but it mainly studies black-box models, whose training process is not available, and thus can not provide insight into how model calibration is affected by different factors in the alignment training process. To conclude, a systematical study on the calibration of aligned language models is still missing, and our work aims to fill this gap. \n\nIn this work, we study the calibration of aligned language models in the entire building cycle and provide evidence on how to achieve decent model calibration. An overview of the scheme of out study is at Figure 1 Besides the understanding and generating ability, factual faithfulness and reasoning capability are two widely considered issues with large language models (Du et al., 2023). We also follow this path to study models' calibration when applied to different tasks. For this purpose, we design three tasks for each of the stages above. (1) To evaluate model calibration on common text generation, we use Causal Language Modeling (CLM) task, which is also the objective of pre-training stage. \n\n(2) To study model calibration on factuality, we designed a facts generation task where the models are asked to generate fact-related content.",
            "score": 0.4024982055363106,
            "section_title": "Introduction",
            "char_start_offset": 1955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1978
                },
                {
                    "start": 1981,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 100,
                    "matchedPaperCorpusId": "121053719"
                },
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78515625
        },
        {
            "corpus_id": "273502432",
            "title": "Language Model Probabilities are Not Calibrated in Numeric Contexts",
            "text": "Why Aren't LLMs Already Calibrated? Should the negative log-likelihood loss endow a pretrained language model with calibration? If there were n \u2192 \u221e examples of Heads and Tails being flipped in the data and the true distribution was reflective of some p heads , then the lowest loss would be achieved via a perfectly calibrated model. Thus, the answer is empirical: What does the training data look like? Finding the disparate and analogous scenarios that call for randomness may be difficult. Non-calibrated outputs frequently occurring for such scenarios might impact model optimization enough to overcome examples where the loss is explicitly aligned with a calibrated model. For example, patterns derived from people preferring the number 7 (out of 10) or 42 (out of 100) or Heads (compared to Tails) or answering with rounded numbers that end in 0 or 5 might reduce calibration. Beyond impacting the loss, it could be the case that some well-trained models are already capable of producing calibrated results; the models may \"know\" the calibrated output but adjust it to match non-random tendencies, akin to pragmatics. \n\nShould Models Be Calibrated? Our work highlights how models that perform remarkably in other types of evaluations fall short in scenarios that call for calibrated outputs. Some may have expected our results given that these models are trained on data generated by people who are also widely biased. Others might have expected that with the range of data seen in training (data in the wild, code outputs, endemic instances of distributions) models would learn to calibrate. However, it seems curious that models are frequently able to correctly generate text describing what the proper calibrated distribution should be for a given scenario, but fail to represent that distribution internally when prompted to simulate the same scenario. \n\nCan We Calibrate Models with FineTuning? We expect that it is possible to fine-tune models to improve calibration. Already we see some improvements with generic alignment procedures, though these improvements often come at a tradeoff of an (overly large) reduction in model entropy. \n\nMitigating this mode collapse behavior is a compelling area for future work.",
            "score": 0.4024982055363106,
            "section_title": "DISCUSSION",
            "char_start_offset": 18868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.638671875
        },
        {
            "corpus_id": "236486129",
            "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
            "text": "Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.",
            "score": 0.4024982055363106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "266902716",
            "title": "A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts into a Verbalizer",
            "text": "Label-word candidates directly queried from an external concept base have noisy and irrelevant information and are over-covered in a label-word space.For example, the concepts for the named entity \"APPLE\" have two categories of \"fruit\" and \"company\", but for the label \"BUSINESS\", \"fruit\" is not a valid label word.In particular, for zero-shot based learning, the bias of a language model toward predicting certain answers (Zhao et al., 2021;Kong et al., 2020) is unavoidable and cannot be ignored.Therefore, the elimination of irrelevant or noisy candidates and the calibration of label words based on task-specific settings are essential.\n\nTo achieve the aforementioned refinement, we devise a cascade calibration procedure involving three steps: anchor creation, language model calibration, and category calibration.\n\nAnchor Creation To calibrate a concept, a prerequisite is to prepare a base value, namely an anchor, to be employed to measure the fitness of a concept to a task-specific scenario.The anchor is created based on a task-specific prompt template and a sample obtained from the task-specific dataset.\n\nTo create the anchor, we firstly compute the probablity of a token t in the vocabulary T of M at a masked location in a task-specific scenario as\n\nwhere the prompt x p denotes a sample wrapped into a task-specific prompt template.\n\nThen for the convenience of computing the scenario-specific probability of each token t, we exploit normalized exponential function on each token t by\n\nwhere t denots a specific item in T to be computed, t \u2032 stands for each word in T .\n\nLanuguage Model Calibration Among the candidate concepts for a verbalizer, there are still some concepts that are less likely than others to be predicted as eligible label words for a class by a PLM in a task-specific scenario.\n\nConcepts with low probability in task-specific scenarios need to be eliminated first.Language model-based calibration is leveraged to remove anchor-based low-probability concepts.\n\nEach candidate concept may not be exactly an item in the vocabulary of the PLM, therefore we define the function S c to compute the average probability of all tokens belonging to the concept as its predicted probability.",
            "score": 0.4024982055363106,
            "section_title": "Cascade Calibration",
            "char_start_offset": 12882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 640
                },
                {
                    "start": 642,
                    "end": 819
                },
                {
                    "start": 821,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1117
                },
                {
                    "start": 1119,
                    "end": 1264
                },
                {
                    "start": 1266,
                    "end": 1349
                },
                {
                    "start": 1351,
                    "end": 1501
                },
                {
                    "start": 1503,
                    "end": 1586
                },
                {
                    "start": 1588,
                    "end": 1815
                },
                {
                    "start": 1817,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1996
                },
                {
                    "start": 1998,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 423,
                    "end": 442,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "254877502",
            "title": "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering",
            "text": "humans in keeping up with new knowledge without requiring annotations for every new domain or concept. For this, the system should be resilient to changes in the document, question, and answer distributions. Unfortunately, the current work in ODQA focus solely on Wikipedia corpus and do not study effectiveness of a model trained on such a general-purpose domain when applied to an unseen domain. To gauge how likely it is for a source domain model to succeed on an unseen domain we need to understand its ability to work out-of-the-box or even adapt to a new target domain, under varying types and degrees of dataset shifts. (Quinonero-Candela et al., 2008).\n\nIn this work, we study the challenges and interventions for generalizing ODQA models to new domains via four contributions. First, to understand how well the state-of-the-art ODQA system (trained on the general-purpose domain) performs on a variety of target distributions, we define a collection of datasets for evaluating domain generalization. We aggregate a set of seven ODQA datasets spanning five different domains ( \u00a72). We observe that the source ODQA model does not generalize well (Fig.1, Top) on this collection ( \u00a74). Second, to automatically determine the type of data shift with only a small number of labeled target domain examples, we propose a generalizability test. This test assesses the type and degree of shift a new domain suffers with respect to the source domain ( \u00a73). Third, to understand the adaptability of the source model to a target domain, we analyze the performance of various intervention schemes, including existing zero-shot in-domain question generation and a novel few-shot language modelaided generation. These schemes create data akin to the target domain which is augmented with the source domain to learn an adapted version of the source model. Overall, we observe improvement in performance across all the target datasets (Fig. 1). The degree of improvement depends on the intervention scheme and underlying dataset shift ( \u00a75). Finally, we propose a simple and effective few-shot method that improves the performance by up to 24% in F1. This method prompts a large language model with 8 examples to generate examples for further adaptation.\n\nPutting it all",
            "score": 0.40244722301718855,
            "section_title": "Introduction",
            "char_start_offset": 1801,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "271218675",
            "title": "CharED: Character-wise Ensemble Decoding for Large Language Models",
            "text": "As large language models (LLMs) have become increasingly ubiquitous and powerful models have been open-sourced, there has been extensive research on methods to achieve improved task-specific performance from these models.The long-standing method for doing this is through finetuning, in which domain-specific datasets are used to update weights of large foundation models to improve performance on certain tasks.However, direct fine-tuning is both timeconsuming and computationally intensive (Strubell et al., 2019).This problem will become worse as model sizes Figure 1.Our CHARED algorithm ensembles models character by character while decoding.Model prompt: \"Sally has four hats, and John has twice as many.How many total hats are there?\"Models M1 and M2 are queried to retrieve next token probabilities, which are marginalized into next character probabilities, combined and sampled, and re-normalized until the next character chosen is the null string.This sequence is then added to the existing answer, which is fed back into both models.continue to grow, increasingly motivating more efficient fine-tuning (Lester et al., 2021;Han et al., 2024) or alternative approaches (Hu et al., 2021) for enhancing or aligning LLM performance.\n\nModel ensembling has been shown to yield improved performance across different domains.An established method for doing this is through shallow fusion, which was originally used to integrate an LLM into a neural machine translation (NMT) model (Gulcehre et al., 2015).Such ensembling methods, which aggregate models during beam search, have shown promise for improving translation quality in NMT settings (Sutskever et al., 2014;Firat et al., 2016;Stahlberg et al., 2018), but require the same vocabulary and tokenization.Twist decoding (Kasai et al., 2022) modifies beam search to bypass the shared vocabulary restriction, but its reliance on beam search reduces the inference speed.Other more recent approaches related to combining language models include proxy tuning (Liu et al., 2024) and Composition to Augment Language Models (CALM) (Bansal et al., 2024).",
            "score": 0.40239652073331245,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 957
                },
                {
                    "start": 957,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1238
                },
                {
                    "start": 1240,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 515,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 1113,
                    "end": 1134,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1687,
                    "end": 1710,
                    "matchedPaperCorpusId": "52157637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "277451521",
            "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs",
            "text": "Kumar et al. [33] delves into post-training techniques for LLMs, emphasizing that while pre-training establishes a robust linguistic foundation, it is subsequent refinementthrough methods such as fine-tuning, reinforcement learning, and testing-time scaling -that truly enhances the models' reasoning abilities, factual accuracy, and overall adaptability. The work systematically examines these post-training strategies, focusing on their potential to address critical challenges such as catastrophic forgetting, reward hacking, and the tradeoffs encountered during inference. Furthermore, it highlights emerging avenues in model alignment and scalable adaptation, underscoring the importance of these techniques in ensuring that LLMs understand language and perform effectively across a diverse range of real-world tasks. \n\nThe survey by Wang et al. [32] offers a systematic overview of various alignment techniques developed to enhance the reliability of LLMs. It emphasizes that despite LLMs achieving remarkable capabilities through massive pretraining, the variability in data quality can still lead to suboptimal or undesired outputs. To mitigate this, the paper categorizes and examines a range of methods-including reinforcement learning from human feedback (RLHF), reinforcement learning from AI feedback (RLAIF), proximal policy optimization (PPO), and direct preference optimization (DPO)-detailing how each approach contributes to aligning model outputs with human expectations. By organizing these techniques into distinct topics, the survey fills a critical gap in the literature and provides a comprehensive framework that not only aids in understanding current alignment strategies but also guides future research toward improving the performance and safety of LLMs. \n\nThe survey by Tie et al. [34] presents a thorough synthesis of post-training methodologies to overcome the inherent limitations of pre-trained large language models. It critically addresses challenges such as constrained reasoning abilities, ethical ambiguities, and performance issues in specialized domains, underscoring the need for advanced post-training strategies. The authors systematically categorize the evolution of post-training language models into five core paradigms: fine-tuning to enhance task-specific accuracy, alignment to better meet human expectations, reasoning to facilitate multistep inference despite reward design challenges, efficiency to optimize resource use amid growing model complexity, and integration and adaptation to extend capabilities across diverse modalities.",
            "score": 0.4023570081805984,
            "section_title": "C. Post-training and Alignment Strategies",
            "char_start_offset": 11867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2584
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "263828737",
            "title": "Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy",
            "text": "Our paper offers a thorough, multi-module testing system to evaluate the generation quality of LLMs on both general and in-domain business perspective, which is illustrated in Figure 1. Specifically, it consists of four core modules as described below: \n\n\u2022 In-domain and general data combination. When users engage with the intelligent customer service assistant, they often pose a variety of questions, some of which may be ambiguous or unclear. Relying solely on company-specific, in-domain knowledge may prove insufficient for the model to furnish accurate and satisfactory responses in such cases. \n\nThe ability to offer prompt and helpful responses that go beyond domain-specific information is thus critical for enhancing user experience. During empirical tests, we observed that Language Learning Models (LLMs) exhibited a notable decline in their general capabilities when fine-tuned exclusively with domain-specific data. To mitigate this performance degradation, we employ a data combination strategy that integrates both in-domain and out-of-domain data across a range of tasks. This approach is designed to maintain the LLMs' proficiency in general interactive capabilities. \u2022 Supervised fine-tune. Through our investigation, we find fine-tuning is necessary to make the model obtain reasonably strong ability in answering questions that require company in-  domain knowledge. We employ instruction-based fine-tuning, a technique proven effective in recent LLM developments [19,25,28]. \n\n\u2022 Test-inference module. In order to furnish a thorough assessment of the quality of generated responses from both in-domain and out-of-domain perspectives, we employ an extensive evaluation protocol consisting of a carefully curated set of 45 questions. This question set encompasses a broad class of scenarios, from specialized domain-specific inquiries to more generalized queries, aiming to challenge and appraise the model's adaptive capabilities. \n\n\u2022 Scoring module. Evaluating Large Language Models (LLMs) for specialized monetization applications is challenging due to the limitations of current testing methods, which often focus solely on general language skills. Our paper proposes a comprehensive, multi-faceted testing system to assess both general and specialized capabilities of LLMs. The system uses a eight-category scoring framework that adapts to model improvements and training stages. This flexible and evolving approach aims to offer a more accurate and complete assessment of LLMs' capabilities.",
            "score": 0.40134638613615675,
            "section_title": "Overview of Framework",
            "char_start_offset": 9876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2299
                },
                {
                    "start": 2300,
                    "end": 2405
                },
                {
                    "start": 2406,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "250551977",
            "title": "Calibrating Zero-shot Cross-lingual (Un-)structured Predictions",
            "text": "We further experiment with two more information extraction tasks, ACE and BETTER, where the training resource is more limited and the ontologies are more complex. For labeling problems, we follow the general setting in section 4.1. For tagging problems, we calibrate the label-wise probability for positive labels as discussed in section 3.3. In case of a linear chain CRF, we marginalize out all other positions to obtain the label-wise probability following Culotta and McCallum (2004) and Reich et al. (2020). For ACE and BETTER we do not evaluate under low-data and very-low-data setting given the relatively small size of their dev sets. \n\nImpact of Task Type and Difficulty Our results align with the discovery of Lauscher et al. (2020), who showed that the transfer performance depends on a hypothetical \"task level\". Here we observe a larger ECE on ACE and BETTER as well as in \"high level\" semantic tasks like XNLI compared to \"low level\" sequence tagging tasks like POS, UDP, NER defined by Lauscher et al. (2020). \n\nAs shown in table 1 and table 2, in general, the structured prediction components (f-ECE) are less calibrated and remain so after temperature scaling, though for ACE there is some irregularity given the sparse event/argument span annotations on the of the training configuration. First, training with more data improves the cross-lingual calibration. \n\nSecond, transferring from English to non-English intensifies miscalibration as the target language is farther from English. Also, larger models are likely to be less miscalibrated when the model is transferred to a different target language zero-shot. Moreover, our result shows that temperature scaling and Gaussian Process calibration methods are among the top-performing methods. While temperature scaling is easy to implement and generalizes well to distant languages, it's less effective when applied to complex structured probabilities. Finally, models are least calibrated on \"high levels\" tasks like XNLI and span extraction. Models are most calibrated on simple \"low-level\" tasks like POS.",
            "score": 0.40094459444275043,
            "section_title": "Evaluating on More Difficult Tasks",
            "char_start_offset": 25400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 511,
                    "matchedPaperCorpusId": "222341831"
                },
                {
                    "start": 720,
                    "end": 742,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1001,
                    "end": 1023,
                    "matchedPaperCorpusId": "226262344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "249062610",
            "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
            "text": "Inspired by recent parameter-efficient adaptation techniques (Houlsby et al., 2019;Zaken et al., 2021;Li and Liang, 2021;Lester et al., 2021), we take this approach further: can we overcome catastrophic forgetting by freezing all of the pre-trained model parameters, and only tuning a much smaller set of task-specific parameters? Parameter-efficient tuning methods are particularly appealing for multilingual NLP, as they would enable reuse of a single frozen model across many combinations of task and language, reducing storage and serving costs. \n\nTo this end, we conduct the first investigation of the XGEN performance of PROMPTTUNING (Lester et al., 2021), a simple parameter-efficient adaptation technique that limits learned parameters to a set of virtual tokens prepended to the text input. We compare PROMPTTUNING with standard fine-tuning (or MODELTUNING, where all model weights are tuned) across different languages and model scales. We find that increasing model size and decreasing tunable parameter capacity are key for overcoming catastrophic forgetting. Despite its inferior performance on the training language (English), PROMPT-TUNING with scale typically outperforms MODEL-TUNING when evaluated on non-English languages, especially on languages more distantly related to English, such as Thai. This corroborates previous findings (Li and Liang, 2021;Lester et al., 2021) that parameter-efficient methods are more robust to domain shifts between training and inference. Motivated by our initial findings, we investigate two approaches to further improve the XGEN performance of PROMPTTUNING and MODELTUNING. Our first approach involves mixing unlabeled data in the target language into the supervised training stage. We show this dramatically alleviates catastrophic forgetting on WIKILINGUA-0. We also introduce a novel approach, \"factorized prompts\", which is specifically designed for PROMPTTUNING. We train prompts on a multi-task multilingual mixture, where each prompt is factorized into composable language and task modules-the first half of the prompt encodes language knowledge, while the second half captures language-agnostic task knowledge. During inference in the zero-shot crosslingual setting, the source language module is replaced with the target language module, while the task module remains unchanged.",
            "score": 0.40059799024251336,
            "section_title": "Multilingual Language Model (mT5)",
            "char_start_offset": 2514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 83,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 83,
                    "end": 102,
                    "matchedPaperCorpusId": "237439234"
                },
                {
                    "start": 102,
                    "end": 121,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 121,
                    "end": 141,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 640,
                    "end": 661,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1351,
                    "end": 1371,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 1371,
                    "end": 1391,
                    "matchedPaperCorpusId": "233296808"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "260680759",
            "title": "KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering",
            "text": "Large pre-trained language models (PLMs) [15] have succeeded remarkably in various NLP downstream tasks. Their achievements can be attributed to two key factors: extensive pre-training on diverse text sources and the ability to fine-tune domain-specific data. PLMs undergo extensive pre-training on vast amounts of text data from various sources such as books, articles, and websites. This process allows them to develop a profound understanding of language and capture a comprehensive range of linguistic patterns and contextual information. Furthermore, PLMs can be fine-tuned on domain-specific datasets, enabling them to specialize and adapt to a particular domain. This fine-tuning process refines the models' knowledge and performance, allowing them to excel in tasks specific to those domains. However, recent research has highlighted the efficacy of incorporating knowledge graphs into language models using diverse techniques [12,18,25,26]. Our paper shows that incorporating relevant structured knowledge from knowledge graphs can further enhance language model performance and domainspecific understanding. \n\nA knowledge graph (KG) is a graph-based structure comprising real-world entities represented as nodes, such as Ginger Rogers and Primrose Path, and relationships between them represented as edges, such as Ginger Rogers | starred_actors | Primrose Path. KGs can be specific to a particular domain [3] or general in nature [24]. These knowledge graphs, which serve as knowledge bases, play a vital role in knowledge-intensive applications like question answering, as they provide structured and organized information for effective retrieval and analysis. \n\nVarious studies have explored different methods for infusing knowledge into language models. One popular approach involves verbalizing triples in the knowledge base and continually pretrain the LLM using a training criteria such as masked language modeling. However, this approach can be computationally demanding. \n\nOther methods like QA-GNN [25] and GreaseLM [26] rely on knowledge graph embeddings [6] to obtain the domain knowledge which requires additional training. The two critical factors in a knowledge infusion method are: i) the quality of infused knowledge, which allows for achieving strong empirical performance, and ii) the simplicity of the architecture. These underscore the need for a knowledge infusion technique that is computationally efficient while maintaining high quality and simplicity.",
            "score": 0.39986934846111233,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2345
                },
                {
                    "start": 2346,
                    "end": 2487
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "248834551"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "247595094"
                },
                {
                    "start": 942,
                    "end": 945,
                    "matchedPaperCorpusId": "233219869"
                },
                {
                    "start": 1416,
                    "end": 1419,
                    "matchedPaperCorpusId": "248970443"
                },
                {
                    "start": 2018,
                    "end": 2022,
                    "matchedPaperCorpusId": "233219869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Our work focused on measuring and improving calibration of MMLMs across different languages and tasks. The languages that we considered in our experiments were the ones for which labelled test sets were available in the 4 multilingual benchmarks that we considered. The number of languages in these benchmarks ranged from 6 in case of MARC to 15 in XNLI, covering mostly high resource languages3 with Swahili being the lowest resource language studied. However, the MMLMs considered in this work supports around 100 languages many of which are arguably even lower resource compared to Swahili. Hence, how well the methods discussed in the paper work towards improving the calibration for such languages needs to be explored but is limited by the current state of multilingual benchmarking (Ahuja et al., 2022). \n\nAdditionally, investigating the state of calibration across the languages for the 4 tasks and 2 MMLMs for different hyper-parameters and random seeds required a reasonably large amount of GPU resources (we used NVIDIA V100 and P100 GPUs). However, the calibration methods that we describe in the paper can work with little (for temperature scaling and few-shot learning) to no (for label smoothing) additional compute over the standard model training.",
            "score": 0.39959592868366767,
            "section_title": "Limitations",
            "char_start_offset": 13462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1264
                }
            ],
            "ref_mentions": [
                {
                    "start": 789,
                    "end": 809,
                    "matchedPaperCorpusId": "248780386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "271544135",
            "title": "Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian",
            "text": "Addressing the challenge of limited annotated data in specialized fields and low-resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English corpora, there is a notable gap in models specifically tailored for Italian, particularly for technical and bureaucratic jargon. This paper explores the feasibility of employing smaller, domain-specific encoder LMs alongside prompting techniques to enhance performance in these specialized contexts. Our study concentrates on the Italian bureaucratic and legal language, experimenting with both general-purpose and further pre-trained encoder-only models. We evaluated the models on downstream tasks such as document classification and entity typing and conducted intrinsic evaluations using Pseudo-Log-Likelihood. The results indicate that while further pre-trained models may show diminished robustness in general knowledge, they exhibit superior adaptability for domain-specific tasks, even in a zero-shot setting. Furthermore, the application of calibration techniques and in-domain verbalizers significantly enhances the efficacy of encoder models. These domain-specialized models prove to be particularly advantageous in scenarios where in-domain resources or expertise are scarce. In conclusion, our findings offer new insights into the use of Italian models in specialized contexts, which may have a significant impact on both research and industrial applications in the digital transformation era.",
            "score": 0.3992533161590781,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "265609739",
            "title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "text": "The approaches for learning with limited labelled data are designed to achieve high performance in machine learning models even with few labels available [76,134]. Under the term learning with limited labelled data, we understand any approach that is designed to work with a lack of labels, without any constraint on how the labelled samples are distributed, i.e., whether all the samples are from a single task or are distributed across different tasks. Although similar to the notion of few-shot learning, it represents a broader scope encompassing a larger number of possible approaches (some of which are incorrectly categorised as few-shot learning in current practice). \n\nTo deal with the limited labels, these approaches utilise additional information from different sources [19,134], such as transferring knowledge from similar tasks and datasets. In the NLP domain, prompting and in-context learning (also called few-shot prompting) have recently emerged. In these techniques, a large pre-trained language model is \"prompted\" to predict a label for a single test sample by presenting it with task instructions and the test sample (and concatenation of a few labelled samples when in-context learning is used), without requiring any parameter update [76]. In addition, it is common to use fine-tuning, where the parameters, or their subset using parameter-efficient fine-tuning (PEFT) methods, of the pre-trained large language model are updated to optimise the model for the specific downstream task using only a few labelled samples [25,33,94]. Finally, meta-learning can be used, where the model is explicitly trained to quickly adapt to a new task with only a handful of examples by learning how best to learn across a large number of related tasks with few labelled samples each [2,49]. \n\nHowever, a significant problem observed for these approaches is their sensitivity to the effects of uncontrolled randomness, which negatively affects their stability. Under stability and its opposite term sensitivity, we understand a property of a learning algorithm or a model that indicates what influence the small-scale and random changes (or perturbations) in the input data and parameters have on its outputs.",
            "score": 0.39923674090370087,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "246867041"
                },
                {
                    "start": 1792,
                    "end": 1795,
                    "matchedPaperCorpusId": "239998536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "270702921",
            "title": "On the Transformations across Reward Model, Parameter Update, and In-Context Prompt",
            "text": "At the first stage, the initial language model (\u03c0 0 ) is prompted to generate continuations from a textual definition of the target entity z.At the second stage, the model parameters are updated so that the distribution of the new model (\u03c0 * (y|x) matches the distribution of the initial model conditioned on the definition (\u03c0 0 (y|x, z)) on the transfer set (D) (i.e., the objective defined in Eq. 9).They find this approach is more effective at making inferences based on injected facts than fine-tuning and other gradient-based knowledge-editing methods (De Cao et al., 2021;Mitchell et al., 2022).\n\nSelf-improving Another type of application of this transformation is to make an LLM to selfimprove its reasoning ability without supervised data.For instance, Huang et al. (2022) first use the initial LLM to generate high-quality answers for unlabeled questions using advanced prompting strategies such as few-shot chain-of-thought prompting (Wei et al., 2022) and self-consistency (Wang et al., 2022), and fine-tune the LLM using those self-generated solutions as target outputs (Eq.10).\n\nTheir experiments show that this approach improves the results of the LLM across a wide range of reasoning tasks without any ground truth label.\n\nLearning from Language Feedback In addition, Scheurer et al. (2023) propose imitation learning from language feedback (ILF), which applies the transformation to internalize language feedback.In ILF, the LLM is first instructed to generate multiple refined outputs given the input, the initial model output, and the external feedback.Then, the most feedback-incorporating refinement is selected and used to fine-tune the LLM (Eq.10).This can be regarded as injecting informative language feedback into model parameters.",
            "score": 0.3990127307690955,
            "section_title": "Applications",
            "char_start_offset": 30070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 601
                },
                {
                    "start": 603,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1091
                },
                {
                    "start": 1093,
                    "end": 1237
                },
                {
                    "start": 1239,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1572
                },
                {
                    "start": 1572,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 578,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 578,
                    "end": 600,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 762,
                    "end": 781,
                    "matchedPaperCorpusId": "253080328"
                },
                {
                    "start": 945,
                    "end": 963,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 985,
                    "end": 1004,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1284,
                    "end": 1306,
                    "matchedPaperCorpusId": "257805110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69970703125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai and Durrett, 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai and Durrett, 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS).",
            "score": 0.39886228064543816,
            "section_title": "Calibration Methods",
            "char_start_offset": 4891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 153,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1081,
                    "end": 1102,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 1140,
                    "end": 1165,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1382,
                    "end": 1405,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1512,
                    "end": 1537,
                    "matchedPaperCorpusId": "212747810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "267034966",
            "title": "Improving Classification Performance With Human Feedback: Label a few, we label the rest",
            "text": "Our investigation into few-shot learning and active learning methodologies is promising in enhancing language models with minimal labeled data. We were able to produce notable improvements in model accuracy, recall, and precision across diverse domain-specific datasets by utilizing a continuous feedback loop and integrating human expertise. The ability to leverage a minimal number of labels to refine a model will be extremely beneficial to businesses. Companies will be able to maintain model performance levels while minimizing the resources typically required for manual labeling processes. While our research demonstrated the potential of few-shot learning with human feedback, there are some limitations. Our study is focused on a limited number of models such as GPT, BERT, and SetFit. To gain a more comprehensive understanding of the applicability of few-shot learning across various model structures, we can expand the work to train a diverse array of models. We could test newer model architectures like T5, Transformer-XL, or different BERT and GPT 4 variations. Experimenting with additional models can provide deeper insights into the effectiveness of these approaches.",
            "score": 0.3987784537321175,
            "section_title": "Conclusion",
            "char_start_offset": 11665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1185
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "247613322",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "text": "PLMs (Qiu et al., 2020;Min et al., 2021) have achieved state-of-the-art performance on a broad spectrum of NLP benchmarks (Rajpurkar et al., 2016(Rajpurkar et al., , 2018Wang et al., 2019a,b) and are increasingly popular in various downstream applications such as question answering (Yoon et al., 2019;Garg et al., 2020), text classification (Arslan et al., 2021;Limsopatham, 2021), and relation extraction (Zhou et al., 2021;Xiao et al., 2022). Consequently, it is paramount for PLMs to faithfully communicate when to (or not to) rely on their predictions for decision-making, especially in high-stakes scenarios. In these cases, we need PLMs to quantify their uncertainty accurately and calibrate well (Abdar et al., 2021), meaning that their predictive confidence should be a valid estimate of how likely they are to make a correct prediction. Consider an example of medical question answering (Yoon et al., 2019; where a PLM is asked to assist doctors when diagnosing diseases. If the PLM is 90% sure that a patient is healthy, the predicted outcome should occur 90% of the time in practice. Otherwise, it may adversely affect doctors' judgment and lead to catastrophic consequences. Hence, since PLMs have become the de facto paradigm for many NLP tasks, it is necessary to assess their calibration quality.\n\nWhen constructing a well-calibrated PLM-based prediction pipeline for NLP tasks, various considerations are involved. To name a few:\n\n1. Due to the use of diverse pre-training datasets and strategies, different PLMs may behave differently regarding calibration. 2. The model size of PLMs may also affect their capability in calibration. 3. Leveraging uncertainty quantifiers (e.g., Temp\n\nScaling (Guo et al., 2017) and MC Dropout (Gal and Ghahramani, 2016)) alongside PLMs in the pipeline may reduce calibration error. 4. Some losses (e.g., Focal Loss (Mukhoti et al., 2020) and Label Smoothing (M\u00fcller",
            "score": 0.39857759446045876,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 23,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 122,
                    "end": 145,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 145,
                    "end": 170,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 283,
                    "end": 302,
                    "matchedPaperCorpusId": "202660890"
                },
                {
                    "start": 302,
                    "end": 320,
                    "matchedPaperCorpusId": "207853043"
                },
                {
                    "start": 342,
                    "end": 363,
                    "matchedPaperCorpusId": "235324756"
                },
                {
                    "start": 363,
                    "end": 381,
                    "matchedPaperCorpusId": "241583607"
                },
                {
                    "start": 407,
                    "end": 426,
                    "matchedPaperCorpusId": "225039888"
                },
                {
                    "start": 426,
                    "end": 444,
                    "matchedPaperCorpusId": "237635295"
                },
                {
                    "start": 897,
                    "end": 916,
                    "matchedPaperCorpusId": "202660890"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "247613322",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "text": "Due to computational constraints, we are unable to pre-train PLMs from scratch with other combinations of pre-training corpora and tasks. Consequently, while our analysis is applicable to existing widely-used PLMs, we do not claim its generalization to new combinations of pre-training corpora and tasks. We believe that this does not invalidate our claims which are primarily targeted toward real-world practitioners using existing PLMs. It is possible that techniques catering to the special needs of PLM-based pipelines (Kong et al., 2020) can mitigate calibration error further. Moreover, although our setup involves domain shift, we do not focus on inspecting how the degree of domain shift affects the calibration performance of PLM-based pipelines. It is also interesting to consider how to construct a well-calibrated PLMbased pipeline for other types of NLP tasks such as cross-lingual text classification and generation, which we leave to future work.",
            "score": 0.39857759446045876,
            "section_title": "Limitation",
            "char_start_offset": 21500,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "277596537",
            "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
            "text": "In the context of LLMs, calibration refers to how well an LLM's predicted confidence aligns with its actual accuracy. As one of the most common calibration approaches, temperature scaling (Guo et al., 2017) is a post-hoc calibration strategy that aligns model predictions with observed probabilities. We adapt the method from (Shen et al., 2024), a temperature scaling calibration approach tailored to LLMs that learns an auxiliary model to map the outputs of the LLM to better-calibrated probabilities. The calibration formula is shown below: \n\nThe key idea is to train an neuro network to fit the logits distribution and then use the network to infer task-specific latent temperatures \u03c4, allowing the model to adapt to new questions with learned parameters. \u03d5(q n ; W) is the feature that the language model produces for the input token sequence q n . \u2211 v \u2032 exp(w T v \u2032 \u03d5(q n ; W)/\u03c4 k ) is the sum of exponential over all possible tokens v \u2032 in the vocabulary. W and w are model parameters and the logit vector transformation, respectively. The method is computationally efficient, to preserve the accuracy of the LLM, and takes a step towards being universal among different tasks.",
            "score": 0.39857759446045876,
            "section_title": "Calibration",
            "char_start_offset": 11341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1184
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 205,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 326,
                    "end": 345,
                    "matchedPaperCorpusId": "268385471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "274965235",
            "title": "The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration",
            "text": "The advent of pre-trained language models (PLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found PLMs to suffer from miscalibration, indicating a lack of accuracy in the confidence estimates provided by these models. Current evaluation methods for PLM calibration often assume that lower calibration error estimates indicate more reliable predictions. However, fine-tuned PLMs often resort to shortcuts, leading to overconfident predictions that create the illusion of enhanced performance but lack generalizability in their decision rules. The relationship between PLM reliability, as measured by calibration error, and shortcut learning, has not been thoroughly explored thus far. This paper aims to investigate this relationship, studying whether lower calibration error implies reliable decision rules for a language model. Our findings reveal that models with seemingly superior calibration portray higher levels of non-generalizable decision rules. This challenges the prevailing notion that well-calibrated models are inherently reliable. Our study highlights the need to bridge the current gap between language model calibration and generalization objectives, urging the development of comprehensive frameworks to achieve truly robust and reliable language models.",
            "score": 0.39857759446045876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "229363301",
            "title": "On Calibration of Scene-Text Recognition Models",
            "text": "The framework defines the notion of \"Events of Interest\" coupled with confidence scores allowing event-level calibration. The practical methods laid out   [7]: (i) AttDec -a variant of [1] with an attention decoder, (ii) CTCDec -a variant of [1] with a CTC decoder, (iii) SCATTER [26] and (iv) SEED [39]. We calibrate using T-scaling coupled with an equal bin size ECE objective applied to the word-level scalar confidence scores. The accuracy here is measured w.r.t exact word match. The figure shows accuracy vs. confidence plotted for equally-sized confidence bins, before and after calibration. Over-confidence can be observed for STR models, where the confidence of the model is higher than the expected accuracy. by Kuleshov and Liang [19], however, predate the recent advances in DNNs. \n\nKumar et al. [20] address the problem of miscalibration in neural machine translation (NMT) systems. The authors show that NMT models are poorly calibrated and propose a calibration method based on a T-scaling variant where the temperature is predicted at each decoding step. They were also able to improve translation performance by applying beam-search to calibrated models. Our experiments find this to be beneficial for the task of STR as well. \n\nDesai et al. [8] suggest the usage of T-Scaling for calibration of pre-trained transformers models (e.g. BERT [9]). the authors differentiate between in and out of domain calibration and propose using T-Scaling, and label-smoothing [37] techniques. We point-out that label smoothing is carried out during the training phase and therefore affects the model accuracy. Here, calibration is also conducted at the individual output level. \n\nIn another work [22], a proposed extension to T-scaling calibration for sequences is presented. The authors employ a parametric decaying exponential to model the temperature for each decoding step. Again, similarly to [20] calibration is performed for each decoding step and not for entire sequences.",
            "score": 0.39823947861946657,
            "section_title": "Related Work",
            "char_start_offset": 6649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "109884250"
                },
                {
                    "start": 185,
                    "end": 188,
                    "matchedPaperCorpusId": "102481180"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "102481180"
                },
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "214641123"
                },
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "218862702"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "2974522"
                },
                {
                    "start": 1259,
                    "end": 1262,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 1356,
                    "end": 1359,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "258378137",
            "title": "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models",
            "text": "Though pre-trained language models achieve notable success in many applications, it\u2019s usually controversial for over-confident predictions. Specifically, the in-distribution (ID) miscalibration and out-of-distribution (OOD) detection are main concerns. Recently, some works based on energy-based models~(EBM) have shown great improvements on both ID calibration and OOD detection for images. However, it\u2019s rarely explored in natural language understanding tasks due to the non-differentiability of text data which makes it more difficult for EBM training. In this paper, we first propose a triple-hybrid EBM which combines the benefits of classifier, conditional generative model and marginal generative model altogether. Furthermore, we leverage contrastive learning to approximately train the proposed model, which circumvents the non-differentiability issue of text data. Extensive experiments have been done on GLUE and six other multiclass datasets in various domains. Our model outperforms previous methods in terms of ID calibration and OOD detection by a large margin while maintaining competitive accuracy.",
            "score": 0.3981669696048351,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "273098373",
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "text": "We further propose a metric to help describe the phenomenon in more details. We hypothesize that this indiscriminate miscalibration occurs because the model treats each sample independently and has not been trained on the corresponding dataset. As a result, the predicted probabilities are not comparable across samples, which can further negatively impact prediction accuracy. \n\nTo this end, we propose a label-free in-context comparative inference method that adds unlabeled samples to the prompt. This encourages the model to adjust and calibrate its predictions without requiring labels for the demonstration examples. The added examples serve as a proxy to help the Figure 1: Simulated reliability diagrams show different miscalibration behaviors but having the same ECE and accuracy. (a) an indiscriminate miscalibration behavior which is also observed in zero-shot and few-shot prompting in our experiments; (b) a regular miscalibration behavior closer to the original calibration paper (Guo et al., 2017). \n\nmodel better understand the test examples and the corresponding task. We delve deeper into the principles of our method and develop an aggregation step for improved calibration and a post-hoc calibration method that can further enhance performance. Through experiments on multiple datasets, we demonstrate significant gains in performance measured by F1 scores, accuracy, and traditional calibration metrics. We also show that our label-free in-context comparative inference method helps to alleviate indiscriminate miscalibration, enabling models to assign higher confidence to correct predictions and lower confidence to incorrect ones.",
            "score": 0.3974101499398145,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 377
                },
                {
                    "start": 380,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1654
                }
            ],
            "ref_mentions": [
                {
                    "start": 994,
                    "end": 1012,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "265220820",
            "title": "Leveraging Code to Improve In-Context Learning for Semantic Parsing",
            "text": "Semantic parsing studies have traditionally used DSLs. We posit that using general-purpose PLs with a structured description of the domain in hand could better exploit the potential of modern LLMs, which are pretrained on a mix of code and natural language. \n\nLeveraging Existing Coding Knowledge. While DSLs tailored to specific domains can be valuable for trained domain experts, their rarity makes it challenging for LLMs to learn them from just a few demonstrations. In contrast, PLs are prevalent in pretraining corpora; by prompting LLMs to generate PLs rather than DSLs, LLMs can leverage their existing coding knowledge without the need to learn the syntax and standard operations for a new language from scratch. \n\nFor instance, consider the operator most in Figure 1. LLMs with no prior knowledge of the given DSL struggle to correctly apply this operator without sufficient demonstrations. However, with Python, the model can exploit its parametric knowledge to perform this operation by employing the built-in max and len operators of Python, along with list comprehension. Another example is filtering sets of items in \u03bb-DCS (Table 1, Overnight). Using a rare DSL, models must learn how to correctly use the filter operator from just a few demonstrations. However, LLMs have likely already seen a myriad of filtering examples during pretraining, e.g. in the form of Python's conditional list comprehension. Domain Descriptions. While using PLs allows the model to leverage its parametric knowledge of the language's generic aspects, the LLM is still tasked with understanding domain-specific functionalities from a few in-context demonstrations. This is challenging, often even impossible, in a true few-shot setup, where the few fixed demonstrations may not cover all the functionality necessary to satisfy the test input request. A line of prior work alleviated this issue by selecting the most relevant demonstrations for every test input (Levy et al., 2023;Gupta et al., 2023), but this approach typically requires a large labeled pool of demonstrations. \n\nTo address this challenge in a true few-shot setup, we propose an intuitive solution that naturally aligns with the use of PLs: providing the model with a Domain Description (DD) outlining the available operators.",
            "score": 0.39701706760704514,
            "section_title": "Domain-Augmented PL Prompts",
            "char_start_offset": 8918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2071
                },
                {
                    "start": 2074,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.638671875
        },
        {
            "corpus_id": "276249447",
            "title": "Reinforced Lifelong Editing for Language Models",
            "text": "We also evaluate various model editing methods using standard metrics on the FEVER dataset. Specifically, given an LLM f W , an editing knowledge pair (x, y), equivalent knowledge x e , and unrelated knowledge pairs (x loc , y loc ), we examine the following three metrics: \n\nEfficacy. This metric measures the success rate of editing knowledge x in f W . It compares whether the top-1 logits output y \u2032 = f W (x) matches the target output y when inputting x into f W : \n\nGeneralization. This metric measures the success rate of editing equivalent knowledge x e in f W . Since we need to examine whether a knowledge edit is truly successful, we must verify if the LLM has genuinely learned the intrinsic relationships of the knowledge and can extend to other equivalent knowledge. We compare whether the top-1 logits output y \u2032 = f W (x e ) matches the target output y when inputting x e into f W : \n\nSpecificity. This metric measures the retention rate of unrelated knowledge x loc after editing, examining whether the knowledge editing maintains locality and only modifies the target knowledge. We compare whether the top-1 logits output y \u2032 = f W (x loc ) matches the original output y loc when inputting x loc into f W : \n\nA.4. GLUE Benchmark GLUE (General Language Understanding Evaluation) (Wang et al., 2019) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. We selected 6 metrics from this benchmark to evaluate how well different methods maintain general language capabilities. \n\n\u2022 Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a dataset consisting of movie review sentences with their associated sentiment labels. This binary classification task requires models to categorize the sentiment expressed in each individual sentence. \n\n\u2022 Massive Multi-task Language Understanding (MMLU) (Hendrycks et al., 2021) is a comprehensive benchmark designed to assess language models' capabilities across multiple domains. It specifically evaluates model performance in zero-shot and few-shot learning scenarios. \n\n\u2022 Microsoft Research Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005) serves as a benchmark for evaluating semantic similarity.",
            "score": 0.39698021790186205,
            "section_title": "FEVER METRICS",
            "char_start_offset": 37018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1315,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1594,
                    "end": 1615,
                    "matchedPaperCorpusId": "990233"
                },
                {
                    "start": 1874,
                    "end": 1898,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7197265625
        },
        {
            "corpus_id": "240354626",
            "title": "Unsupervised Domain Adaptation with Adapter",
            "text": "Specifically, several bottle-necked adapter modules are inserted in a transformer-based PrLM [Vaswani et al., 2017]. These adapters are learned following a two-step process: 1) The domain-fusion training step trains adapters with the Masked-Language-Model (MLM) loss [Devlin et al., 2019] on a mixed corpus containing data from both the source and all the target domains. This step facilitates the capture and fusion of transferable knowledge between different domains; 2) The task fine-tuning step fine-tunes adapters with the task-specific loss on the source domain corpus. Note that parameters of the underlying pre-trained LM are fixed throughout the two learning processes. This helps prevent the drifting of learned generic knowledge and facilitates more effective domain knowledge transferring [Pfeiffer et al., 2020]. In the testing phase, we apply the resulted model to data sampled from the target domain. The results on two benchmark datasets indicate that our method outperforms competitive baselines and is effective in improving the performance of downstream UDA tasks 3 . \n\nOur contributions can be summarized as: \n\n1. We apply adapter modules in the pre-training-based UDA approaches. Specifically, trainable adapters are introduced in a PrLM, and a two-step process is introduced to facilitate the learning of these adapters. \n\n2. Elaborated experiments on two benchmark datasets show that our approach outperforms competitive baselines and is more effective to improve the performance of downstream UDA tasks. The most similar works comparing to our study are the models for zero-shot cross-lingual transfer tasks [Pfeiffer et al., 2020, Vidoni et al., 2020]. However, these models aim to separate language-specific knowledge using adapters, while our UDA task tries to capture common and transferable features across different domains. \n\n3 Method",
            "score": 0.39691289511820693,
            "section_title": "Introduction",
            "char_start_offset": 1794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1086
                },
                {
                    "start": 1089,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1854
                },
                {
                    "start": 1857,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 115,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 267,
                    "end": 288,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "245144954",
            "title": "Local calibration: metrics and recalibration",
            "text": "We choose an intermediate bandwidth, so our metric can be accurately estimated, and provides some measurement on the reliability of individual predictions. \n\nTheoretically, we show that the LCE can be estimated with polynomially many samples if the kernel function is bounded. Empirically, we also show that for intermediate values of the bandwidth, the LCE can be accurately estimated and reveals modes of miscalibration that global metrics (such as ECE) fail to uncover. \n\nIn addition, we introduce a non-parametric, post-hoc localized recalibration method (LoRe), for lowering the LCE. Empirically, LoRe improves fairness by achieving low calibration error on all potentially sensitive subsets of the data, such as racial groups. Notably, it can do so without any prior knowledge of those groups, and is more effective than global methods at this task. In addition, our recalibration method improves decision making when there is a \"safe\" action that is selected whenever the predicted confidence is low. For example, an automated system which classifies tissue samples as cancerous should request a human expert opinion whenever it is unsure about a classification. In a simulation on an image classification dataset, we show that recalibrated prediction models more accurately choose whether to use the \"safe\" action, which improves the overall utility. \n\nIn summary, the contributions of our paper are as follows. \n\n(1) We introduce a local calibration metric, the LCE, that is both easy to compute and can estimate the reliability of individual predictions. (2) We introduce a post-hoc localized recalibration method LoRe, that transforms a model's confidence predictions to improve the local calibration. (3) We empirically evaluate LoRe on several downstream tasks and observe that LoRe improves fairness and decision-making more than existing baselines.",
            "score": 0.39629121917940213,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "260316095",
            "title": "TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety",
            "text": "Due to the swift advancements in LLM and the increasing demands for practical applications across diverse fields, researchers recognized the need to enhance the performance of LLMs by fune turning the models based on domain knowledge. For example, researchers have acknowledged the imperative to improve the performance of LLMs in medical applications, where precision is of utmost importance, primarily due to their limited domain-specific knowledge. To address this issue, they developed ChatDoctor, a specialized model tailored specifically for the biomedical field. The main objective of ChatDoctor is to enhance the understanding of LLMs in this domain. This endeavor underscores the significance of integrating domain-specific knowledge to attain superior outcomes in medical applications [18]. \n\nAnother notable study focused on enhancing LLMs' performance in the specific domain through domain-specific knowledge transfer with different languages. As an illustration, researchers effectively fine-tuned the LLaMA model with Chinese medical knowledge, which contributed to improved comprehension and performance in Chinese LLM applications [19]. This research emphasized the importance of domain-specific tuning to optimize LLMs for specific fields [20]. Several technical approaches were explored to enhance domain knowledge transfer and fine-tuning of LLMs. Effective techniques such as soft fine-tuning [21], large-scale fine-grained categorization [21], and domain-specific transfer learning were employed to optimize LLM performance in specialized domains(). Additionally, the LLaMA-adapter method demonstrated efficient fine-tuning of language models with zero-init attention, contributing to better adaptability in domain-specific tasks [22]. There are some domain knowledge datasets that support LLM fine-tuning, such as Alpaca, which contributes 52k instruction-following data points by leveraging Self-Instruct techniques to encode specific instructions within conversations [23] .The HealthCareMagic-100k dataset encompasses 100k real-world patient-physician conversations, providing valuable insights into authentic medical interactions [18]. On the other hand, GenMedGPT-5k enriches dataset diversity with 5k synthetic conversations between patients and physicians [18].",
            "score": 0.39599716317319356,
            "section_title": "Pre-trained Models with Domain Knowledge Fine-tuning",
            "char_start_offset": 5925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "274131235",
            "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation",
            "text": "Due to a lack of training data for low-resource languages, most LLMs cannot properly translate lowresource languages. Moreover, it is hard to finetune these LLM without sufficient language resources. Thus, various research efforts focus on improving LLMs to perform translation tasks with minimal or no task-specific training, commonly referred to as zero-shot and few-shot learning. \n\nZero-shot translation refers to the capability of LLMs to translate between language pairs without having seen explicit examples of these translations during training. For instance, Zhang et al. proposed strategies such as random online back-translation and language-specific modeling, improving zero-shot performance in multilingual NMT by approximately 10 BLEU score [75]. Gao et al. introduced Cross-lingual Consistency Regularization (CrossConST), which enhances zero-shot performance by bridging the representation gap across languages, proving effective in both high-resource and low-resource language settings [17]. Although zero-shot translation can provide good results for certain languages, particularly when leveraging shared linguistic patterns from pretraining on multilingual corpora, it basically leverages well-resourced languages (often English) to facilitate translation between low-resource language pairs. Thus, the performance will degrade when applied to highly underrepresented languages. This highlights the need for additional strategies such as fine-tuning or incorporating domain-specific data. \n\nCompared with no training examples in zero-shot translation, few-shot translation involves exposing a model to a small number of translation examples before performing the task. In this scenario, LLMs can adapt quickly to specific language pairs or domains with minimal data, offering a highly flexible solution for low-resource languages. Zhang et al. explored few-shot learning in machine translation using LLMs [80]. They compared prompting, few-shot learning, and fine-tuning approaches, finding that few-shot learning often outperforms zero-shot prompting when the model is given even a few translation examples. This research highlighted the flexibility and efficiency of LLMs, especially when fine-tuned using the QLoRA method, which allows the models to adapt to machine translation tasks with minimal data and memory usage.",
            "score": 0.395911353640074,
            "section_title": "Zero-shot and Few-shots Translation",
            "char_start_offset": 24540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "272727920"
                },
                {
                    "start": 1003,
                    "end": 1007,
                    "matchedPaperCorpusId": "258676511"
                },
                {
                    "start": 1925,
                    "end": 1929,
                    "matchedPaperCorpusId": "264869413"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "264555283",
            "title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models",
            "text": "A foundation model is a term that describes a type of machine learning model that is pre-trained on vast amounts of data to capture a wide range of knowledge, and then fine-tuned on more specific tasks or datasets. [3] Their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. [3] 2.3.1 LLMs for Zero-Shot Classification. Recent works on instructionfollowing such as [36] and [7] have demonstrated that fine-tuning large language models on a large pre-formatted corpus of promptresponse pairs is an effective strategy for improving generalization capabilities on arbitrary classification tasks. This suggests that competitive zero-shot performance on this task may be achievable. However, the problems in these datasets tend to have small label sets, clean labels, balanced classes, few implicit hierarchies in the label set, and have largely been focused on NLP rather than tabular classification tasks. \n\nDespite the ubiquity of large language models in the research literature, there have been surprisingly few attempts to apply them to tabular classification problems. One recent contribution from [17] found success on a wide range of problems using a novel prompt serialization method. Unlike their work, our paper focuses on realworld datasets and problems, where the space of potential classes is very large, label pollution and class imbalance are common, and training data is scarce. \n\nIn our discussion below, we refer to certain results as zero-shot. In this work, we follow the definition provided by [4]; no demonstrations are allowed in the prompt, and only an instruction in natural language is given to the model. We note the distinction from other definitions of \"zero-shot\", which sometimes allows for demonstrations (or training examples) provided as part of the prompt. Explainability. The architectures of most closed-source models are not known to the public; nor is it known how much prompt engineering and behind-the-scenes modification of the model output is being conducted. The specifics of the data on which these models are trained is also unknown.",
            "score": 0.39562873330279585,
            "section_title": "LLMs (Foundation Models)",
            "char_start_offset": 10366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2221
                },
                {
                    "start": 2222,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "252992811"
                },
                {
                    "start": 1734,
                    "end": 1737,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38232421875
        },
        {
            "corpus_id": "270248612",
            "title": "GlyReShot: A glyph-aware model with label refinement for few-shot Chinese agricultural named entity recognition",
            "text": "Few-shot NER models aim to perform NER with scarce labeled data.Metric-based models with meta-learning, typified by Prototypical Networks [27], are valued on this task for its advantage of teaching the machine to learn.Fritzler et al. [10] are the first to apply Prototypical Networks to few-shot NER, taking it as a token classification task.Subsequently, several adjustments are made by researchers to address the imprecise prototypical representations, including changing the way of obtaining prototypes [28], exploring the miscellaneous non-entity tokens (a.k.a.\"O\"-labeled tokens) [29], incorporating label semantics [30] and enhancing token representations with contrastive learning [31].Furthermore, in view of the neglect of label dependency of metric-based models, collapsed dependency between labels is introduced to improve few-shot NER inference [11,28].Different from all the aforementioned models, our model introduces a lightweight module to exploit the glyph knowledge of Chinese characters and integrates this information into the metric-based few-shot NER model by a label refinement strategy.\n\nWith the popularity of large language models (LLM) represented by ChatGPT, 1 some researchers have studied few-shot NER with LLM.Many methods focus on retrieving demonstrations, transforming NER to the generation task or designing data augmentation methods.For example, Wang et al. [32] convert NER to a text-generation task by adding special tokens around the entities.Cai et al. [33] develop a method of retrieving demonstrations for few-shot multimodal NER, considering similarity ranks from both modalities.Ye et al. [34] design a data augmentation technique for few-shot NER, which augments data at different levels with LLM.Mo et al. [35] introduce both correct and incorrect samples to create demonstrations for LLM, thus obtaining additional knowledge for information extraction tasks.Compared with these methods, the proposed model exhibits the following characteristics: First, many LLM-based models view NER as a generative task, and their predictions are unstable due to the influence of prompt design.",
            "score": 0.3951109247983428,
            "section_title": "Few-shot NER",
            "char_start_offset": 7575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 64,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1111
                },
                {
                    "start": 1113,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1906
                },
                {
                    "start": 1906,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "309759"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "56482394"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "235669988"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "247518876"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "237532321"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "219558508"
                },
                {
                    "start": 1494,
                    "end": 1498,
                    "matchedPaperCorpusId": "266166708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331298828125
        },
        {
            "corpus_id": "264288909",
            "title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
            "text": "In order to achieve homogeneity and make effective use of pretraining data, it is crucial to design appropriate pre-training tasks in pre-training. Unlike many language foundation models, which often use LM [127] or MLM [54] as pre-training tasks, there are now various forms of pre-training tasks tailored to different GFM model architectures. Whether each type of pre-training task has its own applicable scope and whether there will be a unified pre-training task are worth further exploration. Additionally, enabling graph foundation models to support cross-domain data is a vital concern. Some works use data from different domains as model input for pre-training [98,190], or enable adaptation to data from different domains through methods such as LLMbased embedding [142], condition generation [191] and zero-shot transfer [172]. Finally, apart from fine-tuning and prompting that are introduced in this article, there are other potential training techniques that can be applied to improve efficiency or update knowledge, such as knowledge distillation [192], reinforcement learning from human feedback (RLHF) [127] and model editing [193]. Whether the above-mentioned techniques can be applied to graph foundation models will be a focal point of future research.",
            "score": 0.3950079657683956,
            "section_title": "Model Training",
            "char_start_offset": 89098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1271
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 212,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 774,
                    "end": 779,
                    "matchedPaperCorpusId": "265871676"
                },
                {
                    "start": 802,
                    "end": 807,
                    "matchedPaperCorpusId": "263834643"
                },
                {
                    "start": 831,
                    "end": 836,
                    "matchedPaperCorpusId": "267938438"
                },
                {
                    "start": 1061,
                    "end": 1066,
                    "matchedPaperCorpusId": "258833333"
                },
                {
                    "start": 1118,
                    "end": 1123,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1142,
                    "end": 1147,
                    "matchedPaperCorpusId": "258833129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70166015625
        },
        {
            "corpus_id": "276106916",
            "title": "FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named Entity Recognition in a Multilingual Framework",
            "text": "Techniques such as parameter-sharing and fine-tuning have been employed to optimize representations across different domains, improving adaptability while minimizing the need for extensive labeled data [18]. However, conventional transfer learning methods often assume that source and target domains share the same label space (homogeneous transfer) or require a sufficient number of annotated examples to bridge the distributional gap in heterogeneous settings. Recent advancements in adversarial domain adaptation have attempted to address these challenges by learning domain-invariant representations, effectively enhancing generalization across diverse datasets [19]. Despite these improvements, challenges persist in adapting NER models to domains with extremely scarce target data, highlighting the need for more efficient domain adaptation techniques. \n\nFew-Shot Learning for NER. Few-shot learning has become a crucial area of research for Named Entity Recognition (NER), enabling models to generalize to new entity types with minimal annotated data. Unlike traditional sequence labeling methods, few-shot NER presents unique challenges due to the variability of entity occurrences within a sentence and the absence of a predefined entity class set [20]. To address these challenges, meta-learning techniques such as FewNER have been proposed. FewNER introduces a task-adaptive training approach that parti-tions the network into task-independent and task-specific components, allowing for rapid adaptation with minimal data while mitigating overfitting [21]. Additionally, decomposed meta-learning frameworks have been introduced to sequentially optimize span detection and entity classification, improving generalization across domains [22]. Other advances include the integration of knowledge graphs to enhance prototype-based few-shot NER models [23] and contrastive learning to improve entity cluster separation in low-resource settings [24]. Despite these advancements, few-shot NER continues to face challenges in handling complex entity dependencies and adapting to highly diverse domains, necessitating further innovations in adaptive learning and self-supervised techniques. \n\nMeta-Learning Approaches in NER. Meta-learning has gained significant traction in natural language processing (NLP), particularly for Named Entity Recognition (NER), where labeled data is often scarce.",
            "score": 0.39485124668227134,
            "section_title": "Related Work",
            "char_start_offset": 5509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2192
                },
                {
                    "start": 2195,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2396
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "273254452"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "270353779"
                },
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "236486243"
                },
                {
                    "start": 1562,
                    "end": 1566,
                    "matchedPaperCorpusId": "260172183"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "252917981",
            "title": "Prompting GPT-3 To Be Reliable",
            "text": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",
            "score": 0.3948440835207187,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "277766037",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "text": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we derive model confidence from the distribution of multiple randomly sampled generations, using three measures of consistency. We extensively evaluate eleven open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches in terms of calibration error. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency can potentially enhance model performance. Finally, we offer guidance on choosing suitable consistency metrics for calibration, tailored to model characteristics such as the exposure to instruction-tuning and RLHF.",
            "score": 0.3946961894953656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "276647283",
            "title": "GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration",
            "text": "Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.",
            "score": 0.3946961894953656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "260379149",
            "title": "Calibration in Deep Learning: A Survey of the State-of-the-Art",
            "text": "The LLMs and prompting engineering have become an efficient learning paradigm and can perform numerous natural language tasks without or with only few examples. However, the outcome of this learning paradigm can be unstable and introduces bias with various prompt templates and training examples (Zhao et al., 2021). The introduced biases include majority label bias, recency bias and token bias. To mitigate this bias, (Zhao et al., 2021) proposed contextual calibration procedure to improve the predictive power of GPT-3 on few-shot learning tasks with a context-free input such as \"N/A\". The contextual calibration is performed by using vector scaling (Guo et al., 2017). \n\nwhere W is a diagonal matrix. \n\nLater on (Fei et al., 2023) proposed domain-context calibration (DC) to mitigate label biases in in-context learning (ICL). The method tries to use random words sampled from the task corpus to estimate the effects of different label biases: \n\nwhere W is a diagonal matrix. Similarly, (Zheng, Zhou, Meng, Zhou, & Huang, 2023) investigated the selection bias of LLMs in multi-choice question (MCQ) task and pinpointed that LLMs' token bias is an intrinsic cause of the selection bias. They proposed label-free, inference-time debasing method PriDe, which demonstrates notable effectiveness and efficiency, interpretability, and crossdomain transferability. (Park & Caragea, 2022) extended mixup (Zhang et al., 2018) training to improve model calibration by synthesizing samples based on the Area Under the Margin (AUM) for pre-trained language models. Prototypical calibration (PROCA) (Han et al., 2023) is one of the latest studies in calibrating LLMs. It showed the importance of decision boundary in few-shot classification settings and suggested learning a better boundary with a prototypical cluster. Concretely, it estimates K category-wise clusters with the Gaussian mixture model (GMM): \n\nwhere u k and \u03a3 k are the mean vector and covariance matrix of the distribution.",
            "score": 0.3946961894953656,
            "section_title": "LARGE LANGUAGE MODELS (LLMS)",
            "char_start_offset": 45581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 315,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 420,
                    "end": 439,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 655,
                    "end": 673,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 718,
                    "end": 736,
                    "matchedPaperCorpusId": "258967265"
                },
                {
                    "start": 1364,
                    "end": 1386,
                    "matchedPaperCorpusId": "247450599"
                },
                {
                    "start": 1402,
                    "end": 1422,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1592,
                    "end": 1609,
                    "matchedPaperCorpusId": "248964978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "268723623",
            "title": "Few-Shot Recalibration of Language Models",
            "text": "We evaluate our few-shot recalibrator on two datasets: MMLU (Hendrycks et al., 2021) consists of multiple choice questions categorized into 57 different subjects (e.g.abstract algebra, high school physics, law), each of which serves as a separate domain.XNLI (Conneau et al., 2018) is a natural language inference task, where the model predicts if the given hypothesis entails, contradicts or is neutral to the corresponding premise.Examples are categorized into 10 genres (e.g.travel guides, speeches, etc.) in 15 languages each, for a total of 150 domains.\n\nWe follow Algorithm 1 to construct 20K slices for the training set and 2K unseen slices for the test set, ensuring that examples which appear in the test data's few-shot sets are held out from training.We also construct an UNSEEN test set for XNLI, where 10 domains are entirely held out from the training data and are used to construct a separate set of 2K mixtures.For the main experiments we set k = 20, and for ablation studies, we consider k = {5, 10, 20, 30}.",
            "score": 0.3946961894953656,
            "section_title": "Datasets",
            "char_start_offset": 16170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 558
                },
                {
                    "start": 560,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 84,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 259,
                    "end": 280,
                    "matchedPaperCorpusId": "52271711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "264405904",
            "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
            "text": "\u2022 We show that finetuning large language models degrades their few-shot performance, limiting their adaptation capabilities ( \u00a73.2). In particular, we show that finetuned LLMs perform poorly on domain adaptation scenarios when provided in-context examples. \n\n\u2022 To address this issue, we propose a simple approach that introduces few-shot examples during finetuning ( \u00a74). Our results show that we can recover few-shot capabilities while retaining the benefits of finetuning.",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 1980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 474
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "273098373",
            "title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free Comparative Inference",
            "text": "In this paper, we study a special miscalibration behavior of large language models when being used for zero-shot and few-shot prompting, which we refer to as indiscriminate miscalibration. We propose metrics to quantify the severity of this issue and develop a label-free in-context comparative inference method to mitigate it. We show our label-free method can achieve better classification performance as well as more calibrated predictions on multiple datasets.",
            "score": 0.3946961894953656,
            "section_title": "DISCUSSION",
            "char_start_offset": 25087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 464
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "236486129",
            "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
            "text": "We presented a study of calibration of finetuned language models in the context of text classification, where models are evaluated on in-domain and out-of-domain data. We showed the effectiveness of a few widely-used calibration methods. We illustrated the intuitive connection between distillation and calibration, and described simple yet competitive calibration methods. We conducted experiments to empirically understand whether distillation can be used to distill calibration performance, and showed that the simple methods we described achieved competitive out-of-domain calibration performances. We further presented ablation studies on the usefulness of components of the proposed method and examined the transferability of calibration via distillation. However, our method is limited in that it requires an overhead cost involved in training the student model, which could be expensive in some settings. We leave it to future works to investigate more efficient inferencetime recalibration techniques.",
            "score": 0.3946961894953656,
            "section_title": "Conclusion and Discussion",
            "char_start_offset": 23753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "269009562",
            "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models",
            "text": "The introduction of LLMs has sparked significant excitement and anticipation in many scientific disciplines and in the general population. The large size of LLM training data and the wide scope of potential subject matter has led to massive experimentation. However, this enthusiasm is often met with disappointment when LLMs are utilized in contexts that are highly specialized, require high precision, or require knowledge that is not easily describable through text. This discrepancy between expectation and reality highlights a critical challenge in effectively deploying LLMs across diverse applications. In this paper, we initiate our discussion by acknowledging this phenomenon, recognizing the superior language modeling skills of the LLMs and, at the same time, highlight the need for caution for many applications and domains. By addressing this issue head-on, we aim to contribute to a deeper understanding of the practical considerations and challenges associated with deploying LLMs in real-world scenarios. \n\nThe idea of including domain knowledge for enhancing the performance of large language models is an established doctrine. Traditionally, incorporating domain knowledge into language models has been achieved through methods such as fine-tuning with domain-specific examples or employing specialized language models trained on domainspecific training data, such as BioMedLM 15 . However, these approaches face limitations for specialized domains and complicated downstream processing tasks. \n\nMetadata, while crucial for structuring and contextualizing data, present unique obstacles for language models such as GPT-4. First, metadata are typically not included in the training data provided to large language models, posing a significant barrier to their effective integration. Second, the scarcity of high-quality examples of \"good\" metadata further complicates the training process, limiting the models' ability to learn and adapt to metadata-related tasks. The task of metadata adherence neither has large chunks of suitable text for pre-training nor a good number of examples for fine-tuning or few-shot prompting. Another popular way of incorporating domain knowledge is through the method of knowledge graphs. This method is also unsuitable for the semi-structured nature of metadata. In our study, we have demonstrated how CEDAR templates, in conjunction with GPT-4, offer a promising solution to these challenges. By leveraging the linguistic description of the structured knowledge provided by CEDAR templates, we can guide the language-generation process and enhance metadata adherence.",
            "score": 0.39427271312796486,
            "section_title": "Discussion",
            "char_start_offset": 11385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2443
                },
                {
                    "start": 2444,
                    "end": 2618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "266902885",
            "title": "Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking",
            "text": "Training Large Language Models (LLMs) on numerous, diverse texts results in the embedding of extensive knowledge [56]. \n\nLLMs can also adeptly interpret complex information [16], general reasoning [44], and aiding knowledge-intensive decision-making. Consequently, researchers have been exploring applying LLM-powered tools in domain-specific tasks [47,50,55]. \n\nDespite their potential benefits, the responses generated by LLMs may have two potential issues: (1) outdated information originating from the model's training date, and (2) inaccuracies in factual representation, known as \"hallucinations\" [6,56]. To address these challenges and leverage the capabilities of LLMs in domain-specific knowledgeintensive tasks, several techniques can be used, such as chain-of-thought [45], few-shot prompting [7,13], and retrieval augmented generation [23]. \n\nUsing few-shot prompting to retrieve information across diverse topics, Semnani et al. [33] introduced an opendomain LLM-powered chatbot called WikiChat. WikiChat utilizes a 7-stage pipeline of few-shot prompted LLM that suggests facts verified against Wikipedia, retrieves additional up-to-date information, and generates coherent responses. \n\nThey used a hybrid human-and-LLM method to evaluate the chatbot on different topics for factuality, alignment with real-worth truths and verifiable facts, and conversationality. This compound metric scores how informational, natural, non-repetitive, and temporally correct the response is. Their solution significantly outperforms GPT-3.5 in factuality, with an average improvement of 24.4% while staying on par in conversationality. Others have explored the capabilities of LLMs in domain-specific tasks such as extracting structured data from unstructured healthcare texts [37], providing medical advice [31], simplifying radiology reports [17], Legal Judgement Prediction from multilingual legal documents [40], and scientific writing [3]. \n\nSeveral manufacturers are cautiously adopting LLMs, while seeking solutions to mitigate their associated risks. For example, Mercedes-Benz [30] used AI with ChatGPT integrated through Azure OpenAI Service to enhance quality management and process optimization in vehicle production.",
            "score": 0.39423206186528237,
            "section_title": "LLM-powered Knowledge Management Tools",
            "char_start_offset": 5312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1938,
                    "end": 1941,
                    "matchedPaperCorpusId": "257037938"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "256846523",
            "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
            "text": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL\u2019s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.",
            "score": 0.3939493135683824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "254366253",
            "title": "Discovering Latent Knowledge in Language Models Without Supervision",
            "text": "Methods. We test four main methods: zero-shot, calibrated zero-shot, Contrast-Consistent Search (CCS), and Logistic Regression (LR). Zero-shot works by predicting the answer with the highest log probability according to the language model, averaged across the tokens that make up that label. Calibrated zero-shot works by balancing zero-shot predictions to be 50/50 for each answer, as we describe in more detail below, similar to Zhao et al. (2021). For Logistic Regression we train on the training split for each dataset using ( \u03c6(x + ), \u03c6(x \u2212 )) as the covariates, then evaluate on the corresponding test split. We treat LR as a ceiling since it uses labeled data. \n\nWhen testing CCS, we optimize it 10 times using AdamW (Loshchilov & Hutter, 2017) with learning rate 0.01, then take the run with the lowest unsupervised loss. Unless otherwise specified, we train CCS using all prompts for a single training set (normalized independently for each prompt), then evaluate it on the corresponding test split. \n\nZero-shot baselines. Zero-shot outputs sometimes suffer from miscalibration (Zhao et al., 2021), in which models are biased towards predicting specific answers. Calibrating the outputs to be uniform Table 1: Accuracy of each method and model averaged across all prompts and dataset, with the average standard deviation of accuracy across different prompts shown in parentheses. For most models, CCS outperforms zero-shot accuracy and exhibits lower sensitivity to prompts, even though this was not our goal. This shows that we can recover knowledge from language model activations without supervision, and can do so in a way that is competitive with strong baseline methods that use model outputs. * T0 was trained on 9 out of 10 of the datasets we evaluate on, including some of the data in our test splits, so we ignore it when averaging over models. \n\nover different answers can mitigate this problem. We use a variant of the calibration method presented in Zhao et al. (2021) by balancing predictions to be 50/50 across the two output labels.",
            "score": 0.39388168892611736,
            "section_title": "EXPERIMENTAL SETUP",
            "char_start_offset": 13478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1863
                },
                {
                    "start": 1866,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "paperId": "458bd3e552533a2e764260ffb38363327da1ab5d",
            "corpusId": 275223881,
            "title": "Zero-Shot Learning With Large Language Models Enhances Drilling-Information Retrieval",
            "venue": "Journal of Petroleum Technology",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2118/0125-0092-jpt?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2118/0125-0092-jpt, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "118741978",
                    "name": "C. Carpenter"
                }
            ],
            "abstract": "\n \n This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 217671, \u201cEnhancing Information Retrieval in the Drilling Domain: Zero-Shot Learning With Large Language Models for Question Answering,\u201d by Felix J. Pacis, SPE, University of Stavanger, and Sergey Alyaev and Gilles Pelfrene, SPE, NORCE, et al. The paper has not been peer reviewed.\n \n \n \n Finding information across multiple databases, formats, and documents remains a manual job in the drilling industry. Large language models (LLMs) have proven effective in data-aggregation tasks, including answering questions. However, using LLMs for domain-specific factual responses poses a nontrivial challenge. The expert-labor cost for training domain-specific LLMs prohibits niche industries from developing custom question-answering bots. The complete paper tests several commercial LLMs for information-retrieval tasks for drilling data using zero-shot in-context learning. In addition, the model\u2019s calibration is tested with a few-shot multiple-choice drilling questionnaire.\n \n \n \n While LLMs have proven effective in various tasks ranging from sentiment analysis to text completion, using LLMs for question-answering tasks presents a challenge in providing factual responses. Pretrained LLMs only serve as a parameterized implicit knowledge base and cannot access recent data; thus, information is bounded by the time of training.\n Retrieval augmented generation (RAG) can address some of these issues by extending the utility of LLMs to specific data sources. Fig. 1 shows a simplified RAG-based LLM question/answer application. RAG involves two primary components: document retrieval (green boxes), which retrieves the most relevant context based on the query, and LLM response generation (blue boxes). During the response generation, LLM operates based on the prompt, query, and retrieved context without any change in the model parameters, a process the authors term as \u201cin-context learning.\u201d\n \n \n \n Two experiments have been conducted: The first one is a few-shot multiple-choice experiment evaluated using the SLB drilling glossary; the second is a zero-shot in-context experiment evaluated on drilling reports and company reports.\n Multiple-Choice Experiment.\n SLB Drilling Glossary.\n For the multiple-choice experiment, a publicly available drilling glossary served as a basis for evaluation. A total of 409 term/definition pairs were considered. Five term/definition pairs were chosen, serving as few-shot default values, while the remaining 404 pairs served as the multiple-choice questions. Four choices were given for each term/definition question pair, where one was the correct answer. The three incorrect choices were picked randomly from all possible terms minus the true answer.\n Zero-Shot In-Context Experiment.\n Norwegian Petroleum Directorate (NPD) Database.\n The authors explored the wellbore history of all individual exploration wells drilled in the Norwegian shelf in the NPD database. In this experiment, 12 exploration wells were randomly chosen for evaluation. In addition to these drilling reports, information about the stratigraphy of three additional wells was added.\n Annual Reports.\n Annual reports of two major operators in Norway for 2020 and 2021 also were considered. These consisted of short summaries that presented the main operational and economic results achieved by the company throughout the year. These reports were added to the evaluation to balance the higher technical content of the wellbore-history reports.\n",
            "corpus_id": "275223881",
            "text": "\n \n This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 217671, \u201cEnhancing Information Retrieval in the Drilling Domain: Zero-Shot Learning With Large Language Models for Question Answering,\u201d by Felix J. Pacis, SPE, University of Stavanger, and Sergey Alyaev and Gilles Pelfrene, SPE, NORCE, et al. The paper has not been peer reviewed.\n \n \n \n Finding information across multiple databases, formats, and documents remains a manual job in the drilling industry. Large language models (LLMs) have proven effective in data-aggregation tasks, including answering questions. However, using LLMs for domain-specific factual responses poses a nontrivial challenge. The expert-labor cost for training domain-specific LLMs prohibits niche industries from developing custom question-answering bots. The complete paper tests several commercial LLMs for information-retrieval tasks for drilling data using zero-shot in-context learning. In addition, the model\u2019s calibration is tested with a few-shot multiple-choice drilling questionnaire.\n \n \n \n While LLMs have proven effective in various tasks ranging from sentiment analysis to text completion, using LLMs for question-answering tasks presents a challenge in providing factual responses. Pretrained LLMs only serve as a parameterized implicit knowledge base and cannot access recent data; thus, information is bounded by the time of training.\n Retrieval augmented generation (RAG) can address some of these issues by extending the utility of LLMs to specific data sources. Fig. 1 shows a simplified RAG-based LLM question/answer application. RAG involves two primary components: document retrieval (green boxes), which retrieves the most relevant context based on the query, and LLM response generation (blue boxes). During the response generation, LLM operates based on the prompt, query, and retrieved context without any change in the model parameters, a process the authors term as \u201cin-context learning.\u201d\n \n \n \n Two experiments have been conducted: The first one is a few-shot multiple-choice experiment evaluated using the SLB drilling glossary; the second is a zero-shot in-context experiment evaluated on drilling reports and company reports.\n Multiple-Choice Experiment.\n SLB Drilling Glossary.\n For the multiple-choice experiment, a publicly available drilling glossary served as a basis for evaluation. A total of 409 term/definition pairs were considered. Five term/definition pairs were chosen, serving as few-shot default values, while the remaining 404 pairs served as the multiple-choice questions. Four choices were given for each term/definition question pair, where one was the correct answer. The three incorrect choices were picked randomly from all possible terms minus the true answer.\n Zero-Shot In-Context Experiment.\n Norwegian Petroleum Directorate (NPD) Database.\n The authors explored the wellbore history of all individual exploration wells drilled in the Norwegian shelf in the NPD database. In this experiment, 12 exploration wells were randomly chosen for evaluation. In addition to these drilling reports, information about the stratigraphy of three additional wells was added.\n Annual Reports.\n Annual reports of two major operators in Norway for 2020 and 2021 also were considered. These consisted of short summaries that presented the main operational and economic results achieved by the company throughout the year. These reports were added to the evaluation to balance the higher technical content of the wellbore-history reports.\n",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6103515625
        }
    ],
    "quotes": {
        "cost": 0.13996499999999995,
        "quotes": [
            {
                "idx": 0,
                "key": "[222327644 | Kong et al. | 2020 | Citations: 26]",
                "snippets": "We propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1388,
                        "end": 2405,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples"
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247613322 | Xiao et al. | 2022 | Citations: 96]",
                "snippets": "In this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration...Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 239,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration"
                    },
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 353,
                        "end": 590,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248524894 | Smith et al. | 2022 | Citations: 56]",
                "snippets": "We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration (Zhao et al., 2021), where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231979430 | Zhao et al. | 2021 | Citations: 1428]": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt."
                },
                "metadata": [
                    {
                        "section_title": "Calibration",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1569,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 84
                            },
                            {
                                "start": 85,
                                "end": 272
                            },
                            {
                                "start": 273,
                                "end": 499
                            },
                            {
                                "start": 500,
                                "end": 636
                            },
                            {
                                "start": 637,
                                "end": 753
                            },
                            {
                                "start": 754,
                                "end": 990
                            },
                            {
                                "start": 991,
                                "end": 1176
                            },
                            {
                                "start": 1177,
                                "end": 1296
                            },
                            {
                                "start": 1297,
                                "end": 1474
                            },
                            {
                                "start": 1475,
                                "end": 1569
                            }
                        ],
                        "ref_mentions": [
                            "231979430"
                        ],
                        "quote": "We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration (Zhao et al., 2021), where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[253098773 | Ahuja et al. | 2022 | Citations: 16]",
                "snippets": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[174802983 | Muller et al. | 2019 | Citations: 1954]": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",
                    "[212747810 | Desai et al. | 2020 | Citations: 301]": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
                    "[226262344 | Lauscher et al. | 2020 | Citations: 315]": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "Calibration Methods",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1623,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 94
                            },
                            {
                                "start": 95,
                                "end": 256
                            },
                            {
                                "start": 259,
                                "end": 322
                            },
                            {
                                "start": 323,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 978
                            },
                            {
                                "start": 979,
                                "end": 1406
                            },
                            {
                                "start": 1407,
                                "end": 1623
                            }
                        ],
                        "ref_mentions": [
                            "28671436",
                            "174802983",
                            "212747810",
                            "226262344",
                            "212747810"
                        ],
                        "quote": "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258967945 | He et al. | 2023 | Citations: 22]",
                "snippets": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[202750126 | Lee et al. | 2019 | Citations: 209]": "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.",
                    "[59599816 | Houlsby et al. | 2019 | Citations: 4518]": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1366,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 118
                            },
                            {
                                "start": 121,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 976
                            },
                            {
                                "start": 977,
                                "end": 1366
                            }
                        ],
                        "ref_mentions": [
                            "59599816",
                            "202750126"
                        ],
                        "quote": "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268723623 | Li et al. | 2024 | Citations: 5]",
                "snippets": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)...To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice. Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 468,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 532,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 229
                            },
                            {
                                "start": 229,
                                "end": 530
                            },
                            {
                                "start": 530,
                                "end": 596
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice. Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[269004786 | Detommaso et al. | 2024 | Citations: 19]",
                "snippets": "This paper proposes the use of \"multicalibration\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 962,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This paper proposes the use of \"multicalibration\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.02043,
        "cot": "I need to cluster these quotes into dimensions that address the user's query about methods for improving language model calibration across different knowledge domains, with a focus on techniques like multicalibration and few-shot recalibration.\n\nLooking at the user query, I'll organize the dimensions as follows:\n\n1. \"Introduction to Language Model Calibration\" - This should be a synthesis paragraph introducing the concept of calibration in language models and why it matters across domains. This is essential background information, though we may not have specific quotes for this section.\n\n2. \"Common Calibration Methods for Language Models\" - This should be a synthesis of the general methods mentioned across quotes. Quotes [1], [3], and [4] discuss various calibration methods like Temperature Scaling, Label Smoothing, etc. This will provide a comprehensive overview of available techniques.\n\n3. \"Regularization and Feature Preservation Approaches\" - This should synthesize quotes [0] and [4] which discuss regularization techniques and preserving pre-trained features to improve calibration.\n\n4. \"Domain-Specific Calibration Challenges\" - This should be a synthesis of the challenges of calibration across different domains mentioned in quotes [5] and [6], which discuss how models may appear well-calibrated over broad distributions but hide significant miscalibration within narrower slices.\n\n5. \"Few-Shot Recalibration for Domain Adaptation\" - This should synthesize quotes [3] and [5] which specifically discuss few-shot recalibration approaches for adapting to specific domains.\n\n6. \"Multicalibration for Reliable Confidence Scores\" - This should focus on quote [6] which discusses multicalibration as a technique for yielding reliable confidence scores across various intersecting groupings of data.\n\n7. \"Prompt-Based Calibration Techniques\" - This should synthesize quote [2] which discusses calibration of prompted labeling functions and contextual calibration.\n\nThe format for each dimension should be synthesis rather than list, as we're trying to explain complex techniques and their relationships rather than simply enumerating options.",
        "plan": {
            "Introduction to Language Model Calibration (synthesis)": [],
            "Common Calibration Methods for Language Models (synthesis)": [
                1,
                3,
                4
            ],
            "Regularization and Feature Preservation Approaches (synthesis)": [
                0,
                4
            ],
            "Domain-Specific Calibration Challenges (synthesis)": [
                5,
                6
            ],
            "Few-Shot Recalibration for Domain Adaptation (synthesis)": [
                3,
                5
            ],
            "Multicalibration for Reliable Confidence Scores (synthesis)": [
                6
            ],
            "Prompt-Based Calibration Techniques (synthesis)": [
                2
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Language Model Calibration",
                "tldr": "Language model calibration refers to how well a model's confidence in its predictions aligns with its actual accuracy. Well-calibrated models are crucial for reliable decision-making in high-stakes domains like healthcare and legal applications. (LLM Memory)",
                "text": "\nLanguage model calibration measures the alignment between a model's confidence in its predictions and the actual probability of those predictions being correct. A perfectly calibrated model should be 90% accurate when it expresses 90% confidence in its answers. However, large language models (LLMs) often suffer from overconfidence, where they express high certainty even when incorrect, or underconfidence, where they express low certainty despite being correct. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nCalibration is particularly important for practical applications of language models where reliability and trustworthiness are critical. In high-stakes domains such as healthcare diagnostics, legal reasoning, or financial advising, poor calibration can lead to dangerous outcomes when models express unwarranted confidence in incorrect information. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe calibration problem becomes more complex across different knowledge domains. Models may be well-calibrated on common knowledge but poorly calibrated on specialized domains, leading to domain-specific calibration gaps. This occurs because pre-training data often contains imbalanced representation of different domains, resulting in varying levels of model confidence that don't accurately reflect its true capabilities in those areas. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research has identified several factors affecting calibration quality, including model size, training methods, and evaluation contexts. Larger models tend to be better calibrated than smaller ones, though they may still exhibit significant calibration errors when venturing into specialized domains or novel tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Common Calibration Methods for Language Models",
                "tldr": "Common calibration methods for language models include post-hoc techniques like temperature scaling, training modifications like label smoothing, and feature preservation approaches that maintain pre-trained model characteristics during fine-tuning. (9 sources)",
                "text": "\nSeveral established methods have emerged to address calibration challenges in language models. One of the most widely used post-hoc calibration techniques is **temperature scaling**, which applies a single learnable parameter to scale model logits before the softmax function, effectively softening overconfident predictions <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>. Temperature scaling has consistently proven effective for in-domain calibration and is recommended as a reliable post-hoc recalibration technique <Paper corpusId=\"247613322\" paperTitle=\"(Xiao et al., 2022)\" isShortName></Paper>.\n\nAnother effective approach is **label smoothing**, which penalizes low entropy distributions by assigning a fixed probability to the true label and distributing remaining probability mass across other classes <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Label smoothing prevents networks from becoming overconfident and has been shown to improve both generalization and model calibration <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>. This technique is particularly beneficial in out-of-domain settings where models tend to be poorly calibrated <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nMore recent research has identified that **preserving pre-trained features** can significantly improve calibration of fine-tuned language models. Several methods that maintain the connection to pre-trained weights have demonstrated superior calibration performance, including:\n\n1. **Parameter-efficient tuning** methods that modify only a small subset of model parameters while keeping most pre-trained weights frozen <Paper corpusId=\"59599816\" paperTitle=\"(Houlsby et al., 2019)\" isShortName></Paper>\n\n2. **Pre-trained weight decay** that regularizes fine-tuned weights to stay closer to their pre-trained values\n\n3. **Mixout**, which stochastically mixes the parameters of the pre-trained and fine-tuned models, regularizing learning to minimize deviation from the pre-trained model <Paper corpusId=\"202750126\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>\n\nThese feature preservation approaches are particularly effective for out-of-domain calibration scenarios, where maintaining connection to the pre-trained model helps prevent overconfidence <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\nFor resource-constrained scenarios, **few-shot learning** has emerged as a surprisingly effective calibration technique. By fine-tuning on a small number of examples in a target domain or language, models can significantly improve their calibration in that specific context <Paper corpusId=\"226262344\" paperTitle=\"(Lauscher et al., 2020)\" isShortName></Paper> <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.\n\nModel selection also impacts calibration quality, with larger pre-trained language models generally exhibiting better calibration than smaller ones. Among model architectures, ELECTRA has been identified as particularly effective for well-calibrated predictions <Paper corpusId=\"247613322\" paperTitle=\"(Xiao et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xiao et al., 2022)",
                        "snippets": [
                            "In this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration",
                            "Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage."
                        ],
                        "paper": {
                            "corpus_id": 247613322,
                            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
                            "authors": [
                                {
                                    "authorId": "120727449",
                                    "name": "Yuxin Xiao"
                                },
                                {
                                    "authorId": "28130078",
                                    "name": "Paul Pu Liang"
                                },
                                {
                                    "authorId": "32326200",
                                    "name": "Umang Bhatt"
                                },
                                {
                                    "authorId": "2934259",
                                    "name": "W. Neiswanger"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                },
                                {
                                    "authorId": "49933077",
                                    "name": "Louis-philippe Morency"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 96
                        },
                        "score": 0.9482421875
                    },
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning."
                        ],
                        "paper": {
                            "corpus_id": 253098773,
                            "title": "On the Calibration of Massively Multilingual Language Models",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 16
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Muller et al., 2019)",
                        "snippets": [
                            "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 174802983,
                            "title": "When Does Label Smoothing Help?",
                            "authors": [
                                {
                                    "authorId": "2114054259",
                                    "name": "Rafael M\u00fcller"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1954
                        },
                        "score": 0
                    },
                    {
                        "id": "(Desai et al., 2020)",
                        "snippets": [
                            "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
                        ],
                        "paper": {
                            "corpus_id": 212747810,
                            "title": "Calibration of Pre-trained Transformers",
                            "authors": [
                                {
                                    "authorId": "120777041",
                                    "name": "Shrey Desai"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 301
                        },
                        "score": 0
                    },
                    {
                        "id": "(Houlsby et al., 2019)",
                        "snippets": [
                            "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
                        ],
                        "paper": {
                            "corpus_id": 59599816,
                            "title": "Parameter-Efficient Transfer Learning for NLP",
                            "authors": [
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1911881",
                                    "name": "A. Giurgiu"
                                },
                                {
                                    "authorId": "40569328",
                                    "name": "Stanislaw Jastrzebski"
                                },
                                {
                                    "authorId": "68973833",
                                    "name": "Bruna Morrone"
                                },
                                {
                                    "authorId": "51985388",
                                    "name": "Quentin de Laroussilhe"
                                },
                                {
                                    "authorId": "2813347",
                                    "name": "Andrea Gesmundo"
                                },
                                {
                                    "authorId": "2809991",
                                    "name": "Mona Attariyan"
                                },
                                {
                                    "authorId": "1802148",
                                    "name": "S. Gelly"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4518
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2019)",
                        "snippets": [
                            "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE."
                        ],
                        "paper": {
                            "corpus_id": 202750126,
                            "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "81275395",
                                    "name": "Cheolhyoung Lee"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "1742677",
                                    "name": "Wanmo Kang"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 209
                        },
                        "score": 0
                    },
                    {
                        "id": "(He et al., 2023)",
                        "snippets": [
                            "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."
                        ],
                        "paper": {
                            "corpus_id": 258967945,
                            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
                            "authors": [
                                {
                                    "authorId": "2218509878",
                                    "name": "Guande He"
                                },
                                {
                                    "authorId": "2276707",
                                    "name": "Jianfei Chen"
                                },
                                {
                                    "authorId": "2155220672",
                                    "name": "Jun Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 22
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Lauscher et al., 2020)",
                        "snippets": [
                            "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions."
                        ],
                        "paper": {
                            "corpus_id": 226262344,
                            "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers",
                            "authors": [
                                {
                                    "authorId": "29891652",
                                    "name": "Anne Lauscher"
                                },
                                {
                                    "authorId": "24881798",
                                    "name": "Vinit Ravishankar"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "2472657",
                                    "name": "Goran Glavas"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 315
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Regularization and Feature Preservation Approaches",
                "tldr": "Regularization approaches can significantly improve language model calibration by preventing overconfidence and maintaining connections to pre-trained features. Techniques like on-manifold and off-manifold regularization, parameter-efficient tuning, and mixout have proven particularly effective for addressing domain-specific calibration challenges. (4 sources)",
                "text": "\nBuilding on traditional calibration methods, regularization approaches have emerged as powerful techniques for addressing calibration challenges in language models. These methods work by constraining the model during fine-tuning to prevent overconfident predictions and preserve useful features from pre-training.\n\nData augmentation-based regularization has shown promising results for improving calibration. Kong et al. proposed a dual regularization approach that uses pseudo samples both on and off the data manifold. Their method employs on-manifold regularization by interpolating training data and labels along directions learned from hidden feature space, introducing a smoothness constraint that improves calibration for in-distribution data. Complementing this, they use off-manifold regularization by adding perturbations that point outward from the data manifold and penalizing overconfident predictions on these samples <Paper corpusId=\"222327644\" paperTitle=\"(Kong et al., 2020)\" isShortName></Paper>.\n\nFeature preservation approaches have proven particularly effective for maintaining calibration during fine-tuning. He et al. observed that preserving pre-trained features helps calibrate fine-tuned language models, especially in out-of-domain settings <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>. Their research validated that several methods designed to maintain connections to pre-trained models significantly improve calibration:\n\n1. **Parameter-efficient tuning** methods, which modify only a small subset of model parameters while keeping most pre-trained weights frozen, have demonstrated strong calibration performance <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper> <Paper corpusId=\"59599816\" paperTitle=\"(Houlsby et al., 2019)\" isShortName></Paper>.\n\n2. **Pre-trained weight decay** regularizes fine-tuned weights to stay closer to their initial pre-trained values, helping prevent overconfidence <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\n3. **Mixout** stochastically mixes parameters from pre-trained and fine-tuned models during training, effectively regularizing learning to minimize deviation from the pre-trained model <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper> <Paper corpusId=\"202750126\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>.\n\nThe effectiveness of these feature preservation approaches stems from the observation that pre-trained language models are often well-calibrated on their masked language modeling (MLM) objectives, but lose this calibration when fine-tuned without proper regularization. By maintaining consistency between pre-trained and fine-tuned models, these techniques can achieve lower expected calibration error while maintaining competitive accuracy on natural language understanding tasks <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\nThese regularization and feature preservation approaches are particularly valuable for addressing domain-specific calibration challenges, as they help prevent the model from becoming overconfident when facing examples that differ from its training distribution.",
                "citations": [
                    {
                        "id": "(Kong et al., 2020)",
                        "snippets": [
                            "We propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples"
                        ],
                        "paper": {
                            "corpus_id": 222327644,
                            "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing",
                            "authors": [
                                {
                                    "authorId": "2865034",
                                    "name": "Lingkai Kong"
                                },
                                {
                                    "authorId": "5795999",
                                    "name": "Haoming Jiang"
                                },
                                {
                                    "authorId": "8103389",
                                    "name": "Yuchen Zhuang"
                                },
                                {
                                    "authorId": "2053220976",
                                    "name": "Jie Lyu"
                                },
                                {
                                    "authorId": "36345161",
                                    "name": "T. Zhao"
                                },
                                {
                                    "authorId": "145657504",
                                    "name": "Chao Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 26
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(He et al., 2023)",
                        "snippets": [
                            "Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."
                        ],
                        "paper": {
                            "corpus_id": 258967945,
                            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
                            "authors": [
                                {
                                    "authorId": "2218509878",
                                    "name": "Guande He"
                                },
                                {
                                    "authorId": "2276707",
                                    "name": "Jianfei Chen"
                                },
                                {
                                    "authorId": "2155220672",
                                    "name": "Jun Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 22
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Houlsby et al., 2019)",
                        "snippets": [
                            "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
                        ],
                        "paper": {
                            "corpus_id": 59599816,
                            "title": "Parameter-Efficient Transfer Learning for NLP",
                            "authors": [
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1911881",
                                    "name": "A. Giurgiu"
                                },
                                {
                                    "authorId": "40569328",
                                    "name": "Stanislaw Jastrzebski"
                                },
                                {
                                    "authorId": "68973833",
                                    "name": "Bruna Morrone"
                                },
                                {
                                    "authorId": "51985388",
                                    "name": "Quentin de Laroussilhe"
                                },
                                {
                                    "authorId": "2813347",
                                    "name": "Andrea Gesmundo"
                                },
                                {
                                    "authorId": "2809991",
                                    "name": "Mona Attariyan"
                                },
                                {
                                    "authorId": "1802148",
                                    "name": "S. Gelly"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4518
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2019)",
                        "snippets": [
                            "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE."
                        ],
                        "paper": {
                            "corpus_id": 202750126,
                            "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "81275395",
                                    "name": "Cheolhyoung Lee"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "1742677",
                                    "name": "Wanmo Kang"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 209
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Domain-Specific Calibration Challenges",
                "tldr": "Language models often appear well-calibrated over broad distributions while hiding significant miscalibration within specific knowledge domains, creating critical reliability issues. These domain-specific calibration problems manifest as systematic overconfidence in some areas (like mathematics) and underconfidence in others (like history), requiring targeted solutions for each knowledge domain. (2 sources)",
                "text": "\nWhile language models may demonstrate good overall calibration metrics when evaluated broadly, recent research has revealed that this apparent calibration often masks serious miscalibration within specific knowledge domains. This phenomenon creates significant challenges for deploying language models in specialized contexts where reliability is crucial.\n\nLi et al. discovered that seemingly well-calibrated models frequently exhibit \"significant miscalibration within narrower slices\" of knowledge, where systematic overconfidence in certain domains (like mathematics) can balance out systematic underconfidence in others (like history), creating an illusion of good calibration when measured in aggregate <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This hidden domain-specific miscalibration poses substantial risks when models are deployed in specialized contexts, as the model's confidence scores may not accurately reflect its true likelihood of being correct in that particular knowledge area.\n\nThe challenge becomes more complex because different knowledge domains may require distinct calibration approaches. Traditional calibration methods that work well for general knowledge may fail to address the unique characteristics of specialized domains like medicine, law, or technical subjects. These domains often have their own linguistic patterns, specialized terminology, and reasoning structures that affect how language models perform and express confidence.\n\nDomain-specific calibration challenges are further complicated by the uneven representation of different knowledge areas in pre-training data, resulting in varying degrees of model expertise and confidence across domains. This imbalance can lead to systematic biases in calibration, where models consistently over- or under-estimate their capabilities in specific areas.\n\nTo address these fine-grained calibration issues, researchers have begun developing more targeted approaches. Detommaso et al. proposed \"multicalibration\" as a solution that aims to achieve calibration \"not just marginally, but simultaneously across various intersecting groupings of the data\" <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>. Their approach creates groupings for prompt/completion pairs through embedding space clustering and \"self-annotation,\" where the language model is queried about various aspects of the prompt to identify groups that correlate with correctness probability. This multicalibration approach has shown \"substantial improvements in fine-grained measures of both calibration and accuracy\" compared to existing methods that don't account for domain-specific characteristics <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)",
                            "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice. Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct)."
                        ],
                        "paper": {
                            "corpus_id": 268723623,
                            "title": "Few-Shot Recalibration of Language Models",
                            "authors": [
                                {
                                    "authorId": "2293910776",
                                    "name": "Xiang Lisa Li"
                                },
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Detommaso et al., 2024)",
                        "snippets": [
                            "This paper proposes the use of \"multicalibration\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."
                        ],
                        "paper": {
                            "corpus_id": 269004786,
                            "title": "Multicalibration for Confidence Scoring in LLMs",
                            "authors": [
                                {
                                    "authorId": "2295667267",
                                    "name": "Gianluca Detommaso"
                                },
                                {
                                    "authorId": "2295665717",
                                    "name": "Martin Bertran"
                                },
                                {
                                    "authorId": "2295664744",
                                    "name": "Riccardo Fogliato"
                                },
                                {
                                    "authorId": "2295665299",
                                    "name": "Aaron Roth"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 19
                        },
                        "score": 0.97119140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Few-Shot Recalibration for Domain Adaptation",
                "tldr": "Few-shot recalibration techniques effectively address domain-specific calibration challenges by fine-tuning models with small amounts of domain-specific data. This approach is particularly valuable for adapting language models to specialized knowledge domains or resource-limited languages, providing significant calibration improvements with minimal additional training. (5 sources)",
                "text": "\nFew-shot recalibration has emerged as a powerful approach for adapting language models to specific domains or languages while improving their calibration. This method leverages a small number of examples from the target domain to adjust model confidence, addressing the domain-specific calibration challenges identified in previous sections.\n\nTraditional calibration techniques like temperature scaling, while effective for in-domain calibration, often struggle with domain adaptation challenges. Temperature scaling can be applied using either source language development data or target language development data (Self-TS), with the latter providing better calibration for cross-lingual transfer <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>. However, these methods may not fully address the nuanced calibration issues that arise when transferring to different knowledge domains.\n\nFew-shot learning presents a surprisingly effective solution for improving calibration across domains. By fine-tuning on just a small number of examples from the target domain or language, models can significantly reduce domain shift and improve both performance and calibration <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. This approach has proven particularly valuable for cross-lingual transfer, where fine-tuning on a few target-language instances consistently improves results across various tasks <Paper corpusId=\"226262344\" paperTitle=\"(Lauscher et al., 2020)\" isShortName></Paper>.\n\nRecent research has further refined this approach with \"slice-specific few-shot recalibration\" - a framework that addresses the fine-grained calibration issues hidden within broader distributions. This method uses a small number of unlabeled examples from a specific knowledge slice (e.g., mathematics, history) to recalibrate the language model specifically for that domain. For each slice, a separate recalibration model is trained to map the language model's confidence scores to domain-specific estimates of precision <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThis domain-specific recalibration approach is particularly valuable because language models often exhibit systemic overconfidence in some domains while showing underconfidence in others. These opposing miscalibrations can balance each other out in aggregate evaluations, creating an illusion of good overall calibration while hiding significant domain-specific problems <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of few-shot recalibration techniques is further enhanced when combined with regularization methods like label smoothing, which helps prevent overconfidence by penalizing low entropy distributions. Label smoothing has demonstrated competitive performance with temperature scaling, especially in out-of-domain settings where calibration challenges are more pronounced <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nFor practical applications, few-shot recalibration offers an efficient solution that balances calibration quality with computational efficiency. By requiring only a small number of examples from the target domain, this approach enables effective domain adaptation even in resource-constrained scenarios where extensive labeled data may not be available.",
                "citations": [
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning."
                        ],
                        "paper": {
                            "corpus_id": 253098773,
                            "title": "On the Calibration of Massively Multilingual Language Models",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 16
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Guo et al., 2017)",
                        "snippets": [
                            "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                        ],
                        "paper": {
                            "corpus_id": 28671436,
                            "title": "On Calibration of Modern Neural Networks",
                            "authors": [
                                {
                                    "authorId": "144993411",
                                    "name": "Chuan Guo"
                                },
                                {
                                    "authorId": "10804137",
                                    "name": "Geoff Pleiss"
                                },
                                {
                                    "authorId": "2117103358",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "7446832",
                                    "name": "Kilian Q. Weinberger"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 5869
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lauscher et al., 2020)",
                        "snippets": [
                            "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions."
                        ],
                        "paper": {
                            "corpus_id": 226262344,
                            "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers",
                            "authors": [
                                {
                                    "authorId": "29891652",
                                    "name": "Anne Lauscher"
                                },
                                {
                                    "authorId": "24881798",
                                    "name": "Vinit Ravishankar"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "2472657",
                                    "name": "Goran Glavas"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 315
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)",
                            "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice. Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct)."
                        ],
                        "paper": {
                            "corpus_id": 268723623,
                            "title": "Few-Shot Recalibration of Language Models",
                            "authors": [
                                {
                                    "authorId": "2293910776",
                                    "name": "Xiang Lisa Li"
                                },
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Desai et al., 2020)",
                        "snippets": [
                            "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."
                        ],
                        "paper": {
                            "corpus_id": 212747810,
                            "title": "Calibration of Pre-trained Transformers",
                            "authors": [
                                {
                                    "authorId": "120777041",
                                    "name": "Shrey Desai"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 301
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Multicalibration for Reliable Confidence Scores",
                "tldr": "Multicalibration addresses fine-grained calibration challenges by ensuring language models are well-calibrated across multiple intersecting data groupings simultaneously, not just in aggregate. This approach creates domain-specific groupings through embedding space clustering and self-annotation techniques, providing more reliable confidence scores for specialized knowledge domains. (1 source)",
                "text": "\nTraditional calibration methods often focus on improving overall calibration across a broad distribution, which can mask significant miscalibration within specific knowledge domains or data slices. To address this limitation, multicalibration has emerged as a promising approach that ensures calibration across multiple intersecting groupings of data simultaneously, rather than just marginally across the entire distribution.\n\nDetommaso et al. propose a multicalibration framework specifically designed to yield interpretable and reliable confidence scores for large language model outputs. Their approach focuses on achieving calibration \"not just marginally, but simultaneously across various intersecting groupings of the data,\" ensuring that models are well-calibrated even within specific knowledge domains. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nA key innovation in this multicalibration approach is the method of forming meaningful groupings for prompt/completion pairs that correlate with correctness probability. The researchers employ two primary techniques to create these groupings:\n\n1. **Clustering within embedding space**: This technique identifies natural groupings of related prompts and completions based on their vector representations in the model's embedding space.\n\n2. **Self-annotation**: This novel approach queries the language model itself with yes-or-no questions about the prompt to identify characteristics that might correlate with correctness probability. For example, asking the model whether a prompt requires mathematical reasoning or historical knowledge helps create domain-specific groupings for targeted calibration. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nTo prevent overfitting, which is a common challenge in multicalibration methods, the researchers developed novel algorithmic variants that improve performance stability. These refined algorithms maintain the benefits of multicalibration while reducing the risk of overadjusting to specific data groupings. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nWhen evaluated across various question-answering datasets and different large language models, multicalibration consistently demonstrates \"substantial improvements in fine-grained measures of both calibration and accuracy\" compared to traditional calibration methods. These improvements are particularly significant for specialized knowledge domains where conventional calibration techniques often struggle to address domain-specific confidence biases. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nThe multicalibration approach complements other calibration techniques discussed earlier, such as few-shot recalibration and regularization methods. While few-shot approaches adapt models to specific domains using limited examples, and regularization preserves beneficial pre-trained features, multicalibration ensures reliable confidence scores across multiple intersecting data groupings simultaneously. This comprehensive approach to calibration is particularly valuable for high-stakes applications where reliability within specific knowledge domains is critical.",
                "citations": [
                    {
                        "id": "(Detommaso et al., 2024)",
                        "snippets": [
                            "This paper proposes the use of \"multicalibration\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."
                        ],
                        "paper": {
                            "corpus_id": 269004786,
                            "title": "Multicalibration for Confidence Scoring in LLMs",
                            "authors": [
                                {
                                    "authorId": "2295667267",
                                    "name": "Gianluca Detommaso"
                                },
                                {
                                    "authorId": "2295665717",
                                    "name": "Martin Bertran"
                                },
                                {
                                    "authorId": "2295664744",
                                    "name": "Riccardo Fogliato"
                                },
                                {
                                    "authorId": "2295665299",
                                    "name": "Aaron Roth"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 19
                        },
                        "score": 0.97119140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Prompt-Based Calibration Techniques",
                "tldr": "Prompt-based calibration techniques address language model confidence biases without requiring labeled data by analyzing model responses to \"content-free\" inputs. These methods, like contextual calibration, can significantly improve model accuracy and reduce prediction variance by counteracting inherent model biases toward certain answers. (2 sources)",
                "text": "\nPrompt-based calibration techniques offer effective approaches for improving language model calibration without requiring extensive labeled data, making them particularly valuable for zero-shot and few-shot learning scenarios. Unlike traditional calibration methods that depend on labeled examples, these techniques leverage the model's own responses to specially crafted prompts to identify and address confidence biases.\n\nA prominent approach in this category is contextual calibration, which addresses the inherent biases language models have toward certain predictions. Current language models often exhibit poor calibration with predicted probabilities subject to various forms of bias, such as favoring tokens that appear more frequently in pre-training data or tokens positioned near the end of a prompt <Paper corpusId=\"248524894\" paperTitle=\"(Smith et al., 2022)\" isShortName></Paper>. These biases can significantly impact the reliability of model predictions, especially when using prompts as labeling functions or when thresholding predictions based on confidence scores.\n\nContextual calibration works by estimating scaling weights from the model's predicted token probabilities when queried with \"content-free\" or null input instances. These null inputs - typically tokens like \"N/A\", \"\", or \"<|endoftext|>\" - serve as baselines that reveal the model's inherent biases without any meaningful content to influence predictions. By analyzing how the model responds to these null inputs, researchers can estimate calibration parameters that transform the model's raw predictions to be more balanced across possible answers <Paper corpusId=\"248524894\" paperTitle=\"(Smith et al., 2022)\" isShortName></Paper> <Paper corpusId=\"231979430\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>.\n\nThis approach is particularly valuable for addressing the instability observed in few-shot learning scenarios, where model accuracy can vary dramatically based on superficial factors such as prompt format, training example selection, or even the order of examples within a prompt. Research has shown that by estimating a model's bias toward each answer and fitting calibration parameters to create uniform predictions for content-free inputs, contextual calibration can substantially improve accuracy (by up to 30.0% absolute) and reduce variance across different prompt configurations <Paper corpusId=\"231979430\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>.\n\nThe effectiveness of prompt-based calibration methods stems from their ability to identify and counteract specific biases without requiring labeled data for the target task. This makes them especially suitable for zero-shot and few-shot settings where labeled examples are scarce or unavailable. By addressing the calibration challenges inherent in prompt-based approaches, these techniques enhance both the reliability and performance of language models across diverse knowledge domains.",
                "citations": [
                    {
                        "id": "(Smith et al., 2022)",
                        "snippets": [
                            "We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration (Zhao et al., 2021), where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions."
                        ],
                        "paper": {
                            "corpus_id": 248524894,
                            "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
                            "authors": [
                                {
                                    "authorId": "2165361707",
                                    "name": "Ryan Smith"
                                },
                                {
                                    "authorId": "31592365",
                                    "name": "Jason Alan Fries"
                                },
                                {
                                    "authorId": "34302368",
                                    "name": "Braden Hancock"
                                },
                                {
                                    "authorId": "2870504",
                                    "name": "Stephen H. Bach"
                                }
                            ],
                            "year": 2022,
                            "venue": "ACM / IMS Journal of Data Science",
                            "n_citations": 56
                        },
                        "score": 0.91064453125
                    },
                    {
                        "id": "(Zhao et al., 2021)",
                        "snippets": [
                            "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt."
                        ],
                        "paper": {
                            "corpus_id": 231979430,
                            "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
                            "authors": [
                                {
                                    "authorId": "145914976",
                                    "name": "Tony Zhao"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "2113511266",
                                    "name": "Shi Feng"
                                },
                                {
                                    "authorId": "38666915",
                                    "name": "D. Klein"
                                },
                                {
                                    "authorId": "34650964",
                                    "name": "Sameer Singh"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1428
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.144543
    }
}