{
    "query": "What are the primary differences between sparse retrieval models like BM25 and dense retrieval models such as DPR or SentenceTransformer when used in retrieval-augmented language models?",
    "user_id": "lib_user",
    "task_id": "2f837191-bde9-4435-9a1d-dd8c3035b306",
    "timestamp": "2025-06-23T23:26:07.579951",
    "n_retrieval": 256,
    "n_retrieved": 271,
    "n_candidates": 46,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.6736709999999999,
    "decomposed_query": {
        "rewritten_query": "Primary differences between sparse retrieval models like BM25 and dense retrieval models such as DPR or SentenceTransformer when used in retrieval-augmented language models.",
        "keyword_query": "sparse retrieval BM25 dense retrieval DPR SentenceTransformer retrieval-augmented language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.01026,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "On the Robustness of Generative Information Retrieval Models",
            "venue": "European Conference on Information Retrieval",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.18768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143860482",
                    "name": "Yuansan Liu"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "70414094",
                    "name": "Jiafeng Guo"
                },
                {
                    "authorId": "2287815279",
                    "name": "Changjiang Zhou"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                },
                {
                    "authorId": "2244825947",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Generative information retrieval methods retrieve documents by directly generating their identifiers. Much effort has been devoted to developing effective generative IR models. Less attention has been paid to the robustness of these models. It is critical to assess the out-of-distribution (OOD) generalization of generative IR models, i.e., how would such models generalize to new distributions? To answer this question, we focus on OOD scenarios from four perspectives in retrieval problems: (i)query variations; (ii)unseen query types; (iii)unseen tasks; and (iv)corpus expansion. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of representative generative IR models against dense retrieval models. Our empirical results indicate that the OOD robustness of generative IR models is in need of improvement. By inspecting the OOD robustness of generative IR models we aim to contribute to the development of more reliable IR models. The code is available at \\url{https://github.com/Davion-Liu/GR_OOD}.",
            "corpus_id": 275119098,
            "sentences": [
                {
                    "corpus_id": "275119098",
                    "title": "On the Robustness of Generative Information Retrieval Models",
                    "text": "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], \n\nto probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance. \n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [51,54], late interaction [20]. Recently, researchers have also investigated pre-training tasks for dense retrieval [12,33]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually come with considerable memory demands and computational overheads.",
                    "score": 0.7525128919604424,
                    "section_title": "Sparse and dense retrieval models",
                    "char_start_offset": 3413,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 381
                        },
                        {
                            "start": 384,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 548
                        },
                        {
                            "start": 551,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1650
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 118,
                            "matchedPaperCorpusId": "12255087"
                        },
                        {
                            "start": 207,
                            "end": 211,
                            "matchedPaperCorpusId": "232147859"
                        },
                        {
                            "start": 211,
                            "end": 214,
                            "matchedPaperCorpusId": "2218552"
                        },
                        {
                            "start": 350,
                            "end": 354,
                            "matchedPaperCorpusId": "6473756"
                        },
                        {
                            "start": 376,
                            "end": 380,
                            "matchedPaperCorpusId": "6473756"
                        },
                        {
                            "start": 412,
                            "end": 415,
                            "matchedPaperCorpusId": "2218552"
                        },
                        {
                            "start": 422,
                            "end": 426,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 645,
                            "end": 649,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 649,
                            "end": 652,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 652,
                            "end": 655,
                            "matchedPaperCorpusId": "254044526"
                        },
                        {
                            "start": 973,
                            "end": 977,
                            "matchedPaperCorpusId": "232147859"
                        },
                        {
                            "start": 996,
                            "end": 1000,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1301,
                            "end": 1304,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 1323,
                            "end": 1327,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1413,
                            "end": 1417,
                            "matchedPaperCorpusId": "237581068"
                        },
                        {
                            "start": 1417,
                            "end": 1420,
                            "matchedPaperCorpusId": "251718954"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96630859375
                }
            ],
            "relevance_judgement": 0.96630859375,
            "relevance_judgment_input_expanded": "# Title: On the Robustness of Generative Information Retrieval Models\n# Venue: European Conference on Information Retrieval\n# Authors: Yuansan Liu, Ruqing Zhang, Jiafeng Guo, Changjiang Zhou, M. D. Rijke, Xueqi Cheng\n## Abstract\nGenerative information retrieval methods retrieve documents by directly generating their identifiers. Much effort has been devoted to developing effective generative IR models. Less attention has been paid to the robustness of these models. It is critical to assess the out-of-distribution (OOD) generalization of generative IR models, i.e., how would such models generalize to new distributions? To answer this question, we focus on OOD scenarios from four perspectives in retrieval problems: (i)query variations; (ii)unseen query types; (iii)unseen tasks; and (iv)corpus expansion. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of representative generative IR models against dense retrieval models. Our empirical results indicate that the OOD robustness of generative IR models is in need of improvement. By inspecting the OOD robustness of generative IR models we aim to contribute to the development of more reliable IR models. The code is available at \\url{https://github.com/Davion-Liu/GR_OOD}.\n## Sparse and dense retrieval models\nSparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], \n\nto probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance. \n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [51,54], late interaction [20]. Recently, researchers have also investigated pre-training tasks for dense retrieval [12,33]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually come with considerable memory demands and computational overheads.",
            "reference_string": "[275119098 | Liu et al. | 2024 | Citations: 4]"
        },
        {
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "venue": "European Conference on Information Retrieval",
            "year": 2023,
            "reference_count": 44,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.13416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116028119",
                    "name": "Thong Nguyen"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "2136074457",
                    "name": "Andrew Yates"
                }
            ],
            "abstract": "Learned sparse retrieval (LSR) is a family of first-stage retrieval methods that are trained to generate sparse lexical representations of queries and documents for use with an inverted index. Many LSR methods have been recently introduced, with Splade models achieving state-of-the-art performance on MSMarco. Despite similarities in their model architectures, many LSR methods show substantial differences in effectiveness and efficiency. Differences in the experimental setups and configurations used make it difficult to compare the methods and derive insights. In this work, we analyze existing LSR methods and identify key components to establish an LSR framework that unifies all LSR methods under the same perspective. We then reproduce all prominent methods using a common codebase and re-train them in the same environment, which allows us to quantify how components of the framework affect effectiveness and efficiency. We find that (1) including document term weighting is most important for a method's effectiveness, (2) including query weighting has a small positive impact, and (3) document expansion and query expansion have a cancellation effect. As a result, we show how removing query expansion from a state-of-the-art model can reduce latency significantly while maintaining effectiveness on MSMarco and TripClick benchmarks. Our code is publicly available at https://github.com/thongnt99/learned-sparse-retrieval",
            "corpus_id": 257585074,
            "sentences": [
                {
                    "corpus_id": "257585074",
                    "title": "A Unified Framework for Learned Sparse Retrieval",
                    "text": "In this section, we introduce a conceptual framework consisting of three components (sparse encoder, sparse regularizer, supervision) that captures the key differences we observe between existing learned sparse retrieval methods. Later, we describe how LSR methods in the literature can be fit into this framework. Sparse (Lexical) Encoders. A sparse or lexical encoder encodes queries and passages into weight vectors of equal dimension. This is the main component that determines the effectiveness of a learned sparse retrieval method. There are three distinct characteristics that make sparse encoders different from dense encoders. The first and most straightforward difference is that sparse encoders produce sparse vectors (i.e., most term weights are zero). This sparsity is controlled by sparse regularizers, which we will discuss in the next section. Second, dimensions in sparse weight vectors are usually tied to terms in a vocabulary that contains tens of thousands of terms. Therefore, the size of the vectors is large, equal to the size of the vocabulary; each dimension represents a term (typically a BERT word piece). On the contrary, (single-vector) dense retrieval methods produce condensed vectors (usually fewer than 1000 dimensions) that encode the semantics of the input text without a clear correspondence between terms and dimensions. Term-level dense retrieval methods like ColBERT [13] do preserve this correspondence. \n\nThe third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies. \n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8,35]. There are also recent works proposing hybrid retrieval systems that combine the strength of both dense and sparse representations [3,18,19], which can bring benefits for both in-domain and out-of-domain effectiveness [19].",
                    "score": 0.628059372545893,
                    "section_title": "Unified learned sparse retrieval framework",
                    "char_start_offset": 5623,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1444
                        },
                        {
                            "start": 1447,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1827
                        },
                        {
                            "start": 1830,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2130
                        },
                        {
                            "start": 2131,
                            "end": 2353
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1407,
                            "end": 1411,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 2126,
                            "end": 2129,
                            "matchedPaperCorpusId": "233296016"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: A Unified Framework for Learned Sparse Retrieval\n# Venue: European Conference on Information Retrieval\n# Authors: Thong Nguyen, Sean MacAvaney, Andrew Yates\n## Abstract\nLearned sparse retrieval (LSR) is a family of first-stage retrieval methods that are trained to generate sparse lexical representations of queries and documents for use with an inverted index. Many LSR methods have been recently introduced, with Splade models achieving state-of-the-art performance on MSMarco. Despite similarities in their model architectures, many LSR methods show substantial differences in effectiveness and efficiency. Differences in the experimental setups and configurations used make it difficult to compare the methods and derive insights. In this work, we analyze existing LSR methods and identify key components to establish an LSR framework that unifies all LSR methods under the same perspective. We then reproduce all prominent methods using a common codebase and re-train them in the same environment, which allows us to quantify how components of the framework affect effectiveness and efficiency. We find that (1) including document term weighting is most important for a method's effectiveness, (2) including query weighting has a small positive impact, and (3) document expansion and query expansion have a cancellation effect. As a result, we show how removing query expansion from a state-of-the-art model can reduce latency significantly while maintaining effectiveness on MSMarco and TripClick benchmarks. Our code is publicly available at https://github.com/thongnt99/learned-sparse-retrieval\n## Unified learned sparse retrieval framework\nIn this section, we introduce a conceptual framework consisting of three components (sparse encoder, sparse regularizer, supervision) that captures the key differences we observe between existing learned sparse retrieval methods. Later, we describe how LSR methods in the literature can be fit into this framework. Sparse (Lexical) Encoders. A sparse or lexical encoder encodes queries and passages into weight vectors of equal dimension. This is the main component that determines the effectiveness of a learned sparse retrieval method. There are three distinct characteristics that make sparse encoders different from dense encoders. The first and most straightforward difference is that sparse encoders produce sparse vectors (i.e., most term weights are zero). This sparsity is controlled by sparse regularizers, which we will discuss in the next section. Second, dimensions in sparse weight vectors are usually tied to terms in a vocabulary that contains tens of thousands of terms. Therefore, the size of the vectors is large, equal to the size of the vocabulary; each dimension represents a term (typically a BERT word piece). On the contrary, (single-vector) dense retrieval methods produce condensed vectors (usually fewer than 1000 dimensions) that encode the semantics of the input text without a clear correspondence between terms and dimensions. Term-level dense retrieval methods like ColBERT [13] do preserve this correspondence. \n\nThe third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies. \n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8,35]. There are also recent works proposing hybrid retrieval systems that combine the strength of both dense and sparse representations [3,18,19], which can bring benefits for both in-domain and out-of-domain effectiveness [19].",
            "reference_string": "[257585074 | Nguyen et al. | 2023 | Citations: 29]"
        },
        {
            "title": "CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257349205",
                    "name": "Yuntong Hu"
                },
                {
                    "authorId": "2303415589",
                    "name": "Zhihan Lei"
                },
                {
                    "authorId": "2342896295",
                    "name": "Zhongjie Dai"
                },
                {
                    "authorId": "2275878297",
                    "name": "Allen Zhang"
                },
                {
                    "authorId": "2142231513",
                    "name": "Abhinav Angirekula"
                },
                {
                    "authorId": "2021011947",
                    "name": "Zhengwu Zhang"
                },
                {
                    "authorId": "2257314969",
                    "name": "Liang Zhao"
                }
            ],
            "abstract": "Research question answering requires accurate retrieval and contextual understanding of scientific literature. However, current Retrieval-Augmented Generation (RAG) methods often struggle to balance complex document relationships with precise information retrieval. In this paper, we introduce Contextualized Graph Retrieval-Augmented Generation (CG-RAG), a novel framework that integrates sparse and dense retrieval signals within graph structures to enhance retrieval efficiency and subsequently improve generation quality for research question answering. First, we propose a contextual graph representation for citation graphs, effectively capturing both explicit and implicit connections within and across documents. Next, we introduce Lexical-Semantic Graph Retrieval (LeSeGR), which seamlessly integrates sparse and dense retrieval signals with graph encoding. It bridges the gap between lexical precision and semantic understanding in citation graph retrieval, demonstrating generalizability to existing graph retrieval and hybrid retrieval methods. Finally, we present a context-aware generation strategy that utilizes the retrieved graph-structured information to generate precise and contextually enriched responses using large language models (LLMs). Extensive experiments on research question answering benchmarks across multiple domains demonstrate that our CG-RAG framework significantly outperforms RAG methods combined with various state-of-the-art retrieval approaches, delivering superior retrieval accuracy and generation quality.",
            "corpus_id": 275921950,
            "sentences": [
                {
                    "corpus_id": "275921950",
                    "title": "CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs",
                    "text": "Information retrieval (IR) focuses on extracting relevant information from large corpora. Two primary retrieval techniques dominate the field: sparse retrieval and dense retrieval. Sparse retrieval methods, such as TF-IDF [32] and BM25 [31], rely on term-based representations to evaluate lexical matches between queries and documents. These approaches perform well in scenarios where exact term matching is essential, but they struggle with semantic meaning. In contrast, dense retrieval methods leverage pretrained language models such as BERT [5] to encode queries and documents as continuous, low-dimensional embeddings, capturing semantic similarity through maximum inner product search (MIPS) [16,30,43,44]. Dense retrieval effectively overcomes the lexical gap, retrieving semantically related results even when query terms differ from the document's terminology. Recently, hybrid retrieval techniques have also emerged to combine the strengths of sparse and dense methods while addressing their respective limitations [25,26,29]. By integrating sparse and dense signals, these approaches provide a robust solution for retrieving relevant information from long and complex documents.",
                    "score": 0.5436072801255426,
                    "section_title": "RELATED WORK 2.1 Information Retrieval",
                    "char_start_offset": 6446,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1190
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 222,
                            "end": 226,
                            "matchedPaperCorpusId": "6473756"
                        },
                        {
                            "start": 236,
                            "end": 240,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 699,
                            "end": 703,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 706,
                            "end": 709,
                            "matchedPaperCorpusId": "271114619"
                        },
                        {
                            "start": 1026,
                            "end": 1030,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 1033,
                            "end": 1036,
                            "matchedPaperCorpusId": "252312510"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs\n# Venue: arXiv.org\n# Authors: Yuntong Hu, Zhihan Lei, Zhongjie Dai, Allen Zhang, Abhinav Angirekula, Zhengwu Zhang, Liang Zhao\n## Abstract\nResearch question answering requires accurate retrieval and contextual understanding of scientific literature. However, current Retrieval-Augmented Generation (RAG) methods often struggle to balance complex document relationships with precise information retrieval. In this paper, we introduce Contextualized Graph Retrieval-Augmented Generation (CG-RAG), a novel framework that integrates sparse and dense retrieval signals within graph structures to enhance retrieval efficiency and subsequently improve generation quality for research question answering. First, we propose a contextual graph representation for citation graphs, effectively capturing both explicit and implicit connections within and across documents. Next, we introduce Lexical-Semantic Graph Retrieval (LeSeGR), which seamlessly integrates sparse and dense retrieval signals with graph encoding. It bridges the gap between lexical precision and semantic understanding in citation graph retrieval, demonstrating generalizability to existing graph retrieval and hybrid retrieval methods. Finally, we present a context-aware generation strategy that utilizes the retrieved graph-structured information to generate precise and contextually enriched responses using large language models (LLMs). Extensive experiments on research question answering benchmarks across multiple domains demonstrate that our CG-RAG framework significantly outperforms RAG methods combined with various state-of-the-art retrieval approaches, delivering superior retrieval accuracy and generation quality.\n## RELATED WORK 2.1 Information Retrieval\nInformation retrieval (IR) focuses on extracting relevant information from large corpora. Two primary retrieval techniques dominate the field: sparse retrieval and dense retrieval. Sparse retrieval methods, such as TF-IDF [32] and BM25 [31], rely on term-based representations to evaluate lexical matches between queries and documents. These approaches perform well in scenarios where exact term matching is essential, but they struggle with semantic meaning. In contrast, dense retrieval methods leverage pretrained language models such as BERT [5] to encode queries and documents as continuous, low-dimensional embeddings, capturing semantic similarity through maximum inner product search (MIPS) [16,30,43,44]. Dense retrieval effectively overcomes the lexical gap, retrieving semantically related results even when query terms differ from the document's terminology. Recently, hybrid retrieval techniques have also emerged to combine the strengths of sparse and dense methods while addressing their respective limitations [25,26,29]. By integrating sparse and dense signals, these approaches provide a robust solution for retrieving relevant information from long and complex documents.",
            "reference_string": "[275921950 | Hu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 44,
            "citation_count": 69,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.findings-emnlp.19.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.06918, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1769736",
                    "name": "Xilun Chen"
                },
                {
                    "authorId": "1410624139",
                    "name": "Kushal Lakhotia"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "3377939",
                    "name": "Anchit Gupta"
                },
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "3139260",
                    "name": "Stanislav Peshterliev"
                },
                {
                    "authorId": "2121361882",
                    "name": "Yashar Mehdad"
                },
                {
                    "authorId": "2118343423",
                    "name": "Sonal Gupta"
                },
                {
                    "authorId": "144105277",
                    "name": "Wen-tau Yih"
                }
            ],
            "abstract": "Despite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation of dense models. We rebut this claim by introducing the Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical matching capacity of a sparse model. We show that a dense Lexical Model {\\Lambda} can be trained to imitate a sparse one, and SPAR is built by augmenting a standard dense retriever with {\\Lambda}. Empirically, SPAR shows superior performance on a range of tasks including five question answering datasets, MS MARCO passage retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain evaluation, exceeding the performance of state-of-the-art dense and sparse retrievers. The code and models of SPAR are available at: https://github.com/facebookresearch/dpr-scale/tree/main/spar",
            "corpus_id": 238744204,
            "sentences": [
                {
                    "corpus_id": "238744204",
                    "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?",
                    "text": "Text retrieval is a crucial component for a wide range of knowledge-intensive NLP systems, such as open-domain question answering (ODQA) models and search engines. Recently, dense retrievers (Karpukhin et al., 2020;Xiong et al., 2021) have gained popularity and demonstrated strong performance on a number of retrieval tasks. Dense retrievers employ deep neural networks to learn continuous representations for the queries and documents, and perform retrieval in this dense embedding space using nearest neighbor search (Johnson et al., 2019). Compared to traditional sparse retrievers that rely on discrete bag-of-words representations, dense retrievers can derive more semantically expressive embeddings, thanks to its end-to-end learnability and powerful pre-trained encoders. This helps dense retrievers to overcome several inherent limitations of sparse systems such as vocabulary mismatch (where different words are used for the same meaning) and semantic mismatch (where the same word has multiple meanings).\n\nOn the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, Karpukhin et al. (2020) show that DPR, unlike a sparse BM25 retriever (Robertson and Walker, 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, Sciavolino et al. (2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and Thakur et al. (2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks.\n\nWith dense and sparse retrievers each having their own distinctive pros and cons, researchers have long aspired to develop retriever models that combine the strengths of both. This, however, has proven challenging as dense and sparse retrievers are supported by drastically different algorithms and data structures (inverted index (Bialecki et al.,",
                    "score": 0.6419640583715838,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 191,
                            "end": 215,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 215,
                            "end": 234,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 520,
                            "end": 542,
                            "matchedPaperCorpusId": "926364"
                        },
                        {
                            "start": 1174,
                            "end": 1197,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1244,
                            "end": 1272,
                            "matchedPaperCorpusId": "2218552"
                        },
                        {
                            "start": 1540,
                            "end": 1564,
                            "matchedPaperCorpusId": "237562875"
                        },
                        {
                            "start": 1652,
                            "end": 1672,
                            "matchedPaperCorpusId": "233296016"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.96240234375,
            "relevance_judgment_input_expanded": "# Title: Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xilun Chen, Kushal Lakhotia, Barlas O\u011fuz, Anchit Gupta, Patrick Lewis, Stanislav Peshterliev, Yashar Mehdad, Sonal Gupta, Wen-tau Yih\n## Abstract\nDespite their recent popularity and well-known advantages, dense retrievers still lag behind sparse methods such as BM25 in their ability to reliably match salient phrases and rare entities in the query and to generalize to out-of-domain data. It has been argued that this is an inherent limitation of dense models. We rebut this claim by introducing the Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical matching capacity of a sparse model. We show that a dense Lexical Model {\\Lambda} can be trained to imitate a sparse one, and SPAR is built by augmenting a standard dense retriever with {\\Lambda}. Empirically, SPAR shows superior performance on a range of tasks including five question answering datasets, MS MARCO passage retrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain evaluation, exceeding the performance of state-of-the-art dense and sparse retrievers. The code and models of SPAR are available at: https://github.com/facebookresearch/dpr-scale/tree/main/spar\n## Introduction\nText retrieval is a crucial component for a wide range of knowledge-intensive NLP systems, such as open-domain question answering (ODQA) models and search engines. Recently, dense retrievers (Karpukhin et al., 2020;Xiong et al., 2021) have gained popularity and demonstrated strong performance on a number of retrieval tasks. Dense retrievers employ deep neural networks to learn continuous representations for the queries and documents, and perform retrieval in this dense embedding space using nearest neighbor search (Johnson et al., 2019). Compared to traditional sparse retrievers that rely on discrete bag-of-words representations, dense retrievers can derive more semantically expressive embeddings, thanks to its end-to-end learnability and powerful pre-trained encoders. This helps dense retrievers to overcome several inherent limitations of sparse systems such as vocabulary mismatch (where different words are used for the same meaning) and semantic mismatch (where the same word has multiple meanings).\n\nOn the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, Karpukhin et al. (2020) show that DPR, unlike a sparse BM25 retriever (Robertson and Walker, 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, Sciavolino et al. (2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and Thakur et al. (2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks.\n\nWith dense and sparse retrievers each having their own distinctive pros and cons, researchers have long aspired to develop retriever models that combine the strengths of both. This, however, has proven challenging as dense and sparse retrievers are supported by drastically different algorithms and data structures (inverted index (Bialecki et al.,",
            "reference_string": "[238744204 | Chen et al. | 2021 | Citations: 69]"
        },
        {
            "title": "Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change",
            "venue": "Data Intelligence",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/dint/article-pdf/doi/10.1162/dint_a_00245/2348276/dint_a_00245.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.10945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243283631",
                    "name": "Paulo Pirozelli"
                },
                {
                    "authorId": "2140512811",
                    "name": "M. M. Jos'e"
                },
                {
                    "authorId": "2185349318",
                    "name": "I. Silveira"
                },
                {
                    "authorId": "2135817708",
                    "name": "Fl'avio Nakasato"
                },
                {
                    "authorId": "2125066",
                    "name": "S. M. Peres"
                },
                {
                    "authorId": "119984906",
                    "name": "A. Brand\u00e3o"
                },
                {
                    "authorId": "2243884199",
                    "name": "Anna H. R. Costa"
                },
                {
                    "authorId": "70089890",
                    "name": "F. G. Cozman"
                }
            ],
            "abstract": "ABSTRACT Pir\u00e1 is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pir\u00e1. By creating these baselines, researchers can more easily utilize Pir\u00e1 as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pir\u00e1 dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, repetitions, and other shortcomings. Furthermore, the dataset has been extended in several new directions, so as to face the aforementioned benchmarks: translation of supporting texts from English into Portuguese, classification labels for answerability, automatic paraphrases of questions and answers, and multiple choice candidates. The results described in this paper provide several points of reference for researchers interested in exploring the challenges provided by the Pir\u00e1 dataset.",
            "corpus_id": 262066417,
            "sentences": [
                {
                    "corpus_id": "262066417",
                    "title": "Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change",
                    "text": "Two types of IR models were investigated for this benchmark: BM25 [21], a sparse retrieval, and Dense Passage Retrieval (DPR) [22], a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack.",
                    "score": 0.7525516582411123,
                    "section_title": "Benchmark Setup",
                    "char_start_offset": 26407,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 738
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 66,
                            "end": 70,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 126,
                            "end": 130,
                            "matchedPaperCorpusId": "215737187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change\n# Venue: Data Intelligence\n# Authors: Paulo Pirozelli, M. M. Jos'e, I. Silveira, Fl'avio Nakasato, S. M. Peres, A. Brand\u00e3o, Anna H. R. Costa, F. G. Cozman\n## Abstract\nABSTRACT Pir\u00e1 is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pir\u00e1. By creating these baselines, researchers can more easily utilize Pir\u00e1 as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pir\u00e1 dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, repetitions, and other shortcomings. Furthermore, the dataset has been extended in several new directions, so as to face the aforementioned benchmarks: translation of supporting texts from English into Portuguese, classification labels for answerability, automatic paraphrases of questions and answers, and multiple choice candidates. The results described in this paper provide several points of reference for researchers interested in exploring the challenges provided by the Pir\u00e1 dataset.\n## Benchmark Setup\nTwo types of IR models were investigated for this benchmark: BM25 [21], a sparse retrieval, and Dense Passage Retrieval (DPR) [22], a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack.",
            "reference_string": "[262066417 | Pirozelli et al. | 2023 | Citations: 4]"
        },
        {
            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 33,
            "citation_count": 30,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.00235",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.00235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118384241",
                    "name": "Hang Li"
                },
                {
                    "authorId": "2146514461",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "1630489015",
                    "name": "Shengyao Zhuang"
                },
                {
                    "authorId": "143832672",
                    "name": "Ahmed Mourad"
                },
                {
                    "authorId": "2461713",
                    "name": "Xueguang Ma"
                },
                {
                    "authorId": "145580839",
                    "name": "Jimmy J. Lin"
                },
                {
                    "authorId": "1692855",
                    "name": "G. Zuccon"
                }
            ],
            "abstract": "Current pre-trained language model approaches to information retrieval can be broadly divided into two categories: sparse retrievers (to which belong also non-neural approaches such as bag-of-words methods, e.g., BM25) and dense retrievers. Each of these categories appears to capture different characteristics of relevance. Previous work has investigated how relevance signals from sparse retrievers could be combined with those from dense retrievers via interpolation. Such interpolation would generally lead to higher retrieval effectiveness. In this paper we consider the problem of combining the relevance signals from sparse and dense retrievers in the context of Pseudo Relevance Feedback (PRF). This context poses two key challenges: (1) When should interpolation occur: before, after, or both before and after the PRF process? (2) Which sparse representation should be considered: a zero-shot bag-of-words model (BM25), or a learned sparse representation? To answer these questions we perform a thorough empirical evaluation considering an effective and scalable neural PRF approach (Vector-PRF), three effective dense retrievers (ANCE, TCTv2, DistillBERT), and one state-of-the-art learned sparse retriever (uniCOIL). The empirical findings from our experiments suggest that, regardless of sparse representation and dense retriever, interpolation both before and after PRF achieves the highest effectiveness across most datasets and metrics.",
            "corpus_id": 248496043,
            "sentences": [
                {
                    "corpus_id": "248496043",
                    "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
                    "text": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels [26]. Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers [10,17,18,23,24,28,30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" [32], and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value [26]. \n\nOn the other hand, learned sparse models [2,5,6,8,9,14,19,22,31], also strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformerbased deep language model re-rankers [21]. They use transformerbased language models to learn term weights, and achieve comparable effectiveness to dense retrievers.",
                    "score": 0.6676050059165934,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1763
                        },
                        {
                            "start": 1766,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2127
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 303,
                            "end": 307,
                            "matchedPaperCorpusId": "237366133"
                        },
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 968,
                            "end": 971,
                            "matchedPaperCorpusId": "235720578"
                        },
                        {
                            "start": 971,
                            "end": 974,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 974,
                            "end": 977,
                            "matchedPaperCorpusId": "238857121"
                        },
                        {
                            "start": 977,
                            "end": 980,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1264,
                            "end": 1268,
                            "matchedPaperCorpusId": "235792476"
                        },
                        {
                            "start": 1758,
                            "end": 1762,
                            "matchedPaperCorpusId": "237366133"
                        },
                        {
                            "start": 1812,
                            "end": 1814,
                            "matchedPaperCorpusId": "218521094"
                        },
                        {
                            "start": 1816,
                            "end": 1818,
                            "matchedPaperCorpusId": "235792467"
                        },
                        {
                            "start": 1821,
                            "end": 1824,
                            "matchedPaperCorpusId": "233394068"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Hang Li, Shuai Wang, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy J. Lin, G. Zuccon\n## Abstract\nCurrent pre-trained language model approaches to information retrieval can be broadly divided into two categories: sparse retrievers (to which belong also non-neural approaches such as bag-of-words methods, e.g., BM25) and dense retrievers. Each of these categories appears to capture different characteristics of relevance. Previous work has investigated how relevance signals from sparse retrievers could be combined with those from dense retrievers via interpolation. Such interpolation would generally lead to higher retrieval effectiveness. In this paper we consider the problem of combining the relevance signals from sparse and dense retrievers in the context of Pseudo Relevance Feedback (PRF). This context poses two key challenges: (1) When should interpolation occur: before, after, or both before and after the PRF process? (2) Which sparse representation should be considered: a zero-shot bag-of-words model (BM25), or a learned sparse representation? To answer these questions we perform a thorough empirical evaluation considering an effective and scalable neural PRF approach (Vector-PRF), three effective dense retrievers (ANCE, TCTv2, DistillBERT), and one state-of-the-art learned sparse retriever (uniCOIL). The empirical findings from our experiments suggest that, regardless of sparse representation and dense retriever, interpolation both before and after PRF achieves the highest effectiveness across most datasets and metrics.\n## INTRODUCTION\nTraditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels [26]. Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers [10,17,18,23,24,28,30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" [32], and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value [26]. \n\nOn the other hand, learned sparse models [2,5,6,8,9,14,19,22,31], also strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformerbased deep language model re-rankers [21]. They use transformerbased language models to learn term weights, and achieve comparable effectiveness to dense retrievers.",
            "reference_string": "[248496043 | Li et al. | 2022 | Citations: 30]"
        },
        {
            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146342371",
                    "name": "Taolin Zhang"
                },
                {
                    "authorId": "2257089368",
                    "name": "Dongyang Li"
                },
                {
                    "authorId": "2300139675",
                    "name": "Qizhou Chen"
                },
                {
                    "authorId": "50097294",
                    "name": "Chengyu Wang"
                },
                {
                    "authorId": "2292090586",
                    "name": "Longtao Huang"
                },
                {
                    "authorId": "2292128230",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2257159827",
                    "name": "Xiaofeng He"
                },
                {
                    "authorId": "2272790856",
                    "name": "Junyuan Huang"
                }
            ],
            "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.",
            "corpus_id": 269605025,
            "sentences": [
                {
                    "corpus_id": "269605025",
                    "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
                    "text": "To evaluate the impact of retrieval mechanisms, we implemented benchmarks using both dense and sparse retrievers. Sparse retrievers, such as TF-IDF [28] and BM25 [29], are grounded in token frequency-centric metrics, contrasting with dense retrievers represented by Spider [26], Contriever [11], and DPR [16]. Findings presented in Table 2 support the following conclusions: (1) Dense retrievers surpass sparse alternatives, showcasing superior performance and consistency across tasks. (2) Among dense retrieval models, those employing direct negative sampling techniques, such as DPR [16], notably improve query document discrimination, which is achieved through contrastive learning [13].",
                    "score": 0.615896640013549,
                    "section_title": "The Influence of Different Retrievers",
                    "char_start_offset": 22786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 691
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 148,
                            "end": 152,
                            "matchedPaperCorpusId": "14638345"
                        },
                        {
                            "start": 162,
                            "end": 166,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 273,
                            "end": 277,
                            "matchedPaperCorpusId": "245144844"
                        },
                        {
                            "start": 290,
                            "end": 294,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 304,
                            "end": 308,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 586,
                            "end": 590,
                            "matchedPaperCorpusId": "215737187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models\n# Venue: European Conference on Artificial Intelligence\n# Authors: Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Junyuan Huang\n## Abstract\nRetrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.\n## The Influence of Different Retrievers\nTo evaluate the impact of retrieval mechanisms, we implemented benchmarks using both dense and sparse retrievers. Sparse retrievers, such as TF-IDF [28] and BM25 [29], are grounded in token frequency-centric metrics, contrasting with dense retrievers represented by Spider [26], Contriever [11], and DPR [16]. Findings presented in Table 2 support the following conclusions: (1) Dense retrievers surpass sparse alternatives, showcasing superior performance and consistency across tasks. (2) Among dense retrieval models, those employing direct negative sampling techniques, such as DPR [16], notably improve query document discrimination, which is achieved through contrastive learning [13].",
            "reference_string": "[269605025 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282545265",
                    "name": "Jaewoong Kim"
                },
                {
                    "authorId": "2361711807",
                    "name": "Moohong Min"
                }
            ],
            "abstract": "Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.",
            "corpus_id": 278664642,
            "sentences": [
                {
                    "corpus_id": "278664642",
                    "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
                    "text": "When it comes to the document retrieval, Sparse retrieval methods such as BM25 [Robertson and Zaragoza, 2009;Trotman et al., 2014] had been prevalent due to their straightforward approach to matching keywords. However, they can be limited by their inability to capture the deeper semantic meaning of text. Dense retrieval, on the other hand, showed many advantages over sparse retrieval [Lee et al., 2019;Karpukhin et al., 2020;Li et al., 2021]. Dense retrieval approaches go beyond mere keyword matching; they generate vector representations of documents and queries, facilitating the capture of deep semantic meanings. This is crucial in fields requiring high accuracy and contextual understanding, where the relevancy of documents cannot solely be determined by keyword frequency. Given the advantages of dense retrieval, this method was selected for our model.",
                    "score": 0.5216832426102311,
                    "section_title": "Document preprocessing & similarity search",
                    "char_start_offset": 5487,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 864
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 109,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 109,
                            "end": 130,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 405,
                            "end": 428,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 428,
                            "end": 444,
                            "matchedPaperCorpusId": "236034464"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process\n# Venue: arXiv.org\n# Authors: Jaewoong Kim, Moohong Min\n## Abstract\nRegulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.\n## Document preprocessing & similarity search\nWhen it comes to the document retrieval, Sparse retrieval methods such as BM25 [Robertson and Zaragoza, 2009;Trotman et al., 2014] had been prevalent due to their straightforward approach to matching keywords. However, they can be limited by their inability to capture the deeper semantic meaning of text. Dense retrieval, on the other hand, showed many advantages over sparse retrieval [Lee et al., 2019;Karpukhin et al., 2020;Li et al., 2021]. Dense retrieval approaches go beyond mere keyword matching; they generate vector representations of documents and queries, facilitating the capture of deep semantic meanings. This is crucial in fields requiring high accuracy and contextual understanding, where the relevancy of documents cannot solely be determined by keyword frequency. Given the advantages of dense retrieval, this method was selected for our model.",
            "reference_string": "[278664642 | Kim et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 34,
            "citation_count": 167,
            "influential_citation_count": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.496.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.08535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112021127",
                    "name": "Christopher Sciavolino"
                },
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "46664096",
                    "name": "Jinhyuk Lee"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ],
            "abstract": "Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., \u201cWhere was Arve Furset born?\u201d), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.",
            "corpus_id": 237562875,
            "sentences": [
                {
                    "corpus_id": "237562875",
                    "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
                    "text": "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT. In this paper, we use the popular dense passage retriever (DPR) model (Karpukhin et al., 2020) as our main evaluation, 3 and we also report the evaluation of REALM (Guu et al., 2020) in Appendix A. DPR models the retrieval problem using two encoders, namely the question and the passage encoders, initialized using BERT. DPR uses a contrastive objective during training, with in-batch negatives and hard negatives mined from BM25. During inference, a pre-defined large set of passages (e.g., 21-3 The detailed experimental settings are in Appendix B. million passages in English Wikipedia) are encoded and pre-indexed-for any test question, the top passages with the highest similarity scores are returned. Recently, other advances have been made in improving dense retrieval, including incorporating better hard negatives (Xiong et al., 2021;Qu et al., 2021), or fine-grained phrase retrieval (Lee et al., 2021). We leave them for future investigation.\n\nGeneralization problem Despite the impressive in-domain performance of dense retrievers, their capability of generalizing to unseen questions still remains relatively under-explored. Recently, Lewis et al. (2021a) discover that there is a large overlap between training and testing sets on popular QA benchmarks, concluding that current models tend to memorize training questions and perform significantly worse on non-overlapping questions. AmbER (Chen et al., 2021) test sets are designed to",
                    "score": 0.6642875365418571,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 2371,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.953125
                }
            ],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: Simple Entity-Centric Questions Challenge Dense Retrievers\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, Danqi Chen\n## Abstract\nOpen-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., \u201cWhere was Arve Furset born?\u201d), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.\n## Background and Related Work\nSparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT. In this paper, we use the popular dense passage retriever (DPR) model (Karpukhin et al., 2020) as our main evaluation, 3 and we also report the evaluation of REALM (Guu et al., 2020) in Appendix A. DPR models the retrieval problem using two encoders, namely the question and the passage encoders, initialized using BERT. DPR uses a contrastive objective during training, with in-batch negatives and hard negatives mined from BM25. During inference, a pre-defined large set of passages (e.g., 21-3 The detailed experimental settings are in Appendix B. million passages in English Wikipedia) are encoded and pre-indexed-for any test question, the top passages with the highest similarity scores are returned. Recently, other advances have been made in improving dense retrieval, including incorporating better hard negatives (Xiong et al., 2021;Qu et al., 2021), or fine-grained phrase retrieval (Lee et al., 2021). We leave them for future investigation.\n\nGeneralization problem Despite the impressive in-domain performance of dense retrievers, their capability of generalizing to unseen questions still remains relatively under-explored. Recently, Lewis et al. (2021a) discover that there is a large overlap between training and testing sets on popular QA benchmarks, concluding that current models tend to memorize training questions and perform significantly worse on non-overlapping questions. AmbER (Chen et al., 2021) test sets are designed to",
            "reference_string": "[237562875 | Sciavolino et al. | 2021 | Citations: 167]"
        },
        {
            "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 83,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.10558",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.10558, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145579682",
                    "name": "Gustavo Penha"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ],
            "abstract": "Ranking responses for a given dialogue context is a popular benchmark in which the setup is to re-rank the ground-truth response over a limited set of $n$ responses, where $n$ is typically 10. The predominance of this setup in conversation response ranking has lead to a great deal of attention to building neural re-rankers, while the first-stage retrieval step has been overlooked. Since the correct answer is always available in the candidate list of $n$ responses, this artificial evaluation setup assumes that there is a first-stage retrieval step which is always able to rank the correct response in its top-$n$ list. In this paper we focus on the more realistic task of full-rank retrieval of responses, where $n$ can be up to millions of responses. We investigate both dialogue context and response expansion techniques for sparse retrieval, as well as zero-shot and fine-tuned dense retrieval approaches. Our findings based on three different information-seeking dialogue datasets reveal that a learned response expansion technique is a solid baseline for sparse retrieval. We find the best performing method overall to be dense retrieval with intermediate training, i.e. a step after the language model pre-training where sentence representations are learned, followed by fine-tuning on the target conversational data. We also investigate the intriguing phenomena that harder negatives sampling techniques lead to worse results for the fine-tuned dense retrieval models. The code and datasets are available at https://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues.",
            "corpus_id": 248366550,
            "sentences": [
                {
                    "corpus_id": "248366550",
                    "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
                    "text": "The proposed conceptual framework by [30] argues for a categorization of retrieval models into two dimensions: supervised vs. unsupervised and dense vs. sparse representations 5 . An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods. \n\nA supervised sparse retrieval model such as COIL [12], SPLADE [8], TILDE [78] and DeepImpact [36] can take advantage of the effectiveness of transformer-based language models by changing the terms' weights from collection statistics to something that is learned. DeepCT [6] for example learns term weights with a transformerbased regression model from the supervision of the MSMarco dataset. Approaches that only modify non-zero weights however are not able to address the vocabulary mismatch problem [9], as non-zero terms will not be affected. One way to address such problem in sparse retrieval is by using query expansion methods. RM3 [1] has been shown to be a competitive query expansion technique that uses pseudo-relevance feedback to add new terms to the queries followed by another final retrieval step using the modified query. Document expansion has also been shown to be an effective technique to improve sparse retrieval, which is able to address the vocabulary mismatch problem. The core idea is to create pseudo documents that have expanded terms and use them instead when doing retrieval. Doc2query [41] is an effective approach to document expansion that uses a language model to predict the queries which might be issued to find the document. The predictions of this model are used to create the augmented pseudo documents. Expansion techniques are able to modify non-zero weights by adding terms that did not exist in the query or document. \n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics.",
                    "score": 0.531353613509492,
                    "section_title": "Dense and Sparse Retrieval",
                    "char_start_offset": 8429,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 527
                        },
                        {
                            "start": 530,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1635
                        },
                        {
                            "start": 1636,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 1990
                        },
                        {
                            "start": 1993,
                            "end": 2226
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 237,
                            "end": 241,
                            "matchedPaperCorpusId": "2218552"
                        },
                        {
                            "start": 253,
                            "end": 257,
                            "matchedPaperCorpusId": "2996187"
                        },
                        {
                            "start": 623,
                            "end": 627,
                            "matchedPaperCorpusId": "233394068"
                        },
                        {
                            "start": 1031,
                            "end": 1034,
                            "matchedPaperCorpusId": "3002280"
                        },
                        {
                            "start": 1646,
                            "end": 1650,
                            "matchedPaperCorpusId": "208612557"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9521484375
                },
                {
                    "corpus_id": "248366550",
                    "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
                    "text": "Supervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario. The BEIR benchmark [59] showed that BM25 was superior to dense retrieval from 9-18 (depending on the model) out of the 18 datasets under this evaluation scheme. While the zero-shot scenario offers a fairer comparison of dense models with unsupervised sparse models, learned dense retrieval models should also be compared with learned sparse models, e.g. BM25+doc2query. \n\nUnlike previous work that compares supervised and unsupervised, dense and sparse retrieval models for other tasks such as passage ranking, we provide a novel and comprehensive comparison for the problem of full-rank retrieval of responses for dialogues.",
                    "score": 0.6283550919962977,
                    "section_title": "Dense and Sparse Retrieval",
                    "char_start_offset": 10422,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 920
                        },
                        {
                            "start": 923,
                            "end": 1176
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.9521484375,
            "relevance_judgment_input_expanded": "# Title: Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues\n# Venue: arXiv.org\n# Authors: Gustavo Penha, C. Hauff\n## Abstract\nRanking responses for a given dialogue context is a popular benchmark in which the setup is to re-rank the ground-truth response over a limited set of $n$ responses, where $n$ is typically 10. The predominance of this setup in conversation response ranking has lead to a great deal of attention to building neural re-rankers, while the first-stage retrieval step has been overlooked. Since the correct answer is always available in the candidate list of $n$ responses, this artificial evaluation setup assumes that there is a first-stage retrieval step which is always able to rank the correct response in its top-$n$ list. In this paper we focus on the more realistic task of full-rank retrieval of responses, where $n$ can be up to millions of responses. We investigate both dialogue context and response expansion techniques for sparse retrieval, as well as zero-shot and fine-tuned dense retrieval approaches. Our findings based on three different information-seeking dialogue datasets reveal that a learned response expansion technique is a solid baseline for sparse retrieval. We find the best performing method overall to be dense retrieval with intermediate training, i.e. a step after the language model pre-training where sentence representations are learned, followed by fine-tuning on the target conversational data. We also investigate the intriguing phenomena that harder negatives sampling techniques lead to worse results for the fine-tuned dense retrieval models. The code and datasets are available at https://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues.\n## Dense and Sparse Retrieval\nThe proposed conceptual framework by [30] argues for a categorization of retrieval models into two dimensions: supervised vs. unsupervised and dense vs. sparse representations 5 . An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods. \n\nA supervised sparse retrieval model such as COIL [12], SPLADE [8], TILDE [78] and DeepImpact [36] can take advantage of the effectiveness of transformer-based language models by changing the terms' weights from collection statistics to something that is learned. DeepCT [6] for example learns term weights with a transformerbased regression model from the supervision of the MSMarco dataset. Approaches that only modify non-zero weights however are not able to address the vocabulary mismatch problem [9], as non-zero terms will not be affected. One way to address such problem in sparse retrieval is by using query expansion methods. RM3 [1] has been shown to be a competitive query expansion technique that uses pseudo-relevance feedback to add new terms to the queries followed by another final retrieval step using the modified query. Document expansion has also been shown to be an effective technique to improve sparse retrieval, which is able to address the vocabulary mismatch problem. The core idea is to create pseudo documents that have expanded terms and use them instead when doing retrieval. Doc2query [41] is an effective approach to document expansion that uses a language model to predict the queries which might be issued to find the document. The predictions of this model are used to create the augmented pseudo documents. Expansion techniques are able to modify non-zero weights by adding terms that did not exist in the query or document. \n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics.\n...\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario. The BEIR benchmark [59] showed that BM25 was superior to dense retrieval from 9-18 (depending on the model) out of the 18 datasets under this evaluation scheme. While the zero-shot scenario offers a fairer comparison of dense models with unsupervised sparse models, learned dense retrieval models should also be compared with learned sparse models, e.g. BM25+doc2query. \n\nUnlike previous work that compares supervised and unsupervised, dense and sparse retrieval models for other tasks such as passage ranking, we provide a novel and comprehensive comparison for the problem of full-rank retrieval of responses for dialogues.",
            "reference_string": "[248366550 | Penha et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Learning to Retrieve In-Context Examples for Large Language Models",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 43,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.07164",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.07164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145769448",
                    "name": "Liang Wang"
                },
                {
                    "authorId": "144610884",
                    "name": "Nan Yang"
                },
                {
                    "authorId": "49807919",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.",
            "corpus_id": 259924840,
            "sentences": [
                {
                    "corpus_id": "259924840",
                    "title": "Learning to Retrieve In-Context Examples for Large Language Models",
                    "text": "Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem. Various techniques such as hard negative mining (Karpukhin et al., 2020), knowledge distillation (Ren et al., 2021), and continual pre-training (Wang et al., 2022) have been proposed to enhance the performance of dense retrieval. \n\nRetrieval Augmented LLMs combine the generative power of LLMs with the ability to retrieve relevant information from external sources (Ram et al., 2023;Lewis et al., 2020;Shi et al., 2023). This paradigm has the potential to enhance the factual consistency of generated texts, make LLMs aware of the up-to-date knowledge, as well as provide a natural way for source attribution (Nakano et al., 2021). The retrieved information can be incorporated into LLMs through various mechanisms, such as input concatenation (Shi et al., 2023), intermediate attention fusion (Borgeaud et al., 2022), and output interpolation (Khandelwal et al., 2020). For in-context learning, the goal of retrieval augmentation is to improve the performance of LLMs on downstream tasks by retrieving informative examples (Li et al., 2023;Luo et al., 2023).",
                    "score": 0.5622858764399665,
                    "section_title": "Related Work",
                    "char_start_offset": 5096,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 492
                        },
                        {
                            "start": 495,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1323
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 145,
                            "end": 166,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 311,
                            "end": 335,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 360,
                            "end": 378,
                            "matchedPaperCorpusId": "238857121"
                        },
                        {
                            "start": 407,
                            "end": 426,
                            "matchedPaperCorpusId": "254366618"
                        },
                        {
                            "start": 666,
                            "end": 683,
                            "matchedPaperCorpusId": "256389797"
                        },
                        {
                            "start": 873,
                            "end": 894,
                            "matchedPaperCorpusId": "245329531"
                        },
                        {
                            "start": 1008,
                            "end": 1026,
                            "matchedPaperCorpusId": "256389797"
                        },
                        {
                            "start": 1058,
                            "end": 1081,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 1108,
                            "end": 1133,
                            "matchedPaperCorpusId": "207870430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94873046875
                }
            ],
            "relevance_judgement": 0.94873046875,
            "relevance_judgment_input_expanded": "# Title: Learning to Retrieve In-Context Examples for Large Language Models\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Liang Wang, Nan Yang, Furu Wei\n## Abstract\nLarge language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.\n## Related Work\nCompared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem. Various techniques such as hard negative mining (Karpukhin et al., 2020), knowledge distillation (Ren et al., 2021), and continual pre-training (Wang et al., 2022) have been proposed to enhance the performance of dense retrieval. \n\nRetrieval Augmented LLMs combine the generative power of LLMs with the ability to retrieve relevant information from external sources (Ram et al., 2023;Lewis et al., 2020;Shi et al., 2023). This paradigm has the potential to enhance the factual consistency of generated texts, make LLMs aware of the up-to-date knowledge, as well as provide a natural way for source attribution (Nakano et al., 2021). The retrieved information can be incorporated into LLMs through various mechanisms, such as input concatenation (Shi et al., 2023), intermediate attention fusion (Borgeaud et al., 2022), and output interpolation (Khandelwal et al., 2020). For in-context learning, the goal of retrieval augmentation is to improve the performance of LLMs on downstream tasks by retrieving informative examples (Li et al., 2023;Luo et al., 2023).",
            "reference_string": "[259924840 | Wang et al. | 2023 | Citations: 43]"
        },
        {
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 113,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.02464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238699512",
                    "name": "Abdelrahman Abdallah"
                },
                {
                    "authorId": "1935823995",
                    "name": "Bhawna Piryani"
                },
                {
                    "authorId": "1389526186",
                    "name": "Jamshid Mozafari"
                },
                {
                    "authorId": "2343952059",
                    "name": "Mohammed Ali"
                },
                {
                    "authorId": "2261673463",
                    "name": "Adam Jatowt"
                }
            ],
            "abstract": "Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern applications in information retrieval, question answering, or knowledge-based text generation. However, existing solutions are often fragmented, lacking a unified framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the complexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking. In response to these challenges, we introduce Rankify, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. Rankify supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re-ranking models to enhance retrieval quality. Additionally, Rankify includes a collection of pre-retrieved datasets to facilitate benchmarking, available at Huggingface (https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To encourage adoption and ease of integration, we provide comprehensive documentation (http://rankify.readthedocs.io/), an open-source implementation on GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for easy installation (https://pypi.org/project/rankify/). As a unified and lightweight framework, Rankify allows researchers and practitioners to advance retrieval and re-ranking methodologies while ensuring consistency, scalability, and ease of use.",
            "corpus_id": 276107364,
            "sentences": [
                {
                    "corpus_id": "276107364",
                    "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
                    "text": "Retrieval models have evolved significantly, transitioning from traditional sparse retrievers to sophisticated dense and hybrid retrieval techniques. Sparse retrievers, such as BM25 [92], remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods [55], which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR [120] and ANCE [115], improve retrieval effectiveness by employing contrastive training with in-batch negatives [33] and hard-negative mining [46,116]. Multi-vector approaches like ColBERT [49] enable fine-grained interaction between queries and documents, albeit at the cost of increased computational overhead. To address the limitations of dense retrievers, hybrid models [30,67] combine sparse and dense approaches, utilizing both term matching and semantic understanding for enhanced retrieval performance. \n\nRecent research has also explored retrieval enhancements through knowledge distillation, curriculum learning, and pre-training. Distillation techniques transfer knowledge from cross-encoders or teacher retrievers to student models, improving dense retriever effectiveness [18,86]. Curriculum learning strategies, which progressively train retrievers from easy to hard samples [60,124], have been shown to enhance both supervised and zero-shot retrieval effectiveness. Meanwhile, pre-training methods such as contrastive learning [40] and masked autoencoding [29,63] have significantly improved the generalization of retrieval models. Additionally, approaches like query augmentation [66,70] generate synthetic training data to enhance retrieval robustness.",
                    "score": 0.5434916377239832,
                    "section_title": "Related Work 2.1 Retrievers",
                    "char_start_offset": 5166,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1030
                        },
                        {
                            "start": 1033,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1500
                        },
                        {
                            "start": 1501,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1789
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 182,
                            "end": 186,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 386,
                            "end": 390,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 519,
                            "end": 524,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 534,
                            "end": 539,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 631,
                            "end": 635,
                            "matchedPaperCorpusId": "202718954"
                        },
                        {
                            "start": 661,
                            "end": 665,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 665,
                            "end": 669,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 894,
                            "end": 898,
                            "matchedPaperCorpusId": "232423090"
                        },
                        {
                            "start": 898,
                            "end": 901,
                            "matchedPaperCorpusId": "248003306"
                        },
                        {
                            "start": 1309,
                            "end": 1312,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1413,
                            "end": 1417,
                            "matchedPaperCorpusId": "248426770"
                        },
                        {
                            "start": 1591,
                            "end": 1595,
                            "matchedPaperCorpusId": "237581068"
                        },
                        {
                            "start": 1716,
                            "end": 1720,
                            "matchedPaperCorpusId": "231704318"
                        },
                        {
                            "start": 1720,
                            "end": 1723,
                            "matchedPaperCorpusId": "221802772"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9453125
                }
            ],
            "relevance_judgement": 0.9453125,
            "relevance_judgment_input_expanded": "# Title: Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt\n## Abstract\nRetrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern applications in information retrieval, question answering, or knowledge-based text generation. However, existing solutions are often fragmented, lacking a unified framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the complexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking. In response to these challenges, we introduce Rankify, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. Rankify supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re-ranking models to enhance retrieval quality. Additionally, Rankify includes a collection of pre-retrieved datasets to facilitate benchmarking, available at Huggingface (https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To encourage adoption and ease of integration, we provide comprehensive documentation (http://rankify.readthedocs.io/), an open-source implementation on GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for easy installation (https://pypi.org/project/rankify/). As a unified and lightweight framework, Rankify allows researchers and practitioners to advance retrieval and re-ranking methodologies while ensuring consistency, scalability, and ease of use.\n## Related Work 2.1 Retrievers\nRetrieval models have evolved significantly, transitioning from traditional sparse retrievers to sophisticated dense and hybrid retrieval techniques. Sparse retrievers, such as BM25 [92], remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods [55], which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR [120] and ANCE [115], improve retrieval effectiveness by employing contrastive training with in-batch negatives [33] and hard-negative mining [46,116]. Multi-vector approaches like ColBERT [49] enable fine-grained interaction between queries and documents, albeit at the cost of increased computational overhead. To address the limitations of dense retrievers, hybrid models [30,67] combine sparse and dense approaches, utilizing both term matching and semantic understanding for enhanced retrieval performance. \n\nRecent research has also explored retrieval enhancements through knowledge distillation, curriculum learning, and pre-training. Distillation techniques transfer knowledge from cross-encoders or teacher retrievers to student models, improving dense retriever effectiveness [18,86]. Curriculum learning strategies, which progressively train retrievers from easy to hard samples [60,124], have been shown to enhance both supervised and zero-shot retrieval effectiveness. Meanwhile, pre-training methods such as contrastive learning [40] and masked autoencoding [29,63] have significantly improved the generalization of retrieval models. Additionally, approaches like query augmentation [66,70] generate synthetic training data to enhance retrieval robustness.",
            "reference_string": "[276107364 | Abdallah et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Learning to Tokenize for Generative Retrieval",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 75,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.04171",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.04171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153198380",
                    "name": "Weiwei Sun"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "2117203270",
                    "name": "Zheng Chen"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2387872",
                    "name": "Haichao Zhu"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "1696030",
                    "name": "M. de Rijke"
                },
                {
                    "authorId": "2780667",
                    "name": "Z. Ren"
                }
            ],
            "abstract": "Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability.",
            "corpus_id": 258048596,
            "sentences": [
                {
                    "corpus_id": "258048596",
                    "title": "Learning to Tokenize for Generative Retrieval",
                    "text": "Sparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24]. \n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1]. \n\nGenerative retrieval. Generative retrieval is increasing gaining attention. It retrieves documents by generating their docid using a generative model like T5. Generative retrieval presents an end-toend solution for document retrieval tasks [28,42] and allows for better exploitation of the capabilities of large generative LMs [1].",
                    "score": 0.6658189543339339,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 36186,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 17
                        },
                        {
                            "start": 18,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 708
                        },
                        {
                            "start": 711,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1123
                        },
                        {
                            "start": 1124,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1474
                        },
                        {
                            "start": 1475,
                            "end": 1704
                        },
                        {
                            "start": 1707,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 2038
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 124,
                            "matchedPaperCorpusId": "16829071"
                        },
                        {
                            "start": 143,
                            "end": 147,
                            "matchedPaperCorpusId": "1183147"
                        },
                        {
                            "start": 156,
                            "end": 160,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 335,
                            "end": 339,
                            "matchedPaperCorpusId": "3666085"
                        },
                        {
                            "start": 339,
                            "end": 342,
                            "matchedPaperCorpusId": "5688521"
                        },
                        {
                            "start": 342,
                            "end": 345,
                            "matchedPaperCorpusId": "155300"
                        },
                        {
                            "start": 871,
                            "end": 875,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1053,
                            "end": 1057,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1057,
                            "end": 1060,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1079,
                            "end": 1083,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1083,
                            "end": 1086,
                            "matchedPaperCorpusId": "244799249"
                        },
                        {
                            "start": 1115,
                            "end": 1119,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 1244,
                            "end": 1248,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 1251,
                            "end": 1254,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 1530,
                            "end": 1534,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1633,
                            "end": 1636,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1700,
                            "end": 1703,
                            "matchedPaperCorpusId": "248366293"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Learning to Tokenize for Generative Retrieval\n# Venue: Neural Information Processing Systems\n# Authors: Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin, M. de Rijke, Z. Ren\n## Abstract\nConventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability.\n## RELATED WORK\nSparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24]. \n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1]. \n\nGenerative retrieval. Generative retrieval is increasing gaining attention. It retrieves documents by generating their docid using a generative model like T5. Generative retrieval presents an end-toend solution for document retrieval tasks [28,42] and allows for better exploitation of the capabilities of large generative LMs [1].",
            "reference_string": "[258048596 | Sun et al. | 2023 | Citations: 75]"
        },
        {
            "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies",
            "venue": "Knowledge Discovery and Data Mining",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 41,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3580305.3599903",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166392258",
                    "name": "Yubao Tang"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "1777025",
                    "name": "J. Guo"
                },
                {
                    "authorId": "2108313363",
                    "name": "Jiangui Chen"
                },
                {
                    "authorId": "2218182057",
                    "name": "Zuowei Zhu"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Recently, a new paradigm called Differentiable Search Index (DSI) has been proposed for document retrieval, wherein a sequence-to-sequence model is learned to directly map queries to relevant document identifiers. The key idea behind DSI is to fully parameterize traditional ''index-retrieve'' pipelines within a single neural model, by encoding all documents in the corpus into the model parameters. In essence, DSI needs to resolve two major questions: (1) how to assign an identifier to each document, and (2) how to learn the associations between a document and its identifier. In this work, we propose a Semantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the area of Cognitive Psychology. Our approach advances original DSI in two ways: (1) For the document identifier, we take inspiration from Elaboration Strategies in human learning. Specifically, we assign each document an Elaborative Description based on the query generation technique, which is more meaningful than a string of integers in the original DSI; and (2) For the associations between a document and its identifier, we take inspiration from Rehearsal Strategies in human learning. Specifically, we select fine-grained semantic features from a document as Rehearsal Contents to improve document memorization. Both the offline and online experiments show improved retrieval performance over prevailing baselines.",
            "corpus_id": 258865792,
            "sentences": [
                {
                    "corpus_id": "258865792",
                    "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies",
                    "text": "Sparse retrieval. The key idea of sparse retrieval methods is to utilize exact matching signals to design a relevance scoring function. Specifically, these models consider easily computed statistics (e.g., term frequency, document length, and inverse document frequency) of normalized terms matched exactly between the query and document. Among these models, BM25 [47] is shown to be effective and is still regarded as a strong baseline of many retrieval models nowadays. To enhance the semantic relationships, several works utilize word embeddings as term weights [22,62]. Dense retrieval. To solve the vocabulary mismatch problem in sparse retrieval [23,61], many researchers turn to dense retrieval models [33,59], which first learn dense representations of both queries and documents, and then approximate nearest neighbor search [9,11] is employed to retrieve. Further, pre-trained models are used to enhance dense retrieval [28,41]. Differentiable search index. Differentiable Search Index (DSI) [52] is gaining increasing attention, which retrieves documents by generating their docid using a generative model. It presents an endto-end solution for document retrieval tasks and allows for better exploitation of the capabilities of pre-trained generative models. \n\nFor the docids, the original DSI proposed that the docid could be represented by a single token (atomic integers) or a string of tokens, which can be an arbitrary string or a semantic numeric string [52].",
                    "score": 0.5550452913882975,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 38325,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 17
                        },
                        {
                            "start": 18,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1269
                        },
                        {
                            "start": 1272,
                            "end": 1476
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 364,
                            "end": 368,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 565,
                            "end": 569,
                            "matchedPaperCorpusId": "216144735"
                        },
                        {
                            "start": 569,
                            "end": 572,
                            "matchedPaperCorpusId": "155300"
                        },
                        {
                            "start": 652,
                            "end": 656,
                            "matchedPaperCorpusId": "3002280"
                        },
                        {
                            "start": 656,
                            "end": 659,
                            "matchedPaperCorpusId": "14286159"
                        },
                        {
                            "start": 709,
                            "end": 713,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 834,
                            "end": 837,
                            "matchedPaperCorpusId": "270664"
                        },
                        {
                            "start": 837,
                            "end": 840,
                            "matchedPaperCorpusId": "13091446"
                        },
                        {
                            "start": 930,
                            "end": 934,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 934,
                            "end": 937,
                            "matchedPaperCorpusId": "211572791"
                        },
                        {
                            "start": 1002,
                            "end": 1006,
                            "matchedPaperCorpusId": "246863488"
                        },
                        {
                            "start": 1471,
                            "end": 1475,
                            "matchedPaperCorpusId": "246863488"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93701171875
                }
            ],
            "relevance_judgement": 0.93701171875,
            "relevance_judgment_input_expanded": "# Title: Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies\n# Venue: Knowledge Discovery and Data Mining\n# Authors: Yubao Tang, Ruqing Zhang, J. Guo, Jiangui Chen, Zuowei Zhu, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng\n## Abstract\nRecently, a new paradigm called Differentiable Search Index (DSI) has been proposed for document retrieval, wherein a sequence-to-sequence model is learned to directly map queries to relevant document identifiers. The key idea behind DSI is to fully parameterize traditional ''index-retrieve'' pipelines within a single neural model, by encoding all documents in the corpus into the model parameters. In essence, DSI needs to resolve two major questions: (1) how to assign an identifier to each document, and (2) how to learn the associations between a document and its identifier. In this work, we propose a Semantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the area of Cognitive Psychology. Our approach advances original DSI in two ways: (1) For the document identifier, we take inspiration from Elaboration Strategies in human learning. Specifically, we assign each document an Elaborative Description based on the query generation technique, which is more meaningful than a string of integers in the original DSI; and (2) For the associations between a document and its identifier, we take inspiration from Rehearsal Strategies in human learning. Specifically, we select fine-grained semantic features from a document as Rehearsal Contents to improve document memorization. Both the offline and online experiments show improved retrieval performance over prevailing baselines.\n## RELATED WORK\nSparse retrieval. The key idea of sparse retrieval methods is to utilize exact matching signals to design a relevance scoring function. Specifically, these models consider easily computed statistics (e.g., term frequency, document length, and inverse document frequency) of normalized terms matched exactly between the query and document. Among these models, BM25 [47] is shown to be effective and is still regarded as a strong baseline of many retrieval models nowadays. To enhance the semantic relationships, several works utilize word embeddings as term weights [22,62]. Dense retrieval. To solve the vocabulary mismatch problem in sparse retrieval [23,61], many researchers turn to dense retrieval models [33,59], which first learn dense representations of both queries and documents, and then approximate nearest neighbor search [9,11] is employed to retrieve. Further, pre-trained models are used to enhance dense retrieval [28,41]. Differentiable search index. Differentiable Search Index (DSI) [52] is gaining increasing attention, which retrieves documents by generating their docid using a generative model. It presents an endto-end solution for document retrieval tasks and allows for better exploitation of the capabilities of pre-trained generative models. \n\nFor the docids, the original DSI proposed that the docid could be represented by a single token (atomic integers) or a string of tokens, which can be an arbitrary string or a semantic numeric string [52].",
            "reference_string": "[258865792 | Tang et al. | 2023 | Citations: 41]"
        },
        {
            "title": "A proposed conceptual framework for a representational approach to information retrieval",
            "venue": "SIGIR Forum",
            "year": 2021,
            "reference_count": 88,
            "citation_count": 53,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3527546.3527552",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.01529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145580839",
                    "name": "Jimmy J. Lin"
                }
            ],
            "abstract": "This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access \"technologies\" prior to the dawn of computing.",
            "corpus_id": 238259539,
            "sentences": [
                {
                    "corpus_id": "238259539",
                    "title": "A proposed conceptual framework for a representational approach to information retrieval",
                    "text": "Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as Karpukhin et al. [2020] did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation. \n\nThis taxonomy immediately points to the existence of two other classes of logical scoring models. In fact, they correspond to models described in the literature that we can now categorize and unify in a single conceptual framework: \n\nLearned sparse representations The existence of learned dense representations such as DPR and unsupervised sparse representations such as BM25 suggests that there should exist a class of learned sparse representations. \n\nLearning sparse representations is by no means a new idea. If we fix the dimensions of the output representation to be the vocabulary (i.e., retaining a bag-of-words assumption), models for learned sparse representations become term weighting models-that is, a supervised machine learning approach to learning term weights. The earliest example I am aware of is Gordon [1988], who applied (what we might today call) representational learning on boolean vectors of descriptors using genetic algorithms, based on a small set of relevance judgments.",
                    "score": 0.7138686668279854,
                    "section_title": "Generalization of Logical Scoring Models",
                    "char_start_offset": 12726,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 262
                        },
                        {
                            "start": 265,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 567
                        },
                        {
                            "start": 570,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 790
                        },
                        {
                            "start": 793,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1315
                        },
                        {
                            "start": 1318,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1549
                        },
                        {
                            "start": 1552,
                            "end": 1770
                        },
                        {
                            "start": 1773,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2096
                        },
                        {
                            "start": 2097,
                            "end": 2319
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 154,
                            "matchedPaperCorpusId": "215737187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.935546875
                }
            ],
            "relevance_judgement": 0.935546875,
            "relevance_judgment_input_expanded": "# Title: A proposed conceptual framework for a representational approach to information retrieval\n# Venue: SIGIR Forum\n# Authors: Jimmy J. Lin\n## Abstract\nThis paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access \"technologies\" prior to the dawn of computing.\n## Generalization of Logical Scoring Models\nDense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as Karpukhin et al. [2020] did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation. \n\nThis taxonomy immediately points to the existence of two other classes of logical scoring models. In fact, they correspond to models described in the literature that we can now categorize and unify in a single conceptual framework: \n\nLearned sparse representations The existence of learned dense representations such as DPR and unsupervised sparse representations such as BM25 suggests that there should exist a class of learned sparse representations. \n\nLearning sparse representations is by no means a new idea. If we fix the dimensions of the output representation to be the vocabulary (i.e., retaining a bag-of-words assumption), models for learned sparse representations become term weighting models-that is, a supervised machine learning approach to learning term weights. The earliest example I am aware of is Gordon [1988], who applied (what we might today call) representational learning on boolean vectors of descriptors using genetic algorithms, based on a small set of relevance judgments.",
            "reference_string": "[238259539 | Lin | 2021 | Citations: 53]"
        },
        {
            "title": "Retrieval Augmented Code Generation and Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 60,
            "citation_count": 189,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.232.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.11601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3405393",
                    "name": "Md. Rizwan Parvez"
                },
                {
                    "authorId": "2125973767",
                    "name": "W. Ahmad"
                },
                {
                    "authorId": "47570053",
                    "name": "Saikat Chakraborty"
                },
                {
                    "authorId": "31631000",
                    "name": "Baishakhi Ray"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                }
            ],
            "abstract": "Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.",
            "corpus_id": 237304122,
            "sentences": [
                {
                    "corpus_id": "237304122",
                    "title": "Retrieval Augmented Code Generation and Summarization",
                    "text": "Information retrieval (IR) systems or retriever models are designed to retrieve the top-k relevant documents that presumably best provide the desired information (Manning et al., 2008). Term-based retrieval methods, a.k.a. sparse retrieval models, such as TF-IDF or BM25 (Robertson and Zaragoza, 2009) use sparse vector representations to perform lexical matching and compute relevance scores to rank the documents based on a query. On the other hand, dense retrieval methods encode documents into a fixed-size representations and retrieve documents via maximum inner product search (Sutskever et al., 2014;Guo et al., 2016). Particularly of interests,  propose a Dense Passage Retriever (DPR) model for open-domain question answering (QA). It consists of two encoders (Q(.) and P(.)) that encode queries and passages, respectively. The similarity of a query q and a passage p is defined by the inner product of their encoded vectors sim(p, q) = Q(q) T \u22c5 P (p). Given a query q, a positive (relevant) passage p + , and a set of n irrelevant passages p \u2212 i , DPR optimizes the classification loss:\n\n.  propose to fine-tune DPR using in-batch negatives (Gillick et al., 2019;Yih et al., 2011) with curated \"hard\" negatives us- ing BM25 (candidates with high BM25 scores but contain no sub-string that match the target). We refer to  for details.",
                    "score": 0.6093106490919732,
                    "section_title": "Retriever: DPR",
                    "char_start_offset": 7174,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 583,
                            "end": 607,
                            "matchedPaperCorpusId": "7961699"
                        },
                        {
                            "start": 1173,
                            "end": 1190,
                            "matchedPaperCorpusId": "1965270"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93505859375
                }
            ],
            "relevance_judgement": 0.93505859375,
            "relevance_judgment_input_expanded": "# Title: Retrieval Augmented Code Generation and Summarization\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Md. Rizwan Parvez, W. Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang\n## Abstract\nSoftware developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.\n## Retriever: DPR\nInformation retrieval (IR) systems or retriever models are designed to retrieve the top-k relevant documents that presumably best provide the desired information (Manning et al., 2008). Term-based retrieval methods, a.k.a. sparse retrieval models, such as TF-IDF or BM25 (Robertson and Zaragoza, 2009) use sparse vector representations to perform lexical matching and compute relevance scores to rank the documents based on a query. On the other hand, dense retrieval methods encode documents into a fixed-size representations and retrieve documents via maximum inner product search (Sutskever et al., 2014;Guo et al., 2016). Particularly of interests,  propose a Dense Passage Retriever (DPR) model for open-domain question answering (QA). It consists of two encoders (Q(.) and P(.)) that encode queries and passages, respectively. The similarity of a query q and a passage p is defined by the inner product of their encoded vectors sim(p, q) = Q(q) T \u22c5 P (p). Given a query q, a positive (relevant) passage p + , and a set of n irrelevant passages p \u2212 i , DPR optimizes the classification loss:\n\n.  propose to fine-tune DPR using in-batch negatives (Gillick et al., 2019;Yih et al., 2011) with curated \"hard\" negatives us- ing BM25 (candidates with high BM25 scores but contain no sub-string that match the target). We refer to  for details.",
            "reference_string": "[237304122 | Parvez et al. | 2021 | Citations: 189]"
        },
        {
            "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 69,
            "citation_count": 66,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.09924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "120174856",
                    "name": "Aleksandra Piktus"
                },
                {
                    "authorId": "40052301",
                    "name": "F. Petroni"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2067091563",
                    "name": "Vladimir Karpukhin"
                },
                {
                    "authorId": "113568063",
                    "name": "Dmytro Okhonko"
                },
                {
                    "authorId": "2966239",
                    "name": "Samuel Broscheit"
                },
                {
                    "authorId": "1410231361",
                    "name": "Gautier Izacard"
                },
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "1628391446",
                    "name": "Barlas Ouguz"
                },
                {
                    "authorId": "3024698",
                    "name": "Edouard Grave"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                }
            ],
            "abstract": "In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.",
            "corpus_id": 245334864,
            "sentences": [
                {
                    "corpus_id": "245334864",
                    "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
                    "text": "We consider two retrieval architectures. BM25 (Robertson, 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval.",
                    "score": 0.6617321320466075,
                    "section_title": "Retrieval models",
                    "char_start_offset": 7468,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 40
                        },
                        {
                            "start": 41,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 778
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 46,
                            "end": 63,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 266,
                            "end": 290,
                            "matchedPaperCorpusId": "215737187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93408203125
                }
            ],
            "relevance_judgement": 0.93408203125,
            "relevance_judgment_input_expanded": "# Title: The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus\n# Venue: arXiv.org\n# Authors: Aleksandra Piktus, F. Petroni, Yizhong Wang, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Ouguz, Edouard Grave, Wen-tau Yih, Sebastian Riedel\n## Abstract\nIn order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.\n## Retrieval models\nWe consider two retrieval architectures. BM25 (Robertson, 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval.",
            "reference_string": "[245334864 | Piktus et al. | 2021 | Citations: 66]"
        },
        {
            "title": "Database-Augmented Query Representation for Information Retrieval",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8599185",
                    "name": "Soyeong Jeong"
                },
                {
                    "authorId": "90765684",
                    "name": "Jinheon Baek"
                },
                {
                    "authorId": "2158892171",
                    "name": "Sukmin Cho"
                },
                {
                    "authorId": "2260611009",
                    "name": "Sung Ju Hwang"
                },
                {
                    "authorId": "2109285560",
                    "name": "Jong C. Park"
                }
            ],
            "abstract": "Information retrieval models that aim to search for the documents relevant to the given query have shown many successes, which have been applied to diverse tasks. However, the query provided by the user is oftentimes very short, which challenges the retrievers to correctly fetch relevant documents. To tackle this, existing studies have proposed expanding the query with a couple of additional (user-related) features related to the query. Yet, they may be suboptimal to effectively augment the query, though there is plenty of information available to augment it in a relational database. Motivated by this, we present a novel retrieval framework called Database-Augmented Query representation (DAQu), which augments the original query with various (query-related) metadata across multiple tables. In addition, as the number of features in the metadata can be very large and there is no order among them, we encode them with our graph-based set encoding strategy, which considers hierarchies of features in the database without order. We validate DAQu in diverse retrieval scenarios that can incorporate metadata from the relational database, demonstrating that ours significantly enhances overall retrieval performance, compared to existing query augmentation methods.",
            "corpus_id": 270702658,
            "sentences": [
                {
                    "corpus_id": "270702658",
                    "title": "Database-Augmented Query Representation for Information Retrieval",
                    "text": "Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021).Typically, it can be performed with two types of models: sparse and dense retrievers.Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020;Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it.For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents.Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data.Yet, using only the information within a query for retrieval can be suboptimal, due to the scarcity of information on it.\n\nQuery Augmentation for Retrieval Some studies have proposed augmenting (or expanding) the original query with additional information to enhance the retrieval performance (Carpineto and Romano, 2012;Azad and Deepak, 2019).To be specific, traditional query augmentation methods have focused on utilizing a lexical knowledge base such as the WordNet (Miller, 1992) to expand the original queries (Bhogal et al., 2007;Zhang et al., 2009).",
                    "score": 0.620783746564599,
                    "section_title": "Related Work",
                    "char_start_offset": 5771,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 169,
                            "end": 254
                        },
                        {
                            "start": 254,
                            "end": 597
                        },
                        {
                            "start": 599,
                            "end": 901
                        },
                        {
                            "start": 901,
                            "end": 1183
                        },
                        {
                            "start": 1183,
                            "end": 1439
                        },
                        {
                            "start": 1439,
                            "end": 1560
                        },
                        {
                            "start": 1562,
                            "end": 1783
                        },
                        {
                            "start": 1783,
                            "end": 1996
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 309,
                            "end": 333,
                            "matchedPaperCorpusId": "247839624"
                        },
                        {
                            "start": 551,
                            "end": 575,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 575,
                            "end": 596,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 918,
                            "end": 942,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1200,
                            "end": 1222,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 1732,
                            "end": 1760,
                            "matchedPaperCorpusId": "10393627"
                        },
                        {
                            "start": 1760,
                            "end": 1782,
                            "matchedPaperCorpusId": "8718737"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                }
            ],
            "relevance_judgement": 0.9326171875,
            "relevance_judgment_input_expanded": "# Title: Database-Augmented Query Representation for Information Retrieval\n# Venue: arXiv.org\n# Authors: Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park\n## Abstract\nInformation retrieval models that aim to search for the documents relevant to the given query have shown many successes, which have been applied to diverse tasks. However, the query provided by the user is oftentimes very short, which challenges the retrievers to correctly fetch relevant documents. To tackle this, existing studies have proposed expanding the query with a couple of additional (user-related) features related to the query. Yet, they may be suboptimal to effectively augment the query, though there is plenty of information available to augment it in a relational database. Motivated by this, we present a novel retrieval framework called Database-Augmented Query representation (DAQu), which augments the original query with various (query-related) metadata across multiple tables. In addition, as the number of features in the metadata can be very large and there is no order among them, we encode them with our graph-based set encoding strategy, which considers hierarchies of features in the database without order. We validate DAQu in diverse retrieval scenarios that can incorporate metadata from the relational database, demonstrating that ours significantly enhances overall retrieval performance, compared to existing query augmentation methods.\n## Related Work\nRetrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021).Typically, it can be performed with two types of models: sparse and dense retrievers.Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020;Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it.For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents.Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data.Yet, using only the information within a query for retrieval can be suboptimal, due to the scarcity of information on it.\n\nQuery Augmentation for Retrieval Some studies have proposed augmenting (or expanding) the original query with additional information to enhance the retrieval performance (Carpineto and Romano, 2012;Azad and Deepak, 2019).To be specific, traditional query augmentation methods have focused on utilizing a lexical knowledge base such as the WordNet (Miller, 1992) to expand the original queries (Bhogal et al., 2007;Zhang et al., 2009).",
            "reference_string": "[270702658 | Jeong et al. | 2024 | Citations: 2]"
        },
        {
            "title": "On Complementarity Objectives for Hybrid Retrieval",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.746.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-long.746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135607329",
                    "name": "Dohyeon Lee"
                },
                {
                    "authorId": "2153642272",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "79733119",
                    "name": "Kyungjae Lee"
                },
                {
                    "authorId": "5841595",
                    "name": "Seungtaek Choi"
                },
                {
                    "authorId": "2108106092",
                    "name": "Sunghyun Park"
                }
            ],
            "abstract": "Dense retrieval has shown promising results in various information retrieval tasks, and hybrid retrieval, combined with the strength of sparse retrieval, has also been actively studied. A key challenge in hybrid retrieval is to make sparse and dense complementary to each other. Existing models have focused on dense models to capture \u201cresidual\u201d features neglected in the sparse models. Our key distinction is to show how this notion of residual complementarity is limited, and propose a new objective, denoted as RoC (Ratio of Complementarity), which captures a fuller notion of complementarity. We propose a two-level orthogonality designed to improve RoC, then show that the improved RoC of our model, in turn, improves the performance of hybrid retrieval. Our method outperforms all state-of-the-art methods on three representative IR benchmarks: MSMARCO-Passage, Natural Questions, and TREC Robust04, with statistical significance. Our finding is also consistent in various adversarial settings.",
            "corpus_id": 259370750,
            "sentences": [
                {
                    "corpus_id": "259370750",
                    "title": "On Complementarity Objectives for Hybrid Retrieval",
                    "text": "Representing and matching queries and documents (or answers) is crucial for designing models for Information Retrieval (IR) and open-domain Question Answering (QA). Existing approaches have been categorized into sparse and dense retrieval. \n\nClassic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations. \n\nThese complementary advantages of each model have naturally motivated hybrid models (Gao et al., 2020;Yadav et al., 2020;Ma et al., 2021), which we denote as BM25+DPR, extracting scores from both models and selecting documents with the highest linearly combined scores. \n\nTo illustrate how we advance BM25+DPR baseline, Figure 1(a) shows Recall@10 of BM25+DPR on Natural Questions, where a yellow circle, represents questions answerable by BM25, or S, and a blue circle, represents those answerable by DPR, or D. Desirably, two retrievers together should cover all questions in the universe U, but failure is 46.5%, which corresponds to U \u2212 D \u222a S.",
                    "score": 0.5549684016111387,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 239
                        },
                        {
                            "start": 242,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 626
                        },
                        {
                            "start": 627,
                            "end": 807
                        },
                        {
                            "start": 810,
                            "end": 1063
                        },
                        {
                            "start": 1064,
                            "end": 1215
                        },
                        {
                            "start": 1218,
                            "end": 1487
                        },
                        {
                            "start": 1490,
                            "end": 1865
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 886,
                            "end": 910,
                            "matchedPaperCorpusId": "215737187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9306640625
                }
            ],
            "relevance_judgement": 0.9306640625,
            "relevance_judgment_input_expanded": "# Title: On Complementarity Objectives for Hybrid Retrieval\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Dohyeon Lee, Seung-won Hwang, Kyungjae Lee, Seungtaek Choi, Sunghyun Park\n## Abstract\nDense retrieval has shown promising results in various information retrieval tasks, and hybrid retrieval, combined with the strength of sparse retrieval, has also been actively studied. A key challenge in hybrid retrieval is to make sparse and dense complementary to each other. Existing models have focused on dense models to capture \u201cresidual\u201d features neglected in the sparse models. Our key distinction is to show how this notion of residual complementarity is limited, and propose a new objective, denoted as RoC (Ratio of Complementarity), which captures a fuller notion of complementarity. We propose a two-level orthogonality designed to improve RoC, then show that the improved RoC of our model, in turn, improves the performance of hybrid retrieval. Our method outperforms all state-of-the-art methods on three representative IR benchmarks: MSMARCO-Passage, Natural Questions, and TREC Robust04, with statistical significance. Our finding is also consistent in various adversarial settings.\n## Introduction\nRepresenting and matching queries and documents (or answers) is crucial for designing models for Information Retrieval (IR) and open-domain Question Answering (QA). Existing approaches have been categorized into sparse and dense retrieval. \n\nClassic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations. \n\nThese complementary advantages of each model have naturally motivated hybrid models (Gao et al., 2020;Yadav et al., 2020;Ma et al., 2021), which we denote as BM25+DPR, extracting scores from both models and selecting documents with the highest linearly combined scores. \n\nTo illustrate how we advance BM25+DPR baseline, Figure 1(a) shows Recall@10 of BM25+DPR on Natural Questions, where a yellow circle, represents questions answerable by BM25, or S, and a blue circle, represents those answerable by DPR, or D. Desirably, two retrievers together should cover all questions in the universe U, but failure is 46.5%, which corresponds to U \u2212 D \u222a S.",
            "reference_string": "[259370750 | Lee et al. | 2023 | Citations: 5]"
        },
        {
            "title": "A Large Language Model-based Framework for Semi-Structured Tender Document Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 9,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325913891",
                    "name": "Yilong Zhao"
                },
                {
                    "authorId": "2325898970",
                    "name": "Daifeng Li"
                }
            ],
            "abstract": "The drafting of documents in the procurement field has progressively become more complex and diverse, driven by the need to meet legal requirements, adapt to technological advancements, and address stakeholder demands. While large language models (LLMs) show potential in document generation, most LLMs lack specialized knowledge in procurement. To address this gap, we use retrieval-augmented techniques to achieve professional document generation, ensuring accuracy and relevance in procurement documentation.",
            "corpus_id": 273346484,
            "sentences": [
                {
                    "corpus_id": "273346484",
                    "title": "A Large Language Model-based Framework for Semi-Structured Tender Document Retrieval-Augmented Generation",
                    "text": "Retrieval metrics in retrieval-augmented generation include sparse-vector retrieval (using methods like TF-IDF and BM25), dense-vector retrieval (leveraging pretrained language models for encoding text), and task-specific retrieval (learning a customized retrieval metric optimized for specific objectives). These metrics enhance the relevance and quality of retrieved examples to improve the final generation output. Retrieval metrics play a vital role in selecting relevant examples. These metrics can be classified into sparse-vector retrieval, which involves methods like TF-IDF and BM25 [8], enabling efficient keyword matching using inverted indexes. On the other hand, dense-vector retrieval leverages pre-trained language models, such as those based on BERT, to encode text into low-dimensional dense vectors [9]. Retrieval-augmented text generation has emerged as a promising approach in computational linguistics. Its effectiveness and success rely on three key components: retrieval source, retrieval metric, and integration methods.",
                    "score": 0.6622784535308527,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 4434,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1044
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 592,
                            "end": 595,
                            "matchedPaperCorpusId": "207178704"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: A Large Language Model-based Framework for Semi-Structured Tender Document Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Yilong Zhao, Daifeng Li\n## Abstract\nThe drafting of documents in the procurement field has progressively become more complex and diverse, driven by the need to meet legal requirements, adapt to technological advancements, and address stakeholder demands. While large language models (LLMs) show potential in document generation, most LLMs lack specialized knowledge in procurement. To address this gap, we use retrieval-augmented techniques to achieve professional document generation, ensuring accuracy and relevance in procurement documentation.\n## II. RELATED WORK\nRetrieval metrics in retrieval-augmented generation include sparse-vector retrieval (using methods like TF-IDF and BM25), dense-vector retrieval (leveraging pretrained language models for encoding text), and task-specific retrieval (learning a customized retrieval metric optimized for specific objectives). These metrics enhance the relevance and quality of retrieved examples to improve the final generation output. Retrieval metrics play a vital role in selecting relevant examples. These metrics can be classified into sparse-vector retrieval, which involves methods like TF-IDF and BM25 [8], enabling efficient keyword matching using inverted indexes. On the other hand, dense-vector retrieval leverages pre-trained language models, such as those based on BERT, to encode text into low-dimensional dense vectors [9]. Retrieval-augmented text generation has emerged as a promising approach in computational linguistics. Its effectiveness and success rely on three key components: retrieval source, retrieval metric, and integration methods.",
            "reference_string": "[273346484 | Zhao et al. | 2024 | Citations: 0]"
        },
        {
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 40,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3701228",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187857206",
                    "name": "Yuanjie Lyu"
                },
                {
                    "authorId": "2268429641",
                    "name": "Zhiyu Li"
                },
                {
                    "authorId": "2268393907",
                    "name": "Simin Niu"
                },
                {
                    "authorId": "2268399953",
                    "name": "Feiyu Xiong"
                },
                {
                    "authorId": "2268400606",
                    "name": "Bo Tang"
                },
                {
                    "authorId": "2117833477",
                    "name": "Wenjin Wang"
                },
                {
                    "authorId": "2282083454",
                    "name": "Hao Wu"
                },
                {
                    "authorId": "2304320758",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.",
            "corpus_id": 267320876,
            "sentences": [
                {
                    "corpus_id": "267320876",
                    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                    "text": "A retriever is a key component of the RAG pipeline, which finds relevant documents from a large database based on the user input, and provides contextual information for the large model. There are two main types of retrievers: Keyword-based search-sparse retrieval algorithms, which use keywords and their frequencies to compute the relevance between documents and queries. Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF method, which accounts for factors such as the length and position of words in the document. Dense retrieval algorithms, which use deep learning models to encode documents and queries into low-dimensional vectors, and then measure the cosine similarity between them. This method can capture the semantic and contextual information of words, and improve the retrieval performance. \n\nIn order to combine the advantages of both types of retrievers, we can fuse their retrieval results and randomly sample k from them as contexts for LLMs(Hybrid). Alternatively, we can also use a re-ranking model to re-rank the fused retrieval results, and then select the top-k ones as the context of LLMs(Hybrid+Rerank). In our experiments, we employ the bge-rank as the rerank model. \n\nText Continuation: As Table 5 displays, the performance of the dense retriever is roughly equivalent to that of BM25, except for the key information recall rate. Compared to the keywordbased algorithm, the modern vector search can capture the semantic and contextual information of words, so that more content that does not match keywords but is obviously semantically related can be retrieved. However, the RAG system using BM25 also performs well. In terms of the precision of key information, BM25 even exceeds the dense retriever. This suggests that in the continuation task, which is a creative task, BM25 can retrieve content that is highly relevant to the user's intention, but may overlook some details. \n\nOpen-Domain Multi-Document Summarization: On the overall semantic similarity metric, the performance of the dense retriever is roughly equivalent to that of BM25. On the QuestEval metric, BM25 surpasses dense retriever in terms of key information precision, but slightly trails behind in key information recall.",
                    "score": 0.5537005768355983,
                    "section_title": "Analyzing the Impact of Retriever on RAG Performance in Different Tasks",
                    "char_start_offset": 48972,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 840
                        },
                        {
                            "start": 843,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1392
                        },
                        {
                            "start": 1393,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1942
                        },
                        {
                            "start": 1945,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2256
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huan Liu, Tong Xu, Enhong Chen\n## Abstract\nRetrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.\n## Analyzing the Impact of Retriever on RAG Performance in Different Tasks\nA retriever is a key component of the RAG pipeline, which finds relevant documents from a large database based on the user input, and provides contextual information for the large model. There are two main types of retrievers: Keyword-based search-sparse retrieval algorithms, which use keywords and their frequencies to compute the relevance between documents and queries. Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF method, which accounts for factors such as the length and position of words in the document. Dense retrieval algorithms, which use deep learning models to encode documents and queries into low-dimensional vectors, and then measure the cosine similarity between them. This method can capture the semantic and contextual information of words, and improve the retrieval performance. \n\nIn order to combine the advantages of both types of retrievers, we can fuse their retrieval results and randomly sample k from them as contexts for LLMs(Hybrid). Alternatively, we can also use a re-ranking model to re-rank the fused retrieval results, and then select the top-k ones as the context of LLMs(Hybrid+Rerank). In our experiments, we employ the bge-rank as the rerank model. \n\nText Continuation: As Table 5 displays, the performance of the dense retriever is roughly equivalent to that of BM25, except for the key information recall rate. Compared to the keywordbased algorithm, the modern vector search can capture the semantic and contextual information of words, so that more content that does not match keywords but is obviously semantically related can be retrieved. However, the RAG system using BM25 also performs well. In terms of the precision of key information, BM25 even exceeds the dense retriever. This suggests that in the continuation task, which is a creative task, BM25 can retrieve content that is highly relevant to the user's intention, but may overlook some details. \n\nOpen-Domain Multi-Document Summarization: On the overall semantic similarity metric, the performance of the dense retriever is roughly equivalent to that of BM25. On the QuestEval metric, BM25 surpasses dense retriever in terms of key information precision, but slightly trails behind in key information recall.",
            "reference_string": "[267320876 | Lyu et al. | 2024 | Citations: 40]"
        },
        {
            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350956983",
                    "name": "Sejong Kim"
                },
                {
                    "authorId": "2350857179",
                    "name": "Hyunseo Song"
                },
                {
                    "authorId": "2351317331",
                    "name": "Hyunwoo Seo"
                },
                {
                    "authorId": "2350867590",
                    "name": "Hyunjun Kim"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR.",
            "corpus_id": 277113527,
            "sentences": [
                {
                    "corpus_id": "277113527",
                    "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
                    "text": "Recent advancements in Retrieval-Augmented Generation (RAG) systems have increasingly focused on overcoming the limitations of using a single retrieval modality by fusing the strengths of both dense and sparse retrieval approaches. Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers & Gurevych, 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024) To address these complementary weaknesses, hybrid retrieval methods have been proposed. These methods combine the scores obtained from dense and sparse retrieval, typically through linear weighted fusion or techniques such as Reciprocal Rank Fusion (RRF). For example, the Sawarkar et al. (2024) demonstrates that integrating semantic search techniques with sparse encoder indexes can significantly enhance retrieval performance on benchmarks such as NQ and TREC-COVID, leading to improved overall accuracy in RAG systems (Zhang et al., 2024).",
                    "score": 0.6318915718924702,
                    "section_title": "HYBRID RETRIEVAL",
                    "char_start_offset": 5920,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1405
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 325,
                            "end": 346,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 371,
                            "end": 397,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 664,
                            "end": 683,
                            "matchedPaperCorpusId": "237366133"
                        },
                        {
                            "start": 838,
                            "end": 861,
                            "matchedPaperCorpusId": "266999115"
                        },
                        {
                            "start": 1135,
                            "end": 1157,
                            "matchedPaperCorpusId": "269043117"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9287109375
                }
            ],
            "relevance_judgement": 0.9287109375,
            "relevance_judgment_input_expanded": "# Title: Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems\n# Venue: arXiv.org\n# Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR.\n## HYBRID RETRIEVAL\nRecent advancements in Retrieval-Augmented Generation (RAG) systems have increasingly focused on overcoming the limitations of using a single retrieval modality by fusing the strengths of both dense and sparse retrieval approaches. Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers & Gurevych, 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024) To address these complementary weaknesses, hybrid retrieval methods have been proposed. These methods combine the scores obtained from dense and sparse retrieval, typically through linear weighted fusion or techniques such as Reciprocal Rank Fusion (RRF). For example, the Sawarkar et al. (2024) demonstrates that integrating semantic search techniques with sparse encoder indexes can significantly enhance retrieval performance on benchmarks such as NQ and TREC-COVID, leading to improved overall accuracy in RAG systems (Zhang et al., 2024).",
            "reference_string": "[277113527 | Kim et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data",
            "venue": "ECNLP",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2073867373",
                    "name": "Biplob Biswas"
                },
                {
                    "authorId": "2253261",
                    "name": "R. Ramnath"
                }
            ],
            "abstract": "Expansion-enhanced sparse lexical representation improves information retrieval (IR) by minimizing vocabulary mismatch problems during lexical matching. In this paper, we explore the potential of jointly learning dense semantic representation and combining it with the lexical one for ranking candidate information. We present a hybrid information retrieval mechanism that maximizes lexical and semantic matching while minimizing their shortcomings. Our architecture consists of dual hybrid encoders that independently encode queries and information elements. Each encoder jointly learns a dense semantic representation and a sparse lexical representation augmented by a learnable term expansion of the corresponding text through contrastive learning. We demonstrate the efficacy of our model in single-stage ranking of a benchmark product question-answering dataset containing the typical heterogeneous information available on online product pages. Our evaluation demonstrates that our hybrid approach outperforms independently trained retrievers by 10.95% (sparse) and 2.7% (dense) in MRR@5 score. Moreover, our model offers better interpretability and performs comparably to state-of-the-art cross-encoders while reducing response time by 30% (latency) and cutting computational load by approximately 38% (FLOPs).",
            "corpus_id": 269950696,
            "sentences": [
                {
                    "corpus_id": "269950696",
                    "title": "Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data",
                    "text": "In the field of natural language processing, ranked information retrieval (IR), refers to retrieving information ordered by relevance from a large collection, in response to a query.Ranked IR remains important even with the emergence of advanced large language models (LLMs) as a means of greatly enriching their outputs.\n\nExisting retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson and Walker, 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large.\n\nIn an effort to balance the quality-cost trade-off, a two-stage pipeline is proposed where a quicker retriever first retrieves a smaller set of candidates and then a dense retriever re-ranks them in a second stage.Unfortunately, this approach suffers from two major problems.First, any semantically relevant information pruned due to lack of exact word matches in the first stage is not considered for further ranking.Second, the neural ranker in the last stage lacks interpretability because, for scoring, it uses the inner product of the latent representation of the text which is difficult to explain in human understandable terms.Recently proposed transformer (Vaswani et al., 2017) encoders have the potential to tackle these issues.By utilizing a pre-trained masked language model (MLM), SparTerm (Bai et al., 2020) and SPLADE (Formal et al., 2021) progressively improved the use of expansion-aware sparse lexical representation learners in mitigating vocabulary mismatch problems, while enhancing interpretability.",
                    "score": 0.5238959772029215,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 182,
                            "end": 321
                        },
                        {
                            "start": 323,
                            "end": 406
                        },
                        {
                            "start": 406,
                            "end": 646
                        },
                        {
                            "start": 646,
                            "end": 837
                        },
                        {
                            "start": 837,
                            "end": 987
                        },
                        {
                            "start": 987,
                            "end": 1158
                        },
                        {
                            "start": 1160,
                            "end": 1374
                        },
                        {
                            "start": 1374,
                            "end": 1435
                        },
                        {
                            "start": 1435,
                            "end": 1578
                        },
                        {
                            "start": 1578,
                            "end": 1794
                        },
                        {
                            "start": 1794,
                            "end": 1898
                        },
                        {
                            "start": 1898,
                            "end": 2181
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 573,
                            "end": 601,
                            "matchedPaperCorpusId": "2218552"
                        },
                        {
                            "start": 965,
                            "end": 986,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1824,
                            "end": 1846,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data\n# Venue: ECNLP\n# Authors: Biplob Biswas, R. Ramnath\n## Abstract\nExpansion-enhanced sparse lexical representation improves information retrieval (IR) by minimizing vocabulary mismatch problems during lexical matching. In this paper, we explore the potential of jointly learning dense semantic representation and combining it with the lexical one for ranking candidate information. We present a hybrid information retrieval mechanism that maximizes lexical and semantic matching while minimizing their shortcomings. Our architecture consists of dual hybrid encoders that independently encode queries and information elements. Each encoder jointly learns a dense semantic representation and a sparse lexical representation augmented by a learnable term expansion of the corresponding text through contrastive learning. We demonstrate the efficacy of our model in single-stage ranking of a benchmark product question-answering dataset containing the typical heterogeneous information available on online product pages. Our evaluation demonstrates that our hybrid approach outperforms independently trained retrievers by 10.95% (sparse) and 2.7% (dense) in MRR@5 score. Moreover, our model offers better interpretability and performs comparably to state-of-the-art cross-encoders while reducing response time by 30% (latency) and cutting computational load by approximately 38% (FLOPs).\n## Introduction\nIn the field of natural language processing, ranked information retrieval (IR), refers to retrieving information ordered by relevance from a large collection, in response to a query.Ranked IR remains important even with the emergence of advanced large language models (LLMs) as a means of greatly enriching their outputs.\n\nExisting retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson and Walker, 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large.\n\nIn an effort to balance the quality-cost trade-off, a two-stage pipeline is proposed where a quicker retriever first retrieves a smaller set of candidates and then a dense retriever re-ranks them in a second stage.Unfortunately, this approach suffers from two major problems.First, any semantically relevant information pruned due to lack of exact word matches in the first stage is not considered for further ranking.Second, the neural ranker in the last stage lacks interpretability because, for scoring, it uses the inner product of the latent representation of the text which is difficult to explain in human understandable terms.Recently proposed transformer (Vaswani et al., 2017) encoders have the potential to tackle these issues.By utilizing a pre-trained masked language model (MLM), SparTerm (Bai et al., 2020) and SPLADE (Formal et al., 2021) progressively improved the use of expansion-aware sparse lexical representation learners in mitigating vocabulary mismatch problems, while enhancing interpretability.",
            "reference_string": "[269950696 | Biswas et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "venue": "Knowledge-Based Systems",
            "year": 2025,
            "reference_count": 137,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342276561",
                    "name": "Lilian Some"
                },
                {
                    "authorId": "2341600949",
                    "name": "Wenli Yang"
                },
                {
                    "authorId": "2342277330",
                    "name": "Michael Bain"
                },
                {
                    "authorId": "2341910700",
                    "name": "Byeong Kang"
                }
            ],
            "abstract": null,
            "corpus_id": 275906690,
            "sentences": [
                {
                    "corpus_id": "275906690",
                    "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                    "text": "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109]. \n\nQuery expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122]. Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114]. Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123]. Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105]. \n\nRetrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications. RAG capabilities are experienced in varied settings. In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114]. \n\nIn question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context. This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120]. RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124]. Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125]. Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].",
                    "score": 0.547182168239349,
                    "section_title": "Retrieval augmented generation",
                    "char_start_offset": 50470,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 296
                        },
                        {
                            "start": 299,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 1035
                        },
                        {
                            "start": 1038,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1512
                        },
                        {
                            "start": 1515,
                            "end": 1684
                        },
                        {
                            "start": 1685,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1977
                        },
                        {
                            "start": 1978,
                            "end": 2155
                        },
                        {
                            "start": 2156,
                            "end": 2325
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 600,
                            "end": 604,
                            "matchedPaperCorpusId": "265498312"
                        },
                        {
                            "start": 1808,
                            "end": 1812,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods\n# Venue: Knowledge-Based Systems\n# Authors: Lilian Some, Wenli Yang, Michael Bain, Byeong Kang\n## Abstract\nNone\n## Retrieval augmented generation\nDense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109]. \n\nQuery expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122]. Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114]. Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123]. Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105]. \n\nRetrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications. RAG capabilities are experienced in varied settings. In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114]. \n\nIn question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context. This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120]. RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124]. Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125]. Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].",
            "reference_string": "[275906690 | Some et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 23,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.03367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8427191",
                    "name": "Dingkun Long"
                },
                {
                    "authorId": "2117205083",
                    "name": "Qiong Gao"
                },
                {
                    "authorId": "3271753",
                    "name": "Kuan-sheng Zou"
                },
                {
                    "authorId": "2149131512",
                    "name": "Guangwei Xu"
                },
                {
                    "authorId": "35930962",
                    "name": "Pengjun Xie"
                },
                {
                    "authorId": "2114009505",
                    "name": "Rui Guo"
                },
                {
                    "authorId": "2146054545",
                    "name": "Jianfeng Xu"
                },
                {
                    "authorId": "2084534507",
                    "name": "Guanjun Jiang"
                },
                {
                    "authorId": "30967877",
                    "name": "Luxi Xing"
                },
                {
                    "authorId": "2327836457",
                    "name": "Ping Yang"
                }
            ],
            "abstract": "Passage retrieval is a fundamental task in information retrieval (IR) research, which has drawn much attention recently. In the English field, the availability of large-scale annotated dataset (e.g, MS MARCO) and the emergence of deep pre-trained language models (e.g, BERT) has resulted in a substantial improvement of existing passage retrieval systems. However, in the Chinese field, especially for specific domains, passage retrieval systems are still immature due to quality-annotated dataset being limited by scale. Therefore, in this paper, we present a novel multi-domain Chinese dataset for passage retrieval (Multi-CPR). The dataset is collected from three different domains, including E-commerce, Entertainment video and Medical. Each dataset contains millions of passages and a certain amount of human annotated query-passage related pairs. We implement various representative passage retrieval methods as baselines. We find that the performance of retrieval models trained on dataset from general domain will inevitably decrease on specific domain. Nevertheless, a passage retrieval system built on in-domain annotated dataset can achieve significant improvement, which indeed demonstrates the necessity of domain labeled data for further optimization. We hope the release of the Multi-CPR dataset could benchmark Chinese passage retrieval task in specific domain and also make advances for future studies.",
            "corpus_id": 247292113,
            "sentences": [
                {
                    "corpus_id": "247292113",
                    "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
                    "text": "Passage Retrieval Passage retrieval task aims to recall all potentially relevant passages from a large corpus given an informationseeking query. In practical, passage retrieval is often an important step in other information retrieval tasks [4]. Traditional passage retrieval systems usually rely on term-based retrieval models like BM25 [46]. Recently, with the rapid development in text representation learning research [3] and deep pre-trained language models [21,27,33,51], dense retrieval combined with pre-trained language models, has become a popular paradigm to improve retrieval performance [16,26,42]. In general, dense models significantly outperform traditional term-based retrieval models in terms of effectiveness and benefit downstream tasks. In a basic concept, the core problem of passage retrieval is how to form the text representation and then compute text similarity. Thus, based on the text representation type and corpus index mode, passage retrieval models can be roughly categorized into two main classes. Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]. \n\nFor the above two types of models, the current optimization directions are not the same. Specifically, Sparse retrieval models \n\n(Adults should not put their hands on the chest of a sleeping baby as this is not good for the baby's breathing.) \n\nfocus on improving retrieval performance by either enhancing the bag-of-words (BoW) representations in classical term-based methods or mapping input texts into latent space (e,g. doc2query [37], query expansion [7] and document expansion [39]). The sparse representation has attracted great attention as it can be easily integrated into the inverted index for efficient retrieval. Recently, With the development of deep neural networks, pre-trained language models have been widely employed to improve the capacity of sparse retrieval models, including term re-weighting [9,10], sparse representation learning [24,50], etc. The mainstream of existing studies on improving the performance of dense retrieval models can be roughly divided into three groups. 1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation.",
                    "score": 0.57551178635563,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4763,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1377
                        },
                        {
                            "start": 1380,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1506
                        },
                        {
                            "start": 1509,
                            "end": 1622
                        },
                        {
                            "start": 1625,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 2005
                        },
                        {
                            "start": 2006,
                            "end": 2248
                        },
                        {
                            "start": 2249,
                            "end": 2380
                        },
                        {
                            "start": 2381,
                            "end": 2538
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 244,
                            "matchedPaperCorpusId": "1058977"
                        },
                        {
                            "start": 463,
                            "end": 467,
                            "matchedPaperCorpusId": "219531210"
                        },
                        {
                            "start": 467,
                            "end": 470,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 473,
                            "end": 476,
                            "matchedPaperCorpusId": "195069387"
                        },
                        {
                            "start": 604,
                            "end": 607,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 607,
                            "end": 610,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1836,
                            "end": 1839,
                            "matchedPaperCorpusId": "1815908"
                        },
                        {
                            "start": 2199,
                            "end": 2202,
                            "matchedPaperCorpusId": "162168864"
                        },
                        {
                            "start": 2239,
                            "end": 2242,
                            "matchedPaperCorpusId": "235293983"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Dingkun Long, Qiong Gao, Kuan-sheng Zou, Guangwei Xu, Pengjun Xie, Rui Guo, Jianfeng Xu, Guanjun Jiang, Luxi Xing, Ping Yang\n## Abstract\nPassage retrieval is a fundamental task in information retrieval (IR) research, which has drawn much attention recently. In the English field, the availability of large-scale annotated dataset (e.g, MS MARCO) and the emergence of deep pre-trained language models (e.g, BERT) has resulted in a substantial improvement of existing passage retrieval systems. However, in the Chinese field, especially for specific domains, passage retrieval systems are still immature due to quality-annotated dataset being limited by scale. Therefore, in this paper, we present a novel multi-domain Chinese dataset for passage retrieval (Multi-CPR). The dataset is collected from three different domains, including E-commerce, Entertainment video and Medical. Each dataset contains millions of passages and a certain amount of human annotated query-passage related pairs. We implement various representative passage retrieval methods as baselines. We find that the performance of retrieval models trained on dataset from general domain will inevitably decrease on specific domain. Nevertheless, a passage retrieval system built on in-domain annotated dataset can achieve significant improvement, which indeed demonstrates the necessity of domain labeled data for further optimization. We hope the release of the Multi-CPR dataset could benchmark Chinese passage retrieval task in specific domain and also make advances for future studies.\n## RELATED WORK\nPassage Retrieval Passage retrieval task aims to recall all potentially relevant passages from a large corpus given an informationseeking query. In practical, passage retrieval is often an important step in other information retrieval tasks [4]. Traditional passage retrieval systems usually rely on term-based retrieval models like BM25 [46]. Recently, with the rapid development in text representation learning research [3] and deep pre-trained language models [21,27,33,51], dense retrieval combined with pre-trained language models, has become a popular paradigm to improve retrieval performance [16,26,42]. In general, dense models significantly outperform traditional term-based retrieval models in terms of effectiveness and benefit downstream tasks. In a basic concept, the core problem of passage retrieval is how to form the text representation and then compute text similarity. Thus, based on the text representation type and corpus index mode, passage retrieval models can be roughly categorized into two main classes. Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]. \n\nFor the above two types of models, the current optimization directions are not the same. Specifically, Sparse retrieval models \n\n(Adults should not put their hands on the chest of a sleeping baby as this is not good for the baby's breathing.) \n\nfocus on improving retrieval performance by either enhancing the bag-of-words (BoW) representations in classical term-based methods or mapping input texts into latent space (e,g. doc2query [37], query expansion [7] and document expansion [39]). The sparse representation has attracted great attention as it can be easily integrated into the inverted index for efficient retrieval. Recently, With the development of deep neural networks, pre-trained language models have been widely employed to improve the capacity of sparse retrieval models, including term re-weighting [9,10], sparse representation learning [24,50], etc. The mainstream of existing studies on improving the performance of dense retrieval models can be roughly divided into three groups. 1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation.",
            "reference_string": "[247292113 | Long et al. | 2022 | Citations: 23]"
        },
        {
            "title": "Decoy Effect in Search Interaction: Understanding User Behavior and Measuring System Vulnerability",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2024,
            "reference_count": 82,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.18462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257286538",
                    "name": "Nuo Chen"
                },
                {
                    "authorId": "2265515693",
                    "name": "Jiqun Liu"
                },
                {
                    "authorId": "2293660445",
                    "name": "Hanpei Fang"
                },
                {
                    "authorId": "2293898339",
                    "name": "Yuankai Luo"
                },
                {
                    "authorId": "2257233277",
                    "name": "Tetsuya Sakai"
                },
                {
                    "authorId": "2265517306",
                    "name": "Xiao-Ming Wu"
                }
            ],
            "abstract": "This study addresses (1) the influence of the decoy effect, a cognitive bias where the presence of an inferior item alters preferences between two options, on users\u2019 search interactions and (2) the measurement of information retrieval systems\u2019 vulnerability to the decoy effect.1 From the perspective of user behavior, this study investigates the influence of the decoy effect in information retrieval (IR) by examining how decoy results affect users\u2019 interaction on search engine result pages (SERPs), particularly in terms of click-through likelihood, browsing dwell time, and perceived document usefulness. We conducted an experiment based upon regression analysis on user interaction logs from three user study datasets which in total encompass 24 topics, 841 unique search sessions, and 2,685 queries. The findings indicate that decoys significantly increase the likelihood of document clicks and perceived usefulness. To investigate whether the influence of the decoy varies across different levels of task difficulty and user knowledge, we ran an additional experiment on one of the three datasets, which encompasses 6 topics, 166 search sessions and 652 queries. The results indicate that when the task is less challenging, users are more likely to click on a document with a decoy. Additionally, they spend more time on the target document and assign it a higher usefulness score. Furthermore, users with lower knowledge levels about the topic tend to give higher usefulness ratings to the target document. Regarding IR system evaluation, this study provides empirical insights into measuring the vulnerability of text retrieval models to potential decoy effect. An evaluation metric, namely DEcoy Judgement and Assessment VUlnerability (DEJA-VU), is proposed to evaluate the possibility of a retrieval model ranking results in a way that could trigger decoy biases. The experiments on the Text REtrieval Conference (TREC) 19 Deep Learning (DL) passage retrieval task and the TREC 20 DL passage retrieval task demonstrate that ColBERT and SPLADE show higher relevance-oriented retrieval effectiveness while also displaying lower vulnerability to decoy effect. Overall, this work advances the understanding of decoy effect, a well-established concept in cognitive psychology and behavioral economics, in a novel application field (i.e., Information Retrieval). It contributes to modeling users\u2019 search behavior in the context of cognitive biases, as well as assessment of the vulnerability of systems and ranking algorithms to the decoy effect.",
            "corpus_id": 268724187,
            "sentences": [
                {
                    "corpus_id": "268724187",
                    "title": "Decoy Effect in Search Interaction: Understanding User Behavior and Measuring System Vulnerability",
                    "text": "Traditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46].These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears.However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context.These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.\n\ne., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text.The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space.Ranking models based on dense vectors is referred to as dense retrieval models.Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth.Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2 .\n\nOn the other hand, sparse representations based on pre-training language models (PLMs) have also garnered increasing interest because they inherently inherit desirable properties of lexical models in their design.For example, COntextualized Inverted List (COIL) [29] learns dense term-level representations to perform contextualized lexical matching; uniCOIL [50] further simplifies the approach by learning a single weight per term.",
                    "score": 0.5639939114459903,
                    "section_title": "Ranking Models in Information Retrieval",
                    "char_start_offset": 15846,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 140,
                            "end": 443
                        },
                        {
                            "start": 443,
                            "end": 587
                        },
                        {
                            "start": 587,
                            "end": 686
                        },
                        {
                            "start": 688,
                            "end": 759
                        },
                        {
                            "start": 761,
                            "end": 978
                        },
                        {
                            "start": 978,
                            "end": 1130
                        },
                        {
                            "start": 1130,
                            "end": 1209
                        },
                        {
                            "start": 1209,
                            "end": 1421
                        },
                        {
                            "start": 1421,
                            "end": 1766
                        },
                        {
                            "start": 1768,
                            "end": 1981
                        },
                        {
                            "start": 1981,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 98,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 98,
                            "end": 101,
                            "matchedPaperCorpusId": "2218552"
                        },
                        {
                            "start": 135,
                            "end": 139,
                            "matchedPaperCorpusId": "1183147"
                        },
                        {
                            "start": 1270,
                            "end": 1274,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1359,
                            "end": 1364,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1374,
                            "end": 1378,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1402,
                            "end": 1406,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 1462,
                            "end": 1466,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1508,
                            "end": 1512,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1512,
                            "end": 1515,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1515,
                            "end": 1518,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 1518,
                            "end": 1522,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1583,
                            "end": 1587,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1587,
                            "end": 1591,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1622,
                            "end": 1626,
                            "matchedPaperCorpusId": "201646309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Decoy Effect in Search Interaction: Understanding User Behavior and Measuring System Vulnerability\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Nuo Chen, Jiqun Liu, Hanpei Fang, Yuankai Luo, Tetsuya Sakai, Xiao-Ming Wu\n## Abstract\nThis study addresses (1) the influence of the decoy effect, a cognitive bias where the presence of an inferior item alters preferences between two options, on users\u2019 search interactions and (2) the measurement of information retrieval systems\u2019 vulnerability to the decoy effect.1 From the perspective of user behavior, this study investigates the influence of the decoy effect in information retrieval (IR) by examining how decoy results affect users\u2019 interaction on search engine result pages (SERPs), particularly in terms of click-through likelihood, browsing dwell time, and perceived document usefulness. We conducted an experiment based upon regression analysis on user interaction logs from three user study datasets which in total encompass 24 topics, 841 unique search sessions, and 2,685 queries. The findings indicate that decoys significantly increase the likelihood of document clicks and perceived usefulness. To investigate whether the influence of the decoy varies across different levels of task difficulty and user knowledge, we ran an additional experiment on one of the three datasets, which encompasses 6 topics, 166 search sessions and 652 queries. The results indicate that when the task is less challenging, users are more likely to click on a document with a decoy. Additionally, they spend more time on the target document and assign it a higher usefulness score. Furthermore, users with lower knowledge levels about the topic tend to give higher usefulness ratings to the target document. Regarding IR system evaluation, this study provides empirical insights into measuring the vulnerability of text retrieval models to potential decoy effect. An evaluation metric, namely DEcoy Judgement and Assessment VUlnerability (DEJA-VU), is proposed to evaluate the possibility of a retrieval model ranking results in a way that could trigger decoy biases. The experiments on the Text REtrieval Conference (TREC) 19 Deep Learning (DL) passage retrieval task and the TREC 20 DL passage retrieval task demonstrate that ColBERT and SPLADE show higher relevance-oriented retrieval effectiveness while also displaying lower vulnerability to decoy effect. Overall, this work advances the understanding of decoy effect, a well-established concept in cognitive psychology and behavioral economics, in a novel application field (i.e., Information Retrieval). It contributes to modeling users\u2019 search behavior in the context of cognitive biases, as well as assessment of the vulnerability of systems and ranking algorithms to the decoy effect.\n## Ranking Models in Information Retrieval\nTraditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46].These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears.However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context.These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.\n\ne., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text.The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space.Ranking models based on dense vectors is referred to as dense retrieval models.Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth.Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2 .\n\nOn the other hand, sparse representations based on pre-training language models (PLMs) have also garnered increasing interest because they inherently inherit desirable properties of lexical models in their design.For example, COntextualized Inverted List (COIL) [29] learns dense term-level representations to perform contextualized lexical matching; uniCOIL [50] further simplifies the approach by learning a single weight per term.",
            "reference_string": "[268724187 | Chen et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 401,
            "citation_count": 282,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268718776",
                    "name": "Penghao Zhao"
                },
                {
                    "authorId": "2288557803",
                    "name": "Hailin Zhang"
                },
                {
                    "authorId": "2289597580",
                    "name": "Qinhan Yu"
                },
                {
                    "authorId": "2288675277",
                    "name": "Zhengren Wang"
                },
                {
                    "authorId": "2288532368",
                    "name": "Yunteng Geng"
                },
                {
                    "authorId": "46182701",
                    "name": "Fangcheng Fu"
                },
                {
                    "authorId": "2249513224",
                    "name": "Ling Yang"
                },
                {
                    "authorId": "2277807793",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2277742543",
                    "name": "Bin Cui"
                }
            ],
            "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "corpus_id": 268091298,
            "sentences": [
                {
                    "corpus_id": "268091298",
                    "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                    "text": "Retrieval is to identify and obtain relevant information given an information need. Specifically, let's consider information resources that can be conceptualized as a key-value store, where each key corresponds to a value (keys and values can be identical). Given a query, the objective is to search for the top-k most similar keys using a similarity function, and obtain the paired values. Based on different similarity functions, existing retrieval methods can be categorized into sparse retrieval, dense retrieval, and others. In widely used sparse and dense retrieval, the entire process can be divided into two distinct phases: (i) each object is first encoded into a specific representation; and then (ii) an index is constructed to organize the data source for efficient search. \n\n1) Sparse Retriever: Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage Fig. 2: General architectures of several generators. term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics. \n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores. \n\n2) Dense Retriever: Unlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance.",
                    "score": 0.517915879519205,
                    "section_title": "C. Retriever",
                    "char_start_offset": 12168,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 785
                        },
                        {
                            "start": 788,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1358
                        },
                        {
                            "start": 1361,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1627
                        },
                        {
                            "start": 1630,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 1870
                        },
                        {
                            "start": 1871,
                            "end": 2009
                        },
                        {
                            "start": 2010,
                            "end": 2078
                        },
                        {
                            "start": 2079,
                            "end": 2203
                        },
                        {
                            "start": 2204,
                            "end": 2332
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1050,
                            "end": 1054,
                            "matchedPaperCorpusId": "16829071"
                        },
                        {
                            "start": 1073,
                            "end": 1077,
                            "matchedPaperCorpusId": "1183147"
                        },
                        {
                            "start": 1088,
                            "end": 1092,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 1942,
                            "end": 1946,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 2004,
                            "end": 2008,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 2142,
                            "end": 2146,
                            "matchedPaperCorpusId": "211171605"
                        },
                        {
                            "start": 2159,
                            "end": 2163,
                            "matchedPaperCorpusId": "8810481"
                        },
                        {
                            "start": 2176,
                            "end": 2180,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 2193,
                            "end": 2197,
                            "matchedPaperCorpusId": "81982618"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for AI-Generated Content: A Survey\n# Venue: arXiv.org\n# Authors: Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui\n## Abstract\nAdvancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.\n## C. Retriever\nRetrieval is to identify and obtain relevant information given an information need. Specifically, let's consider information resources that can be conceptualized as a key-value store, where each key corresponds to a value (keys and values can be identical). Given a query, the objective is to search for the top-k most similar keys using a similarity function, and obtain the paired values. Based on different similarity functions, existing retrieval methods can be categorized into sparse retrieval, dense retrieval, and others. In widely used sparse and dense retrieval, the entire process can be divided into two distinct phases: (i) each object is first encoded into a specific representation; and then (ii) an index is constructed to organize the data source for efficient search. \n\n1) Sparse Retriever: Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage Fig. 2: General architectures of several generators. term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics. \n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores. \n\n2) Dense Retriever: Unlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance.",
            "reference_string": "[268091298 | Zhao et al. | 2024 | Citations: 282]"
        },
        {
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2023,
            "reference_count": 34,
            "citation_count": 48,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3539618.3591874",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.03679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111366555",
                    "name": "Xiaohui Xie"
                },
                {
                    "authorId": "2053111975",
                    "name": "Qian Dong"
                },
                {
                    "authorId": "46270259",
                    "name": "Bingning Wang"
                },
                {
                    "authorId": "2134961990",
                    "name": "Feiyang Lv"
                },
                {
                    "authorId": "48577275",
                    "name": "Ting Yao"
                },
                {
                    "authorId": "2213965902",
                    "name": "Weinan Gan"
                },
                {
                    "authorId": "47039225",
                    "name": "Zhijing Wu"
                },
                {
                    "authorId": "2116837949",
                    "name": "Xiangsheng Li"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "46399371",
                    "name": "Y. Liu"
                },
                {
                    "authorId": "2188153417",
                    "name": "Jin Ma"
                }
            ],
            "abstract": "Passage ranking involves two stages: passage retrieval and passage re-ranking, which are important and challenging topics for both academics and industries in the area of Information Retrieval (IR). However, the commonly-used datasets for passage ranking usually focus on the English language. For non-English scenarios, such as Chinese, the existing datasets are limited in terms of data scale, fine-grained relevance annotation and false negative issues. To address this problem, we introduce T2Ranking, a large-scale Chinese benchmark for passage ranking. T2Ranking comprises more than 300K queries and over 2M unique passages from real-world search engines. Expert annotators are recruited to provide 4-level graded relevance scores (fine-grained) for query-passage pairs instead of binary relevance judgments (coarse-grained). To ease the false negative issues, more passages with higher diversities are considered when performing relevance annotations, especially in the test set, to ensure a more accurate evaluation. Apart from the textual query and passage data, other auxiliary resources are also provided, such as query types and XML files of documents which passages are generated from, to facilitate further studies. To evaluate the dataset, commonly used ranking models are implemented and tested on T2Ranking as baselines. The experimental results show that T2Ranking is challenging and there is still scope for improvement. The full data and all codes are available at https://github.com/THUIR/T2Ranking/.",
            "corpus_id": 258041275,
            "sentences": [
                {
                    "corpus_id": "258041275",
                    "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
                    "text": "Baselines. Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents. Generally, most existing dense retrieval methods adhere to the cascade training paradigm [15,20,21]. Therefore, to facilitate easier comparison in future studies on our dataset, we simplify the training process as illustrated in Figure 5 as in [15,20]. Specifically, we utilize the dual-encoder (DE) as the architecture of dense retrieval models, which is illustrated in Figure 6(a). The following methods are employed as our baselines to evaluate the retrieval performance on T 2 Ranking. \n\n\u2022 QL (query likelihood) [19] is a representative statistical language model that measures the relevance of passages by modeling the generation of a query. \u2022 BM25 [23] is a widely-used sparse retrieval baseline. \n\n\u2022 DE w/ BM25 Neg is equivalent to DPR [12], which is the first work that uses the pre-trained language model as the backbone for the passage retrieval task. \u2022 DE w/ Mined Neg enhance the performance of DPR by sampling hard negatives globally from the entire corpus as in ANCE [28] and RocketQA [21]. \n\n\u2022 DPTDR [25] is the first work that employs prompt tuning for dense retrieval. Among them, QL and BM25 are sparse retrieval models, whereas the others are dense retrieval models Implementation details. BM25 is implemented by Pyserini [14] with default parameters. The dual-encoder models are implemented Metrics. The following evaluation metrics are used in our experiments to examine the retrieval performance of baselines on T 2 Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved passages (MRR@10), ( 2) Recall for the top- retrieved passages (Recall@). Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages.",
                    "score": 0.7022810493487253,
                    "section_title": "Retrieval Performance",
                    "char_start_offset": 19754,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 10
                        },
                        {
                            "start": 11,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 935
                        },
                        {
                            "start": 938,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1148
                        },
                        {
                            "start": 1151,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2172
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 535,
                            "end": 539,
                            "matchedPaperCorpusId": "247292113"
                        },
                        {
                            "start": 690,
                            "end": 694,
                            "matchedPaperCorpusId": "247292113"
                        },
                        {
                            "start": 962,
                            "end": 966,
                            "matchedPaperCorpusId": "2759772"
                        },
                        {
                            "start": 1100,
                            "end": 1104,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 1687,
                            "end": 1691,
                            "matchedPaperCorpusId": "235366815"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91748046875
                }
            ],
            "relevance_judgement": 0.91748046875,
            "relevance_judgment_input_expanded": "# Title: T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Y. Liu, Jin Ma\n## Abstract\nPassage ranking involves two stages: passage retrieval and passage re-ranking, which are important and challenging topics for both academics and industries in the area of Information Retrieval (IR). However, the commonly-used datasets for passage ranking usually focus on the English language. For non-English scenarios, such as Chinese, the existing datasets are limited in terms of data scale, fine-grained relevance annotation and false negative issues. To address this problem, we introduce T2Ranking, a large-scale Chinese benchmark for passage ranking. T2Ranking comprises more than 300K queries and over 2M unique passages from real-world search engines. Expert annotators are recruited to provide 4-level graded relevance scores (fine-grained) for query-passage pairs instead of binary relevance judgments (coarse-grained). To ease the false negative issues, more passages with higher diversities are considered when performing relevance annotations, especially in the test set, to ensure a more accurate evaluation. Apart from the textual query and passage data, other auxiliary resources are also provided, such as query types and XML files of documents which passages are generated from, to facilitate further studies. To evaluate the dataset, commonly used ranking models are implemented and tested on T2Ranking as baselines. The experimental results show that T2Ranking is challenging and there is still scope for improvement. The full data and all codes are available at https://github.com/THUIR/T2Ranking/.\n## Retrieval Performance\nBaselines. Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents. Generally, most existing dense retrieval methods adhere to the cascade training paradigm [15,20,21]. Therefore, to facilitate easier comparison in future studies on our dataset, we simplify the training process as illustrated in Figure 5 as in [15,20]. Specifically, we utilize the dual-encoder (DE) as the architecture of dense retrieval models, which is illustrated in Figure 6(a). The following methods are employed as our baselines to evaluate the retrieval performance on T 2 Ranking. \n\n\u2022 QL (query likelihood) [19] is a representative statistical language model that measures the relevance of passages by modeling the generation of a query. \u2022 BM25 [23] is a widely-used sparse retrieval baseline. \n\n\u2022 DE w/ BM25 Neg is equivalent to DPR [12], which is the first work that uses the pre-trained language model as the backbone for the passage retrieval task. \u2022 DE w/ Mined Neg enhance the performance of DPR by sampling hard negatives globally from the entire corpus as in ANCE [28] and RocketQA [21]. \n\n\u2022 DPTDR [25] is the first work that employs prompt tuning for dense retrieval. Among them, QL and BM25 are sparse retrieval models, whereas the others are dense retrieval models Implementation details. BM25 is implemented by Pyserini [14] with default parameters. The dual-encoder models are implemented Metrics. The following evaluation metrics are used in our experiments to examine the retrieval performance of baselines on T 2 Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved passages (MRR@10), ( 2) Recall for the top- retrieved passages (Recall@). Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages.",
            "reference_string": "[258041275 | Xie et al. | 2023 | Citations: 48]"
        },
        {
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2022,
            "reference_count": 47,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.00970, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115569215",
                    "name": "Yifan Wang"
                },
                {
                    "authorId": "2110816708",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ],
            "abstract": "Passage retrieval has been studied for decades, and many recent approaches of passage retrieval are using dense embeddings generated from deep neural models, called \"dense passage retrieval\". The state-of-the-art end-to-end dense passage retrieval systems normally deploy a deep neural model followed by an approximate nearest neighbor (ANN) search module. The model generates embeddings of the corpus and queries, which are then indexed and searched by the high-performance ANN module. With the increasing data scale, the ANN module unavoidably becomes the bottleneck on efficiency. An alternative is the learned index, which achieves significantly high search efficiency by learning the data distribution and predicting the target data location. But most of the existing learned indexes are designed for low dimensional data, which are not suitable for dense passage retrieval with high-dimensional dense embeddings.\n \n In this paper, we propose\n LIDER\n , an efficient high-dimensional\n L\n earned\n I\n ndex for large-scale\n DE\n nse passage\n R\n etrieval. LIDER has a clustering-based hierarchical architecture formed by two layers of core models. As the basic unit of LIDER to index and search data, a\n core model\n includes an adapted recursive model index (RMI) and a dimension reduction component which consists of an extended SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction component reduces the high-dimensional dense embeddings into one-dimensional keys and sorts them in a specific order, which are then used by the RMI to make fast prediction. Experiments show that LIDER has a higher search speed with high retrieval quality comparing to the state-of-the-art ANN indexes on passage retrieval tasks, e.g., on large-scale data it achieves 1.2x search speed and significantly higher retrieval quality than the fastest baseline in our evaluation. Furthermore, LIDER has a better capability of speed-quality trade-off.\n",
            "corpus_id": 248496840,
            "sentences": [
                {
                    "corpus_id": "248496840",
                    "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
                    "text": "There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval. After BERT [6] was proposed, most of the recent dense retrieval models are designed based on it and achieve significant improvement on retrieval quality, e.g., Sentence-BERT [34] and MarkedBERT [2]. But as a heavy model, BERT has high inference latency, which limits its application on online retrieval that requires low-latency serving. To solve this problem, following works have proposed several variants to reduce its complexity, including DistilBERT [36], ColBERT [15], TCT-ColBERT [22], etc. \n\nTo better support the low-latency online retrieval, in addition to deploying more lightweight neural models, most state-of-the-art end-to-end dense retrieval systems also arrange a high-performance ANN search module following the neural model to fast look up the closest documents to the queries based on their embeddings. ANN indexes include four major categories, i.e., hashing, graph, quantization and tree based indexes. Among them the tree based indexes are more suitable to low-dimensional space, so dense retrieval systems normally choose from the other three types of indexes. For example, DPR [14] utilizes a graph based index, HNSW [26], ColBERT deploys IVFADC index which is based on product quantization, BPR [43] integrates learning-to-hash technique into DPR [14], etc. FAISS [13] is one of the most popular ANN index libraries in today's dense retrieval, as it implements high-performance indexes of all the three classes.",
                    "score": 0.5521530324396782,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 10570,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1100
                        },
                        {
                            "start": 1103,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 2040
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.91455078125,
            "relevance_judgment_input_expanded": "# Title: LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval\n# Venue: Proceedings of the VLDB Endowment\n# Authors: Yifan Wang, Haodi Ma, D. Wang\n## Abstract\nPassage retrieval has been studied for decades, and many recent approaches of passage retrieval are using dense embeddings generated from deep neural models, called \"dense passage retrieval\". The state-of-the-art end-to-end dense passage retrieval systems normally deploy a deep neural model followed by an approximate nearest neighbor (ANN) search module. The model generates embeddings of the corpus and queries, which are then indexed and searched by the high-performance ANN module. With the increasing data scale, the ANN module unavoidably becomes the bottleneck on efficiency. An alternative is the learned index, which achieves significantly high search efficiency by learning the data distribution and predicting the target data location. But most of the existing learned indexes are designed for low dimensional data, which are not suitable for dense passage retrieval with high-dimensional dense embeddings.\n \n In this paper, we propose\n LIDER\n , an efficient high-dimensional\n L\n earned\n I\n ndex for large-scale\n DE\n nse passage\n R\n etrieval. LIDER has a clustering-based hierarchical architecture formed by two layers of core models. As the basic unit of LIDER to index and search data, a\n core model\n includes an adapted recursive model index (RMI) and a dimension reduction component which consists of an extended SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction component reduces the high-dimensional dense embeddings into one-dimensional keys and sorts them in a specific order, which are then used by the RMI to make fast prediction. Experiments show that LIDER has a higher search speed with high retrieval quality comparing to the state-of-the-art ANN indexes on passage retrieval tasks, e.g., on large-scale data it achieves 1.2x search speed and significantly higher retrieval quality than the fastest baseline in our evaluation. Furthermore, LIDER has a better capability of speed-quality trade-off.\n\n## RELATED WORK\nThere are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval. After BERT [6] was proposed, most of the recent dense retrieval models are designed based on it and achieve significant improvement on retrieval quality, e.g., Sentence-BERT [34] and MarkedBERT [2]. But as a heavy model, BERT has high inference latency, which limits its application on online retrieval that requires low-latency serving. To solve this problem, following works have proposed several variants to reduce its complexity, including DistilBERT [36], ColBERT [15], TCT-ColBERT [22], etc. \n\nTo better support the low-latency online retrieval, in addition to deploying more lightweight neural models, most state-of-the-art end-to-end dense retrieval systems also arrange a high-performance ANN search module following the neural model to fast look up the closest documents to the queries based on their embeddings. ANN indexes include four major categories, i.e., hashing, graph, quantization and tree based indexes. Among them the tree based indexes are more suitable to low-dimensional space, so dense retrieval systems normally choose from the other three types of indexes. For example, DPR [14] utilizes a graph based index, HNSW [26], ColBERT deploys IVFADC index which is based on product quantization, BPR [43] integrates learning-to-hash technique into DPR [14], etc. FAISS [13] is one of the most popular ANN index libraries in today's dense retrieval, as it implements high-performance indexes of all the three classes.",
            "reference_string": "[248496840 | Wang et al. | 2022 | Citations: 13]"
        },
        {
            "title": "Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299735305",
                    "name": "Jiwei Tang"
                },
                {
                    "authorId": "2295771439",
                    "name": "Jin Xu"
                },
                {
                    "authorId": "2225120527",
                    "name": "Tingwei Lu"
                },
                {
                    "authorId": "2342467015",
                    "name": "Zhicheng Zhang"
                },
                {
                    "authorId": "2323518692",
                    "name": "Yiming Zhao"
                },
                {
                    "authorId": "2342416818",
                    "name": "Lin Hai"
                },
                {
                    "authorId": "2313769137",
                    "name": "Hai-Tao Zheng"
                }
            ],
            "abstract": "Large language models (LLMs) demonstrate exceptional capabilities in various scenarios. However, they suffer from much redundant information and are sensitive to the position of key information in long context scenarios. To address these challenges, we present Perception Compressor, a training-free prompt compression framework. It includes a perception retriever that leverages guiding questions and instruction to retrieve the most relevant demonstrations, a dual-slope ratio allocator to dynamically allocate compression ratios and open-book ratios, and a semi-guided iterative compression that retains key information at the token level while removing tokens that distract the LLM. We conduct extensive experiments on long context benchmarks, i.e., NaturalQuestions, LongBench, and MuSiQue. Experiment results show that Perception Compressor outperforms existing methods by a large margin, achieving state-of-the-art performance.",
            "corpus_id": 275921747,
            "sentences": [
                {
                    "corpus_id": "275921747",
                    "title": "Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios",
                    "text": "The retrieval methods can be divided into two categories: (1) Sparse retrieval. Sparse retrieval retrieves the most similar items from an index based on query keywords or feature vectors, such as BM25. (2) Dense retrieval. Dense retrieval methods (Xiao et al., 2023;Reimers and Gurevych, 2019;Jiang et al., 2023b;G\u00fcnther et al., 2023) involve training retrieval models on large corpora, generating dense vector representations for queries and demonstrations through model inference, and computing similarities based on these vector representations.",
                    "score": 0.5810049219378931,
                    "section_title": "Retrieval Methods",
                    "char_start_offset": 8351,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 548
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 293,
                            "end": 313,
                            "matchedPaperCorpusId": "259858947"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91259765625
                }
            ],
            "relevance_judgement": 0.91259765625,
            "relevance_judgment_input_expanded": "# Title: Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jiwei Tang, Jin Xu, Tingwei Lu, Zhicheng Zhang, Yiming Zhao, Lin Hai, Hai-Tao Zheng\n## Abstract\nLarge language models (LLMs) demonstrate exceptional capabilities in various scenarios. However, they suffer from much redundant information and are sensitive to the position of key information in long context scenarios. To address these challenges, we present Perception Compressor, a training-free prompt compression framework. It includes a perception retriever that leverages guiding questions and instruction to retrieve the most relevant demonstrations, a dual-slope ratio allocator to dynamically allocate compression ratios and open-book ratios, and a semi-guided iterative compression that retains key information at the token level while removing tokens that distract the LLM. We conduct extensive experiments on long context benchmarks, i.e., NaturalQuestions, LongBench, and MuSiQue. Experiment results show that Perception Compressor outperforms existing methods by a large margin, achieving state-of-the-art performance.\n## Retrieval Methods\nThe retrieval methods can be divided into two categories: (1) Sparse retrieval. Sparse retrieval retrieves the most similar items from an index based on query keywords or feature vectors, such as BM25. (2) Dense retrieval. Dense retrieval methods (Xiao et al., 2023;Reimers and Gurevych, 2019;Jiang et al., 2023b;G\u00fcnther et al., 2023) involve training retrieval models on large corpora, generating dense vector representations for queries and demonstrations through model inference, and computing similarities based on these vector representations.",
            "reference_string": "[275921747 | Tang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238699512",
                    "name": "Abdelrahman Abdallah"
                },
                {
                    "authorId": "1389526186",
                    "name": "Jamshid Mozafari"
                },
                {
                    "authorId": "1935823995",
                    "name": "Bhawna Piryani"
                },
                {
                    "authorId": "2343952059",
                    "name": "Mohammed Ali"
                },
                {
                    "authorId": "2261673463",
                    "name": "Adam Jatowt"
                }
            ],
            "abstract": "Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.",
            "corpus_id": 276647280,
            "sentences": [
                {
                    "corpus_id": "276647280",
                    "title": "From Retrieval to Generation: Comparing Different Approaches",
                    "text": "Retrieval plays a fundamental role in the TriviaQA dataset, where models must extract relevant information from large document collections to answer trivia-based, open-domain questions. This section provides a detailed comparison of various retrieval methods, including sparse retrievers like BM25, dense retrievers such as DPR, and generator models. The retrieval effectiveness of these models is measured using Top-1, Top-5, and Top-10 accuracy, which represent the percentage of cases in which a correct document appears within the top-k retrieved results. \n\nThe results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness. MSS-DPR follows closely, with a Top-1 accuracy of 73.5%, suggesting that additional pretraining techniques further enhance retrieval performance. Generative augmentation also proves valuable, as GenRead achieves a Top-1 accuracy of 69.7%, surpassing BM25 and approaching the effectiveness of dense retrievers. \n\nThe advantages of generator approaches become more evident in the Top-5 and Top-10 accuracy metrics. MSS-DPR+Gen leads with an 85.0% Top-5 accuracy, followed closely by DPR+Gen at 84.4%, indicating that the combination of retrieval and generation improves ranking effectiveness. BM25+Gen also sees significant improvements, achieving 84.4% in Top-5 accuracy, compared to BM25 alone at 73.6%. In the Top-10 retrieval setting, hybrid models consistently outperform retrieval-only methods, with DPR+Gen reaching 85.2% and MSS-DPR+Gen achieving 85.0%. These findings confirm that hybrid approaches, which integrate retrieval with generative document expansion, provide more robust and reliable retrieval for complex QA tasks.",
                    "score": 0.5711395734853756,
                    "section_title": "A.1 Retrieval Performance on TriviaQA",
                    "char_start_offset": 28826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 559
                        },
                        {
                            "start": 562,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1239
                        },
                        {
                            "start": 1242,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1963
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.912109375
                },
                {
                    "corpus_id": "276647280",
                    "title": "From Retrieval to Generation: Comparing Different Approaches",
                    "text": "Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.",
                    "score": 0.6841357575321563,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                },
                {
                    "corpus_id": "276647280",
                    "title": "From Retrieval to Generation: Comparing Different Approaches",
                    "text": "Let D = d 1 , d 2 , . . . , d M be a collection of evidence documents representing a retrieval corpus. Given a query q, an Information Retrieval (IR) model selects a subset of relevant passages Z \u2282 D, one or more of which will ideally contain the correct answer to q. Our setup supports passages obtained from any retriever, whether based on sparse representations like BM25 (Robertson and Zaragoza, 2009) or dense representations such as DPR (Karpukhin et al., 2020a), MSS (Sachan et al., 2021), MSS-DPR (Sachan et al., 2021), and Contriever (Izacard et al., 2021). BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively. In addition to DPR, we test models such as MSS, which focuses on masked salient span prediction, and MSS-DPR, which extends DPR with additional pre-training using MSS. Another dense retriever, Contriever, is trained in an unsupervised manner using contrastive learning on text paragraphs. We assume that each retriever provides the top-K most relevant passages, denoted as Z = {z 1 , z 2 , . . . , z K }.",
                    "score": 0.566933199133434,
                    "section_title": "Retriever Models",
                    "char_start_offset": 7254,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 25
                        },
                        {
                            "start": 26,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 375,
                            "end": 405,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 443,
                            "end": 468,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 543,
                            "end": 565,
                            "matchedPaperCorpusId": "249097975"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                },
                {
                    "corpus_id": "276647280",
                    "title": "From Retrieval to Generation: Comparing Different Approaches",
                    "text": "The increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25 (Robertson and Zaragoza, 2009) serving as foundational tools for extracting relevant documents. However, the limitations of keyword-based retrieval prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020a) and Contriever (Izacard et al., 2021), which leverage transformer-based architectures to encode queries and documents into dense representations. While dense retrieval models improve over sparse methods, they introduce new challenges. First, retrieval corpora are typically divided into fixed chunks (Karpukhin et al., 2020a), which can lead to retrieving irrelevant content. Second, dualencoder architectures encode queries and documents separately, limiting direct interaction between them (Khattab et al., 2021). Finally, dense retrieval models require pre-encoding and storing document embeddings, which constrains scalability and hinders their ability to leverage large language models (LLMs) (Levine et al., 2022). \n\nTo address these limitations, generative models such as GPT-3.5 and InstructGPT (Brown et al., 2020;Ouyang et al., 2022) offer an alternative by directly generating contextualized responses instead of retrieving existing documents. Approaches like GenRead (Yu et al., 2022) first generate relevant text and then use it for answer prediction. However, generative models often struggle with factual consistency and may hallucinate information (Huang et al., 2023), making them less reliable for knowledge-intensive tasks. Given the trade-offs between retrieval and generation, hybrid models have emerged to integrate the strengths of both approaches. Merging Generator and Retriever (MGR) (Abdallah and Jatowt, 2023;Zhang et al., 2023) combines generated and retrieved documents, allowing models to refine answers while maintaining factual accuracy.",
                    "score": 0.6006212090258654,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1324
                        },
                        {
                            "start": 1327,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1668
                        },
                        {
                            "start": 1669,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 1975
                        },
                        {
                            "start": 1976,
                            "end": 2174
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 341,
                            "end": 371,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 578,
                            "end": 603,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 619,
                            "end": 641,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 904,
                            "end": 929,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1427,
                            "end": 1447,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89013671875
                }
            ],
            "relevance_judgement": 0.912109375,
            "relevance_judgment_input_expanded": "# Title: From Retrieval to Generation: Comparing Different Approaches\n# Venue: arXiv.org\n# Authors: Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed Ali, Adam Jatowt\n## Abstract\nKnowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.\n## Introduction\nThe increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25 (Robertson and Zaragoza, 2009) serving as foundational tools for extracting relevant documents. However, the limitations of keyword-based retrieval prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020a) and Contriever (Izacard et al., 2021), which leverage transformer-based architectures to encode queries and documents into dense representations. While dense retrieval models improve over sparse methods, they introduce new challenges. First, retrieval corpora are typically divided into fixed chunks (Karpukhin et al., 2020a), which can lead to retrieving irrelevant content. Second, dualencoder architectures encode queries and documents separately, limiting direct interaction between them (Khattab et al., 2021). Finally, dense retrieval models require pre-encoding and storing document embeddings, which constrains scalability and hinders their ability to leverage large language models (LLMs) (Levine et al., 2022). \n\nTo address these limitations, generative models such as GPT-3.5 and InstructGPT (Brown et al., 2020;Ouyang et al., 2022) offer an alternative by directly generating contextualized responses instead of retrieving existing documents. Approaches like GenRead (Yu et al., 2022) first generate relevant text and then use it for answer prediction. However, generative models often struggle with factual consistency and may hallucinate information (Huang et al., 2023), making them less reliable for knowledge-intensive tasks. Given the trade-offs between retrieval and generation, hybrid models have emerged to integrate the strengths of both approaches. Merging Generator and Retriever (MGR) (Abdallah and Jatowt, 2023;Zhang et al., 2023) combines generated and retrieved documents, allowing models to refine answers while maintaining factual accuracy.\n\n## Retriever Models\nLet D = d 1 , d 2 , . . . , d M be a collection of evidence documents representing a retrieval corpus. Given a query q, an Information Retrieval (IR) model selects a subset of relevant passages Z \u2282 D, one or more of which will ideally contain the correct answer to q. Our setup supports passages obtained from any retriever, whether based on sparse representations like BM25 (Robertson and Zaragoza, 2009) or dense representations such as DPR (Karpukhin et al., 2020a), MSS (Sachan et al., 2021), MSS-DPR (Sachan et al., 2021), and Contriever (Izacard et al., 2021). BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively. In addition to DPR, we test models such as MSS, which focuses on masked salient span prediction, and MSS-DPR, which extends DPR with additional pre-training using MSS. Another dense retriever, Contriever, is trained in an unsupervised manner using contrastive learning on text paragraphs. We assume that each retriever provides the top-K most relevant passages, denoted as Z = {z 1 , z 2 , . . . , z K }.\n\n## A.1 Retrieval Performance on TriviaQA\nRetrieval plays a fundamental role in the TriviaQA dataset, where models must extract relevant information from large document collections to answer trivia-based, open-domain questions. This section provides a detailed comparison of various retrieval methods, including sparse retrievers like BM25, dense retrievers such as DPR, and generator models. The retrieval effectiveness of these models is measured using Top-1, Top-5, and Top-10 accuracy, which represent the percentage of cases in which a correct document appears within the top-k retrieved results. \n\nThe results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness. MSS-DPR follows closely, with a Top-1 accuracy of 73.5%, suggesting that additional pretraining techniques further enhance retrieval performance. Generative augmentation also proves valuable, as GenRead achieves a Top-1 accuracy of 69.7%, surpassing BM25 and approaching the effectiveness of dense retrievers. \n\nThe advantages of generator approaches become more evident in the Top-5 and Top-10 accuracy metrics. MSS-DPR+Gen leads with an 85.0% Top-5 accuracy, followed closely by DPR+Gen at 84.4%, indicating that the combination of retrieval and generation improves ranking effectiveness. BM25+Gen also sees significant improvements, achieving 84.4% in Top-5 accuracy, compared to BM25 alone at 73.6%. In the Top-10 retrieval setting, hybrid models consistently outperform retrieval-only methods, with DPR+Gen reaching 85.2% and MSS-DPR+Gen achieving 85.0%. These findings confirm that hybrid approaches, which integrate retrieval with generative document expansion, provide more robust and reliable retrieval for complex QA tasks.",
            "reference_string": "[276647280 | Abdallah et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval",
            "venue": "Text Retrieval Conference",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.01019",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.01019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145580839",
                    "name": "Jimmy J. Lin"
                },
                {
                    "authorId": "1419474794",
                    "name": "David Alfonso-Hermelo"
                },
                {
                    "authorId": "2274328833",
                    "name": "Vitor Jeronymo"
                },
                {
                    "authorId": "2023642",
                    "name": "Ehsan Kamalloo"
                },
                {
                    "authorId": "2131640257",
                    "name": "Carlos Lassance"
                },
                {
                    "authorId": "2274330487",
                    "name": "Rodrigo Nogueira"
                },
                {
                    "authorId": "2166106776",
                    "name": "Odunayo Ogundepo"
                },
                {
                    "authorId": "2066076226",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "47583894",
                    "name": "Nandan Thakur"
                },
                {
                    "authorId": "2109723027",
                    "name": "Jheng-Hong Yang"
                },
                {
                    "authorId": "2118895402",
                    "name": "Xinyu Crystina Zhang"
                }
            ],
            "abstract": "The advent of multilingual language models has generated a resurgence of interest in cross-lingual information retrieval (CLIR), which is the task of searching documents in one language with queries from another. However, the rapid pace of progress has led to a confusing panoply of methods and reproducibility has lagged behind the state of the art. In this context, our work makes two important contributions: First, we provide a conceptual framework for organizing different approaches to cross-lingual retrieval using multi-stage architectures for mono-lingual retrieval as a scaffold. Second, we implement simple yet effective reproducible baselines in the Anserini and Pyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in Persian, Russian, and Chinese. Our efforts are built on a collaboration of the two teams that submitted the most effective runs to the TREC evaluation. These contributions provide a firm foundation for future advances.",
            "corpus_id": 266230831,
            "sentences": [
                {
                    "corpus_id": "266230831",
                    "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval",
                    "text": "After the introduction of dense retrieval models, researchers soon realized that transformer-based encoders could also be coaxed to generate sparse representations, where the vector basis, for example, spans the input vocabulary space. Another way to view these so-called sparse retrieval models is to contrast them with BM25: whereas BM25 term weights are assigned using a heuristic scoring function, sparse retrieval models assign term weights that are learned using pretrained transformers such as BERT. Examples of these learned sparse retrieval models include DeepImpact [29], uniCOIL [24,53], SPLADE [12], as well as many others. \n\nRecently, Lin [23] made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models. The conceptual framework for mono-lingual retrieval provides us with a basis for organizing cross-lingual retrieval approaches, which we discuss next.",
                    "score": 0.6368501494121173,
                    "section_title": "MONO-LINGUAL RETRIEVAL OVERVIEW",
                    "char_start_offset": 6802,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 635
                        },
                        {
                            "start": 638,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 977
                        },
                        {
                            "start": 978,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1599
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 576,
                            "end": 580,
                            "matchedPaperCorpusId": "233394068"
                        },
                        {
                            "start": 606,
                            "end": 610,
                            "matchedPaperCorpusId": "248665596"
                        },
                        {
                            "start": 652,
                            "end": 656,
                            "matchedPaperCorpusId": "238259539"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.9072265625,
            "relevance_judgment_input_expanded": "# Title: Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval\n# Venue: Text Retrieval Conference\n# Authors: Jimmy J. Lin, David Alfonso-Hermelo, Vitor Jeronymo, Ehsan Kamalloo, Carlos Lassance, Rodrigo Nogueira, Odunayo Ogundepo, Mehdi Rezagholizadeh, Nandan Thakur, Jheng-Hong Yang, Xinyu Crystina Zhang\n## Abstract\nThe advent of multilingual language models has generated a resurgence of interest in cross-lingual information retrieval (CLIR), which is the task of searching documents in one language with queries from another. However, the rapid pace of progress has led to a confusing panoply of methods and reproducibility has lagged behind the state of the art. In this context, our work makes two important contributions: First, we provide a conceptual framework for organizing different approaches to cross-lingual retrieval using multi-stage architectures for mono-lingual retrieval as a scaffold. Second, we implement simple yet effective reproducible baselines in the Anserini and Pyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in Persian, Russian, and Chinese. Our efforts are built on a collaboration of the two teams that submitted the most effective runs to the TREC evaluation. These contributions provide a firm foundation for future advances.\n## MONO-LINGUAL RETRIEVAL OVERVIEW\nAfter the introduction of dense retrieval models, researchers soon realized that transformer-based encoders could also be coaxed to generate sparse representations, where the vector basis, for example, spans the input vocabulary space. Another way to view these so-called sparse retrieval models is to contrast them with BM25: whereas BM25 term weights are assigned using a heuristic scoring function, sparse retrieval models assign term weights that are learned using pretrained transformers such as BERT. Examples of these learned sparse retrieval models include DeepImpact [29], uniCOIL [24,53], SPLADE [12], as well as many others. \n\nRecently, Lin [23] made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models. The conceptual framework for mono-lingual retrieval provides us with a basis for organizing cross-lingual retrieval approaches, which we discuss next.",
            "reference_string": "[266230831 | Lin et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.05153",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.05153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211101536",
                    "name": "Yasuto Hoshi"
                },
                {
                    "authorId": "2441156",
                    "name": "D. Miyashita"
                },
                {
                    "authorId": "51194024",
                    "name": "Yasuhiro Morioka"
                },
                {
                    "authorId": "20556792",
                    "name": "Youyang Ng"
                },
                {
                    "authorId": "2422593",
                    "name": "Osamu Torii"
                },
                {
                    "authorId": "49192096",
                    "name": "J. Deguchi"
                }
            ],
            "abstract": "Neural document retrievers, including dense passage retrieval (DPR), have outperformed classical lexical-matching retrievers, such as BM25, when fine-tuned and tested on specific question-answering datasets. However, it has been shown that the existing dense retrievers do not generalize well not only out of domain but even in domain such as Wikipedia, especially when a named entity in a question is a dominant clue for retrieval. In this paper, we propose an approach toward in-domain generalization using the embeddings generated by the frozen language model trained with the entities in the domain. By not fine-tuning, we explore the possibility that the rich knowledge contained in a pretrained language model can be used for retrieval tasks. The proposed method outperforms conventional DPRs on entity-centric questions in Wikipedia domain and achieves almost comparable performance to BM25 and state-of-the-art SPAR model. We also show that the contextualized keys lead to strong improvements compared to BM25 when the entity names consist of common words. Our results demonstrate the feasibility of the zero-shot retrieval method for entity-centric questions of Wikipedia domain, where DPR has struggled to perform.",
            "corpus_id": 257427642,
            "sentences": [
                {
                    "corpus_id": "257427642",
                    "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
                    "text": "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al. 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al. 1995;Robertson and Zaragoza 2009). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models. We refer the reader to Thakur et al. (2021) for details on major retrieval methods. It is also reported that re-ranking the retrieved passages can improve recall in EQ dataset (e.g., Sachan et al. 2022), but note that re-rankers are outside the scope of this paper. \n\nIn-Domain Generalization of Retriever. Dense retrievers still have room for improvement of in-domain generalization, as well as out-of-distribution generalization (Thakur et al. 2021). To generalize in Wikipedia domain, Sciavolino et al. (2021) trained DPR on PAQ dataset (Lewis et al. 2021) which contains many of named entities in Wikipedia. However, the trained model still performed far worse than BM25 in EQ dataset. Sciavolino et al. (2021) also attempted to improve recalls for relational questions by training dedicated question encoders for each relation. While this approach somewhat improved recall, the dedicated retrievers did not reach the performance of BM25 on average and did not solve the poor generalization problem. In contrast, our proposed method employs a pretrained language model without finetuning for retrieval, allowing us to exploit the rich knowledge including named entities in Wikipedia learned by pretraining. \n\nDifference between Dense Retrieval and Sparse Retrieval. Ma et al. (2021) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020).",
                    "score": 0.533664542506345,
                    "section_title": "Related Work",
                    "char_start_offset": 5041,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 18
                        },
                        {
                            "start": 19,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 803
                        },
                        {
                            "start": 806,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1370
                        },
                        {
                            "start": 1371,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1748
                        },
                        {
                            "start": 1751,
                            "end": 1807
                        },
                        {
                            "start": 1808,
                            "end": 1911
                        },
                        {
                            "start": 1912,
                            "end": 2066
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 135,
                            "end": 153,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 275,
                            "end": 298,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 298,
                            "end": 326,
                            "matchedPaperCorpusId": "3641284"
                        },
                        {
                            "start": 561,
                            "end": 581,
                            "matchedPaperCorpusId": "233296016"
                        },
                        {
                            "start": 969,
                            "end": 989,
                            "matchedPaperCorpusId": "233296016"
                        },
                        {
                            "start": 1026,
                            "end": 1050,
                            "matchedPaperCorpusId": "237562875"
                        },
                        {
                            "start": 1078,
                            "end": 1097,
                            "matchedPaperCorpusId": "231924957"
                        },
                        {
                            "start": 1228,
                            "end": 1252,
                            "matchedPaperCorpusId": "237562875"
                        },
                        {
                            "start": 1808,
                            "end": 1824,
                            "matchedPaperCorpusId": "3641284"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.90625,
            "relevance_judgment_input_expanded": "# Title: Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?\n# Venue: arXiv.org\n# Authors: Yasuto Hoshi, D. Miyashita, Yasuhiro Morioka, Youyang Ng, Osamu Torii, J. Deguchi\n## Abstract\nNeural document retrievers, including dense passage retrieval (DPR), have outperformed classical lexical-matching retrievers, such as BM25, when fine-tuned and tested on specific question-answering datasets. However, it has been shown that the existing dense retrievers do not generalize well not only out of domain but even in domain such as Wikipedia, especially when a named entity in a question is a dominant clue for retrieval. In this paper, we propose an approach toward in-domain generalization using the embeddings generated by the frozen language model trained with the entities in the domain. By not fine-tuning, we explore the possibility that the rich knowledge contained in a pretrained language model can be used for retrieval tasks. The proposed method outperforms conventional DPRs on entity-centric questions in Wikipedia domain and achieves almost comparable performance to BM25 and state-of-the-art SPAR model. We also show that the contextualized keys lead to strong improvements compared to BM25 when the entity names consist of common words. Our results demonstrate the feasibility of the zero-shot retrieval method for entity-centric questions of Wikipedia domain, where DPR has struggled to perform.\n## Related Work\nPassage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al. 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al. 1995;Robertson and Zaragoza 2009). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models. We refer the reader to Thakur et al. (2021) for details on major retrieval methods. It is also reported that re-ranking the retrieved passages can improve recall in EQ dataset (e.g., Sachan et al. 2022), but note that re-rankers are outside the scope of this paper. \n\nIn-Domain Generalization of Retriever. Dense retrievers still have room for improvement of in-domain generalization, as well as out-of-distribution generalization (Thakur et al. 2021). To generalize in Wikipedia domain, Sciavolino et al. (2021) trained DPR on PAQ dataset (Lewis et al. 2021) which contains many of named entities in Wikipedia. However, the trained model still performed far worse than BM25 in EQ dataset. Sciavolino et al. (2021) also attempted to improve recalls for relational questions by training dedicated question encoders for each relation. While this approach somewhat improved recall, the dedicated retrievers did not reach the performance of BM25 on average and did not solve the poor generalization problem. In contrast, our proposed method employs a pretrained language model without finetuning for retrieval, allowing us to exploit the rich knowledge including named entities in Wikipedia learned by pretraining. \n\nDifference between Dense Retrieval and Sparse Retrieval. Ma et al. (2021) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020).",
            "reference_string": "[257427642 | Hoshi et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Multi-view Content-aware Indexing for Long Document Retrieval",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.15103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275185317",
                    "name": "Kuicai Dong"
                },
                {
                    "authorId": "2151790894",
                    "name": "Derrick-Goh-Xin Deik"
                },
                {
                    "authorId": "2297951506",
                    "name": "Yi Lee"
                },
                {
                    "authorId": "2298263602",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2297935844",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "2284302963",
                    "name": "Cong Zhang"
                },
                {
                    "authorId": "2297898895",
                    "name": "Yong Liu"
                }
            ],
            "abstract": "Long document question answering (DocQA) aims to answer questions from long documents over 10k words. They usually contain content structures such as sections, sub-sections, and paragraph demarcations. However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking. As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content. Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views. We highlight that MC-indexing requires neither training nor fine-tuning. Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance. Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope. When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments.",
            "corpus_id": 269302715,
            "sentences": [
                {
                    "corpus_id": "269302715",
                    "title": "Multi-view Content-aware Indexing for Long Document Retrieval",
                    "text": "Retrieval Methods.Current approaches to content retrieval are primarily classified into sparse and dense retrieval.There are two widely-used sparse retrieval methods, namely TF-IDF (Salton et al., 1983) and BM25 (Robertson et al., 1995).TF-IDF calculates the relevance of a word to a document in the corpus by multiplying the word frequency with the inverse document frequency.BM25 is an advancement of TF-IDF that introduces nonlinear word frequency saturation and length normalization to improve retrieval accuracy.Recently, dense retrieval methods have shown promising results, by encoding content into highdimensional representations.DPR (Karpukhin et al., 2020) is the pioneering work of dense vector representations for QA tasks.Similarly, Col-BERT (Khattab and Zaharia, 2020b) introduces an efficient question-document interaction model, enhancing retrieval accuracy by allowing fine-grained term matching.Contriever (Izacard et al., 2022) further leverages contrastive learning to improve content dense encoding.E5 (Wang et al., 2022) and BGE (Xiao et al., 2023) propose novel training and data preparation techniques to enhance retrieval performance, e.g., consistency-filtering of noisy web data in E5 and the usage of RetroMAE (Xiao et al., 2022) pre-training paradigm in BGE.Moreover, GTE (Li et al., 2023) integrates graph-based techniques to enhance dense embedding.\n\nIn summary, these systems focus on how to retrieve relevant chunks, but neglecting how text content is chunked.In contrast, MC-indexing can utilize the strengths of existing retrievers, and further improve their retrieval performance.\n\nChunking Methods.Chunking is a crucial step in either QA or Retrieval-Augmented Generation (RAG).When dealing with ultra-long text documents, chunk optimization involves breaking the document into smaller chunks.In practice, fixedlength chunking is a commonly used method that is easy to be implemented.It chunks text at a fixed length, e.g., 200 words.Sentence chunking involves dividing textual content based on sentences.",
                    "score": 0.5557513428594684,
                    "section_title": "Related Work",
                    "char_start_offset": 4799,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 18
                        },
                        {
                            "start": 18,
                            "end": 115
                        },
                        {
                            "start": 115,
                            "end": 237
                        },
                        {
                            "start": 237,
                            "end": 377
                        },
                        {
                            "start": 377,
                            "end": 517
                        },
                        {
                            "start": 517,
                            "end": 638
                        },
                        {
                            "start": 638,
                            "end": 735
                        },
                        {
                            "start": 735,
                            "end": 913
                        },
                        {
                            "start": 913,
                            "end": 1020
                        },
                        {
                            "start": 1020,
                            "end": 1287
                        },
                        {
                            "start": 1287,
                            "end": 1380
                        },
                        {
                            "start": 1382,
                            "end": 1493
                        },
                        {
                            "start": 1493,
                            "end": 1616
                        },
                        {
                            "start": 1618,
                            "end": 1635
                        },
                        {
                            "start": 1635,
                            "end": 1715
                        },
                        {
                            "start": 1715,
                            "end": 1830
                        },
                        {
                            "start": 1830,
                            "end": 1921
                        },
                        {
                            "start": 1921,
                            "end": 1971
                        },
                        {
                            "start": 1971,
                            "end": 2042
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 202,
                            "matchedPaperCorpusId": "207180535"
                        },
                        {
                            "start": 212,
                            "end": 236,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 642,
                            "end": 666,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 755,
                            "end": 782,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 924,
                            "end": 946,
                            "matchedPaperCorpusId": "263831502"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: Multi-view Content-aware Indexing for Long Document Retrieval\n# Venue: arXiv.org\n# Authors: Kuicai Dong, Derrick-Goh-Xin Deik, Yi Lee, Hao Zhang, Xiangyang Li, Cong Zhang, Yong Liu\n## Abstract\nLong document question answering (DocQA) aims to answer questions from long documents over 10k words. They usually contain content structures such as sections, sub-sections, and paragraph demarcations. However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking. As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content. Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views. We highlight that MC-indexing requires neither training nor fine-tuning. Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance. Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope. When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments.\n## Related Work\nRetrieval Methods.Current approaches to content retrieval are primarily classified into sparse and dense retrieval.There are two widely-used sparse retrieval methods, namely TF-IDF (Salton et al., 1983) and BM25 (Robertson et al., 1995).TF-IDF calculates the relevance of a word to a document in the corpus by multiplying the word frequency with the inverse document frequency.BM25 is an advancement of TF-IDF that introduces nonlinear word frequency saturation and length normalization to improve retrieval accuracy.Recently, dense retrieval methods have shown promising results, by encoding content into highdimensional representations.DPR (Karpukhin et al., 2020) is the pioneering work of dense vector representations for QA tasks.Similarly, Col-BERT (Khattab and Zaharia, 2020b) introduces an efficient question-document interaction model, enhancing retrieval accuracy by allowing fine-grained term matching.Contriever (Izacard et al., 2022) further leverages contrastive learning to improve content dense encoding.E5 (Wang et al., 2022) and BGE (Xiao et al., 2023) propose novel training and data preparation techniques to enhance retrieval performance, e.g., consistency-filtering of noisy web data in E5 and the usage of RetroMAE (Xiao et al., 2022) pre-training paradigm in BGE.Moreover, GTE (Li et al., 2023) integrates graph-based techniques to enhance dense embedding.\n\nIn summary, these systems focus on how to retrieve relevant chunks, but neglecting how text content is chunked.In contrast, MC-indexing can utilize the strengths of existing retrievers, and further improve their retrieval performance.\n\nChunking Methods.Chunking is a crucial step in either QA or Retrieval-Augmented Generation (RAG).When dealing with ultra-long text documents, chunk optimization involves breaking the document into smaller chunks.In practice, fixedlength chunking is a commonly used method that is easy to be implemented.It chunks text at a fixed length, e.g., 200 words.Sentence chunking involves dividing textual content based on sentences.",
            "reference_string": "[269302715 | Dong et al. | 2024 | Citations: 3]"
        },
        {
            "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 14,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349603255",
                    "name": "Hsin-Ling Hsu"
                },
                {
                    "authorId": "145168633",
                    "name": "Jengnan Tzeng"
                }
            ],
            "abstract": "Hybrid retrieval techniques in Retrieval-Augmented Generation (RAG) systems enhance information retrieval by combining dense and sparse (e.g., BM25-based) retrieval methods. However, existing approaches struggle with adaptability, as fixed weighting schemes fail to adjust to different queries. To address this, we propose DAT (Dynamic Alpha Tuning), a novel hybrid retrieval framework that dynamically balances dense retrieval and BM25 for each query. DAT leverages a large language model (LLM) to evaluate the effectiveness of the top-1 results from both retrieval methods, assigning an effectiveness score to each. It then calibrates the optimal weighting factor through effectiveness score normalization, ensuring a more adaptive and query-aware weighting between the two approaches. Empirical results show that DAT consistently significantly outperforms fixed-weighting hybrid retrieval methods across various evaluation metrics. Even on smaller models, DAT delivers strong performance, highlighting its efficiency and adaptability.",
            "corpus_id": 277451883,
            "sentences": [
                {
                    "corpus_id": "277451883",
                    "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) systems have emerged as a powerful paradigm for enhancing the factuality and reliability of large language model (LLM) outputs by grounding responses in external knowledge sources. At the core of effective RAG systems lies the retrieval component, which is responsible for identifying and surfacing the most relevant documents from a knowledge base in response to user queries. The quality of retrieval directly impacts the overall performance of RAG systems, making it a critical area for optimization. \n\nHybrid retrieval (Ma et al., 2020;Sawarkar et al., 2024;Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap. While the complementary strengths of these methods are well established, effectively balancing their contributions remains challenging. Current approaches (Bruch et al., 2023) typically employ a fixed weighting parameter (\u03b1) determined through offline tuning on validation datasets. This static weighting scheme, however, fails to account for the diverse nature of user queries, where the optimal balance between keyword matching and semantic similarity varies significantly based on query characteristics and knowledge base structure. \n\nRecent efforts to address this limitation include approaches that assign different \u03b1 values based on query types (e.g., fact-seeking, concept-seeking, etc.) (Theja, 2024). However, these methods still rely on predetermined categories with fixed weights and often overlook the complex interplay between individual queries and the knowledge base. If this assumption holds-that many queries benefit more from extreme values (i.e., pure BM25 or pure dense retrieval)-then using a compromise value such as \u03b1 = 0.5, while seemingly optimal on average, may in fact lead to suboptimal performance for most individual queries. This would pose a significant challenge to hybrid retrieval optimization. \n\nThese limitations and opportunities motivate our research questions: \n\n\u2022 How can we effectively combine sparse and dense retrieval methods to maximize retrieval performance?",
                    "score": 0.5786294694614358,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 238
                        },
                        {
                            "start": 239,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 561
                        },
                        {
                            "start": 564,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1528
                        },
                        {
                            "start": 1531,
                            "end": 1702
                        },
                        {
                            "start": 1703,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 2148
                        },
                        {
                            "start": 2149,
                            "end": 2222
                        },
                        {
                            "start": 2225,
                            "end": 2293
                        },
                        {
                            "start": 2296,
                            "end": 2398
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 57,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 581,
                            "end": 598,
                            "matchedPaperCorpusId": "221668044"
                        },
                        {
                            "start": 598,
                            "end": 620,
                            "matchedPaperCorpusId": "269043117"
                        },
                        {
                            "start": 891,
                            "end": 915,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1148,
                            "end": 1168,
                            "matchedPaperCorpusId": "253080555"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90478515625
                }
            ],
            "relevance_judgement": 0.90478515625,
            "relevance_judgment_input_expanded": "# Title: DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Hsin-Ling Hsu, Jengnan Tzeng\n## Abstract\nHybrid retrieval techniques in Retrieval-Augmented Generation (RAG) systems enhance information retrieval by combining dense and sparse (e.g., BM25-based) retrieval methods. However, existing approaches struggle with adaptability, as fixed weighting schemes fail to adjust to different queries. To address this, we propose DAT (Dynamic Alpha Tuning), a novel hybrid retrieval framework that dynamically balances dense retrieval and BM25 for each query. DAT leverages a large language model (LLM) to evaluate the effectiveness of the top-1 results from both retrieval methods, assigning an effectiveness score to each. It then calibrates the optimal weighting factor through effectiveness score normalization, ensuring a more adaptive and query-aware weighting between the two approaches. Empirical results show that DAT consistently significantly outperforms fixed-weighting hybrid retrieval methods across various evaluation metrics. Even on smaller models, DAT delivers strong performance, highlighting its efficiency and adaptability.\n## Introduction\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020) systems have emerged as a powerful paradigm for enhancing the factuality and reliability of large language model (LLM) outputs by grounding responses in external knowledge sources. At the core of effective RAG systems lies the retrieval component, which is responsible for identifying and surfacing the most relevant documents from a knowledge base in response to user queries. The quality of retrieval directly impacts the overall performance of RAG systems, making it a critical area for optimization. \n\nHybrid retrieval (Ma et al., 2020;Sawarkar et al., 2024;Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap. While the complementary strengths of these methods are well established, effectively balancing their contributions remains challenging. Current approaches (Bruch et al., 2023) typically employ a fixed weighting parameter (\u03b1) determined through offline tuning on validation datasets. This static weighting scheme, however, fails to account for the diverse nature of user queries, where the optimal balance between keyword matching and semantic similarity varies significantly based on query characteristics and knowledge base structure. \n\nRecent efforts to address this limitation include approaches that assign different \u03b1 values based on query types (e.g., fact-seeking, concept-seeking, etc.) (Theja, 2024). However, these methods still rely on predetermined categories with fixed weights and often overlook the complex interplay between individual queries and the knowledge base. If this assumption holds-that many queries benefit more from extreme values (i.e., pure BM25 or pure dense retrieval)-then using a compromise value such as \u03b1 = 0.5, while seemingly optimal on average, may in fact lead to suboptimal performance for most individual queries. This would pose a significant challenge to hybrid retrieval optimization. \n\nThese limitations and opportunities motivate our research questions: \n\n\u2022 How can we effectively combine sparse and dense retrieval methods to maximize retrieval performance?",
            "reference_string": "[277451883 | Hsu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 83,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2215174877",
                    "name": "Mengxuan Hu"
                },
                {
                    "authorId": "2325564703",
                    "name": "Hongyi Wu"
                },
                {
                    "authorId": "2260652610",
                    "name": "Zihan Guan"
                },
                {
                    "authorId": "1778454",
                    "name": "Ronghang Zhu"
                },
                {
                    "authorId": "2246521452",
                    "name": "Dongliang Guo"
                },
                {
                    "authorId": "51892829",
                    "name": "Daiqing Qi"
                },
                {
                    "authorId": "2293760697",
                    "name": "Sheng Li"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness and cost-efficiency in mitigating hallucinations and enhancing the domain-specific generation capabilities of large language models (LLMs). However, is this effectiveness and cost-efficiency truly a free lunch? In this study, we comprehensively investigate the fairness costs associated with RAG by proposing a practical three-level threat model from the perspective of user awareness of fairness. Specifically, varying levels of user fairness awareness result in different degrees of fairness censorship on the external dataset. We examine the fairness implications of RAG using uncensored, partially censored, and fully censored datasets. Our experiments demonstrate that fairness alignment can be easily undermined through RAG without the need for fine-tuning or retraining. Even with fully censored and supposedly unbiased external datasets, RAG can lead to biased outputs. Our findings underscore the limitations of current alignment methods in the context of RAG-based LLMs and highlight the urgent need for new strategies to ensure fairness. We propose potential mitigations and call for further research to develop robust fairness safeguards in RAG-based LLMs.",
            "corpus_id": 273234268,
            "sentences": [
                {
                    "corpus_id": "273234268",
                    "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
                    "text": "A.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
                    "score": 0.5960614009952676,
                    "section_title": "A More details of Retrieval and Generation",
                    "char_start_offset": 32867,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1814
                        },
                        {
                            "start": 1817,
                            "end": 1885
                        },
                        {
                            "start": 1886,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2210
                        },
                        {
                            "start": 2211,
                            "end": 2367
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 609,
                            "end": 612,
                            "matchedPaperCorpusId": "269740933"
                        },
                        {
                            "start": 898,
                            "end": 902,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 1486,
                            "end": 1490,
                            "matchedPaperCorpusId": "254044526"
                        },
                        {
                            "start": 1523,
                            "end": 1527,
                            "matchedPaperCorpusId": "244714119"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users\n# Venue: arXiv.org\n# Authors: Mengxuan Hu, Hongyi Wu, Zihan Guan, Ronghang Zhu, Dongliang Guo, Daiqing Qi, Sheng Li\n## Abstract\nRetrieval-Augmented Generation (RAG) is widely adopted for its effectiveness and cost-efficiency in mitigating hallucinations and enhancing the domain-specific generation capabilities of large language models (LLMs). However, is this effectiveness and cost-efficiency truly a free lunch? In this study, we comprehensively investigate the fairness costs associated with RAG by proposing a practical three-level threat model from the perspective of user awareness of fairness. Specifically, varying levels of user fairness awareness result in different degrees of fairness censorship on the external dataset. We examine the fairness implications of RAG using uncensored, partially censored, and fully censored datasets. Our experiments demonstrate that fairness alignment can be easily undermined through RAG without the need for fine-tuning or retraining. Even with fully censored and supposedly unbiased external datasets, RAG can lead to biased outputs. Our findings underscore the limitations of current alignment methods in the context of RAG-based LLMs and highlight the urgent need for new strategies to ensure fairness. We propose potential mitigations and call for further research to develop robust fairness safeguards in RAG-based LLMs.\n## A More details of Retrieval and Generation\nA.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
            "reference_string": "[273234268 | Hu et al. | 2024 | Citations: 5]"
        },
        {
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2403.14374",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14374, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293238106",
                    "name": "Yuren Mao"
                },
                {
                    "authorId": "2223421628",
                    "name": "Xuemei Dong"
                },
                {
                    "authorId": "2292411692",
                    "name": "Wenyi Xu"
                },
                {
                    "authorId": "2292513358",
                    "name": "Yunjun Gao"
                },
                {
                    "authorId": "2159328709",
                    "name": "Bin Wei"
                },
                {
                    "authorId": "2292448869",
                    "name": "Ying Zhang"
                }
            ],
            "abstract": "Due to the extraordinarily large number of parameters, fine-tuning large language models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a retrieval-augmented generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs\u2019 preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this article proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer which takes the factual information and LLMs\u2019 preferences as labels respectively. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer, which enables FIT-RAG to avoid unnecessary augmentation and reduce augmentation tokens as much as possible. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ, and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.",
            "corpus_id": 268554288,
            "sentences": [
                {
                    "corpus_id": "268554288",
                    "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
                    "text": "Retrieval-Augmented Generation (RAG) is a technique that augments natural language generation models with relevant content retrieved from knowledge sources, aiming at improving the quality and relevance of text generation.Previous works have demonstrated its strong performance in knowledge-intensive tasks such as question answering, fact checking, and content recommendation [12,24,32,41].\n\nRetrievers interact with external corpus to acquire relevant information.For open-domain question answering, the Wikipedia corpus [6] is commonly used.As for retrieval methods, it can broadly be categorized into two types: sparse retrievers and dense retrievers.Sparse retrievers, such as TF-IDF [37] and BM25 [35], predominantly rely on keyword matching for document retrieval.These methods determine the relevance between queries and documents by analyzing the occurrence and distribution of keywords within the documents.Dense retrievers employ dual-encoders to generate dense vector representations of text for more accurate semantic matching.Consequently, dense retrievers are considered more suitable for retrieval-augmented applications.Some techniques like vector quantization [25,47] and embedding optimization [48] also improves the efficiency of dense retrievers.Common dense retrievers include DPR [20], ANCE [49] and Contriever [13].Specifically, DPR [20] is trained with supervised learning on question-answer pairs, and focuses on extracting relevant passages by analyzing the semantic content of both questions and answers.ANCE [49] leverages approximate nearest neighbor search and contrastive learning to enhance the model's ability to discern between relevant and non-relevant documents in a dense vector space.Contriever [13] employs unsupervised contrastive learning to adapt to inherent data structure, especially beneficial when the annotated training data is scarce.To enhance the quality of retrieved documents, some work conduct further reranking to these documents for personalization [3,40,56] and diversification [27,38].\n\nRecent work has explored different ways for language models to leverage retrieved or generated text as external knowledge.One approach is to integrate retrieval into language model pre-training or fine-tuning.For instance, REALM [9] integrates external document retrieval into pre-training, enhancing performance in downstream tasks by retrieving relevant information.",
                    "score": 0.6086114307992867,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 8684,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 222
                        },
                        {
                            "start": 222,
                            "end": 391
                        },
                        {
                            "start": 393,
                            "end": 466
                        },
                        {
                            "start": 466,
                            "end": 544
                        },
                        {
                            "start": 544,
                            "end": 655
                        },
                        {
                            "start": 655,
                            "end": 771
                        },
                        {
                            "start": 771,
                            "end": 917
                        },
                        {
                            "start": 917,
                            "end": 1040
                        },
                        {
                            "start": 1040,
                            "end": 1137
                        },
                        {
                            "start": 1137,
                            "end": 1267
                        },
                        {
                            "start": 1267,
                            "end": 1339
                        },
                        {
                            "start": 1339,
                            "end": 1532
                        },
                        {
                            "start": 1532,
                            "end": 1723
                        },
                        {
                            "start": 1723,
                            "end": 1883
                        },
                        {
                            "start": 1883,
                            "end": 2043
                        },
                        {
                            "start": 2045,
                            "end": 2167
                        },
                        {
                            "start": 2167,
                            "end": 2254
                        },
                        {
                            "start": 2254,
                            "end": 2413
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 377,
                            "end": 381,
                            "matchedPaperCorpusId": "257637217"
                        },
                        {
                            "start": 384,
                            "end": 387,
                            "matchedPaperCorpusId": "221507798"
                        },
                        {
                            "start": 387,
                            "end": 390,
                            "matchedPaperCorpusId": "4711425"
                        },
                        {
                            "start": 689,
                            "end": 693,
                            "matchedPaperCorpusId": "2996187"
                        },
                        {
                            "start": 703,
                            "end": 707,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 1178,
                            "end": 1182,
                            "matchedPaperCorpusId": "259949894"
                        },
                        {
                            "start": 1182,
                            "end": 1185,
                            "matchedPaperCorpusId": "247922621"
                        },
                        {
                            "start": 1213,
                            "end": 1217,
                            "matchedPaperCorpusId": "245986472"
                        },
                        {
                            "start": 1303,
                            "end": 1307,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1314,
                            "end": 1318,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1334,
                            "end": 1338,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 1357,
                            "end": 1361,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1537,
                            "end": 1541,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1734,
                            "end": 1738,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 2005,
                            "end": 2008,
                            "matchedPaperCorpusId": "1292249"
                        },
                        {
                            "start": 2008,
                            "end": 2011,
                            "matchedPaperCorpusId": "316030"
                        },
                        {
                            "start": 2011,
                            "end": 2014,
                            "matchedPaperCorpusId": "240230721"
                        },
                        {
                            "start": 2035,
                            "end": 2039,
                            "matchedPaperCorpusId": "220730230"
                        },
                        {
                            "start": 2039,
                            "end": 2042,
                            "matchedPaperCorpusId": "235792531"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9033203125
                }
            ],
            "relevance_judgement": 0.9033203125,
            "relevance_judgment_input_expanded": "# Title: FIT-RAG: Black-Box RAG with Factual Information and Token Reduction\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang\n## Abstract\nDue to the extraordinarily large number of parameters, fine-tuning large language models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a retrieval-augmented generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs\u2019 preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this article proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer which takes the factual information and LLMs\u2019 preferences as labels respectively. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer, which enables FIT-RAG to avoid unnecessary augmentation and reduce augmentation tokens as much as possible. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ, and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.\n## Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) is a technique that augments natural language generation models with relevant content retrieved from knowledge sources, aiming at improving the quality and relevance of text generation.Previous works have demonstrated its strong performance in knowledge-intensive tasks such as question answering, fact checking, and content recommendation [12,24,32,41].\n\nRetrievers interact with external corpus to acquire relevant information.For open-domain question answering, the Wikipedia corpus [6] is commonly used.As for retrieval methods, it can broadly be categorized into two types: sparse retrievers and dense retrievers.Sparse retrievers, such as TF-IDF [37] and BM25 [35], predominantly rely on keyword matching for document retrieval.These methods determine the relevance between queries and documents by analyzing the occurrence and distribution of keywords within the documents.Dense retrievers employ dual-encoders to generate dense vector representations of text for more accurate semantic matching.Consequently, dense retrievers are considered more suitable for retrieval-augmented applications.Some techniques like vector quantization [25,47] and embedding optimization [48] also improves the efficiency of dense retrievers.Common dense retrievers include DPR [20], ANCE [49] and Contriever [13].Specifically, DPR [20] is trained with supervised learning on question-answer pairs, and focuses on extracting relevant passages by analyzing the semantic content of both questions and answers.ANCE [49] leverages approximate nearest neighbor search and contrastive learning to enhance the model's ability to discern between relevant and non-relevant documents in a dense vector space.Contriever [13] employs unsupervised contrastive learning to adapt to inherent data structure, especially beneficial when the annotated training data is scarce.To enhance the quality of retrieved documents, some work conduct further reranking to these documents for personalization [3,40,56] and diversification [27,38].\n\nRecent work has explored different ways for language models to leverage retrieved or generated text as external knowledge.One approach is to integrate retrieval into language model pre-training or fine-tuning.For instance, REALM [9] integrates external document retrieval into pre-training, enhancing performance in downstream tasks by retrieving relevant information.",
            "reference_string": "[268554288 | Mao et al. | 2024 | Citations: 10]"
        },
        {
            "title": "MultiConIR: Towards multi-condition Information Retrieval",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349551985",
                    "name": "Xuan Lu"
                },
                {
                    "authorId": "2349802894",
                    "name": "Sifan Liu"
                },
                {
                    "authorId": "2349465013",
                    "name": "Bochao Yin"
                },
                {
                    "authorId": "2349749730",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2325183464",
                    "name": "Xinghao Chen"
                },
                {
                    "authorId": "2349762405",
                    "name": "Hui Su"
                },
                {
                    "authorId": "2349879907",
                    "name": "Yaohui Jin"
                },
                {
                    "authorId": "2349955598",
                    "name": "Wenjun Zeng"
                },
                {
                    "authorId": "2287612923",
                    "name": "Xiaoyu Shen"
                }
            ],
            "abstract": "In this paper, we introduce MultiConIR, the first benchmark designed to evaluate retrieval models in multi-condition scenarios. Unlike existing datasets that primarily focus on single-condition queries from search engines, MultiConIR captures real-world complexity by incorporating five diverse domains: books, movies, people, medical cases, and legal documents. We propose three tasks to systematically assess retrieval and reranking models on multi-condition robustness, monotonic relevance ranking, and query format sensitivity. Our findings reveal that existing retrieval and reranking models struggle with multi-condition retrieval, with rerankers suffering severe performance degradation as query complexity increases. We further investigate the performance gap between retrieval and reranking models, exploring potential reasons for these discrepancies, and analysis the impact of different pooling strategies on condition placement sensitivity. Finally, we highlight the strengths of GritLM and Nv-Embed, which demonstrate enhanced adaptability to multi-condition queries, offering insights for future retrieval models. The code and datasets are available at https://github.com/EIT-NLP/MultiConIR.",
            "corpus_id": 276928453,
            "sentences": [
                {
                    "corpus_id": "276928453",
                    "title": "MultiConIR: Towards multi-condition Information Retrieval",
                    "text": "Retriever: From Sparse To Dense Traditional sparse retrieval methods are based on BM25 (Robertson and Zaragoza, 2009), TF-IDF (Ramos et al., 2003), etc., rely on keyword matching and statistical weighting to evaluate relevance, which suffers from the well-known issue of lexical gap (Berger et al., 2000), restricting their ability to effectively capture semantic relationships (Luan et al., 2021;Nian et al., 2024). \n\nDense retrieval addresses this limitation by encoding both queries and documents as embeddings within a joint latent space, where the semantic relationship is captured through the similarity scores between their embeddings (Li et al., 2023a). Pretrained language models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), are widely used as backbone encoders for dense retrieval (Li et al., 2023b;Sturua et al., 2024;Xiao et al., 2023). \n\nRecent advancements have shown that LLMs offer significant potential as backbone encoders for dense retrieval (Wang et al., 2024a;Weller et al., 2024c;BehnamGhader et al., 2024). For instance, Repllama (Ma et al., 2023) enhanced retrieval performance by fine-tuning Llama-2 to serve as dense retrievers. GritLM (Muennighoff et al., 2024) unified text embedding and generation within a single LLM. LLM2Vec (BehnamGhader et al., 2024) introduced an unsupervised approach for transforming decoder-only LLMs into dense retrievers. \n\nBenchmarks In Complex Retrieval Tasks Existing datasets for information retrieval, such as MS MARCO (Nguyen et al., 2016), Natural Questions (Kwiatkowski et al., 2019), andMTEB (Muennighoff et al., 2022), primarily focus on queries sourced from search engines. The relationships between queries and documents are typically simple and direct (Su et al., 2024). \n\nRecent studies have expanded retrieval benchmarks to address more complex scenarios.",
                    "score": 0.5586174762351196,
                    "section_title": "Related Works",
                    "char_start_offset": 4870,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 416
                        },
                        {
                            "start": 419,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 865
                        },
                        {
                            "start": 868,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1171
                        },
                        {
                            "start": 1172,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1394
                        },
                        {
                            "start": 1397,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1756
                        },
                        {
                            "start": 1759,
                            "end": 1843
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 87,
                            "end": 117,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 126,
                            "end": 146,
                            "matchedPaperCorpusId": "14638345"
                        },
                        {
                            "start": 283,
                            "end": 304,
                            "matchedPaperCorpusId": "6749682"
                        },
                        {
                            "start": 378,
                            "end": 397,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 699,
                            "end": 720,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 728,
                            "end": 749,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 978,
                            "end": 998,
                            "matchedPaperCorpusId": "266693831"
                        },
                        {
                            "start": 1538,
                            "end": 1569,
                            "matchedPaperCorpusId": "86611921"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90087890625
                }
            ],
            "relevance_judgement": 0.90087890625,
            "relevance_judgment_input_expanded": "# Title: MultiConIR: Towards multi-condition Information Retrieval\n# Venue: arXiv.org\n# Authors: Xuan Lu, Sifan Liu, Bochao Yin, Yongqi Li, Xinghao Chen, Hui Su, Yaohui Jin, Wenjun Zeng, Xiaoyu Shen\n## Abstract\nIn this paper, we introduce MultiConIR, the first benchmark designed to evaluate retrieval models in multi-condition scenarios. Unlike existing datasets that primarily focus on single-condition queries from search engines, MultiConIR captures real-world complexity by incorporating five diverse domains: books, movies, people, medical cases, and legal documents. We propose three tasks to systematically assess retrieval and reranking models on multi-condition robustness, monotonic relevance ranking, and query format sensitivity. Our findings reveal that existing retrieval and reranking models struggle with multi-condition retrieval, with rerankers suffering severe performance degradation as query complexity increases. We further investigate the performance gap between retrieval and reranking models, exploring potential reasons for these discrepancies, and analysis the impact of different pooling strategies on condition placement sensitivity. Finally, we highlight the strengths of GritLM and Nv-Embed, which demonstrate enhanced adaptability to multi-condition queries, offering insights for future retrieval models. The code and datasets are available at https://github.com/EIT-NLP/MultiConIR.\n## Related Works\nRetriever: From Sparse To Dense Traditional sparse retrieval methods are based on BM25 (Robertson and Zaragoza, 2009), TF-IDF (Ramos et al., 2003), etc., rely on keyword matching and statistical weighting to evaluate relevance, which suffers from the well-known issue of lexical gap (Berger et al., 2000), restricting their ability to effectively capture semantic relationships (Luan et al., 2021;Nian et al., 2024). \n\nDense retrieval addresses this limitation by encoding both queries and documents as embeddings within a joint latent space, where the semantic relationship is captured through the similarity scores between their embeddings (Li et al., 2023a). Pretrained language models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), are widely used as backbone encoders for dense retrieval (Li et al., 2023b;Sturua et al., 2024;Xiao et al., 2023). \n\nRecent advancements have shown that LLMs offer significant potential as backbone encoders for dense retrieval (Wang et al., 2024a;Weller et al., 2024c;BehnamGhader et al., 2024). For instance, Repllama (Ma et al., 2023) enhanced retrieval performance by fine-tuning Llama-2 to serve as dense retrievers. GritLM (Muennighoff et al., 2024) unified text embedding and generation within a single LLM. LLM2Vec (BehnamGhader et al., 2024) introduced an unsupervised approach for transforming decoder-only LLMs into dense retrievers. \n\nBenchmarks In Complex Retrieval Tasks Existing datasets for information retrieval, such as MS MARCO (Nguyen et al., 2016), Natural Questions (Kwiatkowski et al., 2019), andMTEB (Muennighoff et al., 2022), primarily focus on queries sourced from search engines. The relationships between queries and documents are typically simple and direct (Su et al., 2024). \n\nRecent studies have expanded retrieval benchmarks to address more complex scenarios.",
            "reference_string": "[276928453 | Lu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Unsupervised Domain Adaption for Neural Information Retrieval",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.09350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258720245",
                    "name": "Carlos Dominguez"
                },
                {
                    "authorId": "1602998334",
                    "name": "Jon Ander Campos"
                },
                {
                    "authorId": "2257267295",
                    "name": "Eneko Agirre"
                },
                {
                    "authorId": "2481918",
                    "name": "Gorka Azkune"
                }
            ],
            "abstract": "Neural information retrieval requires costly annotated data for each target domain to be competitive. Synthetic annotation by query generation using Large Language Models or rule-based string manipulation has been proposed as an alternative, but their relative merits have not been analysed. In this paper, we compare both methods head-to-head using the same neural IR architecture. We focus on the BEIR benchmark, which includes test datasets from several domains with no training data, and explore two scenarios: zero-shot, where the supervised system is trained in a large out-of-domain dataset (MS-MARCO); and unsupervised domain adaptation, where, in addition to MS-MARCO, the system is fine-tuned in synthetic data from the target domain. Our results indicate that Large Language Models outperform rule-based methods in all scenarios by a large margin, and, more importantly, that unsupervised domain adaptation is effective compared to applying a supervised IR system in a zero-shot fashion. In addition we explore several sizes of open Large Language Models to generate synthetic data and find that a medium-sized model suffices. Code and models are publicly available for reproducibility.",
            "corpus_id": 264147039,
            "sentences": [
                {
                    "corpus_id": "264147039",
                    "title": "Unsupervised Domain Adaption for Neural Information Retrieval",
                    "text": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson and Zaragoza, 2009) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages. The two most important differences are: (i) SBERT uses tied encoders (shared weights), whereas DPR uses two independent encoders; (ii) SBERT uses mean pooling to obtain the final vector, while DPR makes use of the [CLS] token. Modern IR models allow fine-grained token-level interaction to improve the performance but with higher inference cost. Two of such models are ColBERT (Khattab and Zaharia, 2020) and SPLADE (Formal et al., 2021). The main difference between DPR and ColBERT is in their approach to encode the document and query representations. ColBERT uses a joint space approach and a late interaction strategy, while DPR uses a dual-encoder architec-ture and a dense retrieval approach.",
                    "score": 0.6578614116945405,
                    "section_title": "Retrieval models",
                    "char_start_offset": 5638,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1333
                        },
                        {
                            "start": 1334,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1805
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 183,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 467,
                            "end": 491,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 518,
                            "end": 546,
                            "matchedPaperCorpusId": "201646309"
                        },
                        {
                            "start": 1523,
                            "end": 1544,
                            "matchedPaperCorpusId": "235792467"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89892578125
                }
            ],
            "relevance_judgement": 0.89892578125,
            "relevance_judgment_input_expanded": "# Title: Unsupervised Domain Adaption for Neural Information Retrieval\n# Venue: arXiv.org\n# Authors: Carlos Dominguez, Jon Ander Campos, Eneko Agirre, Gorka Azkune\n## Abstract\nNeural information retrieval requires costly annotated data for each target domain to be competitive. Synthetic annotation by query generation using Large Language Models or rule-based string manipulation has been proposed as an alternative, but their relative merits have not been analysed. In this paper, we compare both methods head-to-head using the same neural IR architecture. We focus on the BEIR benchmark, which includes test datasets from several domains with no training data, and explore two scenarios: zero-shot, where the supervised system is trained in a large out-of-domain dataset (MS-MARCO); and unsupervised domain adaptation, where, in addition to MS-MARCO, the system is fine-tuned in synthetic data from the target domain. Our results indicate that Large Language Models outperform rule-based methods in all scenarios by a large margin, and, more importantly, that unsupervised domain adaptation is effective compared to applying a supervised IR system in a zero-shot fashion. In addition we explore several sizes of open Large Language Models to generate synthetic data and find that a medium-sized model suffices. Code and models are publicly available for reproducibility.\n## Retrieval models\nIn the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson and Zaragoza, 2009) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages. The two most important differences are: (i) SBERT uses tied encoders (shared weights), whereas DPR uses two independent encoders; (ii) SBERT uses mean pooling to obtain the final vector, while DPR makes use of the [CLS] token. Modern IR models allow fine-grained token-level interaction to improve the performance but with higher inference cost. Two of such models are ColBERT (Khattab and Zaharia, 2020) and SPLADE (Formal et al., 2021). The main difference between DPR and ColBERT is in their approach to encode the document and query representations. ColBERT uses a joint space approach and a late interaction strategy, while DPR uses a dual-encoder architec-ture and a dense retrieval approach.",
            "reference_string": "[264147039 | Dominguez et al. | 2023 | Citations: 0]"
        },
        {
            "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 25,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.18443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265148831",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2263890944",
                    "name": "Wenqi Shi"
                },
                {
                    "authorId": "2218865512",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "8103389",
                    "name": "Yuchen Zhuang"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2237844925",
                    "name": "M. D. Wang"
                },
                {
                    "authorId": "2263536473",
                    "name": "Joyce C. Ho"
                },
                {
                    "authorId": "2256776233",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2237940940",
                    "name": "Carl Yang"
                }
            ],
            "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the lack of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever\u2019s efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at https://huggingface.co/BMRetriever to ensure transparency, reproducibility, and application to new domains.",
            "corpus_id": 269448997,
            "sentences": [
                {
                    "corpus_id": "269448997",
                    "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
                    "text": "Sparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents. \n\nIn our experiments, we consider dense retrieval models at various scales for a comprehensive evaluation: (1) Base Size (<1B parameters), (2) Large Size (1B-5B), and (3) XL Size (>5B). \n\n\u2022 Contriever (Izacard et al., 2022) is a dense retrieval model (110M) pre-trained via contrastive learning on documents sampled from Wikipedia and CC-Net corpora. \n\n\u2022 Dragon (Lin et al., 2023) is a BERT-base-sized dense retrieval model (110M) that undergoes progressive training using a data augmentation approach, incorporating diverse queries and sources of supervision. \n\n\u2022 SPECTER 2.0 (Singh et al., 2023) is a scientific document representation model (110M) pretrained using multi-format representation learning, enabling tailored embeddings for a diverse range of task formats. \n\n\u2022 SciMult (Zhang et al., 2023) is a scientific dense retrieval model (110M) that employs a multitask contrastive learning framework with taskaware specialization and instruction tuning to enhance performance on scientific literature retrieval tasks. \n\n\u2022 COCO-DR (Yu et al., 2022) is a lightweight dense retrieval model (110M) pre-trained using continuous contrastive learning and implicit distributionally robust optimization on domainspecific corpora, enabling adaptation to various downstream tasks, including those in the biomedical domain. \n\n\u2022 QExt (Meng et al., 2022) is a data augmentation method that trains dense retrieval models by selecting salient spans from the original document, and generating pseudo queries using transferred language models. We report the performance of QExt (110M) in the unsupervised dense retrieval setting only.",
                    "score": 0.5482787372362213,
                    "section_title": "C.1 Baselines for Retrieval Tasks in Main Experiments",
                    "char_start_offset": 42174,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 24
                        },
                        {
                            "start": 25,
                            "end": 138
                        },
                        {
                            "start": 141,
                            "end": 386
                        },
                        {
                            "start": 389,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 534
                        },
                        {
                            "start": 537,
                            "end": 720
                        },
                        {
                            "start": 723,
                            "end": 885
                        },
                        {
                            "start": 888,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1306
                        },
                        {
                            "start": 1309,
                            "end": 1558
                        },
                        {
                            "start": 1561,
                            "end": 1852
                        },
                        {
                            "start": 1855,
                            "end": 2066
                        },
                        {
                            "start": 2067,
                            "end": 2157
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 148,
                            "end": 172,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 736,
                            "end": 757,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 897,
                            "end": 915,
                            "matchedPaperCorpusId": "256868909"
                        },
                        {
                            "start": 1112,
                            "end": 1131,
                            "matchedPaperCorpusId": "254018137"
                        },
                        {
                            "start": 1319,
                            "end": 1338,
                            "matchedPaperCorpusId": "258841021"
                        },
                        {
                            "start": 1571,
                            "end": 1587,
                            "matchedPaperCorpusId": "253157773"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                }
            ],
            "relevance_judgement": 0.8955078125,
            "relevance_judgment_input_expanded": "# Title: BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, M. D. Wang, Joyce C. Ho, Chao Zhang, Carl Yang\n## Abstract\nDeveloping effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the lack of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever\u2019s efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at https://huggingface.co/BMRetriever to ensure transparency, reproducibility, and application to new domains.\n## C.1 Baselines for Retrieval Tasks in Main Experiments\nSparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents. \n\nIn our experiments, we consider dense retrieval models at various scales for a comprehensive evaluation: (1) Base Size (<1B parameters), (2) Large Size (1B-5B), and (3) XL Size (>5B). \n\n\u2022 Contriever (Izacard et al., 2022) is a dense retrieval model (110M) pre-trained via contrastive learning on documents sampled from Wikipedia and CC-Net corpora. \n\n\u2022 Dragon (Lin et al., 2023) is a BERT-base-sized dense retrieval model (110M) that undergoes progressive training using a data augmentation approach, incorporating diverse queries and sources of supervision. \n\n\u2022 SPECTER 2.0 (Singh et al., 2023) is a scientific document representation model (110M) pretrained using multi-format representation learning, enabling tailored embeddings for a diverse range of task formats. \n\n\u2022 SciMult (Zhang et al., 2023) is a scientific dense retrieval model (110M) that employs a multitask contrastive learning framework with taskaware specialization and instruction tuning to enhance performance on scientific literature retrieval tasks. \n\n\u2022 COCO-DR (Yu et al., 2022) is a lightweight dense retrieval model (110M) pre-trained using continuous contrastive learning and implicit distributionally robust optimization on domainspecific corpora, enabling adaptation to various downstream tasks, including those in the biomedical domain. \n\n\u2022 QExt (Meng et al., 2022) is a data augmentation method that trains dense retrieval models by selecting salient spans from the original document, and generating pseudo queries using transferred language models. We report the performance of QExt (110M) in the unsupervised dense retrieval setting only.",
            "reference_string": "[269448997 | Xu et al. | 2024 | Citations: 25]"
        },
        {
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 42,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.09612",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264184691",
                    "name": "Noah Ziems"
                },
                {
                    "authorId": "38767143",
                    "name": "W. Yu"
                },
                {
                    "authorId": "72871419",
                    "name": "Zhihan Zhang"
                },
                {
                    "authorId": "2152153656",
                    "name": "Meng Jiang"
                }
            ],
            "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.",
            "corpus_id": 258714822,
            "sentences": [
                {
                    "corpus_id": "258714822",
                    "title": "Large Language Models are Built-in Autoregressive Search Engines",
                    "text": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
                    "score": 0.5755534497756001,
                    "section_title": "Traditional Document Retrievers",
                    "char_start_offset": 4664,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 944
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 148,
                            "end": 178,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 178,
                            "end": 196,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 220,
                            "end": 244,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 558,
                            "end": 578,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 594,
                            "end": 614,
                            "matchedPaperCorpusId": "258865472"
                        },
                        {
                            "start": 637,
                            "end": 655,
                            "matchedPaperCorpusId": "230435683"
                        },
                        {
                            "start": 824,
                            "end": 848,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 848,
                            "end": 867,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 921,
                            "end": 943,
                            "matchedPaperCorpusId": "220302658"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8935546875
                }
            ],
            "relevance_judgement": 0.8935546875,
            "relevance_judgment_input_expanded": "# Title: Large Language Models are Built-in Autoregressive Search Engines\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Noah Ziems, W. Yu, Zhihan Zhang, Meng Jiang\n## Abstract\nDocument retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.\n## Traditional Document Retrievers\nTraditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
            "reference_string": "[258714822 | Ziems et al. | 2023 | Citations: 42]"
        },
        {
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
            "corpus_id": 271571401,
            "sentences": [
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "The retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models. \n\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios. \n\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25. \n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]. \n\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness. \n\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment. \n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples.",
                    "score": 0.5296931925363634,
                    "section_title": "C. Retrieval",
                    "char_start_offset": 21512,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 48
                        },
                        {
                            "start": 49,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 412
                        },
                        {
                            "start": 415,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 939
                        },
                        {
                            "start": 942,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1599
                        },
                        {
                            "start": 1602,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1997
                        },
                        {
                            "start": 2000,
                            "end": 2208
                        },
                        {
                            "start": 2209,
                            "end": 2329
                        },
                        {
                            "start": 2332,
                            "end": 2361
                        },
                        {
                            "start": 2362,
                            "end": 2466
                        },
                        {
                            "start": 2467,
                            "end": 2583
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n## Abstract\nRetrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.\n## C. Retrieval\nThe retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models. \n\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios. \n\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25. \n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]. \n\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness. \n\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment. \n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples.",
            "reference_string": "[271571401 | Gao et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 34,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13729",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13729, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158892171",
                    "name": "Sukmin Cho"
                },
                {
                    "authorId": "8599185",
                    "name": "Soyeong Jeong"
                },
                {
                    "authorId": "2148402840",
                    "name": "Jeong-yeon Seo"
                },
                {
                    "authorId": "2109285560",
                    "name": "Jong C. Park"
                }
            ],
            "abstract": "Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.",
            "corpus_id": 258840999,
            "sentences": [
                {
                    "corpus_id": "258840999",
                    "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
                    "text": "We use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson and Zaragoza, 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers.",
                    "score": 0.5708438319111951,
                    "section_title": "A.3 Retrievers",
                    "char_start_offset": 15196,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 380
                        },
                        {
                            "start": 383,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 702
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 143,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 390,
                            "end": 414,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 516,
                            "end": 537,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Sukmin Cho, Soyeong Jeong, Jeong-yeon Seo, Jong C. Park\n## Abstract\nRe-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.\n## A.3 Retrievers\nWe use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson and Zaragoza, 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers.",
            "reference_string": "[258840999 | Cho et al. | 2023 | Citations: 24]"
        },
        {
            "title": "A Quantitative Analysis of Noise Impact on Document Ranking",
            "venue": "IEEE International Conference on Systems, Man and Cybernetics",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.science/hal-04284004/file/SMC%202023%20-%20A%20Quantitative%20Analysis%20of%20Noise%20Impact.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SMC53992.2023.10394665?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SMC53992.2023.10394665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2181131501",
                    "name": "Edward Giamphy"
                },
                {
                    "authorId": "2212071788",
                    "name": "K\u00e9vin Sanchis"
                },
                {
                    "authorId": "144101598",
                    "name": "G. Dashyan"
                },
                {
                    "authorId": "2281888390",
                    "name": "Jean-Loup Guillaume"
                },
                {
                    "authorId": "2924500",
                    "name": "Ahmed Hamdi"
                },
                {
                    "authorId": "2281878315",
                    "name": "Lilian Sanselme"
                },
                {
                    "authorId": "2241553562",
                    "name": "Antoine Doucet"
                }
            ],
            "abstract": "After decades of massive digitization, a sub-stantial amount of documents exists in digital form. The accessibility of these documents is strongly impacted by the quality of document indexing. Most of these documents are indexed in noisy versions that include numerous errors. The noise can be due to manual input mistakes or optical character recognition process and results in errors like spelling mistakes, missing characters, and others. This paper presents a study of the impact of noise on document ranking, an essential task in natural language processing (NLP) with wide-ranging practical applications. We provide a deep and quantitative analysis of the impact of recognition errors on document ranking by testing two popular ranking models on several noisy versions of a subset of the MS MARCO passage ranking dataset, with various levels and types of noise. Our study provides insights into the challenges of document ranking under noisy conditions and advocates for developing ranking models that are more robust to noise.",
            "corpus_id": 267328301,
            "sentences": [
                {
                    "corpus_id": "267328301",
                    "title": "A Quantitative Analysis of Noise Impact on Document Ranking",
                    "text": "The aim of this study is to evaluate the performance document ranking models on the aforementioned noisy datasets.We use two classical ones, namely BM25 [5] and DistilBERT [6] on the passage ranking task.We use the Pyserini python toolkit to run our experiments [35].\n\nBM25 is a well-known model in the field of information retrieval that has been widely used for various retrieval tasks, such as web search and document ranking.BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks.DistilBERT is a pre-trained model that can be fine-tuned for various NLP tasks, including passage ranking.Precisely, it is a distilled version [36] of BERT [20], retaining 97% performance but being 60% faster and using only half the number of parameters.Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors.By comparing these two types of models in a noisy context, we can gain insight into the strengths and weaknesses of sparse and dense representation models, and how they perform under different levels and types of noise.This comparison is particularly interesting because the two models use fundamentally different approaches to represent text.Moreover, we believe that BM25 and DistilBERT are representative of their family of models and we would expect similar results if we conducted experiments with other models.",
                    "score": 0.5195214984614263,
                    "section_title": "D. Ranking models",
                    "char_start_offset": 16399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 114,
                            "end": 204
                        },
                        {
                            "start": 204,
                            "end": 267
                        },
                        {
                            "start": 269,
                            "end": 429
                        },
                        {
                            "start": 429,
                            "end": 592
                        },
                        {
                            "start": 592,
                            "end": 731
                        },
                        {
                            "start": 731,
                            "end": 837
                        },
                        {
                            "start": 837,
                            "end": 985
                        },
                        {
                            "start": 985,
                            "end": 1153
                        },
                        {
                            "start": 1155,
                            "end": 1245
                        },
                        {
                            "start": 1245,
                            "end": 1286
                        },
                        {
                            "start": 1286,
                            "end": 1352
                        },
                        {
                            "start": 1352,
                            "end": 1367
                        },
                        {
                            "start": 1367,
                            "end": 1586
                        },
                        {
                            "start": 1586,
                            "end": 1710
                        },
                        {
                            "start": 1710,
                            "end": 1883
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 262,
                            "end": 266,
                            "matchedPaperCorpusId": "235366815"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: A Quantitative Analysis of Noise Impact on Document Ranking\n# Venue: IEEE International Conference on Systems, Man and Cybernetics\n# Authors: Edward Giamphy, K\u00e9vin Sanchis, G. Dashyan, Jean-Loup Guillaume, Ahmed Hamdi, Lilian Sanselme, Antoine Doucet\n## Abstract\nAfter decades of massive digitization, a sub-stantial amount of documents exists in digital form. The accessibility of these documents is strongly impacted by the quality of document indexing. Most of these documents are indexed in noisy versions that include numerous errors. The noise can be due to manual input mistakes or optical character recognition process and results in errors like spelling mistakes, missing characters, and others. This paper presents a study of the impact of noise on document ranking, an essential task in natural language processing (NLP) with wide-ranging practical applications. We provide a deep and quantitative analysis of the impact of recognition errors on document ranking by testing two popular ranking models on several noisy versions of a subset of the MS MARCO passage ranking dataset, with various levels and types of noise. Our study provides insights into the challenges of document ranking under noisy conditions and advocates for developing ranking models that are more robust to noise.\n## D. Ranking models\nThe aim of this study is to evaluate the performance document ranking models on the aforementioned noisy datasets.We use two classical ones, namely BM25 [5] and DistilBERT [6] on the passage ranking task.We use the Pyserini python toolkit to run our experiments [35].\n\nBM25 is a well-known model in the field of information retrieval that has been widely used for various retrieval tasks, such as web search and document ranking.BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks.DistilBERT is a pre-trained model that can be fine-tuned for various NLP tasks, including passage ranking.Precisely, it is a distilled version [36] of BERT [20], retaining 97% performance but being 60% faster and using only half the number of parameters.Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors.By comparing these two types of models in a noisy context, we can gain insight into the strengths and weaknesses of sparse and dense representation models, and how they perform under different levels and types of noise.This comparison is particularly interesting because the two models use fundamentally different approaches to represent text.Moreover, we believe that BM25 and DistilBERT are representative of their family of models and we would expect similar results if we conducted experiments with other models.",
            "reference_string": "[267328301 | Giamphy et al. | 2023 | Citations: 2]"
        },
        {
            "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
            "venue": "IEEE International Conference on e-Science",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://zenodo.org/record/8413983/files/2023.conference.escience.nali.camera.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/e-Science58273.2023.10254859?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/e-Science58273.2023.10254859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2085129",
                    "name": "N. Li"
                },
                {
                    "authorId": "2247665349",
                    "name": "Yangjun Zhang"
                },
                {
                    "authorId": "2247992622",
                    "name": "Zhiming Zhao"
                }
            ],
            "abstract": "The discovery and reutilization of scientific codes are crucial in many research activities. Computational notebooks have emerged as a particularly effective medium for sharing and reusing scientific codes. Nevertheless, effectively locating relevant computational notebooks is a significant challenge. First, computational notebooks encompass multi-modal data comprising unstructured text, source code, and other media, posing complexities in representing such data for retrieval purposes. Second, the absence of evaluation datasets for the computational notebook search task hampers fair performance assessments within the research community. Prior studies have either treated computational notebook search as a code-snippet search problem or focused solely on content-based approaches for searching computational notebooks. To address the aforementioned difficulties, we present DeCNR, tackling the information needs of researchers in seeking computational notebooks. Our approach leverages a fused sparse-dense retrieval model to represent computational notebooks effectively. Additionally, we construct an evaluation dataset including actual scientific queries, computational notebooks, and relevance judgments for fair and objective performance assessment. Experimental results demonstrate that the proposed method surpasses baseline approaches in terms of F1@5 and NDCG@5. The proposed system has been implemented as a web service shipped with REST APIs, allowing seamless integration with other applications and web services.",
            "corpus_id": 262947262,
            "sentences": [
                {
                    "corpus_id": "262947262",
                    "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
                    "text": "Sparse retrieval models [8] usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning. We propose a fusion-based approach to overcome this issue, combining sparse and dense retrieval models to derive the final ranking. Hybrid retrieval methods have been studied to improve the effectiveness of a retrieval system via the fusion of different retrieval strategies [23]- [25]. Chen et al. [25] propose a simple yet effective zero-shot hybrid model that combines BM25 with NPR [26] to address the out-of-domain generalization problem. They also point out that the dense retrieval models could be incompetent in modeling long documents, and sparse retrieval models can compensate for these weaknesses. Computational notebooks and scientific queries are out-of-domain data for most large-scale pre-trained language models, e.g., SBERT [15] and computational notebook retrieval also faces the long document problem. Therefore, the combination of sparse and dense models can be a potential solution for out-ofdomain and long document problems in computational notebook retrieval. Hence, different from the previous studies [4], [5] that use only one type of model, we propose a fused sparsedense model for computational notebook retrieval.",
                    "score": 0.5233039728961253,
                    "section_title": "C. Hybrid Sparse-dense Retrieval",
                    "char_start_offset": 7551,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1635
                        },
                        {
                            "start": 1636,
                            "end": 1795
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 24,
                            "end": 27,
                            "matchedPaperCorpusId": "41563977"
                        },
                        {
                            "start": 932,
                            "end": 936,
                            "matchedPaperCorpusId": "246285831"
                        },
                        {
                            "start": 950,
                            "end": 954,
                            "matchedPaperCorpusId": "246285831"
                        },
                        {
                            "start": 1037,
                            "end": 1041,
                            "matchedPaperCorpusId": "243865679"
                        },
                        {
                            "start": 1679,
                            "end": 1682,
                            "matchedPaperCorpusId": "231749675"
                        },
                        {
                            "start": 1684,
                            "end": 1687,
                            "matchedPaperCorpusId": "245144651"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks\n# Venue: IEEE International Conference on e-Science\n# Authors: N. Li, Yangjun Zhang, Zhiming Zhao\n## Abstract\nThe discovery and reutilization of scientific codes are crucial in many research activities. Computational notebooks have emerged as a particularly effective medium for sharing and reusing scientific codes. Nevertheless, effectively locating relevant computational notebooks is a significant challenge. First, computational notebooks encompass multi-modal data comprising unstructured text, source code, and other media, posing complexities in representing such data for retrieval purposes. Second, the absence of evaluation datasets for the computational notebook search task hampers fair performance assessments within the research community. Prior studies have either treated computational notebook search as a code-snippet search problem or focused solely on content-based approaches for searching computational notebooks. To address the aforementioned difficulties, we present DeCNR, tackling the information needs of researchers in seeking computational notebooks. Our approach leverages a fused sparse-dense retrieval model to represent computational notebooks effectively. Additionally, we construct an evaluation dataset including actual scientific queries, computational notebooks, and relevance judgments for fair and objective performance assessment. Experimental results demonstrate that the proposed method surpasses baseline approaches in terms of F1@5 and NDCG@5. The proposed system has been implemented as a web service shipped with REST APIs, allowing seamless integration with other applications and web services.\n## C. Hybrid Sparse-dense Retrieval\nSparse retrieval models [8] usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning. We propose a fusion-based approach to overcome this issue, combining sparse and dense retrieval models to derive the final ranking. Hybrid retrieval methods have been studied to improve the effectiveness of a retrieval system via the fusion of different retrieval strategies [23]- [25]. Chen et al. [25] propose a simple yet effective zero-shot hybrid model that combines BM25 with NPR [26] to address the out-of-domain generalization problem. They also point out that the dense retrieval models could be incompetent in modeling long documents, and sparse retrieval models can compensate for these weaknesses. Computational notebooks and scientific queries are out-of-domain data for most large-scale pre-trained language models, e.g., SBERT [15] and computational notebook retrieval also faces the long document problem. Therefore, the combination of sparse and dense models can be a potential solution for out-ofdomain and long document problems in computational notebook retrieval. Hence, different from the previous studies [4], [5] that use only one type of model, we propose a fused sparsedense model for computational notebook retrieval.",
            "reference_string": "[262947262 | Li et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2085891275",
                    "name": "Iman Saberi"
                },
                {
                    "authorId": "2283136625",
                    "name": "Fatemeh Fard"
                }
            ],
            "abstract": "Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly improved code generation, but, they frequently face difficulties when dealing with challenging and complex problems. Retrieval-Augmented Generation (RAG) addresses this issue by retrieving and integrating external knowledge at the inference time. However, retrieval models often fail to find most relevant context, and generation models, with limited context capacity, can hallucinate when given irrelevant data. We present a novel framework that leverages a Programming Knowledge Graph (PKG) to semantically represent and retrieve code. This approach enables fine-grained code retrieval by focusing on the most relevant segments while reducing irrelevant context through a tree-pruning technique. PKG is coupled with a re-ranking mechanism to reduce even more hallucinations by selectively integrating non-RAG solutions. We propose two retrieval approaches-block-wise and function-wise-based on the PKG, optimizing context granularity. Evaluations on the HumanEval and MBPP benchmarks show our method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art models by up to 34% on MBPP. Our contributions include PKG-based retrieval, tree pruning to enhance retrieval precision, a re-ranking method for robust solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic code augmentation with relevant comments and docstrings.",
            "corpus_id": 273549875,
            "sentences": [
                {
                    "corpus_id": "273549875",
                    "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
                    "text": "Additionally, we use two types of retrievers: BM25 as a sparse retriever (SR) and Voyage-Code-2 as a dense retriever (DR). \n\nTo analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data. \n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods. \n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data. \n\nFinally, when comparing Func-PKG to Block-PKG, the results demonstrate that leveraging more granular data, particularly at the block level, significantly enhances model accuracy. Block-PKG enhances precision by retrieving relevant individual code blocks instead of entire functions. This approach involves pruning irrelevant branches from the DAG associated with the selected blocks, ensuring that only the most pertinent contextual information is leveraged. By focusing on finergrained code structures, Block-PKG achieves superior performance across most models, offering a more targeted and efficient retrieval process. \n\nRQ3: Which problem topics benefit more from RAG, and which benefit less? \n\nThis research question explores the performance of RAG across various problem categories.",
                    "score": 0.578692546686022,
                    "section_title": "RQ1: Does PKG improve code generation?",
                    "char_start_offset": 23753,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 125,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 717
                        },
                        {
                            "start": 720,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1070
                        },
                        {
                            "start": 1073,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1415
                        },
                        {
                            "start": 1418,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2039
                        },
                        {
                            "start": 2042,
                            "end": 2114
                        },
                        {
                            "start": 2117,
                            "end": 2206
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8857421875
                }
            ],
            "relevance_judgement": 0.8857421875,
            "relevance_judgment_input_expanded": "# Title: Context-Augmented Code Generation Using Programming Knowledge Graphs\n# Venue: arXiv.org\n# Authors: Iman Saberi, Fatemeh Fard\n## Abstract\nLarge Language Models (LLMs) and Code-LLMs (CLLMs) have significantly improved code generation, but, they frequently face difficulties when dealing with challenging and complex problems. Retrieval-Augmented Generation (RAG) addresses this issue by retrieving and integrating external knowledge at the inference time. However, retrieval models often fail to find most relevant context, and generation models, with limited context capacity, can hallucinate when given irrelevant data. We present a novel framework that leverages a Programming Knowledge Graph (PKG) to semantically represent and retrieve code. This approach enables fine-grained code retrieval by focusing on the most relevant segments while reducing irrelevant context through a tree-pruning technique. PKG is coupled with a re-ranking mechanism to reduce even more hallucinations by selectively integrating non-RAG solutions. We propose two retrieval approaches-block-wise and function-wise-based on the PKG, optimizing context granularity. Evaluations on the HumanEval and MBPP benchmarks show our method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art models by up to 34% on MBPP. Our contributions include PKG-based retrieval, tree pruning to enhance retrieval precision, a re-ranking method for robust solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic code augmentation with relevant comments and docstrings.\n## RQ1: Does PKG improve code generation?\nAdditionally, we use two types of retrievers: BM25 as a sparse retriever (SR) and Voyage-Code-2 as a dense retriever (DR). \n\nTo analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data. \n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods. \n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data. \n\nFinally, when comparing Func-PKG to Block-PKG, the results demonstrate that leveraging more granular data, particularly at the block level, significantly enhances model accuracy. Block-PKG enhances precision by retrieving relevant individual code blocks instead of entire functions. This approach involves pruning irrelevant branches from the DAG associated with the selected blocks, ensuring that only the most pertinent contextual information is leveraged. By focusing on finergrained code structures, Block-PKG achieves superior performance across most models, offering a more targeted and efficient retrieval process. \n\nRQ3: Which problem topics benefit more from RAG, and which benefit less? \n\nThis research question explores the performance of RAG across various problem categories.",
            "reference_string": "[273549875 | Saberi et al. | 2024 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "262066417",
            "title": "Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change",
            "text": "Two types of IR models were investigated for this benchmark: BM25 [21], a sparse retrieval, and Dense Passage Retrieval (DPR) [22], a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack.",
            "score": 0.7525516582411123,
            "section_title": "Benchmark Setup",
            "char_start_offset": 26407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 738
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], \n\nto probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance. \n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [51,54], late interaction [20]. Recently, researchers have also investigated pre-training tasks for dense retrieval [12,33]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually come with considerable memory demands and computational overheads.",
            "score": 0.7525128919604424,
            "section_title": "Sparse and dense retrieval models",
            "char_start_offset": 3413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1650
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "12255087"
                },
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 412,
                    "end": 415,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 649,
                    "end": 652,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 652,
                    "end": 655,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1417,
                    "end": 1420,
                    "matchedPaperCorpusId": "251718954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "238259539",
            "title": "A proposed conceptual framework for a representational approach to information retrieval",
            "text": "Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as Karpukhin et al. [2020] did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation. \n\nThis taxonomy immediately points to the existence of two other classes of logical scoring models. In fact, they correspond to models described in the literature that we can now categorize and unify in a single conceptual framework: \n\nLearned sparse representations The existence of learned dense representations such as DPR and unsupervised sparse representations such as BM25 suggests that there should exist a class of learned sparse representations. \n\nLearning sparse representations is by no means a new idea. If we fix the dimensions of the output representation to be the vocabulary (i.e., retaining a bag-of-words assumption), models for learned sparse representations become term weighting models-that is, a supervised machine learning approach to learning term weights. The earliest example I am aware of is Gordon [1988], who applied (what we might today call) representational learning on boolean vectors of descriptors using genetic algorithms, based on a small set of relevance judgments.",
            "score": 0.7138686668279854,
            "section_title": "Generalization of Logical Scoring Models",
            "char_start_offset": 12726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 154,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Baselines. Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents. Generally, most existing dense retrieval methods adhere to the cascade training paradigm [15,20,21]. Therefore, to facilitate easier comparison in future studies on our dataset, we simplify the training process as illustrated in Figure 5 as in [15,20]. Specifically, we utilize the dual-encoder (DE) as the architecture of dense retrieval models, which is illustrated in Figure 6(a). The following methods are employed as our baselines to evaluate the retrieval performance on T 2 Ranking. \n\n\u2022 QL (query likelihood) [19] is a representative statistical language model that measures the relevance of passages by modeling the generation of a query. \u2022 BM25 [23] is a widely-used sparse retrieval baseline. \n\n\u2022 DE w/ BM25 Neg is equivalent to DPR [12], which is the first work that uses the pre-trained language model as the backbone for the passage retrieval task. \u2022 DE w/ Mined Neg enhance the performance of DPR by sampling hard negatives globally from the entire corpus as in ANCE [28] and RocketQA [21]. \n\n\u2022 DPTDR [25] is the first work that employs prompt tuning for dense retrieval. Among them, QL and BM25 are sparse retrieval models, whereas the others are dense retrieval models Implementation details. BM25 is implemented by Pyserini [14] with default parameters. The dual-encoder models are implemented Metrics. The following evaluation metrics are used in our experiments to examine the retrieval performance of baselines on T 2 Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved passages (MRR@10), ( 2) Recall for the top- retrieved passages (Recall@). Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages.",
            "score": 0.7022810493487253,
            "section_title": "Retrieval Performance",
            "char_start_offset": 19754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "238419331",
            "title": "Adversarial Retriever-Ranker for dense text retrieval",
            "text": "The comparison of retrieval performance on NQ, Trivia QA, and MS-MARCO are presented in Table 1. \n\nWe compare AR2-G with previous state-of-the-art methods, including sparse and dense retrieval models. The top block shows the performance of sparse retrieval methods. BM25 (Yang et al., 2017) is a traditional sparse retriever based on the exact term matching. DeepCT (Dai & Callan, 2019) uses BERT to dynamically generate lexical weights to augment BM25 Systems. doc2Query (Nogueira et al., 2019b), docTTTTTQuery (Nogueira et al., 2019a), and GAR (Mao et al., 2021a) use text generation to expand queries or documents to make better use of BM25. The middle block lists the results of strong dense retrieval methods, including DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), RDR (Yang & Seo, 2020), RocketQA (Qu et al., 2021), Joint and Individual Top-k (Sachan et al., 2021a), PAIR (Ren et al., 2021), DPR-PAQ (Oguz et al., 2021), Condenser (Gao & Callan, 2021b). coCondenser (Gao & Callan, 2021a), ME-BERT (Luan et al., 2021), CoIL (Gao et al., 2021). These methods improve the performance of dense retrieval by constructing hard negative samples, jointly training the retriever and downstream tasks, pre-training, knowledge distillation, and multi-vector representations. \n\nThe bottom block in Table 1 shows the results of proposed AR2 models. AR2-G 0 refers to the warm-up retrieval model in AR2 (details can be found in section 4.2) which leverages the existing continuous pre-training technique for dense text retrieval tasks. i.e., it shows a better performance compared with DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021), etc approaches that do not adopt the continuous pre-training procedure.",
            "score": 0.6925030131251146,
            "section_title": "Performance of Retriever AR2-G:",
            "char_start_offset": 14056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1721
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 290,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 366,
                    "end": 386,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 546,
                    "end": 564,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 729,
                    "end": 753,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 815,
                    "end": 832,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 890,
                    "end": 908,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1015,
                    "end": 1034,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1041,
                    "end": 1059,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "We use representative samples of models from different families: \n\n-BM25 [42] is a representative sparse retrieval model that estimates the relevance based on term frequency, document length, and document frequency. -DPR [19] is a representative dense retrieval model that uses dual-encoder architecture and is trained with in-batch negatives and a few hard negatives selected with BM25. -BART [23] is a Seq2Seq model applicable for sequence generation tasks. Following [3,7], we extract the query-title pairs from each dataset and fine-tune the BART for generative retrieval. -CorpusBrain [3] is a pre-trained generative IR model for knowledge-intensive language tasks. We fine-tune CorpusBrain on every specific downstream KILT task.",
            "score": 0.6858816180856667,
            "section_title": "Retrieval models",
            "char_start_offset": 14328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 67,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 735
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 473,
                    "end": 475,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 590,
                    "end": 593,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "267200117",
            "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
            "text": "Retrieval-augmented Generation (RAG) has been considered as an effective method to overcome several limitations of LLMs, such as hallucinations [21,46], factuality [52], long-term memory [64] and etc [11]. Usually, an external retriever is first used to retrieve relevant textual knowledge from one specific knowledge source (e.g., Wikipedia), then the reader takes the relevant textual knowledge as external context for generating knowledge-grounded response [25]. Most of previous works try to optimize the retriever and reader independently [11]. During the initial phases, people use sparse retriever, such as BM25 [41] and TF-IDF [59], to make relevance decisions and retrieve corresponding evidence. \n\nHowever, sparse approaches fall short in extracting the semantic features inherent in text content [13]. To overcome this issue, researchers have proposed language model-based dense retrieval methods by encoding documents and queries as dense vectors, which effectively represent the semantic features of text content [4,22,26]. As the first dense retriever, DPR [22] uses two pre-trained language models to encode documents and queries separately, allowing for a more nuanced understanding of the content. In addition, Glass et al. [12] propose a retrieve and re-rank framework for leveraging both advantages of sparse retrieval and dense retrieval. There are also several attempts which optimize the retriever and reader simultaneously [1]. For example, Guu et al. [15] augment language model pretraining with a latent knowledge retriever, allowing the model to retrieve and attend over documents used during pre-training, fine-tuning and inference. However, the complex architectures and interactions make them infeasible and inefficient in the era of LLMs. \n\nMore recently, there are a handful of works exploring the performance of LLMs as retriever [30,45,50,73].",
            "score": 0.6842279187839765,
            "section_title": "Retrieval-augmented Generation",
            "char_start_offset": 9479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1876
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 148,
                    "end": 151,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "247411350"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "18303048"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 1026,
                    "end": 1029,
                    "matchedPaperCorpusId": "253080555"
                },
                {
                    "start": 1029,
                    "end": 1032,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.",
            "score": 0.6841357575321563,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "258841762",
            "title": "BM25 Query Augmentation Learned End-to-End",
            "text": "We propose a novel approach for learning to augment BM25 end-to-end with a language model. Our method improves over BM25 on three different datasets while retaining its efficiency. Additionally, we show that such improvements are able to generalize out-of-distribution. With its simple formulation, our method can be easily integrated into existing sparse retrieval frameworks. And we believe it might serve as a stronger sparse baseline for future work in retrieval.",
            "score": 0.6835828535296633,
            "section_title": "Conclusion",
            "char_start_offset": 11865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5634765625
        },
        {
            "corpus_id": "269983057",
            "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
            "text": "\u2022 Dense models: We focus on two models: MultiQA embeddings (Talmor & Berant, 2019;Wang et al., 2020) and Siamese network embeddings (SIAM) (Koch et al., 2015).\n\n\u2022 Sparse models: We focus on BM25 (Robertson et al., 2009), a widely-used information retrieval technique which employs a probabilistic model to rank documents based on the frequency and distribution of query terms within them.\n\n\u2022 Hybrid models: We consider ensembles of Dense and Sparse base retrievers where the output is re-ranked using a cross-encoder model (Yadav et al., 2022).We refer to the models as DPR (SIAM plus BM25) and DPRV2 (MultiQA plus BM25) bellow.\n\nOur analysis covers a spectrum of retrieval models, including contemporary models like MultiQA from Sentence Transformers and Cross-encoders in DPR and DPRV2, which are among the most used in the community (resp.1.6M and 1.3M monthly downloads on HuggingFace, at the time of publication).BM25 is a standard bearer in Information Retrieval known for its robustness over modern methods.Our set of models is a combination of dense, sparse and hybrid models to ensure that our results are representative of all main classes.\n\nLLM Variants.We employ Mistral-7B, LlamaV2-13B and LlamaV2-70B (Jiang et al., 2023;Touvron et al., 2023).We chose these three LLMs with the objective of investigating the spectrum of performance across different scales, aiming to gain insights into how the size of a model influences its language processing capabilities.These models offer a balance between advanced features, optimal performance at the  1.Description of all four tasks used in the experiment benchmark.Word count is computed using NLKT word tokenizer and punctuation remover (Bird et al., 2009).\n\ntime of the publication, community support, and practical considerations like resource availability and computational efficiency.Our original analysis also considered LlamaV2-7B, Falcon-40B and Alpaca-13B, which we discarded as they were consistently outperformed.",
            "score": 0.6685040536514589,
            "section_title": "RAG Pipelines",
            "char_start_offset": 20845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 161,
                    "end": 388
                },
                {
                    "start": 390,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 628
                },
                {
                    "start": 630,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1014
                },
                {
                    "start": 1014,
                    "end": 1150
                },
                {
                    "start": 1152,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1559
                },
                {
                    "start": 1559,
                    "end": 1622
                },
                {
                    "start": 1622,
                    "end": 1715
                },
                {
                    "start": 1717,
                    "end": 1846
                },
                {
                    "start": 1846,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "13874643"
                },
                {
                    "start": 195,
                    "end": 219,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "248496043",
            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
            "text": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels [26]. Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers [10,17,18,23,24,28,30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" [32], and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value [26]. \n\nOn the other hand, learned sparse models [2,5,6,8,9,14,19,22,31], also strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformerbased deep language model re-rankers [21]. They use transformerbased language models to learn term weights, and achieve comparable effectiveness to dense retrievers.",
            "score": 0.6676050059165934,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 977,
                    "end": 980,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 1812,
                    "end": 1814,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1816,
                    "end": 1818,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "258048596",
            "title": "Learning to Tokenize for Generative Retrieval",
            "text": "Sparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24]. \n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1]. \n\nGenerative retrieval. Generative retrieval is increasing gaining attention. It retrieves documents by generating their docid using a generative model like T5. Generative retrieval presents an end-toend solution for document retrieval tasks [28,42] and allows for better exploitation of the capabilities of large generative LMs [1].",
            "score": 0.6658189543339339,
            "section_title": "RELATED WORK",
            "char_start_offset": 36186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "3666085"
                },
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "155300"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1057,
                    "end": 1060,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1633,
                    "end": 1636,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1700,
                    "end": 1703,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "237562875",
            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
            "text": "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT. In this paper, we use the popular dense passage retriever (DPR) model (Karpukhin et al., 2020) as our main evaluation, 3 and we also report the evaluation of REALM (Guu et al., 2020) in Appendix A. DPR models the retrieval problem using two encoders, namely the question and the passage encoders, initialized using BERT. DPR uses a contrastive objective during training, with in-batch negatives and hard negatives mined from BM25. During inference, a pre-defined large set of passages (e.g., 21-3 The detailed experimental settings are in Appendix B. million passages in English Wikipedia) are encoded and pre-indexed-for any test question, the top passages with the highest similarity scores are returned. Recently, other advances have been made in improving dense retrieval, including incorporating better hard negatives (Xiong et al., 2021;Qu et al., 2021), or fine-grained phrase retrieval (Lee et al., 2021). We leave them for future investigation.\n\nGeneralization problem Despite the impressive in-domain performance of dense retrievers, their capability of generalizing to unseen questions still remains relatively under-explored. Recently, Lewis et al. (2021a) discover that there is a large overlap between training and testing sets on popular QA benchmarks, concluding that current models tend to memorize training questions and perform significantly worse on non-overlapping questions. AmbER (Chen et al., 2021) test sets are designed to",
            "score": 0.6642875365418571,
            "section_title": "Background and Related Work",
            "char_start_offset": 2371,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "273346484",
            "title": "A Large Language Model-based Framework for Semi-Structured Tender Document Retrieval-Augmented Generation",
            "text": "Retrieval metrics in retrieval-augmented generation include sparse-vector retrieval (using methods like TF-IDF and BM25), dense-vector retrieval (leveraging pretrained language models for encoding text), and task-specific retrieval (learning a customized retrieval metric optimized for specific objectives). These metrics enhance the relevance and quality of retrieved examples to improve the final generation output. Retrieval metrics play a vital role in selecting relevant examples. These metrics can be classified into sparse-vector retrieval, which involves methods like TF-IDF and BM25 [8], enabling efficient keyword matching using inverted indexes. On the other hand, dense-vector retrieval leverages pre-trained language models, such as those based on BERT, to encode text into low-dimensional dense vectors [9]. Retrieval-augmented text generation has emerged as a promising approach in computational linguistics. Its effectiveness and success rely on three key components: retrieval source, retrieval metric, and integration methods.",
            "score": 0.6622784535308527,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1044
                }
            ],
            "ref_mentions": [
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "245334864",
            "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
            "text": "We consider two retrieval architectures. BM25 (Robertson, 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval.",
            "score": 0.6617321320466075,
            "section_title": "Retrieval models",
            "char_start_offset": 7468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 778
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 63,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 266,
                    "end": 290,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "255825673",
            "title": "Do the Findings of Document and Passage Retrieval Generalize to the Retrieval of Responses for Dialogues?",
            "text": "A supervised sparse retrieval model can take advantage of the effectiveness of transformer-based language models by changing the terms' weights from collection statistics to something that is learned. Document expansion with a learned model can be considered a learned sparse retrieval approach [19]. The core idea is to create pseudo documents that have expanded terms and use them instead when doing retrieval. Doc2query [25] is a strong supervised sparse retrieval baseline that uses a language model to predict queries that might be issued to find a document. The predictions of this model are used to create the augmented pseudo documents. \n\nContext for F3 and F4 Supervised dense retrieval models7 , such as ANCE [46] and coCodenser [7], represent query and documents in a small fixed-length space, for example of 768 dimensions. Dense retrieval models without access to target data for training-known as the zero-shot scenario-have underperformed sparse methods (F3). For example, the BEIR benchmark [41] showed that BM25 was superior to dense retrieval from 9-18 (depending on the model) out of the 18 datasets in the zero-shot scenario. In contrast, when having access to enough supervision from target data, dense retrieval models have shown to consistently outperform strong sparse baselines [7,15,34]",
            "score": 0.6612900438720566,
            "section_title": "Context for F2",
            "char_start_offset": 6529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1312
                }
            ],
            "ref_mentions": [
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "208612557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "264147039",
            "title": "Unsupervised Domain Adaption for Neural Information Retrieval",
            "text": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson and Zaragoza, 2009) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages. The two most important differences are: (i) SBERT uses tied encoders (shared weights), whereas DPR uses two independent encoders; (ii) SBERT uses mean pooling to obtain the final vector, while DPR makes use of the [CLS] token. Modern IR models allow fine-grained token-level interaction to improve the performance but with higher inference cost. Two of such models are ColBERT (Khattab and Zaharia, 2020) and SPLADE (Formal et al., 2021). The main difference between DPR and ColBERT is in their approach to encode the document and query representations. ColBERT uses a joint space approach and a late interaction strategy, while DPR uses a dual-encoder architec-ture and a dense retrieval approach.",
            "score": 0.6578614116945405,
            "section_title": "Retrieval models",
            "char_start_offset": 5638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 183,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 467,
                    "end": 491,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 518,
                    "end": 546,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1523,
                    "end": 1544,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "273185905",
            "title": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA",
            "text": "Retrieval Traditional retrieval techniques, such as TF-IDF (Jones, 1972) and BM25 (Robertson et al., 1995;Robertson & Zaragoza, 2009), retrieve information based on word terms. Subsequently, deep learning-based retrieval methods quickly became popular. REALM (Guu et al., 2020) augments the language model pre-training with a latent knowledge retriever using masked language modeling. DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) encodes queries and documents as dense embeddings, with similarity computed between them. ColBERT (Khattab & Zaharia, 2020;Santhanam et al., 2022) produces multi-vector representations at the token level. JPR (Joint Passage Retrieval) (Min et al., 2021) is a joint passage retrieval model with an autoregressive reranker that selects a sequence of passages. DHR (Dense Hierarchical Retrieval) (Liu et al., 2021) leverages both macroscopic document-level semantics and microscopic passage-level semantics. Fusion-in-Decoder (Izacard & Grave, 2021) employs both DPR and BM25 in a knowledge distillation manner, which does not require annotated query-document pairs. CPT-text (Neelakantan et al., 2022) utilizes contrastive pre-training on unsupervised data. NCI (Wang et al., 2022) directly generates relevant document identifiers for a given query. Atlas (Izacard et al., 2022) fine-tunes an encoder-decoder model with a retriever to address knowledge-intensive tasks with minimal training examples. RETRO (Borgeaud et al., 2022;Wang et al., 2023a) conditions on document chunks based on local similarity with preceding tokens. HHR (Hybrid Hierarchical Retrieval) (Arivazhagan et al., 2023) combines sparse and dense retrieval methods across both document and passage retrieval stages. SimLM (Wang et al., 2023b) proposes a new loss function to reduce the mismatch between pre-training and fine-tuning input distributions.",
            "score": 0.6484691488592688,
            "section_title": "RELATED WORK",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 106,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 106,
                    "end": 133,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 259,
                    "end": 277,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 415,
                    "end": 439,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 538,
                    "end": 563,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 563,
                    "end": 586,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 675,
                    "end": 692,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 833,
                    "end": 851,
                    "matchedPaperCorpusId": "240288895"
                },
                {
                    "start": 963,
                    "end": 986,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1200,
                    "end": 1219,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "258170263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "In this section, we will introduce some widely-adopted passage retrieval models (including both sparse and dense models) for experiments. BM25 BM25 is the most widely used term-based passage retrieval method. Practically, BM25 ranks a set of passages based on the  query terms appearing in each passage, regardless of their proximity within the passage. Doc2Query [39] Doc2Query is still a term-based passage retrieval method. Doc2Query alleviates the term mismatch problem in the BM25 via training a neural sequence-to-sequence model to generate potential queries from passages, and indexes the queries as passage expansion terms. Different from the BM25 method, the implementation of the doc2query method relies on labeled query-passage pairs. DPR [26] DPR is the most widely used dense passage retrieval method, which provides a strong baseline performance. It learns dense embeddings for the query and passage with a BERT-based encoder separately. The embeddings of query and passage are then fed into a \"similarity\" function to compute the final relevance score. The retrieval performance of the DPR model is mainly determined by two factors: the BERT backbone network and the labeled query-passage dataset adopted. Therefore, in order to gain a deep understanding of the DPR model in domain passage retrieval, we conduct various settings based on the DPR model architecture by replacing the original BERT model with a BERT model that has continuously trained on in-domain raw text (DPR-2) or leveraging different domain labeled datasets to carry out the training process (DPR-1).",
            "score": 0.6428008563739325,
            "section_title": "Methods",
            "char_start_offset": 19895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "238744204",
            "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?",
            "text": "Text retrieval is a crucial component for a wide range of knowledge-intensive NLP systems, such as open-domain question answering (ODQA) models and search engines. Recently, dense retrievers (Karpukhin et al., 2020;Xiong et al., 2021) have gained popularity and demonstrated strong performance on a number of retrieval tasks. Dense retrievers employ deep neural networks to learn continuous representations for the queries and documents, and perform retrieval in this dense embedding space using nearest neighbor search (Johnson et al., 2019). Compared to traditional sparse retrievers that rely on discrete bag-of-words representations, dense retrievers can derive more semantically expressive embeddings, thanks to its end-to-end learnability and powerful pre-trained encoders. This helps dense retrievers to overcome several inherent limitations of sparse systems such as vocabulary mismatch (where different words are used for the same meaning) and semantic mismatch (where the same word has multiple meanings).\n\nOn the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, Karpukhin et al. (2020) show that DPR, unlike a sparse BM25 retriever (Robertson and Walker, 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, Sciavolino et al. (2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and Thakur et al. (2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks.\n\nWith dense and sparse retrievers each having their own distinctive pros and cons, researchers have long aspired to develop retriever models that combine the strengths of both. This, however, has proven challenging as dense and sparse retrievers are supported by drastically different algorithms and data structures (inverted index (Bialecki et al.,",
            "score": 0.6419640583715838,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 215,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 215,
                    "end": 234,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 520,
                    "end": 542,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1174,
                    "end": 1197,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1244,
                    "end": 1272,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 1540,
                    "end": 1564,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1652,
                    "end": 1672,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "237940558",
            "title": "MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents",
            "text": "We first evaluate the performance of BM25, DPR and fine-tuned DPR on a passage retrieval task. The query is the combination of current turn and dialogue history from latest to earliest turn up to 128 tokens. Table 5 presents the retrieval results on the validation set. BM25 performs better than DPR-nq but worse than DPR-ft. However, it almost shows no difference for D token and D struct . DPRft shows significant improvement over DPR-nq. Both DPR-nq and DPR-ft seem to benefit from  the document-based structure as they show better performance on D struct than D token . Table 4 and 6 present the evaluation results on test and validation set for the two tasks respectively. All numbers on in tables are the mean of three runs with different random seeds. We omit the standard deviation numbers as they suggest low variance in our experiments. Even though BM25 outperforms DPR-nq in Table 5, BM25 performs much worse than DPR-nq for the generation tasks as shown in Table 4 and 6. RAG models with different DPR-based retrievers generally perform better with D struct than D token on the generation tasks. This is consistent with the DPR-based retrieval results in Table 5. RAG models with DPR-ft show improvement for both D struct and D token over the ones with DPR-nq, which confirms the importance of positive and negative examples even in small quantity (Karpukhin et al., 2020;Khattab et al., 2020b). We also see the retrieval performance gap between D token and D struct is reduced after training the fine-tuned question encoder in RAG. Overall, the retrieval performances for the two tasks seem comparable but generation metric scores for Task II are much lower Task I as the agent responses are free-formed natural language.",
            "score": 0.6399166808812803,
            "section_title": "Passage Retrieval Results",
            "char_start_offset": 20547,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1360,
                    "end": 1384,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.374267578125
        },
        {
            "corpus_id": "266230831",
            "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval",
            "text": "After the introduction of dense retrieval models, researchers soon realized that transformer-based encoders could also be coaxed to generate sparse representations, where the vector basis, for example, spans the input vocabulary space. Another way to view these so-called sparse retrieval models is to contrast them with BM25: whereas BM25 term weights are assigned using a heuristic scoring function, sparse retrieval models assign term weights that are learned using pretrained transformers such as BERT. Examples of these learned sparse retrieval models include DeepImpact [29], uniCOIL [24,53], SPLADE [12], as well as many others. \n\nRecently, Lin [23] made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models. The conceptual framework for mono-lingual retrieval provides us with a basis for organizing cross-lingual retrieval approaches, which we discuss next.",
            "score": 0.6368501494121173,
            "section_title": "MONO-LINGUAL RETRIEVAL OVERVIEW",
            "char_start_offset": 6802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1599
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 580,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 652,
                    "end": 656,
                    "matchedPaperCorpusId": "238259539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "277113527",
            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
            "text": "Recent advancements in Retrieval-Augmented Generation (RAG) systems have increasingly focused on overcoming the limitations of using a single retrieval modality by fusing the strengths of both dense and sparse retrieval approaches. Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers & Gurevych, 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024) To address these complementary weaknesses, hybrid retrieval methods have been proposed. These methods combine the scores obtained from dense and sparse retrieval, typically through linear weighted fusion or techniques such as Reciprocal Rank Fusion (RRF). For example, the Sawarkar et al. (2024) demonstrates that integrating semantic search techniques with sparse encoder indexes can significantly enhance retrieval performance on benchmarks such as NQ and TREC-COVID, leading to improved overall accuracy in RAG systems (Zhang et al., 2024).",
            "score": 0.6318915718924702,
            "section_title": "HYBRID RETRIEVAL",
            "char_start_offset": 5920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1405
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 346,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 371,
                    "end": 397,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 664,
                    "end": 683,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 838,
                    "end": 861,
                    "matchedPaperCorpusId": "266999115"
                },
                {
                    "start": 1135,
                    "end": 1157,
                    "matchedPaperCorpusId": "269043117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "248366550",
            "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
            "text": "Supervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario. The BEIR benchmark [59] showed that BM25 was superior to dense retrieval from 9-18 (depending on the model) out of the 18 datasets under this evaluation scheme. While the zero-shot scenario offers a fairer comparison of dense models with unsupervised sparse models, learned dense retrieval models should also be compared with learned sparse models, e.g. BM25+doc2query. \n\nUnlike previous work that compares supervised and unsupervised, dense and sparse retrieval models for other tasks such as passage ranking, we provide a novel and comprehensive comparison for the problem of full-rank retrieval of responses for dialogues.",
            "score": 0.6283550919962977,
            "section_title": "Dense and Sparse Retrieval",
            "char_start_offset": 10422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1176
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "257585074",
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "text": "In this section, we introduce a conceptual framework consisting of three components (sparse encoder, sparse regularizer, supervision) that captures the key differences we observe between existing learned sparse retrieval methods. Later, we describe how LSR methods in the literature can be fit into this framework. Sparse (Lexical) Encoders. A sparse or lexical encoder encodes queries and passages into weight vectors of equal dimension. This is the main component that determines the effectiveness of a learned sparse retrieval method. There are three distinct characteristics that make sparse encoders different from dense encoders. The first and most straightforward difference is that sparse encoders produce sparse vectors (i.e., most term weights are zero). This sparsity is controlled by sparse regularizers, which we will discuss in the next section. Second, dimensions in sparse weight vectors are usually tied to terms in a vocabulary that contains tens of thousands of terms. Therefore, the size of the vectors is large, equal to the size of the vocabulary; each dimension represents a term (typically a BERT word piece). On the contrary, (single-vector) dense retrieval methods produce condensed vectors (usually fewer than 1000 dimensions) that encode the semantics of the input text without a clear correspondence between terms and dimensions. Term-level dense retrieval methods like ColBERT [13] do preserve this correspondence. \n\nThe third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies. \n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8,35]. There are also recent works proposing hybrid retrieval systems that combine the strength of both dense and sparse representations [3,18,19], which can bring benefits for both in-domain and out-of-domain effectiveness [19].",
            "score": 0.628059372545893,
            "section_title": "Unified learned sparse retrieval framework",
            "char_start_offset": 5623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2353
                }
            ],
            "ref_mentions": [
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 2126,
                    "end": 2129,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "267406766",
            "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
            "text": "4.3.1 Baselines for Retrieval Tasks. Retrieval tasks are divided into two main approaches: Sparse&Dense Retrieval and Generative Retrieval. For Sparse&Dense Retrieval, we employ the following models: BM25 [42], a classic sparse retrieval model; DPR [22] and its multi-task variant MT-DPR [34] for dense passage retrieval; RAG [27], which combines dense retrieval with seq2seq models for an enhanced generation; E5 [51], a state-of-the-art text embedding model; and SimLM [52], a dense passage retriever with effective pre-training methods. The Generative Retrieval methods comprise T5 [39], a pre-trained encoder-decoder model for multitask learning, BART [26], a denoising autoencoder for text generation; SEAL [1], generating sub-strings as document identifiers; CorpusBrain [7], incorporating pre-training strategies for KILT retrieval task; and Llama2 [50], an open-source pre-trained LLM. \n\nThe baseline retrieval models are finetuned with labeled retrieval data from KILT datasets. Except the DPR model is finetuned on each specific dataset, other dense and generative retrieval models are multi-task finetuned using retrieval data across all datasets, as multi-task training in KILT leads to improved performance [7,34].",
            "score": 0.6254790366431637,
            "section_title": "Baselines",
            "char_start_offset": 23016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1227
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "230435546"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "250311114"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 656,
                    "end": 660,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 777,
                    "end": 780,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1223,
                    "end": 1226,
                    "matchedPaperCorpusId": "230435546"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "248780080",
            "title": "R3 : Refined Retriever-Reader pipeline for Multidoc2dial",
            "text": "We present the results for different retriever configurations at Recall@10 and Recall@100 in Table 2: Performance of the retriever for different model configurations at Recall@10 and Recall@100. X-PT refers to the pretrained X model while X-FT implies that X was finetuned on MultiDoc2dial. DPR-FT was the retriever employed for the MultiDoc2Dial baseline. \n\n2. It is evident that the pretrained sparse retrieval frameworks, Splade and DistilSPLADE, achieve better retrieval performance in comparison to the pretrained DPR model. This suggests that the exact matching over keywords and over the paraphrases generated for functional words achieves good retrieval performance. Unsurprisingly, the performance for all models improve significantly when they are fine-tuned on Multidoc2Dial dataset, with the sparse-retrievers still outperforming DPR. The performance shows a further boost when we use the fine-tuned DPR model to mine hard-negatives. \n\nReranking the validation passages increases the R@10 to 85% (Ref Table 2). This further leads to improvements in metrics in both the normal and the Gold setting.",
            "score": 0.6240898590674975,
            "section_title": "Retrieval improvement",
            "char_start_offset": 15149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1109
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490966796875
        },
        {
            "corpus_id": "268509926",
            "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
            "text": "Hyperparameter: \n\nThe hyperparameters are all presented in Appendix E. \n\nRetriever: We adopt BM25 as our retrieval model based on findings from (Ram et al., 2023), which demonstrated its superior performance in Retrieval-Augmented Generation, even outperforming some dense retrieval models. We also explored the impact of replacing BM25 with a SOTA dense retrieval method SGPT (Muennighoff, 2022), which is detailed in Section 5.5. \n\nStopwords: For the identification of stopwords within the RIND module, we utilized the en_core_web_sm language model from the Spacy library, a tool recognized for its effectiveness and efficiency in Natural Language Processing tasks as evidenced by previous research (Shelar et al., 2020).",
            "score": 0.6218784066382437,
            "section_title": "Implementation Details",
            "char_start_offset": 20079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 18,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 723
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "219480808"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39013671875
        },
        {
            "corpus_id": "270702658",
            "title": "Database-Augmented Query Representation for Information Retrieval",
            "text": "Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021).Typically, it can be performed with two types of models: sparse and dense retrievers.Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020;Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it.For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents.Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data.Yet, using only the information within a query for retrieval can be suboptimal, due to the scarcity of information on it.\n\nQuery Augmentation for Retrieval Some studies have proposed augmenting (or expanding) the original query with additional information to enhance the retrieval performance (Carpineto and Romano, 2012;Azad and Deepak, 2019).To be specific, traditional query augmentation methods have focused on utilizing a lexical knowledge base such as the WordNet (Miller, 1992) to expand the original queries (Bhogal et al., 2007;Zhang et al., 2009).",
            "score": 0.620783746564599,
            "section_title": "Related Work",
            "char_start_offset": 5771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 597
                },
                {
                    "start": 599,
                    "end": 901
                },
                {
                    "start": 901,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1560
                },
                {
                    "start": 1562,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 333,
                    "matchedPaperCorpusId": "247839624"
                },
                {
                    "start": 551,
                    "end": 575,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 575,
                    "end": 596,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 918,
                    "end": 942,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1200,
                    "end": 1222,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1732,
                    "end": 1760,
                    "matchedPaperCorpusId": "10393627"
                },
                {
                    "start": 1760,
                    "end": 1782,
                    "matchedPaperCorpusId": "8718737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "247450700",
            "title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation",
            "text": "the large language model. However, since our interpolation and perturbation methods only manipulate the already   obtained representations of the documents from the encoder E D , we don't have to newly generate document texts and also to forward generated documents into the model, which greatly saves time and memory (see Table 3). We provide a detailed analysis and discussion of efficiency in Appendix B.  (Kwiatkowski et al., 2019); 2) TriviaQA (TQA) is a QA collection scraped from the Web (Joshi et al., 2017).\n\nRetrieval Models 1) BM25 is a sparse termbased retrieval model based on TF-IDF (Robertson et al., 1994). 2) Dense Passage Retriever (DPR) is a dense retrieval model with a dual-encoder of query-document pairs (Karpukhin et al., 2020). 3) DPR with Query Augmentation (DPR w/ QA) augments pairs with query generation for the document, adopting (Liang et al., 2020;Mao et al., 2021a). 4) DPR with Document Augmentation (DPR w/ DA) augments pairs by replacing words in the document (Ma, 2019). 5) DPR with Axiomatic Regularization (DPR w/ AR) regularizes the retrieval model to satisfy certain axioms (Rosset et al., 2019). 6) DAR is ours with interpolation and perturbation of document representations. Implementation Details For the dense retrieval model based on the DPR framework, we refer to the publicly available code from DPR (Karpukhin et al., 2020). We set the training epoch as 25 and batch size as 32 under academic budgets with a single GeForce RTX 3090 GPU having 24GB memory. We use in-batch negative sampling as our negative sampling strategy without hard negative samples. Also, we retrieve 100 passages per question. We use both interpolation and perturbation schemes for our augmentation methods. Specifically, for the interpolation method, we set \u03bb \u2208 [0, 1] in equation 3 to be sampled from the uniform distribution. Also, for the perturbation method, we set the dropping rate as 0.1, and the",
            "score": 0.6173977382034959,
            "section_title": "Method",
            "char_start_offset": 10233,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 515,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 597,
                    "end": 621,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 727,
                    "end": 751,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 880,
                    "end": 898,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 1348,
                    "end": 1372,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "271534382",
            "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
            "text": "Dense retrieval has become an important research area following the development of pre-trained Transformer language models (PLMs) [2], [19]- [22]. To enhance text retrieval performance, dense retrieval leverages PLM-based text embeddings to encode queries and documents into a shared semantic vector space, focusing on matching semantic contents beyond mere keywords. This text embedding application in retrieval is fundamental to Retrieval-Augmented Generation (RAG) [23], which reduces the hallucinations in LLMs. Recent advancements in dense retrieval include architectural innovations, optimized training methodologies, and efficient indexing techniques, all of which contribute to improved retrieval accuracy and efficiency [4]- [8], [24]. Since the introduction of Sentence-BERT [25] and Dense Passage Retrieval (DPR) [2], numerous sentence embedding models have been proposed to enhance dense passage retrieval. Advanced sentence embedding models, which have been highlighted in the retrieval task of massive text embedding benchmark (MTEB) [26], include Contriever [27], M3E 2 , BGE [9], etc. Our text augmentation method serves as a preprocessing module for dense retrieval and is compatible with various embedding models mentioned above.",
            "score": 0.6172428494574147,
            "section_title": "A. Dense Retrieval",
            "char_start_offset": 10070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "236772078"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "149698624"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "269605025",
            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
            "text": "To evaluate the impact of retrieval mechanisms, we implemented benchmarks using both dense and sparse retrievers. Sparse retrievers, such as TF-IDF [28] and BM25 [29], are grounded in token frequency-centric metrics, contrasting with dense retrievers represented by Spider [26], Contriever [11], and DPR [16]. Findings presented in Table 2 support the following conclusions: (1) Dense retrievers surpass sparse alternatives, showcasing superior performance and consistency across tasks. (2) Among dense retrieval models, those employing direct negative sampling techniques, such as DPR [16], notably improve query document discrimination, which is achieved through contrastive learning [13].",
            "score": 0.615896640013549,
            "section_title": "The Influence of Different Retrievers",
            "char_start_offset": 22786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 691
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "245144844"
                },
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "Information retrieval (IR) is the task of finding relevant knowledge or passages corresponding to a given query. Traditional exact lexical-matching approaches, such as TF-IDF (term frequency and inverse document frequency) or BM25 (Robertson et al. 1995;Robertson and Zaragoza 2009), have performed well on some IR tasks. Recently, neural retrieval with contextualized dense sentence embedding has shown to be effective for tasks such as open-domain question answering (e.g., Karpukhin et al. 2020;Xiong et al. 2021). \n\nDense retriever, or dense passage retrieval (DPR), uses a pair of neural language models as a bi-encoder to obtain latent representations of questions and passages (Lee, Chang, and Toutanova 2019). Bi-encoders require to be finetuned with contrastive learning to embed a question and the relevant passages on semantically similar sentence vectors. Recent works on DPR reported in-domain (tested on Accepted to Workshop on Knowledge Augmented Methods for Natural Language Processing, in conjunction with AAAI 2023. the same dataset which was used to fine-tune) retrieval performances exceeding those from sparse retrievers such as BM25 (Karpukhin et al. 2020). \n\nHowever, it has been shown that conventional DPRs struggle with retrieval on BEIR benchmark (Thakur et al. 2021) including various domains and even on EntityQuestions (EQ) (Sciavolino et al. 2021) built within a confined domain (Wikipedia), whereas BM25 shows better performance on both of them. This indicates that the generalizability of DPR is limited not only out of domain but even in domain when a named entity in a question is a dominant clue for retrieval. In order to address this, Sciavolino et al. (2021) showed that DPR trained with PAQ dataset (Lewis et al. 2021) which consists of 65 million of question-answer pairs containing many of named entities in Wikipedia still performs far worse than BM25 in EQ dataset. The results showed that supervised learning with a huge dataset that covers the domain extensively could not lead to good generalization within the domain.",
            "score": 0.6140356943114926,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 254,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 254,
                    "end": 282,
                    "matchedPaperCorpusId": "3641284"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 684,
                    "end": 716,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1155,
                    "end": 1177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1274,
                    "end": 1294,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1354,
                    "end": 1377,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1673,
                    "end": 1697,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1739,
                    "end": 1757,
                    "matchedPaperCorpusId": "231924957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "257687445",
            "title": "Modular Retrieval for Generalization and Interpretation",
            "text": "As shown in Table 10, compared to the fine-tuned DPR model (DPR-vanilla), REMOP can achieve comparable zero-shot retrieval performance with fewer trainable parameters. The DPR-prompt model, which is only trained on a single prefix prompt (2.3M parameters), shows relatively poor performance on zero-shot retrieval tasks; and REMOP, using module composition to enhance model generalizability, outperforms the DPR-prompt on all the evaluation tasks. Compared to the other listed retrieval methods, REMOP shows its advantage in parameter-efficiency and generalizability that only training on a small number of parameters (2.3M\u00d77) can achieve comparable performance. It is worth noting that most of the dense passage retrieval methods and sparse passage retrieval methods underperform the basic lexical retrieval method BM25 on zero-shot retrieval tasks, and we give an analysis of this phenomenon in Section 5.3.",
            "score": 0.611865427496063,
            "section_title": "Experimental Results",
            "char_start_offset": 28784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 909
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37744140625
        },
        {
            "corpus_id": "270702658",
            "title": "Database-Augmented Query Representation for Information Retrieval",
            "text": "We operationalize query augmentation approaches with two widely used dense retrieval models, namely DPR and Contriever, as follows: DPR is a supervised dense retrieval model that requires a pair of a query and its relevant document for training (Karpukhin et al., 2020); Contriever is another widely used dense retriever, but is trained in an unsupervised fashion (Izacard et al., 2022).In addition, as an indicator, we report the performance of the sparse retriever (BM25).\n\nAugmentation Models We compare our DAQu against relevant query augmentation models as follows: 1) No Expansion (No Expan.):This model directly uses the given query for retrieval without expanding it.2) Na\u00efve Query Expansion (Na\u00efve Expan.):This baseline concatenates a given query with all the textual terms of the associated metadata from the database.3) Query Expansion w/ BM25 (Expan.w/ BM25): Similar to Deng et al. (2021), this model also appends the metadata terms to the given query.However, before expanding the query, it employs a BM25 model to select meta-  data terms that are most relevant to the query, and only these selected terms are appended.4) DAQu (Ours): This is our model that augments the query representation by incorporating the metadata representation on a latent space, which is generated with the graph-structured set encoding strategy.",
            "score": 0.6113984235300962,
            "section_title": "Retrieval Models",
            "char_start_offset": 24063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 474
                },
                {
                    "start": 476,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1338
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 269,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 364,
                    "end": 386,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 883,
                    "end": 901,
                    "matchedPaperCorpusId": "245502546"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "251594672",
            "title": "CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks",
            "text": "We adopt two types of baseline methods for comparison, including traditional IR models and model-based IR models. \n\n4.2.1 Traditional IR Models. We take several representative models that are widely used for KILT tasks as the baselines, including the sparse retrieval and dense retrieval methods. \n\n\u2022 BM25 [40] is a highly effective retrieval model that represents the classical probabilistic retrieval model. \u2022 TF-IDF [5] is a traditional sparse vector space retrieval model that combines bigram hashing and TF-IDF matching to return relevant documents. \u2022 DPR [21] is a BERT-based dual-encoder model trained with inbatch negatives and a few hard negatives selected with BM25. \n\n\u2022 DPR+BERT [38] combines a BERT-base classifier with passages returned from DPR where the query and retrieved passages are the input. \n\n\u2022 DPR+BART [38] incorporates an explicit retrieval step in addition to the generative pre-training together with DPR and BART. \n\n\u2022 RAG [29] combines pre-trained parametric and non-parametric memory for generation. \u2022 MT-DPR [36] jointly trains a DPR model on an extensive selection of retrieval tasks. \u2022 BLINK+flair [38] combines BLINK [46] and flair [1] retrieval solution that ranks pages according to entities in the input.",
            "score": 0.6098862453398961,
            "section_title": "Baselines",
            "char_start_offset": 20574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1240
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 422,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "230435546"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "263877300"
                },
                {
                    "start": 1165,
                    "end": 1168,
                    "matchedPaperCorpusId": "181704107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "237304122",
            "title": "Retrieval Augmented Code Generation and Summarization",
            "text": "Information retrieval (IR) systems or retriever models are designed to retrieve the top-k relevant documents that presumably best provide the desired information (Manning et al., 2008). Term-based retrieval methods, a.k.a. sparse retrieval models, such as TF-IDF or BM25 (Robertson and Zaragoza, 2009) use sparse vector representations to perform lexical matching and compute relevance scores to rank the documents based on a query. On the other hand, dense retrieval methods encode documents into a fixed-size representations and retrieve documents via maximum inner product search (Sutskever et al., 2014;Guo et al., 2016). Particularly of interests,  propose a Dense Passage Retriever (DPR) model for open-domain question answering (QA). It consists of two encoders (Q(.) and P(.)) that encode queries and passages, respectively. The similarity of a query q and a passage p is defined by the inner product of their encoded vectors sim(p, q) = Q(q) T \u22c5 P (p). Given a query q, a positive (relevant) passage p + , and a set of n irrelevant passages p \u2212 i , DPR optimizes the classification loss:\n\n.  propose to fine-tune DPR using in-batch negatives (Gillick et al., 2019;Yih et al., 2011) with curated \"hard\" negatives us- ing BM25 (candidates with high BM25 scores but contain no sub-string that match the target). We refer to  for details.",
            "score": 0.6093106490919732,
            "section_title": "Retriever: DPR",
            "char_start_offset": 7174,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 607,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1173,
                    "end": 1190,
                    "matchedPaperCorpusId": "1965270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "Retrieval-Augmented Generation (RAG) is a technique that augments natural language generation models with relevant content retrieved from knowledge sources, aiming at improving the quality and relevance of text generation.Previous works have demonstrated its strong performance in knowledge-intensive tasks such as question answering, fact checking, and content recommendation [12,24,32,41].\n\nRetrievers interact with external corpus to acquire relevant information.For open-domain question answering, the Wikipedia corpus [6] is commonly used.As for retrieval methods, it can broadly be categorized into two types: sparse retrievers and dense retrievers.Sparse retrievers, such as TF-IDF [37] and BM25 [35], predominantly rely on keyword matching for document retrieval.These methods determine the relevance between queries and documents by analyzing the occurrence and distribution of keywords within the documents.Dense retrievers employ dual-encoders to generate dense vector representations of text for more accurate semantic matching.Consequently, dense retrievers are considered more suitable for retrieval-augmented applications.Some techniques like vector quantization [25,47] and embedding optimization [48] also improves the efficiency of dense retrievers.Common dense retrievers include DPR [20], ANCE [49] and Contriever [13].Specifically, DPR [20] is trained with supervised learning on question-answer pairs, and focuses on extracting relevant passages by analyzing the semantic content of both questions and answers.ANCE [49] leverages approximate nearest neighbor search and contrastive learning to enhance the model's ability to discern between relevant and non-relevant documents in a dense vector space.Contriever [13] employs unsupervised contrastive learning to adapt to inherent data structure, especially beneficial when the annotated training data is scarce.To enhance the quality of retrieved documents, some work conduct further reranking to these documents for personalization [3,40,56] and diversification [27,38].\n\nRecent work has explored different ways for language models to leverage retrieved or generated text as external knowledge.One approach is to integrate retrieval into language model pre-training or fine-tuning.For instance, REALM [9] integrates external document retrieval into pre-training, enhancing performance in downstream tasks by retrieving relevant information.",
            "score": 0.6086114307992867,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 391
                },
                {
                    "start": 393,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 2043
                },
                {
                    "start": 2045,
                    "end": 2167
                },
                {
                    "start": 2167,
                    "end": 2254
                },
                {
                    "start": 2254,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 381,
                    "matchedPaperCorpusId": "257637217"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "259949894"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "247922621"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "245986472"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 2005,
                    "end": 2008,
                    "matchedPaperCorpusId": "1292249"
                },
                {
                    "start": 2008,
                    "end": 2011,
                    "matchedPaperCorpusId": "316030"
                },
                {
                    "start": 2011,
                    "end": 2014,
                    "matchedPaperCorpusId": "240230721"
                },
                {
                    "start": 2035,
                    "end": 2039,
                    "matchedPaperCorpusId": "220730230"
                },
                {
                    "start": 2039,
                    "end": 2042,
                    "matchedPaperCorpusId": "235792531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "273654290",
            "title": "Quam: Adaptive Retrieval through Query Affinity Modelling",
            "text": "We use both sparse (BM25) and dense (TCT) retrieval models. BM25 is a sparse retrieval method based on the query terms present in the documents. We use top  \u2208 [50, 100, 1000] results from BM25 using a PISA [12] index. We use default parameters for retrieval. TCT is a dense retrieval model, a distilled version of the ColBERT model. We retrieve (exhaustively) top  \u2208 [50, 100, 1000] documents using the TCT-ColBERT-HNP [7] model from huggingface 2 . \n\n2 https://huggingface.co/castorini/tct_colbert-v2-hnp-msmarco",
            "score": 0.60699889982392,
            "section_title": "Retrieval Methods.",
            "char_start_offset": 19657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 513
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "198120175"
                },
                {
                    "start": 419,
                    "end": 422,
                    "matchedPaperCorpusId": "235720578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "259224595",
            "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective",
            "text": "\u2022 DPR [12] is a representative dense retrieval model which uses dual-encoder architecture and is trained with in-batch negatives and a few hard negatives selected with BM25. \u2022 BART [16] is a Seq2Seq model applicable for sequence generation tasks. Following [5,7], we extract the query-title pairs from each dataset and fine-tune the BART for generative retrieval. \u2022 CorpusBrain [5] (C.B. for short) is a pre-trained generative retrieval model for knowledge-intensive language tasks. We finetune CorpusBrain on every specific downstream KILT task.",
            "score": 0.6059030622752042,
            "section_title": "Retrieval models",
            "char_start_offset": 11687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 546
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 10,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 260,
                    "end": 262,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1644287109375
        },
        {
            "corpus_id": "243986045",
            "title": "Recent Advances in Automated Question Answering In Biomedical Domain",
            "text": "DrQA [43] TFIDF [160] LSTM [79] predicting the answer span from retrieved text DPR [101] BERT [51] based dual tower dense retriever BERT [51] based model predicting answer span from retrieved text. \n\nRAG [116] DPR retriever BART [114] based model generating answers using retrieved text. \n\nFiD [87] DPR retriever T5 [152] based model generating answers using retrieved text. \n\nGAR [130] BART [114] based model for query expansion; BM25 based retriever DPR Reader (BERT based model predicting answer span from retrieved text.) performance of the entire system significantly. A variety of methods ranging from sparse lexical models to dense deep learning based models have been used for retrieving relevant passages. DrQA [43] uses a TFIDF [153] based retrieval model to retrieve relevant Wikipedia passages. TFIDF and similar sparse lexical models like BM25 [160] have been used extensively for document retrieval in ODQA as well as other information retrieval tasks. First an inverted index of different words / word-tokens in the entire text corpus is created [129]. This produces a sparse high dimensional representation of the document. The query terms are used to match with the inverted index and a list of top scoring documents (scored by BM25 or TFIDF) is returned. These sparse retrieval models can also be modified to include higher order ngrams (bigrams or trigrams) along with the usual unigrams (individual word tokens), as they lead to better retrieval [7], albeit at the cost of reduced efficiency due to the increase in the number of features in the index. The sparse retrieval models only compare the exact matches, i.e. the lexical similarity among terms present in the documents and the query, and not their semantic relevance [101]. This is a major drawback which can be resolved using dense semantic encodings / embeddings produced by neural networks [101]. \n\nFigure 4: Dual Encoder or Two Tower architecture [42]. Query Encoder (also referred to as Query Tower) is used for encoding the question, Document Encoder (also referred to as Document Tower) is used to encode the documents.",
            "score": 0.6048291356088296,
            "section_title": "Method Retriver Reader",
            "char_start_offset": 33665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 316,
                    "end": 321,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "258947132",
            "title": "Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering",
            "text": "We compare EAR with 1) DPR (Karpukhin et al., 2020): a standard BERT-based dense retriever; 2) BM25 (Robertson et al., 2009): a standard sparse retriever based on term matching; 3) GAR (Mao et al., 2021a): generation-augmented retrieval with BM25; 4) Liu et al. (2022): a concurrent work that uses a GAR-like generative model to perform beam search decoding, followed by filtering to obtain multiple expanded queries for performing multiple retrievals with BM25, and then fusion of the results; and 5) SEAL (Bevilacqua et al., 2022): an autoregressive search engine, proposing constrained decoding with the FM-index data structure that enables autoregressive models to retrieve passages.",
            "score": 0.6037031733956557,
            "section_title": "Baselines",
            "char_start_offset": 13984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 51,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 100,
                    "end": 124,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 185,
                    "end": 203,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 507,
                    "end": 532,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "265213270",
            "title": "Scalable and Effective Generative Information Retrieval",
            "text": "MSMARCO Dev TREC DL 2019 TREC DL 2020 MRR@10 Recall@10 NDCG@10 Recall@10 NDCG@10 Recall@10 \u2022 MINDER [23]: An extension of SEAL, MINDER constructs document identifiers from multiple document views, such as titles, pseudo queries, and n-grams. \n\n\u2022 LTRGR [22]: LTRGR utilizes multi-view document identifiers, akin to MINDER, but shifts the loss function to a pairwise-based learning-to-rank algorithm. \n\nWe also compare our model with other document retrieval paradigms: sparse retrieval and dense retrieval. \n\n\u2022 BM25 [37]: a simple yet effective bag-of-word sparse retrieval model that uses term frequency, inverse document frequency, and document length for computing the relevance score. \u2022 DPR [19]: DPR is a bi-encoder dense retrieval models. It incorporates in-batch negatives and BM25 negatives for training. \u2022 ANCE [46]: ANCE is a dense retrieval model with asynchronous hard negative sampling. \u2022 MarginMSE [16]: MarginMSE is a dense retrieval model with a distinctive loss function based on the konwledge distillation. It aims to minimize the discrepancy between the predicted margin from the dense retrieval model and the golden margin from the cross-encoder (teacher) model. \u2022 TAS-B [17]: Building upon MarginMSE, TAS-B designs a topicaware sampling algorithm to enhance the model's effectiveness.",
            "score": 0.6019515294686795,
            "section_title": "Model",
            "char_start_offset": 23783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1304
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "258947148"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "The increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25 (Robertson and Zaragoza, 2009) serving as foundational tools for extracting relevant documents. However, the limitations of keyword-based retrieval prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020a) and Contriever (Izacard et al., 2021), which leverage transformer-based architectures to encode queries and documents into dense representations. While dense retrieval models improve over sparse methods, they introduce new challenges. First, retrieval corpora are typically divided into fixed chunks (Karpukhin et al., 2020a), which can lead to retrieving irrelevant content. Second, dualencoder architectures encode queries and documents separately, limiting direct interaction between them (Khattab et al., 2021). Finally, dense retrieval models require pre-encoding and storing document embeddings, which constrains scalability and hinders their ability to leverage large language models (LLMs) (Levine et al., 2022). \n\nTo address these limitations, generative models such as GPT-3.5 and InstructGPT (Brown et al., 2020;Ouyang et al., 2022) offer an alternative by directly generating contextualized responses instead of retrieving existing documents. Approaches like GenRead (Yu et al., 2022) first generate relevant text and then use it for answer prediction. However, generative models often struggle with factual consistency and may hallucinate information (Huang et al., 2023), making them less reliable for knowledge-intensive tasks. Given the trade-offs between retrieval and generation, hybrid models have emerged to integrate the strengths of both approaches. Merging Generator and Retriever (MGR) (Abdallah and Jatowt, 2023;Zhang et al., 2023) combines generated and retrieved documents, allowing models to refine answers while maintaining factual accuracy.",
            "score": 0.6006212090258654,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 371,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 578,
                    "end": 603,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 904,
                    "end": 929,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1427,
                    "end": 1447,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "221802772",
            "title": "Generation-Augmented Retrieval for Open-Domain Question Answering",
            "text": "Comparison w. the state-of-the-art. We next evaluate the effectiveness of GAR for retrieval. In Table 4, we show the top-k retrieval accuracy of BM25, BM25 with query expansion (RM3) (Abdul-Jaleel et al., 2004), DPR , GAR (i.e., BM25 with generationaugmented queries), and GAR +DPR.\n\nOn the NQ dataset, while BM25 clearly underperforms DPR regardless of the number of retrieved  Top-20 Top-100 Top-500 Top-1000 Top-20 Top-100 Top-500 Top- Table 4: Top-k retrieval accuracy of sparse and dense methods on the test sets of NQ and Trivia. GAR helps BM25 to achieve comparable or better performance than DPR. passages, the gap between GAR and DPR is significantly smaller and negligible when k \u2265 100. When k \u2265 500, GAR is slightly better than DPR despite that it simply uses BM25 for retrieval. In contrast, the classic query expansion method RM3, while showing marginal improvement over the vanilla BM25, does not achieve comparable performance with GAR or DPR. By fusing the results of GAR and DPR in the same way as described in Sec. 3.2, we further obtain consistently higher performance than both methods, with top-100 accuracy 88.8% and top-1000 accuracy 93.2%. On the Trivia dataset, the results are even more encouraging -GAR achieves consistently better retrieval accuracy than DPR when k \u2265 20. On the other hand, the difference between BM25 and BM25 +RM3 is negligible, which suggests that naively considering top-ranked passages as relevant (pseudo relevance feedback) for query expansion does not always work. Results on more cutoffs of k can be found in App. A.\n\nEffectiveness of various query contexts. In Fig. 1, we show the performance of GAR when different query contexts are used to augment the",
            "score": 0.5997919922072239,
            "section_title": "Generation-Augmented Retrieval",
            "char_start_offset": 16476,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 210,
                    "matchedPaperCorpusId": "16221853"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "The sparse retrieval baselines are as follows: \n\n\u2022 BM25 [37] is a classical sparse retrieval algorithm based on probabilistic relevance framework and term frequency statistics. \n\n\u2022 DocT5Query [28] expands documents by generating potential queries using a fine-tuned T5 model. \n\nThe dense retrieval baselines are as follows: \n\n\u2022 DPR [19] is a dual-encoder model trained with in-batch negative sampling. We fine-tune DPR on our training datasets to obtain DPR-FT, following the official implementation and hyperparameter settings. \n\n\u2022 BGE [50] is a state-of-the-art universal embedding model trained on approximately 200 million text pairs using contrastive learning. We employ the bge-large-en-v1.5 variant and fine-tune it on our training datasets to obtain BGE-FT. The fine-tuning process uses a learning rate of 1e-5, batch size of 128, and runs for 10 epochs. \n\n\u2022 Sentence-T5 [31] employs a dual-encoder T5 architecture to generate semantic embeddings through contrastive learning for efficient retrieval. \n\n\u2022 GTR-XL [32] is a dense retrieval model based on Sentence-T5, pre-trained on billions of question-answer pairs. \n\n\u2022 Text-embedding-ada-002 is a powerful embedding model developed by OpenAI, accessible through their API service. \n\n\u2022 GritLM [29] is built upon the Mistral 7B language model and optimized using both embedding and generation objectives. \n\nThe generative retrieval baselines are as follows: \n\n\u2022 DSI [42] is a sequence-to-sequence model that directly maps queries to document identifiers. \n\n\u2022 DSI-QG [59] enhances the DSI framework by incorporating a doc2query model for dataset augmentation. \n\n\u2022 SEAL [5] utilizes n-gram as the document identifiers and constrains the generation process using FM-index. \n\n\u2022 NCI+BGE-Reranker-FT. NCI [49] employs a sequence-to-sequence architecture with a prefix-aware weight-adaptive decoder.",
            "score": 0.5991873742536074,
            "section_title": "B Baselines",
            "char_start_offset": 29318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "273850019",
            "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation RAG aims to enhance language models by retrieving and integrating relevant information from external knowledge sources, demonstrating significant improvements in handling knowledge-intensive tasks (Lewis et al. 2020;Guu et al. 2020). RAG models excel in various NLP applications, like question answering (Izacard and Grave 2021; Ram et al. 2023) and summarization (Lin et al. 2023). Several retrieval models have been developed to support RAG frameworks. BM25 (Robertson, Zaragoza et al. 2009), a sparse retrieval model, has been a foundational method for text retrieval tasks. DPR (Karpukhin et al. 2020a) indexes passages into a dense vector space for efficient retrieval and has been widely adopted in subsequent RAG models. Other notable dense retrieval models include Contriever (Izacard et al. 2021), SBERT (Reimers and Gurevych 2019a), and BGE (Xiao and Liu 2023), each contributing to the robustness and effectiveness of retrieval-augmented systems. \n\nRecent advancements in retrieval strategies have optimized the interaction between retrievers and language models. Models like Atlas (Izacard et al. 2023) and RETRO (Borgeaud et al. 2022) employ joint training and architectural modifications to better integrate retrieved information, though these methods are resource-intensive. \n\nLLM-Supervised Learning LLMs have demonstrated remarkable capabilities in natural language understanding and generation (Brown et al. 2020;Chowdhery et al. 2022). Leveraging the knowledge and capabilities of LLMs to guide the training of other models or themselves has recently emerged as a promising direction. Reinforcement Learning from AI Feedback (RLAIF) (Lee et al. 2023) proposes using an LLM to generate preference labels to train a reward model, which then guides the reinforcement learning process, achieving performance comparable to traditional human feedback-based approaches. \n\nIn addition, Wang et al. (2022) propose an LLMbootstrapping approach called self-instruct, where an LLM iteratively generates additional training data for itself. Wang, Yang, and Wei (2023) introduce the LLM-R framework which trains dense retrievers to identify high-quality incontext examples using feedback from LLMs.",
            "score": 0.5988000454885583,
            "section_title": "Related Work",
            "char_start_offset": 5310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 360,
                    "end": 376,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 491,
                    "end": 524,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 844,
                    "end": 872,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1462,
                    "end": 1484,
                    "matchedPaperCorpusId": "144546246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "273350956",
            "title": "Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs",
            "text": "SBERT Reimers & Gurevych (2019). Contriever is a self-supervised dense retriever that learns unsupervised document embeddings for information retrieval, DPR (Dense Passage Retriever) is a bi-encoder model that retrieves relevant passages by training on questionpassage pairs, Dragon is a dense retrieval model optimized through diverse augmentation for generalizable dense retrieval, and SBERT (Sentence-BERT) is a modification of BERT that generates semantically meaningful sentence embeddings for tasks like similarity and clustering using a siamese network structure. \n\n\u2022 BM25+DPR. BM25+DPR with Reciprocal Rerank Fusion is a hybrid retrieval method that combines the strengths of BM25's lexical matching and DPR's dense embeddings by reranking results from both models using a reciprocal rank fusion strategy to improve retrieval accuracy. \n\n\u2022 Gemma-8K \u2022 Full Context. We feed all inputs to LLMs for summary generation. If the input length exceeds the context window limit, we randomly sample continuous text spans of maximum length multiple times to feed into LLMs and calculate the average result. \n\n\u2022 Thought-R Feng et al. (2024). Thought Retriever (Thought-R) generates thoughts for a series of simulated queries and appends them to the retrieval corpus as high-level knowledge.",
            "score": 0.5969076702184059,
            "section_title": "A EXPERIMENTAL DETAILS",
            "char_start_offset": 31205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1286
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "A.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
            "score": 0.5960614009952676,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 32867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "244714119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "254854303",
            "title": "Controlling Styles in Neural Machine Translation with Activation Prompt",
            "text": "The pre-trained model like GPT-3 (Brown et al., 2020) cannot precisely remember all the data and it is important to retrieve external knowledge to solve different NLP tasks . There are two main representations used for retrieval: bag-ofword (BoW) based sparse representations (Chen et al., 2017) and dense representations generated by the pre-trained language models (Karpukhin et al., 2020). As for the sparse representations, they focus on a rule-based score such as Okapi BM25 (Robertson and Zaragoza, 2009) for retrieval.  use this method to improve model performance on four NLP tasks. The dense representations are generated by the pre-trained autoencoder model and have higher recall performance on most NLP tasks such as machine translation (Cai et al., 2021). For the sake of accuracy and storage, we use dense representations for retrieval in this paper.",
            "score": 0.5939028599780157,
            "section_title": "Retrieval-based Methods",
            "char_start_offset": 5671,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 295,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 367,
                    "end": 391,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 749,
                    "end": 767,
                    "matchedPaperCorpusId": "235166182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "245144844",
            "title": "Learning to Retrieve Passages without Supervision",
            "text": "It is well established that the strong lexical matching skills of sparse models such as BM25 (Robertson and Zaragoza, 2009) are complementary to dense representation models.  demonstrated strong improvements by using hybrid dense-sparse retrieval, based on BM25 and DPR. Specifically, they define the joint score of a hybrid retriever via a linear combination of the scores given by the two models, i.e. s hybrid (q, p) = s(q, p) + \u03b1 \u00b7 BM25(q, p). They tune \u03b1 on a validation set of each of the datasets. Since tuning hyperparameters is unrealistic in our settings, we simply set \u03b1 = 1.0 for all hybrid models. Thus, we define:\n\nWe adopt the normalization technique from : If a passage p is found in the top-k (with k > k) of a dense retriever but not of BM25, then BM25(q, p) is set to the minimum value from the top-k results of BM25 (and vice versa).",
            "score": 0.5934466984325824,
            "section_title": "Hybrid Dense-Sparse Retrieval",
            "char_start_offset": 11475,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "249395549",
            "title": "A Neural Corpus Indexer for Document Retrieval",
            "text": "In this section, we briefly introduce the related works and leave more discussions in Appendix A. \n\nSparse retrieval. Traditional document retrieval methods are based on Sparse Retrieval, which is built upon inverted index with term matching metrics such as TF-IDF [45], query likelihood [33] or BM25 [44]. In industry-scale web search, BM25 is a difficult-to-beat baseline owing to its outstanding trade-off between accuracy and efficiency. In recent years, there are some attempts to incorporate the power of neural networks into inverted index. The Standalone Neural Ranking Model (SNRM) [57] learns high-dimensional sparse representations for query and documents, which enables the construction of inverted index for efficient document retrieval. Doc2Query [41] predicts relevant queries to augment the content of each document before building the BM25 index, and DocT5Query [40] improves the performance of query generation by the pre-trained language model T5 [5]. Furthermore, DeepCT [9] calculates context-aware term importance through neural networks to improve the term matching metrics of BM25. \n\nDense retrieval. Another line of research lies in Dense Retrieval, which presents query and documents in dense vectors and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain dense representations for queries and documents. At inference time, efficient Approximate Nearest Neighbor (ANN) search algorithms, such as k-dimensional trees [3], localitysensitive hashing [10], and graph-based indexes (e.g., HNSW [38], DiskANN [27] and SPANN [7]) can be utilized to retrieve relevant documents within a sublinear time. Besides, Luan et al. [37] analyze the limited capacity of dual encoders, and propose a combination of sparse and dense retrieval methods with multi-vector encoding to achieve better search quality. \n\nAutoregressive retrieval. The other way to approach retrieval is utilizing an end-to-end autoregressive model.",
            "score": 0.5927163362556401,
            "section_title": "Related work",
            "char_start_offset": 7589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "13091446"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "747083"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "266359654",
            "title": "UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models",
            "text": "We choose several baseline models for the retrieval and QA tasks, categorized into different classes. \n\nFor the retrieval task, we select three classes of models. The first class consists of Sparse Retrieval models, which include BM25 (Robertson, Zaragoza et al. 2009) and DocT5Query (Nogueira, Lin, and Epistemic 2019). The second class comprises Dense Retrieval models, such as DPR (Karpukhin et al. 2020) and ANCE (Xiong et al. 2020). Lastly, the Generative Retrieval models class includes DSI (Tay et al. 2022), DSI-QG (Zhuang et al. 2022), NCI (Wang et al. 2022), and Ultron (Zhou et al. 2022b). \n\nRegarding the QA task, we consider three types of baseline models. The first type is Closed-book Generation models, represented by T5 (Raffel et al. 2020) and BART (Lewis et al. 2019). The second type is Retrieval-augmented Generation models, which incorporate RAG (Lewis et al. 2020) and a combination model that utilizes DPR, NCI, Ultron, and Fusions-in-Decoder (Izacard and Grave 2020). The last type is LLM-based Generation models, where we directly evaluate the QA performance of gpt-3.5-turbo-0613 and LLaMA2-13B-Chat (Touvron et al. 2023b).",
            "score": 0.5925625903420435,
            "section_title": "Baselines",
            "char_start_offset": 17899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1150
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 268,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 284,
                    "end": 319,
                    "matchedPaperCorpusId": "260460088"
                },
                {
                    "start": 737,
                    "end": 757,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 868,
                    "end": 886,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5458984375
        },
        {
            "corpus_id": "259145189",
            "title": "Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard",
            "text": "Top-k Retrieval \n\nFigure 1: A bi-encoder architecture for information retrieval that captures both dense retrieval models (e.g., DPR), sparse retrieval models (e.g., SPLADE), as well as traditional \"bagof-words\" lexical retrieval models (e.g., BM25). \n\nThe design of encoders in such a bi-encoder architecture is dictated primarily by two choices: (1) the basis of the vector space and (2) how the vector weights are assigned. For example, both dense models such as DPR and sparse models such as SPLADE use pretrained transformers to encode queries and documents into vectors; both take advantage of large amounts of manually labeled data. However, the critical difference is the representational basis of their vectors-DPR generates dense vectors, typically with the same width as the output contextual embeddings (for many models, 768 dimensions), whereas SPLADE \"projects\" the scalar weights of each dimension back into the input vocabulary space, generating, in essence, bag-of-words vectors. BM25 can be understood in this bi-encoder architecture as having a document \"encoder\" that was heuristically designed (i.e., the BM25 scoring function) and a query \"encoder\" that generates multi-hot vectors. \n\nViewing retrieval as representation learning not only helps us understand the relationship between different models, but immediately illuminates open research questions. The dominant approach today is based on supervised learning with (manually) labeled datasets such as the MS MARCO test collections [2]. This naturally begs the question: What happens when models are applied to out-of-distribution data? Examples include applying retrieval models trained on one type of text (e.g., passages from the web) to another type of text (e.g., text from the medical or legal domain), or arXiv:2306.07471v1 [cs.IR] 13 Jun 2023 differences between training and test queries (e.g., keyword queries vs. well-formed natural language questions). This is where BEIR (Benchmarking IR) [34] comes in.",
            "score": 0.5924962206040985,
            "section_title": "Doc Encoder Query Encoder",
            "char_start_offset": 1435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 18,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 1992
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "258762633",
            "title": "TOME: A Two-stage Approach for Model-based Retrieval",
            "text": "For comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\n\nBM25 (Robertson et al., 2009) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.",
            "score": 0.5921320023796967,
            "section_title": "Baselines",
            "char_start_offset": 18341,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 150,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 260,
                    "end": 284,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 294,
                    "end": 314,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 425,
                    "end": 442,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 667,
                    "end": 692,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "269982279",
            "title": "Control Token with Dense Passage Retrieval",
            "text": "This study addresses the hallucination problem in large language models (LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a technique that involves embedding relevant information in the prompt to obtain accurate answers. However, RAG also faced inherent issues in retrieving correct information. To address this, we employed the Dense Passage Retrieval(DPR) (Karpukhin et al., 2020) model for fetching domain-specific documents related to user queries. Despite this, the DPR model still lacked accuracy in document retrieval. We enhanced the DPR model by incorporating control tokens, achieving significantly superior performance over the standard DPR model, with a 13% improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",
            "score": 0.5904599214265127,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "262947262",
            "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
            "text": "The results suggest that when using uni-modal data for indexing, i.e., text or code, dense retrieval models consistently outperform BM25. But BM25 tends to work better when using bi-modal data. \n\nOn the one hand, two dense retrieval models beat BM25 on all reported metrics under the 'text' and 'code' groups. Under the 'text' group, compared with BM25, SBERT2 sees increases of 14.3%, 30.3%, and 20.3% in terms of P@5, R@5, and F1@5 respectively (Line 1, 7). Under the 'code' group, compared with BM25, SBERT1 sees increases of 37.4%, 22.6%, and 25.0% in terms of P@5, R@5, and F1@5 respectively (Line 2, 5). The improvement of dense retrieval models compared with the BM25 method likely stems from the models' semantic representation of input data, which maps semantically similar sentences to closer places in a highdimensional vector space, whereas BM25 matches words in a lexical manner. On the other hand, under the 'text+code' group, BM25 surpasses dense retrieval models w.r.t. P@k, R@k and F1@k, k = 5, 10. It implies that dense retrieval models are effective on single data modality, but applying them in computational notebook search tasks requires extra effort. \n\n3) Effectiveness of the fusion method (RQ3): To examine the effectiveness of the fusion strategy and the effect of model selection for retrieval performance, we compare three variants of fused models, i.e., SBERT1+SBERT2, BM25+SBERT1, and BM25+SBERT2, applied on 'text+code' contents. The first method combines two dense retrieval models, while the last two coalesce a sparse retrieval model with a dense retrieval model. We show the results in Table V. They illustrate that incorporating a sparse retrieval model to form a keywordaware retrieval method can significantly boost the performance of dense retrieval models.",
            "score": 0.5882589753852411,
            "section_title": "Table IV and Table V list the experimental results.",
            "char_start_offset": 28982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1796
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "239616073",
            "title": "Wacky Weights in Learned Sparse Representations and the Revenge of Score-at-a-Time Query Evaluation",
            "text": "As points of comparison, we adopted the following baselines: BM25 simply performs retrieval using the ubiquitous BM25 scoring function [38] over bag-of-words representations of the passages in the corpus. We set  1 = 0.82 and  = 0.68, based on tuning on a selection of training instances on the MS MARCO passage ranking test collection [19]. \n\nBM25 w/ doc2query-T5 [34,35] (BM25-T5 for short) augments passages in the corpus with query predictions generated by the T5 [37] neural sequence-to-sequence model. The expanded passages are scored using BM25 at retrieval time, with the same BM25 formulation and parameters above. Thus, while neural models are involved in corpus preparation, the assignment of term weights does not involve any neural networks. We examined the following retrieval models that leverage sparse learned representations using transformers: DeepImpact [30] uses doc2query-T5 to identify dimensions in the passage's bag-of-words representation that should have nonzero weights (i.e., expansion terms) and learns a term weighting model based on a pairwise loss between relevant and non-relevant passages with respect to a query. uniCOIL + doc2query-T5 [18] (uniCOIL-T5 for short) is a simplified variant of COIL [12] that assigns scalar weights to terms (as opposed to vector weights in the original COIL formulation). This model additionally benefits from doc2query-T5 expansions. uniCOIL + TILDE [47] (uniCOIL-TILDE for short) can be best characterized as replacing the doc2query-T5 expansion component with an alternative model based on TILDE [48] that has lower inference costs but appears to be just as effective. SPLADEv2 [10] represents an improvement over SPLADEv1 [11], which itself builds on SparTerm [2]. For this family of sparse retrieval models, the expansion component can be best characterized as being based on masked language modeling. SPLADEv2 further improves effectiveness via distillation techniques.",
            "score": 0.5877379476804188,
            "section_title": "Retrieval Models",
            "char_start_offset": 10476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "216642169"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1693,
                    "end": 1697,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "271038977",
            "title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models",
            "text": "Vector retrieval algorithms are crucial for semantic queries and have become increasingly integral to the deployment of Large Language Models (LLMs). Effective interaction with LLMs frequently necessitates the provision of relevant or similar examples to elicit enhanced responses (Liu et al., 2022). The introduction of Retrieval Augmented Generation (RAG) has notably advanced the capabilities in knowledge-intensive tasks (Lewis et al., 2020), underscoring the growing importance of retrieval methods. Empirical evidence suggests that employing the BM25 algorithm to select examples from the training set markedly improves LLMs performance over random selection (Liu et al., 2022;Luo et al., 2023). Moreover, leveraging existing text embedding models for example retrieval often surpasses BM25, particularly in specific contexts (Reimers & Gurevych, 2019;Wang et al., 2022). And the advent of Dense Retrieval, which employs dense vectors for semantic matching in latent spaces (Chen et al., 2017;Lee et al., 2019), represents a evolution over traditional sparse retrieval methods like BM25 by utilizing the robust modeling capabilities of pre-trained language models to learn relevance functions (Devlin et al., 2019). Innovations such as the applying the dual encoder framework (Karpukhin et al., 2020) and dynamic listwise distillation (Ren et al., 2021) have further refined the effectiveness of dense retrieval techniques. Subsequent enhancements in semantic parsing and in-context learning (Pasupat et al., 2021), facilitated by feedback from LLMs (Rubin et al., 2022), have enabled more precise example selection and improved answer accuracy. Despite ongoing advancements in retrieval methods, the broadening application scope of LLMs necessitates retrieval approaches that balance relevance with diversity-specifically, a relevance-focused diversity rather than an unrestricted diversity. Additionally, the RAG framework's ability to augment the LLMs' external data access also underscores the need for simple yet efficient algorithms that can streamline the retrieval process. \n\nConsidering the balance between similarity and diversity, the Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998) is an effective algorithm and has been widely applied in vector retrieval practices.",
            "score": 0.5872788433890077,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2087
                },
                {
                    "start": 2090,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 425,
                    "end": 445,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 665,
                    "end": 683,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 832,
                    "end": 858,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 980,
                    "end": 999,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1199,
                    "end": 1220,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1282,
                    "end": 1306,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1341,
                    "end": 1359,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1498,
                    "end": 1520,
                    "matchedPaperCorpusId": "239016988"
                },
                {
                    "start": 1556,
                    "end": 1576,
                    "matchedPaperCorpusId": "245218561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "The related works are reviewed from two aspects: dense retrieval, LLM-based retriever. \n\nFirst of all, in the realm of neural retrievers, dense retrieval models have consistently demonstrated superior performance over traditional sparse models like BM25 across a wide array of retrieval tasks (Karpukhin et al., 2020;Ni et al., 2021;Muennighoff et al., 2022). A critical factor contributing to the success of dense retrieval models is the utilization of powerful pre-trained language models as their initialization. \n\nOver the past few years, pre-trained language models such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020) have been intensively used as backbone encoders for dense retrieval. For instance, GTR (Ni et al., 2021) highlights the in-domain accuracy and generalization capabilities of T5-based dense retrieval models, with model parameters reaching up to 4.8 billion. Fang et al. (2024) explores scaling laws for dense retrieval models but restricts their study to BERT backbones with up to 110 million parameters and only explores the in-domain situation. Currently, state-ofthe-art dense retrievers employ models with more than 7 billion parameters or more as backbones. Neelakantan et al. (2022) discuss large-scale unsupervised text embedding pre-training, observing consistent performance improvements when scaling up GPT-based dense retrieval model sizes from 300 million to 175 billion parameters. Additionally, recent studies such as Wang et al. (2023) have shown that fine-tuning directly with labeled data can achieve strong performance. Our study focuses on fine-tuning directly using labeled data while comparing various backbone encoders. \n\nLarge Language Models (LLMs) have recently demonstrated significant potential as backbone encoders for dense retrieval, attributed to their vast number of parameters and extensive pre-training. Repllama (Ma et al., 2023) fine-tuned Llama-2-7B and Llama-2-13B to function both as dense retrievers and pointwise rerankers.",
            "score": 0.5864744041750065,
            "section_title": "Related Work",
            "char_start_offset": 6353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 613,
                    "end": 634,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7705078125
        },
        {
            "corpus_id": "277786967",
            "title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models",
            "text": "Different from the prevalent dense retrieval method (Karpukhin et al., 2020;Xiong et al., 2021, inter alia) that represents a document with a dense vector, the sparse retrieval method represents a document with a vocabulary-sized vector where most of the elements are zeros, hence the term \"sparse\". This sparse vector representation can be subsequently used in an inverted index for efficient retrieval. Examples of sparse retrieval include classical methods such as the boolean model (Salton, 1984) and probabilistic retrieval models like BM25 (Robertson et al., 1995). \n\nTraditional sparse retrieval methods focus on capturing lexical match signals, which hinders performance is finding semantically relevant documents (Yates et al., 2024). Learned sparse retrieval emerges as a way to leverage pre-trained language models to mitigate this weakness. At a higher level, LSR can be viewed as a way to learn token importance or \"impact\" scores from data (Dai and Callan, 2019;Bai et al., 2020;Mallia et al., 2021).",
            "score": 0.5863099702191084,
            "section_title": "Sparse Retrieval",
            "char_start_offset": 5865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1014
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 76,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 486,
                    "end": 500,
                    "matchedPaperCorpusId": "15148967"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "274596690"
                },
                {
                    "start": 954,
                    "end": 976,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 993,
                    "end": 1013,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "257985191",
            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
            "text": "Passage retrieval involves searching a large corpus for passages that are relevant to a particular query, and is essential for various applications such as web search (Fan et al., 2021;Guo et al., 2022;Lin et al., 2021a), question answering (Karpukhin et al., 2020;Lee et al., 2020;Zhu et al., 2021), and dialogue systems (Gao et al., 2022;Yu et al., 2021). Sparse retrieval methods, such as BM25, are practical and dominant approaches. However, retrieval methods based on pre-trained language models (PLMs) (Devlin et al., 2018;Liu et al., 2019) have gained popularity in recent years. PLMs are utilized to improve (1) sparse retrieval models that rely on lexical match; (2) dense retrieval models that model the semantic interaction between queries and passages in latent semantic space. \n\nTo improve sparse retrieval, PLM mitigates vocabulary mismatch by projecting each term in the query and passage to a vocabulary-sized weight vector. Each dimension in the weight vector represents the weight of a term in the PLM vocabulary, and the correlation between query and passage depends on lexical matching (Formal et al., 2021b,a;Shen et al., 2022). PLM-based sparse vectors model the features of passage on every lexical term (denoted by PLM logits output), and thus naturally capture the semantic at the lexicon level. SPLADE (Formal et al., 2021b,a) uses learnable PLM, e.g., BERT, to produce sparse vectors and fine-tune the retriever with contrastive learning loss. PLM-based SPLADE outperforms traditional sparse retrieval like BM25. LexMAE (Shen et al., 2022) further expands on SPLADE by proposing to pre-train a lexical-bottlenecked masked autoencoder to learn importance-aware sparse representations. It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck.",
            "score": 0.5844487985193451,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 340,
                    "end": 356,
                    "matchedPaperCorpusId": "234343311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "The overall experimental results on the test set are shown in Table 5, from which we can observe that: \n\n(1) On three different domain datasets in Multi-CPR, the retrieval performance of dense models outperforms sparse models. Taking the BM25 model and the DPR-1 model as an example, the average MRR@10 value over the three datasets are 0.2124 and 0.2837 respectively. The retrieval performance on the MRR@10 metric is largely improved by 33.57%, which points out the value of high-quality labeled data for the optimization of dense passage retrieval models. (2) For sparse methods, the BM25 method provides a strong baseline for all the three domain datasets. Especially on the dataset, the retrieval performance of BM25 is even slightly better than the DPR model (MRR@10: 0.2253 vs 0.2106, Recall@1000: 0.8150 vs 0.7750). We infer that the reason for this phenomenon is that the average length of query and passage in the domain of E-commerce is relatively short, and the search intent is explicit to some content. The method based on exact term matching can provide satisfactory retrieval results. This observation illustrates that traditional unsupervised term-based retrieval methods such as BM25 can still provide valuable results for passage retrieval in some specific domains. Moreover, as an optimization method that has been verified in previous work, Doc2Query has also achieved significant improvement on all three datasets as expected. \n\n(3) For dense methods, we conduct the analysis from two aspects. In the dataset aspect, we can find that the DPR model trained on in-domain labeled dataset has achieved remarkable performance improvement compared to the dense model trained with general domain data, even though the size of labeled dataset is much smaller. As such, we can conclude that labeled data in general domain is helpful for training dense retrieval model on specific domain to some extent, but the in-domain labeled data could provide more effective and valuable information for model training.",
            "score": 0.5842979026263524,
            "section_title": "Results",
            "char_start_offset": 22398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "275921747",
            "title": "Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios",
            "text": "The retrieval methods can be divided into two categories: (1) Sparse retrieval. Sparse retrieval retrieves the most similar items from an index based on query keywords or feature vectors, such as BM25. (2) Dense retrieval. Dense retrieval methods (Xiao et al., 2023;Reimers and Gurevych, 2019;Jiang et al., 2023b;G\u00fcnther et al., 2023) involve training retrieval models on large corpora, generating dense vector representations for queries and demonstrations through model inference, and computing similarities based on these vector representations.",
            "score": 0.5810049219378931,
            "section_title": "Retrieval Methods",
            "char_start_offset": 8351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 548
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 313,
                    "matchedPaperCorpusId": "259858947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Dense retrievers have shown difficulties in out-ofdomain settings (Sciavolino et al., 2021;Thakur et al., 2021), where even sparse models like BM25 significantly outperform them. We now offer an intuitive explanation to these failures via token amnesia. We focus on setups where BM25 outperforms dense models and ask: why do dense retrievers fail to model lexical overlap signals? To answer this question, we consider subsets of NQ and EntityQs where BM25 is able to retrieve the correct passage in its top-5 results. We focus on these subsets as they contain significant lexical overlap between questions and passages (by definition, as BM25 successfully retrieved the correct passage). Let q be a question and p the passage retrieved by BM25 for q, and Q and P be their corresponding vocabulary projections for some dense retriever. Also, let T \u2286 V be the set of tokens that appear in both q and p (excluding stop words). Figure 5 shows the maximum (i.e., lowest) rank of tokens from T in the distributions P (left) and Q (right) as a function of whether DPR is able to retrieve this passage (i.e., the rank of p in the retrieval results of DPR). Indeed, the median max-rank over questions for which DPR succeeds to fetch p in its top-5 results (blue box) is much lower than that of questions for which DPR fails to retrieve the passage (red box). As expected (due to the fact that questions contain less tokens than passages), the ranks of shared tokens in question projections Q are much higher. However, the trend is present in Q as well. Additional figures (for EntityQs; as well as median ranks instead of max ranks) are given in App. C. Overall, these findings indicate a correlation between token amnesia and failures of DPR. Next, we introduce a method to address token amnesia in dense retrievers, via lexical enrichment of dense representations.",
            "score": 0.5806281999398373,
            "section_title": "Token Amnesia is Correlated with Retriever Failures",
            "char_start_offset": 17126,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "Furthermore we see that dense retrieval (PARM-VRRF DPR) outperforms lexical retrieval (PARM-RRF BM25) at cut-off values above 500. \n\nIn order to analyze the differences between PARM and document-level retrieval further, we analyze in Figure 3, how many relevant documents are retrieved with PARM or with document-level retrieval with lexical (BM25) or dense methods (DPR). Furthermore we investigate how many relevant documents are retrieved by both PARM and document-level retrieval (PARM \u2229 Doc), and how many relevant documents are retrieved only with PARM and not with document-level retrieval (PARM\\Doc) and vice versa (Doc\\PARM). When comparing the performance of PARM and document-level retrieval, we find that PARM retrieves more relevant documents in total for both test collections. PARM retrieves 142 \u2212 380 of the relevant documents that did not get retrieved with document-level retrieval (PARM\\Doc), which are 15 \u2212 52% of the total number of relevant documents. This analysis demonstrates that PARM largely retrieves many of relevant documents that are not retrieved with document-level retrieval. We conclude that PARM is not only beneficial for dense but also for lexical retrieval.",
            "score": 0.5800859255677234,
            "section_title": "RQ2: PARM VRRF vs Document-level retrieval",
            "char_start_offset": 23008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "265213270",
            "title": "Scalable and Effective Generative Information Retrieval",
            "text": "Fine-tune the Pre-trained language models (LMs) [11,24,34,39] on information retrieval (IR) tasks have proven to be more effective compared to traditional models [19,30,46], such as BM 25 in various scenarios. This might because LMs, pre-traiend on vast amounts of text data, can have more deep understanding of language semantics. The contextualized representations by LMs also provide flexibility to make it adapt to different IR model designs. The integration of these LMs with neural IR models can be broadly categorized into four main streams: (1) neural sparse retrieval models, (2) neural re-ranking models, (3) dense retrieval models, and (4) generative retrieval models. \n\nNeural sparse retrieval models, inspired by conventional bag-ofwords approaches like TF-IDF [36] and BM25 [37], adapt BERT to reweight subwords, thereby enhancing IR performance. To maintain the sparsity of high-dimensional vectors, they utilize L1 [48] or Flop [13] regularizers. This characteristic sparsity allows them to be incorporated into fast search frameworks based on the inverted index [38]. \n\nRe-ranking with LMs is another approach where LMs serve as re-rankers [30,52]. By feeding a concatenated query and document, these models produce a relevance score. Despite their often superior performance, they are only suited for document re-ranking due to efficiency constraints. \n\nDense retrieval models are based on bi-encoder architectures [16,17,19,20,33,46,49,50]. These models, typically leveraging BERT, encode each document and query into dense representations. For efficient retrieval, they employ approximated nearest neighbor (ANN) search [26,46]. Lastly, the generative retrieval paradigm [3,41] is an innovative approach drawing inspiration from successful generative LMs [8,31,34]. In this paradigm, models like T5 are treated as retrievers. Each document is mapped to a distinct sequence, often denoted as a DocID. At inference, given a specific query, a constrained beam search [41,51] retrieves a list of the most probable DocIDs.",
            "score": 0.5793019451941688,
            "section_title": "RELATED WORK",
            "char_start_offset": 32782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 1440,
                    "end": 1443,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "8915893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "273549875",
            "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
            "text": "Additionally, we use two types of retrievers: BM25 as a sparse retriever (SR) and Voyage-Code-2 as a dense retriever (DR). \n\nTo analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data. \n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods. \n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data. \n\nFinally, when comparing Func-PKG to Block-PKG, the results demonstrate that leveraging more granular data, particularly at the block level, significantly enhances model accuracy. Block-PKG enhances precision by retrieving relevant individual code blocks instead of entire functions. This approach involves pruning irrelevant branches from the DAG associated with the selected blocks, ensuring that only the most pertinent contextual information is leveraged. By focusing on finergrained code structures, Block-PKG achieves superior performance across most models, offering a more targeted and efficient retrieval process. \n\nRQ3: Which problem topics benefit more from RAG, and which benefit less? \n\nThis research question explores the performance of RAG across various problem categories.",
            "score": 0.578692546686022,
            "section_title": "RQ1: Does PKG improve code generation?",
            "char_start_offset": 23753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2039
                },
                {
                    "start": 2042,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2206
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "277451883",
            "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) systems have emerged as a powerful paradigm for enhancing the factuality and reliability of large language model (LLM) outputs by grounding responses in external knowledge sources. At the core of effective RAG systems lies the retrieval component, which is responsible for identifying and surfacing the most relevant documents from a knowledge base in response to user queries. The quality of retrieval directly impacts the overall performance of RAG systems, making it a critical area for optimization. \n\nHybrid retrieval (Ma et al., 2020;Sawarkar et al., 2024;Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap. While the complementary strengths of these methods are well established, effectively balancing their contributions remains challenging. Current approaches (Bruch et al., 2023) typically employ a fixed weighting parameter (\u03b1) determined through offline tuning on validation datasets. This static weighting scheme, however, fails to account for the diverse nature of user queries, where the optimal balance between keyword matching and semantic similarity varies significantly based on query characteristics and knowledge base structure. \n\nRecent efforts to address this limitation include approaches that assign different \u03b1 values based on query types (e.g., fact-seeking, concept-seeking, etc.) (Theja, 2024). However, these methods still rely on predetermined categories with fixed weights and often overlook the complex interplay between individual queries and the knowledge base. If this assumption holds-that many queries benefit more from extreme values (i.e., pure BM25 or pure dense retrieval)-then using a compromise value such as \u03b1 = 0.5, while seemingly optimal on average, may in fact lead to suboptimal performance for most individual queries. This would pose a significant challenge to hybrid retrieval optimization. \n\nThese limitations and opportunities motivate our research questions: \n\n\u2022 How can we effectively combine sparse and dense retrieval methods to maximize retrieval performance?",
            "score": 0.5786294694614358,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2222
                },
                {
                    "start": 2225,
                    "end": 2293
                },
                {
                    "start": 2296,
                    "end": 2398
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 581,
                    "end": 598,
                    "matchedPaperCorpusId": "221668044"
                },
                {
                    "start": 598,
                    "end": 620,
                    "matchedPaperCorpusId": "269043117"
                },
                {
                    "start": 891,
                    "end": 915,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1148,
                    "end": 1168,
                    "matchedPaperCorpusId": "253080555"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "269430346",
            "title": "ExcluIR: Exclusionary Neural Information Retrieval",
            "text": "To evaluate the performance of various retrieval models on exclusionary retrieval, we select three types of retrieval models with different architectures: sparse retrieval, dense retrieval, and generative retrieval.Sparse retrieval methods calculate the relevance score of documents using term matching metrics such as TF-IDF (Robertson and Walker, 1997).\n\n\u2022 BM25 (Robertson et al., 2009) is a classical probabilistic retrieval method based on the normalization of the frequency of the term and the length of the document.\n\n\u2022 DocT5Query (Nogueira et al., 2019) expands documents by generating pseudo queries using a fine-tuned T5 model before building the BM25 index (Raffel et al., 2020).\n\nDense retrieval utilizes pre-trained language models (PLMs) as the backbones to represent queries and documents as dense vectors for computing relevance scores.\n\n\u2022 DPR (Karpukhin et al., 2020) is a dense retrieval model based on dual-encoder architecture, which uses the representation of the [CLS] token of BERT (Kenton and Toutanova, 2019).\n\n\u2022 Sentence-T5 (Ni et al., 2022a) uses a fine-tuned T5 encoder model to encode queries and documents into dense vectors.\n\n\u2022 GTR (Ni et al., 2022b) has the same architecture as Sentence-T5 and has been pretrained on two billion question-answer pairs collected from the Web.\n\n\u2022 ColBERT (Khattab and Zaharia, 2020) is a late interaction model that learns embeddings for each token in queries and documents, and then uses a MaxSim operator to calculate the relevance score.\n\nGenerative retrieval is an end-to-end retrieval paradigm.\n\n\u2022 GENRE (De Cao et al., 2020) retrieves entities by generating their names through a seq-to-seq model, it can be applied to document retrieval by directly generating document titles.The original GENRE is trained based on BART as the backbone, and we reproduce it using T5.\n\n\u2022 SEAL (Bevilacqua et al., 2022) retrieves documents by generating n-grams within them.",
            "score": 0.5779595640268114,
            "section_title": "Methods for comparison",
            "char_start_offset": 10291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1306
                },
                {
                    "start": 1308,
                    "end": 1503
                },
                {
                    "start": 1505,
                    "end": 1562
                },
                {
                    "start": 1564,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1836
                },
                {
                    "start": 1838,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 354,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 364,
                    "end": 387,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 537,
                    "end": 560,
                    "matchedPaperCorpusId": "208612557"
                },
                {
                    "start": 667,
                    "end": 688,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 859,
                    "end": 883,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1049,
                    "end": 1066,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1572,
                    "end": 1593,
                    "matchedPaperCorpusId": "222125277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "258841762",
            "title": "BM25 Query Augmentation Learned End-to-End",
            "text": "While recent dense retrievers have shown strong retrieval performance (Reimers and Gurevych, 2019;Karpukhin et al., 2020), their high latencies limit their application to first-stage retrieval. To improve efficiency, late-interaction approaches have been proposed (Khattab and Zaharia, 2020;Gao et al., 2021;Formal et al., 2021). Here, documents are first retrieved with an inverted-index and then scored by aggregating term embeddings precomputed while indexing. Although such methods reduce retrieval latencies, the need to store dense representations of documents significantly increases index sizes (Thakur et al., 2021). Document expansion methods, such as Doc2query (Nogueira et al., 2019;Nogueira and Lin, 2019), allow indexing and retrieval using standard BM25. Nogueira and Lin (2019) use a language model to predict possible queries given a document; augmented documents are then constructed by appending these queries. By adding to the document, this approach addresses the term mismatching issue affecting sparse retrieval methods. However, this approach also requires running a language model over every document, which is expensive, especially if new documents are added incrementally. It is also potentially infeasible to do this when documents are very long. In contrast, we restrict our method to only perform neural operations on the queries, which are usually much shorter than documents. \n\nTo the best of our knowledge, only a few recent methods meet this requirement of only modifying queries. Nogueira and Cho (2017) use reinforcement learning to predict discrete augmentations. GAR (Mao et al., 2021) and SEAL (Bevilacqua et al., 2022) train language models to generate target documents or n-grams. Our approach differs from these methods by optimizing for BM25 retrieval in an end-to-end fashion, and in being significantly faster.",
            "score": 0.5766910211860894,
            "section_title": "Related Work",
            "char_start_offset": 9995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 98,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 98,
                    "end": 121,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 291,
                    "end": 308,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 603,
                    "end": 624,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1515,
                    "end": 1538,
                    "matchedPaperCorpusId": "125545"
                },
                {
                    "start": 1605,
                    "end": 1623,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 1633,
                    "end": 1658,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "Examining the overall performance of all retrieval models in Table 6, we observe that the generalizability defects for unseen tasks are common among models. In the entity linking (EL) task, the models' generalization performance drops significantly, likely due to the task's distinct format compared to the others. DPR lags behind BM25 almost across the board when faced with unseen tasks. The reason may be the large differences in data distribution across tasks. The semantic representations that DPR learns by learning from the original task are empirical and difficult to flexibly migrate to the new task. While dense retrieval models have excellent performance, there are situations where traditional sparse retrieval models are rather more to be relied upon. \n\nWhen we observe the performance between dense and generative IR models, we find that, in general, generative IR models have higher DR OOD on slot-filling (SF) task. This could be because the format of this downstream task aligns with the pre-training tasks of the backbone generative models. Comparing BART and CorpusBrain from the generative IR models, we observe that CorpusBrain outperforms BART in most (13 out of 20) unseen task scenarios. This may be attributed to the pre-training tasks of CorpusBrain. CorpusBrain includes three tasks: Inner Sentence Selection (ISS), Lead Paragraph Selection (LPS), and Hyperlink Identifier Prediction (HIP). ISS models the semantic granularity differences between queries and documents in various retrieval requirements, helping to bridge the gap between different downstream tasks. This finding is consistent with the original analysis of CorpusBrain [3].",
            "score": 0.5763402908573474,
            "section_title": "Analysis of OOD generalizability on unseen tasks",
            "char_start_offset": 19589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "256389427",
            "title": "Finding the Law: Enhancing Statutory Article Retrieval via Graph Neural Networks",
            "text": "We compare our approach against three strong retrieval systems. As a sparse baseline model, we follow prior work and consider BM25 (Robertson et al., 1994), 4 a popular bag-of-words retrieval function based on exact term matching. We then examine the document expansion technique docT5query (Nogueira and Lin, 2019), which augments each article with a pre-defined number of synthetic queries generated by a finetuned mT5 model, 3 and then uses a traditional BM25 lexical index from the augmented articles for retrieval. Last, we include the results of a supervised dense passage retrievers (Karpukhin et al., 2020, DPR) pre-finetuned on more than 90.5k question-context pairs from a combination of three French QA datasets. 5",
            "score": 0.5760876467720757,
            "section_title": "Baselines",
            "char_start_offset": 17390,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 155,
                    "matchedPaperCorpusId": "41563977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
            "score": 0.5755534497756001,
            "section_title": "Traditional Document Retrievers",
            "char_start_offset": 4664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 178,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 178,
                    "end": 196,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 220,
                    "end": 244,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 558,
                    "end": 578,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 594,
                    "end": 614,
                    "matchedPaperCorpusId": "258865472"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 824,
                    "end": 848,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 921,
                    "end": 943,
                    "matchedPaperCorpusId": "220302658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "Passage Retrieval Passage retrieval task aims to recall all potentially relevant passages from a large corpus given an informationseeking query. In practical, passage retrieval is often an important step in other information retrieval tasks [4]. Traditional passage retrieval systems usually rely on term-based retrieval models like BM25 [46]. Recently, with the rapid development in text representation learning research [3] and deep pre-trained language models [21,27,33,51], dense retrieval combined with pre-trained language models, has become a popular paradigm to improve retrieval performance [16,26,42]. In general, dense models significantly outperform traditional term-based retrieval models in terms of effectiveness and benefit downstream tasks. In a basic concept, the core problem of passage retrieval is how to form the text representation and then compute text similarity. Thus, based on the text representation type and corpus index mode, passage retrieval models can be roughly categorized into two main classes. Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]. \n\nFor the above two types of models, the current optimization directions are not the same. Specifically, Sparse retrieval models \n\n(Adults should not put their hands on the chest of a sleeping baby as this is not good for the baby's breathing.) \n\nfocus on improving retrieval performance by either enhancing the bag-of-words (BoW) representations in classical term-based methods or mapping input texts into latent space (e,g. doc2query [37], query expansion [7] and document expansion [39]). The sparse representation has attracted great attention as it can be easily integrated into the inverted index for efficient retrieval. Recently, With the development of deep neural networks, pre-trained language models have been widely employed to improve the capacity of sparse retrieval models, including term re-weighting [9,10], sparse representation learning [24,50], etc. The mainstream of existing studies on improving the performance of dense retrieval models can be roughly divided into three groups. 1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation.",
            "score": 0.57551178635563,
            "section_title": "RELATED WORK",
            "char_start_offset": 4763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2380
                },
                {
                    "start": 2381,
                    "end": 2538
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "1058977"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "1815908"
                },
                {
                    "start": 2199,
                    "end": 2202,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 2239,
                    "end": 2242,
                    "matchedPaperCorpusId": "235293983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "262947262",
            "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
            "text": "They illustrate that incorporating a sparse retrieval model to form a keywordaware retrieval method can significantly boost the performance of dense retrieval models. However, the fusion of two dense retrieval models does not improve and even impairs the performance compared to using a single dense retrieval model. The sparse-dense fused model gains the best performance for top-5 retrieved results. \n\nFirst, both dense models achieve large performance increases when combined with the BM25 model. Compared with SBERT1 only, BM25+SBERT1 shows 42.1% and 12.5% raises on F1@k, for k = 5, 10 respectively (Line 2, 5). Compared with SBERT2 only, BM25+SBERT2 also manifests a performance improvement of 13.8% on F1@5 and 9.3% on F1@10 (Line 3, 6). In contrast, the SBERT1+SBERT2 combination causes a performance drop compared with each individual model (Line 2, 3, 4). Second, BM25+SBERT1 is the best-performed model over the top-5 retrieved computational notebooks. Compared with BM25, the P@5, R@5, F1@5 and NDCG@5 increase by 4.9%, 9.0%, 5.2% and 9.3%, respectively (Line 1, 5). An exception is P@10, R@10 and F1@10, the reason why the scores are low in metrics@10 could be that dense retrieval models introduce more irrelevant computational notebooks in the top-10 results. It provides the community with an opportunity to develop more effective fusion strategies for sparse and dense retrieval models to leverage the full set advantages of both models. Table VI shows top-3 ranking examples for the query \"visual saliency\" with BM25(text+code) and BM25+SBERT1(text+code) methods. Due to the page limit, we only display descriptive texts. Readers can access the whole computational notebooks via given URLs. Compared with BM25, the fused model of BM25 and SBERT1 improves the ranking by shifting relevant results one spot higher.",
            "score": 0.5753159963159492,
            "section_title": "Table IV and Table V list the experimental results.",
            "char_start_offset": 30612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "score": 0.5750640587588091,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3876953125
        },
        {
            "corpus_id": "267406766",
            "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
            "text": "In this section, we evaluate the retrieval performance of our proposed CorpusLM models based on T5 and LLama2, compared with a range of baseline models, as presented in Table 2. \n\nComparison with Sparse&Dense Retrievers: Despite the competitive performance of dense retrievers like MT-DPR, E5, RAG, and SimLM, which have been fine-tuned through multi-task training, our CorpusLM models achieve superior performance in most datasets by a significant margin (t-test,  < 0.05). Specifically, on the FEVER dataset, T5 and LLama2-based CorpusLM outperform MT-DPR by 18.10% and 18.99% respectively. Similarly, on the zsRE dataset, CorpusLM (T5) and CorpusLM (LLama2) surpass SimLM by 12.44% and 12.03% respectively. We also observe that the bag-of-words method BM25, a sparse retrieval model, falls behind neural retrieval models in all tasks, highlighting the need for neural models due to the complexity of these tasks.",
            "score": 0.5743546060948039,
            "section_title": "Retrieval Performance",
            "char_start_offset": 26530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 915
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "This section presents a detailed zero-shot evaluation of various retrieval and reranking methods within a retrieval-augmented generation (RAG) framework. The performance of these methods is assessed across three widely used questionanswering benchmarks: Natural Questions (NQ), TriviaQA, and WebQuestions (WebQ). The study compares different retriever and reranker models, including BM25, Multi-Stage Search (MSS), Contriever, Dense Passage Retrieval (DPR), and hybrid approaches such as retrieval + generation (R+G) and generation + retrieval (G+R). The results are reported for multiple state-of-the-art large language models, including LLaMA-3 8B, LLaMA-3.1 8B, Gemma-2 (2B and 9B), LLaMA-2-13B, and Mistral-7B-v0.1.",
            "score": 0.5739754186315948,
            "section_title": "A.3 QA performance comparison on multiple benchmarks",
            "char_start_offset": 33493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 719
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "235490586",
            "title": "Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval",
            "text": "To test the effectiveness of our proposed dense PRF approach, we compare with four families of baseline models, for which we vary the use of a BERT-based reranker (namely BERT or ColBERT). For the BERT reranker, we use OpenNIR [21] and capreolus/ bert-base-msmarco fine-tuned model from [19]. For the ColBERT reranker, unless otherwise noted, we use the existing pre-indexed ColBERT representation of documents for efficient reranking. The four families are: Lexical Retrieval Approaches: These are traditional retrieval models using a sparse inverted index, with and without BERT and ColBERT rerankers, namely: (i) BM25 (ii) BM25+BERT (iii) BM25+ColBERT, (iv) BM25+RM3, (v) BM25+RM3+BERT and (vi) BM25+RM3+ColBERT. Neural Augmentation Approaches: These use neural components to augment the (sparse) inverted index: (i) BM25+DeepCT and (ii) BM25+docT5query, both without and with BERT and ColBERT rerankers. For BM25+docT5query+ColBERT, the ColBERT reranker is applied on expanded document texts encoded at querying time, rather than the indexed ColBERT representation. The response time for BM25+docT5query+ColBERT reflects this difference. Dense Retrieval Models: This family consists of the dense retrieval approaches: (i) ANCE: The ANCE [34] model is a single representation dense retrieval model. We use the trained models provided by the authors trained on MSMARCO training data.",
            "score": 0.5735902277550597,
            "section_title": "Baselines",
            "char_start_offset": 24973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "245144556",
            "title": "Large Dual Encoders Are Generalizable Retrievers",
            "text": "We consider various baselines, including sparse retrieval models: BM25, DocT5Query, and dense retrieval models: DPR, ANCE, TAS-B, and GenQ (Thakur et al., 2021). We conduct experiments on four different sizes of our GTR models (GTR- GTR-Base outperforms BM25 on 9 tasks with larger GTR models continuing improving on these 9 tasks. GTR-XXL catches-up/surpasses BM25 on another 5 tasks and only under-performs on the remaining 5. \n\nBase, GTR-Large, GTR-XL, and GTR-XXL). We also consider three different settings for GTR to investigate the effect of scaling up for different training stages: \n\n\u2022 GTR: the full GTR models that conduct both pre-training and fine-tuning. \u2022 GTR-FT: only fine-tuned on MS Marco without pre-training. \u2022 GTR-PT: only pre-trained on CommunityQA without fine-tuning. We evaluate our models on BEIR (Thakur et al., 2021) as discussed in Section 2.2. We consider two retrieval metrics: NDCG@10 and Recall@100 following BEIR. Due to space limitations, we report the Recall@100 results in Appendix A.",
            "score": 0.5731481888779717,
            "section_title": "Models for comparison",
            "char_start_offset": 10348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 160,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 822,
                    "end": 843,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "265033969",
            "title": "GLEN: Generative Retrieval via Lexical Index Learning",
            "text": "We compare GLEN with three types of baseline models, including two sparse retrieval models (BM25 (Robertson and Walker, 1994) and DocT5Query (Nogueira and Lin, 2020)), four dense retrieval models (DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), Sentence-T5 (Ni et al., 2022a), and GTR (Ni et al., 2022b)), and six generative retrieval models. For generative retrieval methods, we categorize them following the Table 1. (i) Static numeric identifier. DSI (Tay et al., 2022) uses a sequence-to-sequence model to generate numeric identifiers built by hierarchical k-means clustering. DSI-QG (Zhuang et al., 2023) and NCI (Wang et al., 2022) are built upon DSI while adopting augmented data via query generation and prefix-aware weight-adaptive decoder, respectively. (ii) Static lexical identifier. GENRE (Cao et al., 2021) utilizes a title as an identifier. SEAL (Bevilacqua et al., 2022) generates arbitrary n-grams to retrieve relevant documents, utilizing the FM-Index structure. TOME (Ren et al., 2023) performs retrieval by generating the document URLs via a two-stage generation architecture. (iii) Dynamic numeric identifier. GENRET (Sun et al., 2023) learns how to assign numeric identifiers based on a discrete auto-encoding scheme. (iv) Dynamic lexical identifier. To our knowledge, GLEN is the first work to employ the dynamic lexical identifier. For details of sparse and dense retrieval models, see Section A.1.",
            "score": 0.5724462335842926,
            "section_title": "Baselines",
            "char_start_offset": 18663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 125,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 201,
                    "end": 225,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 232,
                    "end": 252,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 294,
                    "end": 311,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 597,
                    "end": 618,
                    "matchedPaperCorpusId": "249890267"
                },
                {
                    "start": 811,
                    "end": 829,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 870,
                    "end": 895,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 995,
                    "end": 1013,
                    "matchedPaperCorpusId": "258762633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44775390625
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Retrieval Models: We used five retrieval models in our experiments: BM25, a sparse vectorbased method; DPR, a dense dual-encoder model that maximizes similarity between questions and relevant passages; MSS, a dense retriever pretrained on predicting masked spans like named entities; MSS-DPR, combining MSS pre-training with DPR's fine-tuning for improved performance; and Contriever, an unsupervised dense retriever  optimized for zero-shot performance through contrastive learning. \n\nGenerative Models: For the generation-based retrieval, we employ GenRead (Yu et al., 2022) a generative model designed for open-domain QA tasks, which first generates contextual documents based on the query and then predicts the final answer using those generated documents.",
            "score": 0.5718765439362248,
            "section_title": "Retrieval and Generative Models",
            "char_start_offset": 15564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 760
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "Firstly, we examine the effectiveness of the obtained external feedback for sparse retrieval. In Table 5, for four query sets (Robust04 title-only, Robust04 description-only, WT10G title-only and WT10G description-only), we report results when external retrieval is performed using sparse models (BM25 & DPH), as well using dense retrieval (ANCE & ColBERT). In the table, columns are used to show the processes for each retrieval stage. On each test collection, we firstly report the performance of the four sparse retrieval baselines, including the BM25, BM25 with RM3 query expansion model, DPH, and DPH with Bo1 query expansion model. Then, we measure the retrieval effectiveness of the traditional external expansion models on the sparse retrieval and the external expansion models for sparse retrieval but with feedback documents obtained using dense retrieval. \n\nOn analysing Table 5, firstly, we compare the performance of the external expansion models on sparse retrieval using RM3 and Bo1 query expansion models with the sparse retrieval models without any PRF mechanism applied and the sparse PRF models applied only on the target collection on both Robust04 and WT10G test query sets. We notice that both RM3 and Bo1 based external expansion models on sparse retrieval can significantly improve over the models without the external expansion technique applied, which attest the usefulness of the MSMARCO passage as an external collection for both the Robust04 and WT10G corpora. Secondly, we examine the performance of the external expansion with passages produced by dense retrieval. There are four models reported under this external expansion scheme. We observe that the highest values for most of the metrics reported are given by the external expansion models. Similarly, all the external expansion models with dense retrieved passages are significantly improved over all the baselines without external expansion technique applied. When comparing the external expansion models combining dense retrieval with sparse PRF, we find that the former models can significantly improve over the traditional external expansion models using sparse retrieval, which indicates the superiority of the feedback documents produced by dense retrieval viz. the feedback documents produced by the sparse retrieval, e.g. compared to baselines (e) & (f) in Table 5.",
            "score": 0.5716157700150195,
            "section_title": "RQ1: Dense external expansion for sparse retrieval",
            "char_start_offset": 33922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2316
                },
                {
                    "start": 2317,
                    "end": 2360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "258762633",
            "title": "TOME: A Two-stage Approach for Model-based Retrieval",
            "text": "Text Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009). In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019;Raffel et al., 2020), dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020;Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021). The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020;Ren et al., 2021a,b;Asai et al., 2021;Zhou et al., 2022a). Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\n\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021;Tay et al., 2022). These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021), which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c;Bevilacqua et al., 2022;Zhuang et al., 2022;Wang et al., 2022;Lee et al., 2022;Chen et al., 2022;Zhou et al., 2022b). As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to",
            "score": 0.571323860505049,
            "section_title": "Related Works",
            "char_start_offset": 4771,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 273,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 387,
                    "end": 408,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 528,
                    "end": 555,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 555,
                    "end": 578,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 578,
                    "end": 597,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 597,
                    "end": 613,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 798,
                    "end": 817,
                    "matchedPaperCorpusId": "253080762"
                },
                {
                    "start": 1220,
                    "end": 1237,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1416,
                    "end": 1434,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1621,
                    "end": 1641,
                    "matchedPaperCorpusId": "247187834"
                },
                {
                    "start": 1641,
                    "end": 1665,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Retrieval plays a fundamental role in the TriviaQA dataset, where models must extract relevant information from large document collections to answer trivia-based, open-domain questions. This section provides a detailed comparison of various retrieval methods, including sparse retrievers like BM25, dense retrievers such as DPR, and generator models. The retrieval effectiveness of these models is measured using Top-1, Top-5, and Top-10 accuracy, which represent the percentage of cases in which a correct document appears within the top-k retrieved results. \n\nThe results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness. MSS-DPR follows closely, with a Top-1 accuracy of 73.5%, suggesting that additional pretraining techniques further enhance retrieval performance. Generative augmentation also proves valuable, as GenRead achieves a Top-1 accuracy of 69.7%, surpassing BM25 and approaching the effectiveness of dense retrievers. \n\nThe advantages of generator approaches become more evident in the Top-5 and Top-10 accuracy metrics. MSS-DPR+Gen leads with an 85.0% Top-5 accuracy, followed closely by DPR+Gen at 84.4%, indicating that the combination of retrieval and generation improves ranking effectiveness. BM25+Gen also sees significant improvements, achieving 84.4% in Top-5 accuracy, compared to BM25 alone at 73.6%. In the Top-10 retrieval setting, hybrid models consistently outperform retrieval-only methods, with DPR+Gen reaching 85.2% and MSS-DPR+Gen achieving 85.0%. These findings confirm that hybrid approaches, which integrate retrieval with generative document expansion, provide more robust and reliable retrieval for complex QA tasks.",
            "score": 0.5711395734853756,
            "section_title": "A.1 Retrieval Performance on TriviaQA",
            "char_start_offset": 28826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1963
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Passage retrieval aims to retrieve the relevant passages of a query from a large corpus, which benefits many downstream applications, such as web search (Fan et al. 2021;Guo et al. 2022;Lin, Nogueira, and Yates 2021), question answering (Karpukhin et al. 2020;Lee et al. 2020;Zhu et al. 2021) and dialogue systems (Gao et al. 2022a;Yu et al. 2021).\n\nFor a long time, sparse retrieval represented by BM25 (Robertson, Zaragoza et al. 2009) was the dominant retrieval method. Recently, dense retrieval has received increasing attention with the development of pre-trained language models (PLM) (Devlin et al. 2018;Liu et al. 2019). Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al. 2021;Humeau et al. 2019;Xiong et al. 2020;Zhan et al. 2021Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al. 2021;Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
            "score": 0.5710732012907471,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 170,
                    "end": 186,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 186,
                    "end": 216,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 332,
                    "end": 347,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 404,
                    "end": 436,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 834,
                    "end": 858,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 895,
                    "end": 911,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1402,
                    "end": 1418,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "258840999",
            "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
            "text": "We use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson and Zaragoza, 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers.",
            "score": 0.5708438319111951,
            "section_title": "A.3 Retrievers",
            "char_start_offset": 15196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 702
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 143,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 390,
                    "end": 414,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 516,
                    "end": 537,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "276774808",
            "title": "Tailoring Table Retrieval from a Field-aware Hybrid Matching Perspective",
            "text": "Typical text retrievers can be classified into three types based on representations used: sparse, dense, and hybrid retrieval which combines them. \n\nSparse retrievers refer to models that use sparse representations such as TF-IDF (Sparck Jones, 1972) and BM25 (Robertson and Walker, 1994). Building on pre-trained language models (PLMs), SparTerm (Bai et al., 2020) and SPLADE (Formal et al., 2021) aim to generate term distributions over vocabulary. Dense retrievers usually map inputs to continuous vectors based on PLMs (Devlin et al., 2019;Liu et al., 2019). The hidden state of [CLS] as the representation of the query and passages are used for matching. \n\nDense and Sparse retrieval have distinct advantages. While dense retrieval generally outperforms sparse retrieval, the latter can be more effective in cold-start situations or when relevant data is limited. Hybrid retrieval can combine the advantages of them (Craswell et al., 2020;Bajaj et al., 2018). A straightforward approach is to train two different types of retrievers independently and then combine their outputs linearly to give a final relevance score (Chen et al., 2021;Kuzi et al., 2020;Lin and Lin, 2021;Luan et al., 2020;Guo et al., 2025). Inspired by the boosting technique, CLEAR (Gao et al., 2020) tries to learn a BERT-based retriever from the residual of BM25. In addition, knowledge distillation is also an approach to realize the hybrid retrieval (Shen et al., 2023).",
            "score": 0.5705566705872542,
            "section_title": "Text Retrieval",
            "char_start_offset": 4809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 149,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1450
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 250,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 1197,
                    "end": 1214,
                    "matchedPaperCorpusId": "265033839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "To test the effectiveness of our proposed dense PRF approach, we compare with five families of baseline models, for which we vary the use of a BERT-based reranker (namely BERT or ColBERT). For the BERT reranker, we use OpenNIR [24] and capreolus/ bert-base-msmarco fine-tuned model from [21]. For the ColBERT reranker, unless otherwise noted, we use the existing pre-indexed ColBERT representation of passages for efficient reranking. The five families are: Lexical Retrieval Approaches: These are traditional retrieval models using a sparse inverted index, with and without BERT and ColBERT rerankers, namely: (i) BM25 (ii) BM25+BERT (iii) BM25+ColBERT, (iv) BM25+RM3, (v) BM25+RM3+BERT and (vi) BM25+RM3+ColBERT. Neural Augmentation Approaches: These use neural components to augment the (sparse) inverted index: (i) BM25+DeepCT and (ii) BM25+docT5query, both without and with BERT and ColBERT rerankers. For BM25+docT5query+ColBERT, the ColBERT reranker is applied on expanded passage texts encoded at querying time, rather than the indexed ColBERT representation. The response time for BM25+docT5query+ColBERT reflects this difference. Dense Retrieval Models: This family consists of the dense retrieval approaches: (i) ANCE: The ANCE [46] model is a single representation dense retrieval model. We use the trained models provided by the authors trained on MSMARCO training data. (ii) ANCE-PRF: The ANCE-PRF [48] is a PRF variant of ANCE model -we use the results released by the authors.",
            "score": 0.5702600086012146,
            "section_title": "Baselines.",
            "char_start_offset": 28415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "210883962"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "267771785",
            "title": "Generate to Understand for Representation in One Pre-training Stage",
            "text": "Retrieval is typically perceived as a complex system. BM25 [4] serves as a powerful and straightforward sparse retrieval model. Recently, pretraining representation models and interaction models have been incorporated into dense retrieval models [13]. Some research focuses on identifying similar samples, such as the Inverse Cloze Task (ICT) [14], and employs contrastive learning to enhance text representation [15].",
            "score": 0.5698868781011583,
            "section_title": "B. Retrieval",
            "char_start_offset": 5167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 418
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "245144844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020). More recently, Chen et al. (2021) attempted to train a dense retriever (called as dense Lexical Model \u039b) to imitate BM25 prediction. Though \u039b model underperforms BM25 by itself on EQ dataset, when combined with DPR trained on multiple QA datasets (DPRmulti), the combined model (SPAR) outperforms BM25. SPAR requires two individually trained bi-encoders (thus four BERT models) with tangled architecture, whereas our motivation is to investigate if dense retrievers could generalize with a single language model without ensemble. We also investigate whether dense retrieval has any strengths in areas where sparse retrieval is lacking. We experimentally demonstrate the differences between dense and sparse retrieval by using the IDF value to quantify the generality and rarity of an entity name in a question. \n\nMultiple Keys for Passage Retrieval. Several methods have been proposed for calculating fine-grained interactions between a question and passages using multiple retrieval keys. SPARTA (Sparse Transformer Matching, (Zhao, Lu, and Lee 2021)) and COIL (Contextualized Inverted List, (Gao, Dai, and Callan 2021)) produce passage representations for each token embedded by fine-tuned language models and store them in the inverted index. ColBERT (Khattab, Potts, and Zaharia 2021) leverages the embedding of all subword tokens in each passage to form a large collection of keys. However, all of the methods listed here require the encoders for questions and passages to be fine-tuned. During this fine-tuning for retrieval, multiple index updates are required, which is computationally expensive. We seek a retrieval method that works in zero-shot setting, so we propose a method that only requires the inference operation of a pretrained language model together with named-entity recognition (NER). \n\nLUKE model (Yamada et al. 2020) is the state-of-the-art language model for tasks related to named entities, including NER.",
            "score": 0.5697744588267339,
            "section_title": "Related Work",
            "char_start_offset": 6953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 152,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1182,
                    "end": 1206,
                    "matchedPaperCorpusId": "221971009"
                },
                {
                    "start": 1248,
                    "end": 1275,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "261076036",
            "title": "Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track",
            "text": "The text retrieval task is usually divided into two sub tasks according to the length of the text: passage retrieval and document retrieval. Whether it is a Document ranking or Passage ranking task, under the setting of full ranking, it generally needs to be processed in two stages: retrieval and ranking. Among them, the retrieval stage needs to quickly find the most relevant top-k candidates in the entire corpus set given the input query, and then a more complex and accurate ranking stage will be performed over the top-k candidates thus producing the final ranked result. \n\nFor the retrieval stage, BM25 [RZ + 09] is a bag-of-words retrieval method that ranks a set of documents based on the query terms appearing in each document and is one of the best retrieval algorithms. Recently, various improved methods [NLE19, FLPC21] based on BM25 have been proposed in the sparse retrieval research field. With the development of deep neural network models, the performance of dense retrieval models such as DPR [KOM + 20], and coCondenser [GC21,GC22] have surpassed traditional methods with a large margin. For the ranking stage, since the amount of processed data is greatly reduced, a more complex model structure can be adopted. Benefit from the excellent performance of large-scale pre-trained language model (e,g. BERT [DCLT19]) on various natural language processing tasks, the ranking model is also gradually turning to the BERT model as the relevance scoring model. Specifically, the query and doc will be concatenated as the input of BERT or other pre-trained language models like RoBerta [LOG + 19], ERNIE [LLL + 22], ELEC-TRA [CLLM20], and Deberta [HLGC20]. \n\nBased on this multi-stage processing method, our main optimization methods in this evaluation are: 1. In the retrieval stage, a hybrid retrieval method that combines traditional sparse retrieval and dense retrieval model is adopted, and the ROM [LZXX22] model we proposed for retrieval tasks is used on the dense retrieval pre-training model.",
            "score": 0.5690554597424052,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1047,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1047,
                    "end": 1052,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1326,
                    "end": 1334,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1661,
                    "end": 1669,
                    "matchedPaperCorpusId": "219531210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "233296511",
            "title": "Contextualized Query Embeddings for Conversational Search",
            "text": "First-stage retrieval comparisons. Table 3 reports the sparse and dense retrieval effectiveness of various methods. Overall, dense retrieval yields better effectiveness than BM25 retrieval. Observing the first block in Table 3, CQE-sparse yields reasonable effectiveness compared to the other CQR models, indicating that CQE can be well represented with text. As for dense retrieval, CQE is able to beat the other CQR models. Although NTR (T5) and CQE yield comparable top-ranking accuracy, it is worth mentioning that unlike CQE, the other CQR modules are built independently. Thus, when incorporated with dense retrieval, the overall memory and latency required increase, i.e., # params of NTR (T5) increases from 220M to 330M and is much slower. Finally, we also conduct CQE dense-sparse hybrid retrieval using their linear score combination (denoted by CQE-hybrid); see Appendix A for detailed settings. CQE-hybrid retrieval effectiveness shows significant gains over CQE dense only. The gains from the dense-sparse hybrid suggest that the textual interpretation of CQE not only helps us understand the query reformulation mechanism in dense retrieval but also improves effectiveness, all using a single, unified model.\n\nA comparison of win (tie) entries shows that CQE has more wins against human queries than all the other CQR models. On the other hand, the other CQR models have relatively more ties against human queries than CQE. The difference between  Figure 4: Case studies. We choose cases based on nDCG dense retrieval scores; the CQE text shown is for sparse retrieval. Underline denotes terms not appearing in human queries. CQR + BM25 + BERT-base: latency = 5,350 ms QuReTec (Voskarides et al., 2020) .476 Few-Shot Rewriter (Yu et al., 2020) .492 3CQR + BM25 + BERT-base: latency = 8,025 ms (est.) MVR (Kumar and Callan, 2020) .565\n\nCQR + BM25 + BERT-large: latency = 16,450 ms Transformer++ (Vakulen",
            "score": 0.5689445081409878,
            "section_title": "Results on CAsT",
            "char_start_offset": 18246,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1741,
                    "end": 1758,
                    "matchedPaperCorpusId": "219559295"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458984375
        },
        {
            "corpus_id": "246863456",
            "title": "Saving Dense Retriever from Shortcut Dependency in Conversational Search",
            "text": "To examine how dense retriever trained on the dataset behave, we contrast a dense retriever with its lower and upper bound models in terms of dependency on the retrieval shortcut. For this, we train two Dense Passage Retriever (DPR) models with in-batch negatives (Karpukhin et al., 2020) by feeding X t and X t \\{q t } as input query to each model. 2 We denote the latter one as DPR \u2297 , and it represents the lower bound model that does not consider the current question q t at all. Surprisingly, we find the DPR \u2297 performs 78% of R@10 and 85% Figure 2: Analysis on QReCC (Anantha et al., 2021) for identifying the shortcut. We denote X t as a conversational input including the current question q t while X t \\{q t } does not contain the q t . (a) Lexical assessment using BM25 to quantify the shortcut in the dataset. BM25 shows small performance drop in R@10 even without considering the current question. (i.e. X t \\{q t } as an input.) (b) Comparison of original DPR (Karpukhin et al., 2020) with its lower bound in terms of shortcut dependency, DPR \u2297 taking only X t \\{q t } and upper bound, GPT2QR generating and using standalone question q \u2032 t to retrieve. DPR \u2297 shows comparable performance without using the current question. (c) Breakdown results of each model by three question types in R@10. Most of the performance gain comes from no-switch questions on both original and shortcut-dependent DPRs. \n\nof R@100 compared to DPR as shown in Figure 2 (b). Thus, we presume the original DPR model is also likely to depend on the shortcut. Next, we introduce the upper bound model, GPT2QR (Anantha et al., 2021). It is less likely to be exposed to the shortcut since it first generates standalone question q \u2032 t , and then its BM25 retriever only takes the decontextualized q \u2032 t as input.",
            "score": 0.5687280688421911,
            "section_title": "Lower and Upper bounds Analysis",
            "char_start_offset": 6087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 288,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 573,
                    "end": 595,
                    "matchedPaperCorpusId": "222290679"
                },
                {
                    "start": 973,
                    "end": 997,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1596,
                    "end": 1618,
                    "matchedPaperCorpusId": "222290679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270751953125
        },
        {
            "corpus_id": "269302879",
            "title": "Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding",
            "text": "Classic Neural IR Models: With the emergence of large language models (LLMs) [14,17,39,43,50] and large-scale information retrieval datasets [4,32], neural-based IR models have demonstrated superior results over the traditional lexical-matching models, such as BM25 [55].In general, these IR models can fall into three categories: (1) cross-encoder models [42,48,79], (2) dense retrieval models [22,24,28,30,38,72], and (3) sparse retrieval models [12,13,18,19].The cross-encoder model is often parameterized with LLMs, such as BERT [17] or T5 [50], and takes the concatenation of query and document pair as input to predict their relevant score.This model is effective but slow and is usually used for re-ranking.As for retrieval, the dense retrieval model often uses the bi-encoder architecture to encode the query and document separately into the low-dimensional hidden space and apply the approximate nearest neighborhood (ANN) [40,68] search for fast retrieval.Sparse retrieval is an alternative method for retrieval, in which it encodes the query and document into the high-dimensional vector space, and usually, each element in the vector represents the importance score of a certain token.To filter out those useful tokens, the L1 [69] or FLOPs [18,19,44] regularizer will be incorporated into the objective function to sparsify the high-dimension vectors.For retrieval, the inverted index will be employed similar to BM25.Generative Retrieval Models: Generate Retrieval (GR), diverges from the traditional \"index-then-retrieve\" paradigm used in the sparse and dense retrieval models, offering a novel approach for document retrieval.In GR, each document is represented as a unique document identifier (DocID), and a sequence-to-sequence model is trained to generate relevant DocIDs given a query.\n\nDocIDs are usually fixed in the fine-tuning stage and hence serving as bottleneck for affecting the effectiveness of GR models.",
            "score": 0.5677359176037866,
            "section_title": "RELATED WORK",
            "char_start_offset": 5336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1642
                },
                {
                    "start": 1642,
                    "end": 1805
                },
                {
                    "start": 1807,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 84,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "252993059"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "252212320"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "265455958",
            "title": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights",
            "text": "While Large Language Models have demonstrated remarkable capabilities in natural language processing tasks, they are prone to hallucinations, which can result in incorrect diagnoses and treatments, leading to adverse effects on patients. Retrieval-Augmented Generation (RAG) is a method that can be used to reduce hallucinations in LLMs, as well as increase trust in AI-based tools by explicitly linking their output to external knowledge. \n\nRAG involves augmenting LLMs with information retrieval (IR) systems, which can provide relevant content retrieved from external corpora as references. By incorporating external knowledge, retrieval-augmented LLMs can answer indomain questions that cannot be answered by solely relying on the world knowledge stored in the model's own parameters. \n\nInformation retrieval systems often make use of pretrained models for Dense Passage Retrieval (DPR) and baseline systems such as BM25, a traditional IR strategy that uses term frequency-inverse document frequency (TF-IDF) weighting to rank documents based on their relevance to a query. In this work, we focus on creating a dense concept and sentence representation model which is well suited for use, among other tasks, as a DPR model within a biomedical RAG pipeline.",
            "score": 0.5675258845588824,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 9020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1260
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Let D = d 1 , d 2 , . . . , d M be a collection of evidence documents representing a retrieval corpus. Given a query q, an Information Retrieval (IR) model selects a subset of relevant passages Z \u2282 D, one or more of which will ideally contain the correct answer to q. Our setup supports passages obtained from any retriever, whether based on sparse representations like BM25 (Robertson and Zaragoza, 2009) or dense representations such as DPR (Karpukhin et al., 2020a), MSS (Sachan et al., 2021), MSS-DPR (Sachan et al., 2021), and Contriever (Izacard et al., 2021). BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively. In addition to DPR, we test models such as MSS, which focuses on masked salient span prediction, and MSS-DPR, which extends DPR with additional pre-training using MSS. Another dense retriever, Contriever, is trained in an unsupervised manner using contrastive learning on text paragraphs. We assume that each retriever provides the top-K most relevant passages, denoted as Z = {z 1 , z 2 , . . . , z K }.",
            "score": 0.566933199133434,
            "section_title": "Retriever Models",
            "char_start_offset": 7254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 405,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 443,
                    "end": 468,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 543,
                    "end": 565,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "For PARM with BM25 we also investigate which aggregation strategy leads to the highest retrieval effectiveness in order to have a strong baseline. As BM25 does not provide dense embeddings only rank-based aggregation strategies are applicable. \n\nParagraph and document-level labelled training (RQ3). We train a DPR model on a paragraph-and another document-level labelled collection and compare the retrieval performance of PARM for document-to-document retrieval. As bi-encoders for DPR we choose BERT [12] and LegalBERT [8]. We train DPR on the paragraph-level labelled collection COLIEEPara train and additionally on the document-level labelled collection COLIEEDoc train as described in Section 3.3. We use the public code 4 and train DPR according to Karpukhin et al. [19]. We sample the negative paragraphs randomly from randomly sampled negative documents and take the 20 paragraphs of a positive document as positive samples, which have the highest BM25 score to the query paragraph. This training procedure lead to the highest recall compared to training with all positive paragraphs or with BM25 sampled negative paragraphs. We also experimented with the DPR model pre-trained on open-domain QA as well as TAS-balanced DPR model [18], but initial experiments did not show a performance improvement. We train each DPR model for 40 epochs and take the best checkpoint according to COLIEEPara test/COL-IEEDoc val. We use batch size of 22 and a learning rate of 2 * 10 \u22125 , after comparing three commonly used learning rates (2 * 10 \u22125 , 1 * 10 \u22125 , 5 * 10 \u22126 ) for [19].",
            "score": 0.5667556385483008,
            "section_title": "Baselines",
            "char_start_offset": 18241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1572,
                    "end": 1576,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346435546875
        },
        {
            "corpus_id": "273350862",
            "title": "A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies",
            "text": "This section describes three cross-lingual retrieval baselines and the retriever settings used in our ex-periment. Figure 2 provides an overview of these three baseline approaches for the cross-lingual SAR task. The Sparse Retrieval approach involves first translating the query into the same language as the corpus and then performing term-based retrieval to obtain relevant documents. In contrast, the Dense Retrieval approach directly uses a multilingual embedding model to perform cross-lingual retrieval. \n\nAdditionally, we introduce a third method, LLM-Augmented Retrieval, designed to enhance retrieval performance by utilizing LLM-generated content. This method incorporates Answer Expansion, Statutory Article Expansion, and LLM-based Reranking. Please refer to Subsection 4.1 for more details. \n\nRetrieval Model We use BM254 as our base model for sparse retrieval. Note that all the queries were translated into the same language as the Statu-",
            "score": 0.5661789129020953,
            "section_title": "Methodology",
            "char_start_offset": 7264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 953
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "In this paper we address the challenges of using dense passage retrieval (DPR) in first stage retrieval for document-to-document tasks with limited labelled data. We propose the paragraph aggregation retrieval model (PARM), which liberates dense passage retrieval models from their limited input length and which takes the paragraph-level relevance for document retrieval into account. We demonstrate on two test collections higher first stage recall for dense document-to-document retrieval with PARM than with document-level retrieval. We also show that dense retrieval with PARM outperforms lexical retrieval with BM25 in terms of recall at higher cut-off values. As part of PARM we propose the novel vector-based aggregation with reciprocal rank fusion weighting (VRFF), which combines the advantages of rank-based aggregation with RRF [10] and topical aggregation with dense embeddings. We demonstrate the highest retrieval effectiveness for PARM with VRRF aggregation compared to rank and vector-based aggregation baselines. Furthermore we investigate how to train dense retrieval models for dense document-to-document retrieval with PARM. We find the interesting result that training DPR models on more, but noisy document-level data does not always lead to overall higher retrieval performance compared to training on less, but more accurate paragraph-level labelled data. Finally, we analyze how PARM retrieves relevant paragraphs and find that the dense retrieval model learns a structural paragraph relation which it exhibits with PARM and therefore benefits the retrieval effectiveness.",
            "score": 0.5657988376799257,
            "section_title": "Conclusion",
            "char_start_offset": 27275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1598
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "12408211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68408203125
        },
        {
            "corpus_id": "266844358",
            "title": "Building Efficient and Effective OpenQA Systems for Low-Resource Languages",
            "text": "Table 15 shows the resource usage of the retriever models for each stage of the retrieval process. We observe that BM25 exhibits the lowest resource footprint across nearly all categories in each phase, offering the fastest indexing and retrieval speeds. In contrast, dense retrievers demonstrate varying resource consumption across different phases. \n\nIn training, DPR uses more memory than ColBERT-QA, mainly because DPR depends on in-batch negatives and its performance is tied to batch size. Additionally, DPR's final model size is twice as large as that of ColBERT-QA because DPR uses two separate BERT models for query and document encoding. In contrast, ColBERT-QA employs a single shared BERT model for both encoders. \n\nDuring indexing, BM25 demonstrates exceptional speed when indexing the knowledge source compared to dense retrievers. It also maintains a low memory footprint and minimal index size. DPR and ColBERT-QA have a similar memory footprint. DPR exhibits slower indexing performance relative to ColBERT-QA. (For all models, indexing speed can be boosted using multiple parallel processes.) Additionally, DPR produces a smaller index than ColBERT-QA, in line with the fact that DPR generates fixed-size vectors while ColBERT-QA produces a sequence of token embeddings. \n\nIn the retrieval and ranking step, BM25 maintains the minimum footprint across all categories while achieving the fastest speed. DPR and ColBERT-QA, on the other hand, both demand a considerable amount of memory because they load the entire index into memory during retrieval. The size of memory that DPR and ColBERT-QA consume is in line with the size of their index. Meanwhile, BM25 minimizes memory usage by accessing the index directly from disk.",
            "score": 0.5657240746876553,
            "section_title": "Resource Usage of Retriever Models",
            "char_start_offset": 84485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1741
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "264145947",
            "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
            "text": "The baseline models used for comparing each components are as follows: \n\nRetriever: (i) Sparse embeddings: BM25 (Robertson et al., 2009) (ii) Static embedding models: FastText (Athiwaratkun et al., 2018) and doc2vec (Lau and Baldwin, 2016) (iii) Contextual embedding models: Dense Passage Retrieval (DPR) (Karpukhin et al., 2020b) and Sentence-transformers (Zhang et al., 2022) (iv) Hybrid embedding models: SPLADE (Formal et al., 2021). \n\nReader: (i) Encoder-based (LM) models: pretrained reader models including Albert-large (Lan et al., 2020) (ii) Decoder-based (LLM) models: GPT-3.5-turbo and Luminous-extended. \n\nGenerator: (i) GPT-3.5-turbo (ii) Luminousextended.",
            "score": 0.5654781638181543,
            "section_title": "A.2 Baselines",
            "char_start_offset": 23776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 669
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 136,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 176,
                    "end": 203,
                    "matchedPaperCorpusId": "47012788"
                },
                {
                    "start": 305,
                    "end": 330,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 357,
                    "end": 377,
                    "matchedPaperCorpusId": "246822639"
                },
                {
                    "start": 527,
                    "end": 545,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "237213465",
            "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval",
            "text": "Retrieval in each language was performed using Facebook's Faiss library for nearest neighbor search (Johnson et al., 2017); we used the FlatIP indexes. Experiments were conducted using the same codebase as the DPR replication experiments of Ma et al. (2021), with the Pyserini toolkit (Lin et al., 2021a) . \n\nOur choice of zero-shot mDPR as a baseline deserves some discussion. At a high level, we are interested in the generalizability of dense retrieval techniques in out-of-distribution settings (in this case, primarily different languages). Operationally, our experimental setup captures the scenario where the model does not benefit from any exposure to the target task, even (question, relevant passage) pairs in the English portion of Mr. TYDI. This makes the comparison \"fair\" to BM25, which is similarly not provided any labeled data from the target task (in the case with default parameters). \n\nSparse-Dense Hybrid Our hybrid technique combines the scores of sparse (BM25) and dense (mDPR) retrieval results. The final fusion score of each document is calculated by s sparse + \u03b1 \u2022 s dense , where s sparse and s dense represent the scores from sparse and dense retrieval, respectively. This strategy is similar to the one described by Ma et al. (2021). We take 1000 hits from mDPR and 1000 hits from BM25 and normalize the scores from each into [0, 1] since the range of the two types of scores otherwise are quite different. If one hit isn't found in the other, the normalized score for that hit is set to zero. The weight \u03b1 was tuned in [0, 1] with a simple line search on the development set by optimizing MRR@100 with step size 0.01.",
            "score": 0.5652561159781146,
            "section_title": "Baselines",
            "char_start_offset": 17150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 304,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "277786967",
            "title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models",
            "text": "Recently, the main research focus in information retrieval (IR) has been on dense retrieval and related techniques (Karpukhin et al., 2020;Lin et al., 2022;Zhu et al., 2023a;Xu et al., 2025, inter alia). Dense retrieval encodes queries and documents into high-dimensional sparse vectors. Although effective, these dense vectors are difficult for humans to interpret in terms of their semantic meanings. Moreover, encoding and storing the dense vectors for the whole document collection can be resourceintensive. For example, encoded flat index of MS MARCO passage corpus (Bajaj et al., 2016) with LLAMA-2-7B dense retriever takes up 135G disk space (Ma et al., 2024), which is over 50 times larger than the 2.6G Lucene index from BM25. \n\nTo mitigate these drawbacks of dense retrieval, a different line of works investigates learned sparse retrieval (LSR). Inspired by traditional sparse retrieval models (Sparck Jones, 1972;Robertson et al., 1995), LSR encodes queries and documents into vocabulary-sized vectors with a backbone language model and the language model head, where each dimension of the vector represents the \"impact\" of the corresponding token (Formal et al., 2021b,a;Mallia et al., 2021). A canonical example of LSR is SPLADE (Formal et al., 2021b,a). It encodes text with BERT (Devlin et al., 2019), then applies pooling and log-saturation (Fang et al., 2004) to ensure the resulting vocabulary-sized vector contains non-negative values in each dimension, making it suitable for use in an inverted index. Combined with established training methodologies in dense retrieval such as contrastive learning (Oord et al., 2018), hard negatives mining (Karpukhin et al., 2020;Xiong et al., 2021) and knowledge distillation (Hofst\u00e4tter et al., 2020), LSR has demonstrated competitive performance with BERT-style encoder-only masked language models (Kong et al., 2023;Lassance et al., 2024).",
            "score": 0.5650736014686364,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 139,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 649,
                    "end": 666,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 905,
                    "end": 925,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 925,
                    "end": 948,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1184,
                    "end": 1204,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1295,
                    "end": 1316,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1358,
                    "end": 1377,
                    "matchedPaperCorpusId": "15200693"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "274437524",
            "title": "Efficient Learning Content Retrieval with Knowledge Injection",
            "text": "Information retrieval in LLMs is challenging because learned information is stored implicitly in the model parameters for most of the pretrained LLMs [12], [25], [2]. This makes LLMs hard to adapt for generating factual responses in Q&A systems. RAG approaches are used to overcome this problem and improve language models by easy alignment for various specific tasks. \n\nThe idea of using a non-parametric memory for retrieving external data for improving parametric language models is the starting point for developing RAG. Early studies generally use the retrieval model in the fine-tuning stage and train both retriever and generator models [26]. Vector space representations also evolved in time in retrieval-augmented language models. Methods such as TF-IDF and BM25 are examples of strong sparse vector space representations. [11] shows that DPR could outperform these traditional methods for Q&A systems. It could also be used as a retriever component to provide latent documents that are conditioned on the input. DPR method is also used with small datasets in [27]. [27] uses BLEU and ROUGE metrics for the evaluation. Human evaluation is also very important for evaluating and preparing this kind of system for customer interaction and production. A custom human evaluation interface is also proposed in this study. [28] is another study that proposes a retriever model evaluated for open Q&A task. It is a latent knowledge retriever model that supports unsupervised training by explicitly exposing the role of world knowledge by asking the model to decide, unlike traditional language models. Retrieval-augmented architectures could also be used to improve model alignment with few-shot learning without increasing the parameter size. [26] is an example of these kinds of studies which shows that jointly pretraining the retriever model and language model together improves the few-shot learning performance. RALM systems also could be used as in-context learning and could improve language model performances by keeping model parameters unchanged and without any further training [29].",
            "score": 0.5649300818188053,
            "section_title": "C. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 8282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5830078125
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "The results showed that supervised learning with a huge dataset that covers the domain extensively could not lead to good generalization within the domain. \n\nThough BM25 seems to be relatively robust, it may have an inherent drawback: It is not good at searching for common terms in a query, even when the term is important, such as a part of a named entity. Specifically, BM25 uses the inverse document frequency (IDF) weight of the query terms. This property is based on the assumption that rare terms (of higher IDF values) are more informative than common terms (of lower IDF values). However, there are not few cases where named entity consists of common words and a retriever must retrieve relevant passages using it as the dominant clue. For instance, given a question \"Who is the author of Inside Job?\", the entity name \"Inside Job\" consists of common words, and the IDF values of these words are relatively low. In this case, BM25 may have difficulty in retrieving relevant passages. \n\nOne of the reasons for the poor generalization of DPRs can be their poor retrieval accuracy on salient phrases such as named entities (e.g., Karpukhin et al. 2020). To address the problem, Chen et al. (2021) proposed the Salient Phrase Aware Retriever (SPAR) by combining existing DPR and a dense retriever which is trained so as to imitate BM25 prediction, and SPAR showed better performance than BM25 on EQ dataset. Looking at SPAR from a perspective of modeling, it can be considered as the ensemble of two different dense retrievers. In contrast, our motivation is to investigate if dense retrievers could generalize with a single language model without ensemble. \n\nIn this paper, we investigate whether a pretrained language model without supervised fine-tuning can be used for passage retrieval. The approach is inspired by the idea that language models can have a lot more useful knowledge right after pretraining, but it has been forgotten during finetuning (e.g., Chen et al. 2020). We propose Zero-shot Neural Retrieval (Zero-NeR), a simple dense retrieval method that uses multiple contextualized keys for each passage generated from a frozen pretrained language model.",
            "score": 0.5648734530293751,
            "section_title": "Introduction",
            "char_start_offset": 1925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 1136,
                    "end": 1158,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1968,
                    "end": 1985,
                    "matchedPaperCorpusId": "216553067"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "267412148",
            "title": "Domain Adaptation of Multilingual Semantic Search - Literature Review",
            "text": "Ranking adaptation of dense retrievers is performed by integrating sparse-retrieval to combine the strengths of both approaches. Either the dense and sparse retriever relevance scores are directly combined [76,168,173] or an unsupervised retriever is improved by interpolating BM25 and DR relevance scores [134,142]. Another approach utilises RM3 [72] built on top of the sparse results for selecting deep retriever results as the final list [80] and Chen et al. [23] use robust rank-based scores leading to significant zero-shot performance improvements. Another option is to use dense retrievers to generate term weights for improving sparse retrieval results [30] or to apply a classifier to decide whether to use dense, sparse or hybrid retrieval results for each query [5]. However, while integrating sparse retrieval into dense retrieval systems enhances their term-matching capabilities, this approach requires maintaining two indexing systems simultaneously, which is often not feasible in practice. Nevertheless, some approaches that do not require a dual index but still enhance the term-matching capacities of dense retrieval systems exist. It is, for example, possible to learn low-dimensional dense lexical representations [93,94] or to train a dense lexical retriever based on weakly supervised data constructed by BM25 [24]. Another option are pre-trained language models which have lexical awareness (like SPLADE [40]) [188]. They are trained using lexicon-augmented contrastive learning and rank-consistent regularisation.",
            "score": 0.5648595406032009,
            "section_title": "Ranking Adaptation",
            "char_start_offset": 48844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1539
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "222132914"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "246285831"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 774,
                    "end": 777,
                    "matchedPaperCorpusId": "237593050"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "238744204"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "271270735",
            "title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval",
            "text": "4.1 EXPERIMENTAL SETUP We evaluate 13 representative retrieval models, ranging from traditional bag-of-words models to large dense retrieval models, including the top performers from the retrieval set of the MTEB leaderboard (Muennighoff et al., 2023), BEIR (Thakur et al., 2021). First, we employ BM25 (Robertson et al., 2009) as our primary sparse, lexical-based retrieval model, which demonstrates strong performance on BEIR (Thakur et al., 2021), comparable to that of larger trained dense retrieval models. We also evaluate a diverse set of open-source dense retrieval models: the small (<1B) models are SentenceBERT (109M; Reimers & Gurevych, 2019) Table 3: Average reranking performance on BRIGHT. We also include the retrieval results (reranker=None) for comparison. Detailed scores can be found in Table 41 and 42. 2024), GritLM (7.1B; Muennighoff et al., 2024), and gte-Qwen1.5 (7.7B; Li et al., 2023b). Notably, all large dense models and Instructor-Large are instruction-tuned. Lastly, we include proprietary models from Cohere (Cohere), Voyage (Voyage AI), OpenAI (OpenAI), and Google(1.2B) (Lee et al., 2024). We provide details of each model in \u00a7 A.1. Following prior work (Thakur et al., 2021;Bajaj et al., 2018;Voorhees & Tice, 2000), we use nDCG@10 as the main metric.",
            "score": 0.5641206800836095,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 21244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1286
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 251,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 303,
                    "end": 327,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1228,
                    "end": 1250,
                    "matchedPaperCorpusId": "11465263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447265625
        },
        {
            "corpus_id": "238259539",
            "title": "A proposed conceptual framework for a representational approach to information retrieval",
            "text": "The conceptual framework described in this paper clarifies the relationship between recently proposed dense and sparse retrieval methods, and experimental results presented in the previous section begin to help us understand the impact of different design choices. Furthermore, this proposed framework suggests a number of open research questions, which provide a roadmap for future work. I discuss these below: \n\nOut-of-distribution inference In the logical scoring model, explicitly establishing a contrast between supervised (learned) vs. unsupervised representations makes it obvious why DPR is more effective than BM25. However, in a supervised machine-learning paradigm, we are immediately led to the obvious follow-up question: What happens if the trained models are applied to out-of-distribution data? Phrased differently, what is the effectiveness of learned representations in a zero-shot setting? Cast into the same parlance for comparison purposes, BM25 is always applied in a \"zero-shot\" manner (although admittedly, such a statement sounds odd). \n\nIn the information retrieval context, since training data typically comprise (query, relevant document) pairs, out of distribution could mean a number of different things: (1) the document encoder is fed text from a different domain, genre, register, etc. than the training documents, (2) the query encoder is fed queries that are different from the training queries, (3) the relationship between input query-document pairs at inference time differs from the relationship captured in the training data (e.g., task variations), or (4) a combination of all of the above. \n\nIn fact, we already know the answer, at least in part: learned representations often perform terribly in out-of-distribution settings when applied in a zero-shot manner. Evidence comes from the BEIR benchmark [Thakur et al., 2021], which aims to evaluate the effectiveness of dense retrieval models across diverse domains. Results show that, in many cases, directly applying a dense retrieval model trained on one dataset to another dataset sometimes yields effectiveness that is worse than BM25. Complementary evidence comes from Li et al. [2021], who found that for passage retrieval in question answering, training DPR on one dataset and testing on another can lead to poor results.",
            "score": 0.5640312234314047,
            "section_title": "Discussion",
            "char_start_offset": 44152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "268724187",
            "title": "Decoy Effect in Search Interaction: Understanding User Behavior and Measuring System Vulnerability",
            "text": "Traditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46].These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears.However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context.These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.\n\ne., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text.The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space.Ranking models based on dense vectors is referred to as dense retrieval models.Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth.Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2 .\n\nOn the other hand, sparse representations based on pre-training language models (PLMs) have also garnered increasing interest because they inherently inherit desirable properties of lexical models in their design.For example, COntextualized Inverted List (COIL) [29] learns dense term-level representations to perform contextualized lexical matching; uniCOIL [50] further simplifies the approach by learning a single weight per term.",
            "score": 0.5639939114459903,
            "section_title": "Ranking Models in Information Retrieval",
            "char_start_offset": 15846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 686
                },
                {
                    "start": 688,
                    "end": 759
                },
                {
                    "start": 761,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1209
                },
                {
                    "start": 1209,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1766
                },
                {
                    "start": 1768,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1359,
                    "end": 1364,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1402,
                    "end": 1406,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1512,
                    "end": 1515,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1515,
                    "end": 1518,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1583,
                    "end": 1587,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1587,
                    "end": 1591,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1622,
                    "end": 1626,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "257585074",
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "text": "Learned sparse retrieval (LSR) uses a query encoder f Q and a document encoder f D to project queries and documents to sparse vectors of vocabulary size: \n\nThe score between a query a document is the dot product between their corresponding vectors: \n\nThis formulation is closely connected to traditional sparse retrieval methods like BM25; indeed, BM25 [32,33] can be formulated as: \n\nWith BM25 the IDF and TF components can be viewed as query/document term weights. LSR differs by using neural models, typically transformers, to predict term weights. LSR is compatible with many techniques from sparse retrieval, such as inverted indexing and accompanying query processing algorithms. However, differences in LSR weights can mean that existing query processing optimizations become much less helpful, motivating new optimizations [21,22,24].",
            "score": 0.5638671249003134,
            "section_title": "Learned sparse retrieval",
            "char_start_offset": 4734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 842
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "254686753"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "248376951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "258762711",
            "title": "ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval",
            "text": "Effect of Different Components. Table 3 shows the result of ablation studies on four datasets 7 , which demonstrates the superiority of retrieving texts from the corpus for training data creation as well as conducting multi-step retrieval. Besides, label smoothing also results in performance gain as it mitigates the effect of noisy labels for fine-tuning. Besides, we plot the result over different rounds 7 More results on other datasets are in Appendix H. of retrieval in Fig. 1. It is clear that both multistep retrieval and filtering progressively enhance the performance of target tasks, justifying their necessity for improving the quality of training data. \n\nWe have also attempted to conduct more retrieval rounds, but do not observe significant performance gains. \n\nStudy of Dense Retrievers. We compare the retrieval model R \u03b8 with other off-the-shelf unsupervised retrieval models. Here we choose one sparse model BM25 (Robertson et al., 2004) and three DR models: Condenser (Gao and Callan, 2021), Sim-CSE (Gao et al., 2021b), and Contriever (Izacard et al., 2022a). From Figure 2, we observe that the performance of BM25 is not satisfactory, since simply using lexical similarity is insufficient to retrieve a set of diverse documents for fine-tuning. Besides, our retrieval model outperforms other unsupervised DR models for two reasons: (1) Condenser and SimCSE are pretrained over short sentences, and the learning objective is suboptimal for long documents; \n\n(2) these models are not pretrained on the corpus used in our study and suffer from the distribution shifts (Yu et al., 2022b). Instead, our strategy can better adapt the PLM for the retrieval task. In the following sections, we mainly compare REGEN with Mining (van de Kar et al., 2022) and SuperGen (Meng et al., 2022) as they are closest baselines to us.",
            "score": 0.5624128517947649,
            "section_title": "Ablation Studies",
            "char_start_offset": 19342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 932,
                    "end": 956,
                    "matchedPaperCorpusId": "16628332"
                },
                {
                    "start": 1020,
                    "end": 1039,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 1587,
                    "end": 1605,
                    "matchedPaperCorpusId": "253157773"
                },
                {
                    "start": 1749,
                    "end": 1766,
                    "matchedPaperCorpusId": "253116650"
                },
                {
                    "start": 1780,
                    "end": 1799,
                    "matchedPaperCorpusId": "246680398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488037109375
        },
        {
            "corpus_id": "259924840",
            "title": "Learning to Retrieve In-Context Examples for Large Language Models",
            "text": "Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem. Various techniques such as hard negative mining (Karpukhin et al., 2020), knowledge distillation (Ren et al., 2021), and continual pre-training (Wang et al., 2022) have been proposed to enhance the performance of dense retrieval. \n\nRetrieval Augmented LLMs combine the generative power of LLMs with the ability to retrieve relevant information from external sources (Ram et al., 2023;Lewis et al., 2020;Shi et al., 2023). This paradigm has the potential to enhance the factual consistency of generated texts, make LLMs aware of the up-to-date knowledge, as well as provide a natural way for source attribution (Nakano et al., 2021). The retrieved information can be incorporated into LLMs through various mechanisms, such as input concatenation (Shi et al., 2023), intermediate attention fusion (Borgeaud et al., 2022), and output interpolation (Khandelwal et al., 2020). For in-context learning, the goal of retrieval augmentation is to improve the performance of LLMs on downstream tasks by retrieving informative examples (Li et al., 2023;Luo et al., 2023).",
            "score": 0.5622858764399665,
            "section_title": "Related Work",
            "char_start_offset": 5096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1323
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 166,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 311,
                    "end": 335,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 360,
                    "end": 378,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 407,
                    "end": 426,
                    "matchedPaperCorpusId": "254366618"
                },
                {
                    "start": 666,
                    "end": 683,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 873,
                    "end": 894,
                    "matchedPaperCorpusId": "245329531"
                },
                {
                    "start": 1008,
                    "end": 1026,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1058,
                    "end": 1081,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1108,
                    "end": 1133,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "253157773",
            "title": "COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning",
            "text": "The document distribution shift is measured directly at the lexicon Figure 2 plots the distribution shifts from MARCO to BEIR tasks and the corresponding performance differences between dense retrieval and sparse retrieval. We use BM25 as the sparse retrieval method and ANCE starting from pretrained BERT (Xiong et al., 2021) and coCondenser (Gao and Callan, 2022) as representative DR models. \n\nThe average similarity between MS MARCO and BEIR tasks are 32.4% and 34.6% for queries and documents, indicating the existence of significant distribution shifts from MARCO to BEIR. Furthermore, these shifts are correlated with the performance degradation of dense retrieval models, as DR models perform much worse than BM25 on BEIR tasks that are less similar to MS MARCO. The contrastive learning on MARCO does not address this challenge; ANCE initialized from coCondenser still underperforms BM25 on BEIR tasks where distribution shifts are severe.",
            "score": 0.5622780043851054,
            "section_title": "ZeroDR and Distribution Shifts",
            "char_start_offset": 10734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 948
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 326,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 343,
                    "end": 365,
                    "matchedPaperCorpusId": "236987190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "276724936",
            "title": "WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense Retrieval",
            "text": "When comparing the fine-tuned model with BM25, we find that our model outperforms the sparse retrieval approach on WebFAQ. However, on Mr. TyDi and MIRACL, the performance gap between BM25 and the fine-tuned dense retriever narrows. This outcome is expected, as these datasets are evaluated in a zero-shot setting. As noted by Zhang et al. [32], dense retrievers typically struggle in zero-shot retrieval scenarios due to their reliance on in-domain training for learning effective representations. \n\nHybrid. Lastly, we investigate whether our fine-tuned model retains useful relevance signals, even in cases where BM25 outperforms the dense retriever. To explore this, we employ a hybrid retrieval approach that integrates sparse (BM25) and dense (WebFAQ-trained XLM-RoBERTa) retrieval methods. The hybrid model first performs the regular retrieval step for both the sparse and dense model, retrieving the top 1000 documents, respectively. The retrieved sets are then merged by computing a combined similarity score for query  and document   , following Ma et al. [23]: \n\nwhere (,   ) represents the cosine similarity between  and   using XLM-RoBERTa, and BM25 provides the corresponding sparse retrieval score. Our implementation follows the original paper in setting  = 1.1. If a document appears in only one retrieval set, its missing score is assigned as zero. Our findings indicate that the fine-tuned model continues to capture valuable relevance signals, enhancing retrieval performance beyond the BM25 baseline in several cases, even when BM25 alone outperforms the dense retriever.",
            "score": 0.5618615853732705,
            "section_title": "BM25.",
            "char_start_offset": 24579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1591
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "237213465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.638671875
        },
        {
            "corpus_id": "233219855",
            "title": "A Replication Study of Dense Passage Retriever",
            "text": "BM25 retrieval was performed using Lucene with parameters b = 0.4 and k 1 = 0.9. However, the DPR repo does not appear to contain code for reproducing the BM25 and hybrid fusion results. \n\nWe attempted to replicate the retriever results reported in Karpukhin et al. (2020) with Pyserini, an IR toolkit that our group has been developing since 2019 (Lin et al., 2021). The toolkit supports sparse retrieval (i.e., BM25) via integration with another toolkit called Anserini (Yang et al., 2017), which is built on Lucene. Like in the original DPR work, Pyserini supports dense retrieval via integration with Facebook's Faiss library. Combining dense and sparse retrieval, our toolkit supports hybrid retrieval as well. \n\nTo be clear, we started with model checkpoint releases in the DPR repo and did not retrain the query and passage encoders from scratch. Otherwise, our implementation does not share any code with the DPR repo, other than evaluation scripts to ensure that results are comparable. \n\nSimilar to the original work, we calculated hybrid retrieval scores by linear combination of dense and sparse scores, as follows: Sim(q, D j ) + \u03b1 \u2022 BM25(q, D j ). \n\nNote that, contrary to the original work, we placed the \u03b1 weight on the BM25 score because this yields a more natural way to answer the pertinent research question: Given dense retrieval as a starting point, does adding BM25 as an additional relevance signal provide any value? This question is answered by a setting of \u03b1 = 0, which is equivalent to discarding BM25 results. \n\nFinally, there are a few more details of exactly how to combine BM25 and DPR scores worth exploring. As a baseline, we tried using the raw scores directly in the linear combination (exactly as above). However, we noticed that the range of scores from DPR and BM25 can be quite different.",
            "score": 0.5611325974463417,
            "section_title": "Retriever",
            "char_start_offset": 5528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1828
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 272,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "1340183"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499755859375
        },
        {
            "corpus_id": "267308609",
            "title": "Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control",
            "text": "Learned sparse retrieval (LSR) [6,7,41] typically employs transformer-based encoders to encode queries and documents into sparse lexical vectors (i.e., bags of weighted terms) that are compatible with traditional inverted index. LSR has several nice properties. It provides an approach for effective and efficient neural retrieval, like dense retrieval, but with different advantages and trade-offs. For example, sparse representations have the potential to be interpretable because they are aligned with a vocabulary, and they leverage inverted index software rather than approximate nearest neighbor search [41]. Empirically, LSR also shows advantages over single-vector dense models on retrieval generalization benchmarks [6,17]. \n\nWhile LSR and dense retrieval are common in text retrieval, dense retrieval has taken the lead in multi-modal search. This is evident in state-of-the-art text-image pretraining methods like BLIP [23] and ALBEF [24], which rely on dense architectures. The preference for dense models arises because images, unlike text, consist of continuous pixel values, presenting a challenge when they are mapped to discrete lexical terms. \n\n\u22c6 Co-first author.",
            "score": 0.5597609837205911,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1181
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 34,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 34,
                    "end": 36,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 36,
                    "end": 39,
                    "matchedPaperCorpusId": "257585074"
                },
                {
                    "start": 609,
                    "end": 613,
                    "matchedPaperCorpusId": "257585074"
                },
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 945,
                    "end": 949,
                    "matchedPaperCorpusId": "236034189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77587890625
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Retrieval systems have the potential to mitigate serious problems caused by language models, like factual inaccuracies. However, retrieval failures may lead to undesirable behavior of downstream models, like wrong answers in QA or incorrect generations for other tasks. Also, since retrieval models are based on pretrained language models, they may suffer from similar biases. BERT  We use BERT for dense retrieval, mainly as a baseline for DPR, as DPR was initialized from BERT. This allows us to track where behaviors we observe stem from: pretraining or retrieval fine-tuning. We use both CLS and mean pooling for BERT.\n\nS-MPNet is a supervised model trained for Sentence Transformers (Reimers and Gurevych, 2019) using many available datasets for retrieval, sentence similarity, inter alia. It uses cosine similarity, rather than dot product, for relevance scores. It was initialized from MPNet-base (Song et al., 2020), and thus we use this model's MLM head.\n\nSpider (Ram et al., 2022) is an unsupervised dense retriever trained using the recurring span retrieval pretraining task. It was also initialized from BERT-base, and we therefore use the same MLM head for projection as the one used for DPR.\n\nBM25 (Robertson and Zaragoza, 2009) is a lexical model based on tf-idf. We use two variants of BM25: (1) vanilla BM25, and (2) BM25 over BERT/MPNet tokens (e.g., \"Reba\" \u2192 \"re ##ba\"). 6 We consider this option to understand whether the advantages of BM25 stem from its use of different word units from the transformer models.\n\nB Analysis: Further Results Figure 6 gives an analysis of the top-k tokens in the question projection Q and passage projection P . Figure 7 gives further analyses of token amnesia: It contains the results for EntityQuestions, as well as analysis of median ranks in addition to max ranks (complements Figure 5). Table 9 gives the results of our method on the BEIR and MTEB benchmarks for all 19 datasets (complements Table 3). Table 6, Table 7 and Table 8",
            "score": 0.5590775513993637,
            "section_title": "Ethics Statement",
            "char_start_offset": 28778,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490234375
        },
        {
            "corpus_id": "253080762",
            "title": "SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval",
            "text": "We compare our approach with a variety of methods, including sparse and dense retrieval models. \n\n\u2022 BM25 (Yang et al., 2017) is a widely-used sparse retriever based on exact matching. \n\n\u2022 GAR (Mao et al., 2021), doc2query (Nogueira et al., 2019a), DeepCT (Dai and Callan, 2019) and docTTTTTquery (Nogueira et al., 2019b) enhance BM25 by incorporating neural models. \n\n\u2022 DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021) and STAR (Zhan et al., 2021) are dense retrieval methods that adopt top-k hard negatives to improve training. \n\n\u2022 COIL (Gao et al., 2021b) and ME-BERT (Luan et al., 2021) combine sparse and dense representations for text retrieval. \n\n\u2022 Joint and Individual top-k (Sachan et al., 2021) propose to train the dense retrieval model in an end-to-end manner. \n\n\u2022 RocketQA (Qu et al., 2021), RDR (Yang and Seo, 2020), RocketQAv2 (Ren et al., 2021b) and ERNIE-search (Lu et al., 2022) utilize knowledge distillation technique that leverages a teacher model to guide the training of the dense retrieval model. \n\n\u2022 PAIR (Ren et al., 2021a), DPR-PAQ (Oguz et al., 2022), Condenser (Gao and Callan, 2021) and coCondenser (Gao and Callan, 2022) design special pre-training tasks to improve the backbone model for the dense retrieval task. \n\n\u2022 AR2 (Zhang et al., 2021) incorporates an adversarial framework to jointly train the retriever and the ranker. As it has achieved state-of-the-art performance on most datasets, we implement our approach on it to verify its effectiveness.",
            "score": 0.5586900068110486,
            "section_title": "C More Details on Baselines",
            "char_start_offset": 22307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 124,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 192,
                    "end": 210,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 255,
                    "end": 277,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 374,
                    "end": 398,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 545,
                    "end": 564,
                    "matchedPaperCorpusId": "235422120"
                },
                {
                    "start": 577,
                    "end": 596,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 792,
                    "end": 809,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1036,
                    "end": 1055,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1065,
                    "end": 1084,
                    "matchedPaperCorpusId": "236493551"
                },
                {
                    "start": 1135,
                    "end": 1157,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1260,
                    "end": 1280,
                    "matchedPaperCorpusId": "238419331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "276928453",
            "title": "MultiConIR: Towards multi-condition Information Retrieval",
            "text": "Retriever: From Sparse To Dense Traditional sparse retrieval methods are based on BM25 (Robertson and Zaragoza, 2009), TF-IDF (Ramos et al., 2003), etc., rely on keyword matching and statistical weighting to evaluate relevance, which suffers from the well-known issue of lexical gap (Berger et al., 2000), restricting their ability to effectively capture semantic relationships (Luan et al., 2021;Nian et al., 2024). \n\nDense retrieval addresses this limitation by encoding both queries and documents as embeddings within a joint latent space, where the semantic relationship is captured through the similarity scores between their embeddings (Li et al., 2023a). Pretrained language models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), are widely used as backbone encoders for dense retrieval (Li et al., 2023b;Sturua et al., 2024;Xiao et al., 2023). \n\nRecent advancements have shown that LLMs offer significant potential as backbone encoders for dense retrieval (Wang et al., 2024a;Weller et al., 2024c;BehnamGhader et al., 2024). For instance, Repllama (Ma et al., 2023) enhanced retrieval performance by fine-tuning Llama-2 to serve as dense retrievers. GritLM (Muennighoff et al., 2024) unified text embedding and generation within a single LLM. LLM2Vec (BehnamGhader et al., 2024) introduced an unsupervised approach for transforming decoder-only LLMs into dense retrievers. \n\nBenchmarks In Complex Retrieval Tasks Existing datasets for information retrieval, such as MS MARCO (Nguyen et al., 2016), Natural Questions (Kwiatkowski et al., 2019), andMTEB (Muennighoff et al., 2022), primarily focus on queries sourced from search engines. The relationships between queries and documents are typically simple and direct (Su et al., 2024). \n\nRecent studies have expanded retrieval benchmarks to address more complex scenarios.",
            "score": 0.5586174762351196,
            "section_title": "Related Works",
            "char_start_offset": 4870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 117,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 126,
                    "end": 146,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "6749682"
                },
                {
                    "start": 378,
                    "end": 397,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 699,
                    "end": 720,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 728,
                    "end": 749,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 978,
                    "end": 998,
                    "matchedPaperCorpusId": "266693831"
                },
                {
                    "start": 1538,
                    "end": 1569,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Locality-Sensitive Hashing[6, 45] \u00d7 Spectral Hashing [242] \u00d7 Deep Hashing [143] \u2713 K-means tree [228] \u00d7 ANNOY -HNSW [161] \u00d7 Product Quantization [99] \u00d7 Table 3. Knowledge Retrieval Models 5.5.1 Retrieval Strategies. The objective of retrieval is to identify and extract the most relevant knowledge based on the input query. Specifically, the task involves retrieving the top-k most relevant chunks by employing a similarity function. \n\nBased on different similarity functions, retrieval strategies can be categorized into into three types: sparse retrieval, dense retrieval, and hybrid retrieval. Sparse Retrieval. Sparse retrieval strategies leverage sparse vectors to retrieve documents or chunks through term analysis and matching. Traditional sparse retrieval strategies utilize term matching metrics such as BM25 [204], TF-IDF [205] and query likelihood [120], which estimate the relevance of documents to queries by calculating the frequency of term occurrences and inverse document frequency. Despite simplicity, these strategies remain prevalent in RAG due to their computational efficiency .Recently, some research explored the use of transformer language models as sparse retrievers. Models such as uniCOIL [139],TILDE [290] and SPLADE [68] encode text into high-dimensional sparse vectors, effectively representing documents in a bag-of-words fashion. Dense Retrieval. Dense retrieval strategies encode both queries and documents into a low-dimensional vector space, where relevance is measured by the dot product or cosine similarity between their vector representations. Dense retrievers, such as DPR [112] and ANCE [252], are based on BERT architecture pre-training language model and fine-tuned on unsupervised data to generate high-quality representations for queries and documents. More recently, \n\nLLMs have shown remarkable performance in terms of semantic understanding and representational power. Based on the powerful capabilities of LLMs, efforts have been made to generate discriminative text embeddings using \n\nLLMs [129,159,169,174,278].Llama2Vec [129] is a lightweight method that adapts LLMs for dense retrieval using two unsupervised pretraining tasks, EBAE and EBAR.",
            "score": 0.557862475074967,
            "section_title": "Approximate Nearest Neighbor Search",
            "char_start_offset": 54506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 58,
                    "matchedPaperCorpusId": "268118396"
                },
                {
                    "start": 74,
                    "end": 79,
                    "matchedPaperCorpusId": "15281206"
                },
                {
                    "start": 95,
                    "end": 100,
                    "matchedPaperCorpusId": "225118467"
                },
                {
                    "start": 817,
                    "end": 822,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 831,
                    "end": 836,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 858,
                    "end": 863,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 1228,
                    "end": 1233,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1245,
                    "end": 1249,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "270710900",
            "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs",
            "text": "We evaluate diverse retrieval models with publicly available checkpoints, shown in Table 5. Due to the length limit of transformer based models, we restrict the document length to first 512 word pieces, as done in prior work [46].We select diverse retrieval models based on their unique characteristics.BM25 and SPLADE serve as strong lexical and sparse retrievers respectively.DPR is a well known off-the-shelf retrieval model employed for opne-domain QA.Tas-b is chosen due to the strong training objective that uses dual supervision.ANCE employs better sampling of negatives during training and MPNET is chosen due to different pre-training objective compared to BERT based language models.ColBERTv2 is a well known late-interaction model that mitigates the cost of cross-encoder by employing late interaction based attention mechanism while providing superior performance to bi-encoder based dense retrieval models.",
            "score": 0.557659644865943,
            "section_title": "C Further Experimental Details C.1 Retriever",
            "char_start_offset": 35341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 919
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437744140625
        },
        {
            "corpus_id": "266163955",
            "title": "Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback",
            "text": "To investigate the distinctions between the three relevance scenarios, we partitioned the test queries into three categories based on the performance of sparse, dense, and generative retrieval. These categories align with the dimensions of term-based overlap, semantic similarity, and contextual dependency, respectively. We then conducted a comparative analysis of the model's performance before and after reinforcement learning. \n\nFrom Figure 3, we find that the BM25 and DPR models have demonstrated noticeable advantages on their respective query sets. Although the SFT model exhibited certain gaps in comparison, the introduction of reinforcement learning allowed us to incorporate relevance signals from both the sparse and dense models. Furthermore, the SFT model was augmented by incorporating the knowledge of the LLM. As a result, the retrieval quality across the three query sets experienced improvements. These enhancements signify the effectiveness of reinforcement learning process and diverse relevance signals, leading to enhanced retrieval performance.",
            "score": 0.5568890794030543,
            "section_title": "Performance on Different Query Sets",
            "char_start_offset": 24890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "269302715",
            "title": "Multi-view Content-aware Indexing for Long Document Retrieval",
            "text": "Retrieval Methods.Current approaches to content retrieval are primarily classified into sparse and dense retrieval.There are two widely-used sparse retrieval methods, namely TF-IDF (Salton et al., 1983) and BM25 (Robertson et al., 1995).TF-IDF calculates the relevance of a word to a document in the corpus by multiplying the word frequency with the inverse document frequency.BM25 is an advancement of TF-IDF that introduces nonlinear word frequency saturation and length normalization to improve retrieval accuracy.Recently, dense retrieval methods have shown promising results, by encoding content into highdimensional representations.DPR (Karpukhin et al., 2020) is the pioneering work of dense vector representations for QA tasks.Similarly, Col-BERT (Khattab and Zaharia, 2020b) introduces an efficient question-document interaction model, enhancing retrieval accuracy by allowing fine-grained term matching.Contriever (Izacard et al., 2022) further leverages contrastive learning to improve content dense encoding.E5 (Wang et al., 2022) and BGE (Xiao et al., 2023) propose novel training and data preparation techniques to enhance retrieval performance, e.g., consistency-filtering of noisy web data in E5 and the usage of RetroMAE (Xiao et al., 2022) pre-training paradigm in BGE.Moreover, GTE (Li et al., 2023) integrates graph-based techniques to enhance dense embedding.\n\nIn summary, these systems focus on how to retrieve relevant chunks, but neglecting how text content is chunked.In contrast, MC-indexing can utilize the strengths of existing retrievers, and further improve their retrieval performance.\n\nChunking Methods.Chunking is a crucial step in either QA or Retrieval-Augmented Generation (RAG).When dealing with ultra-long text documents, chunk optimization involves breaking the document into smaller chunks.In practice, fixedlength chunking is a commonly used method that is easy to be implemented.It chunks text at a fixed length, e.g., 200 words.Sentence chunking involves dividing textual content based on sentences.",
            "score": 0.5557513428594684,
            "section_title": "Related Work",
            "char_start_offset": 4799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 18,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1380
                },
                {
                    "start": 1382,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1616
                },
                {
                    "start": 1618,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 1971
                },
                {
                    "start": 1971,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 202,
                    "matchedPaperCorpusId": "207180535"
                },
                {
                    "start": 212,
                    "end": 236,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 642,
                    "end": 666,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 755,
                    "end": 782,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 924,
                    "end": 946,
                    "matchedPaperCorpusId": "263831502"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "263835099",
            "title": "Retrieve Anything To Augment Large Language Models",
            "text": "The related works are reviewed from two perspectives: retrieval augmented large language models, and dense retrieval. \n\n\u2022 Retrieval Augmented LLMs. Large language models (LLMs) are praised for their unprecedented capability on language understanding and generation. Compared with the conventional methods, LLMs exhibit overwhelming generality and notable advantages on typical NLP tasks [17,19,78]. Despite such superiority, LLMs still face a series of severe challenges, such as hallucination, human alignment, and long-term memory. Many of the existing problems are caused by the inherent boundaries, which cannot be addressed by LLMs alone, but to rely on support from the external world. The retrieval-augmented LLMs are regarded as a go-to option to bridge LLMs with the external assistance [4,51]. For the past few years, they have been widely applied to several critical scenarios. One common case is the knowledge enhancement. The internal knowledge of LLMs can be incomplete, static, and limited by the popularity bias. When dealing with knowledge intensive tasks, the retrieval augmented LLMs will look for necessary information from an external database, where the generated content can be grounded on proper knowledge [15,31,32,41]. Besides, the retrieval augmented LLMs are also used to retrieve historical context to establish longterm memory [71,85], retrieve examples to improve the instruction following capability [18,83], and retrieve tools to engage with the physical world [62]. \n\nThe retrieval augmented LLMs consist of two basic parts: generator and retriever. According to previous studies [32,41,83,96], the retrieval augmentation effect is highly influenced by the retrieved content. In practice, there are two common types of retrievers. One is to leverage the general purpose retrievers, such as sparse models like BM25 [69], and dense models, like DPR [37], contriever [30], E5 [81], BGE [89], OpenAI text embedding [56]. The other option is develop task-specific retriever, e.g., AAR for knowledge enhancement [96], LLM-R [85] for in-context learning. The general purpose methods are praised for their generality and simplicity for usage, but may suffer from an inferior retrieval quality.",
            "score": 0.5550907832107264,
            "section_title": "RELATED WORKS",
            "char_start_offset": 33911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67578125
        },
        {
            "corpus_id": "258865792",
            "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies",
            "text": "Sparse retrieval. The key idea of sparse retrieval methods is to utilize exact matching signals to design a relevance scoring function. Specifically, these models consider easily computed statistics (e.g., term frequency, document length, and inverse document frequency) of normalized terms matched exactly between the query and document. Among these models, BM25 [47] is shown to be effective and is still regarded as a strong baseline of many retrieval models nowadays. To enhance the semantic relationships, several works utilize word embeddings as term weights [22,62]. Dense retrieval. To solve the vocabulary mismatch problem in sparse retrieval [23,61], many researchers turn to dense retrieval models [33,59], which first learn dense representations of both queries and documents, and then approximate nearest neighbor search [9,11] is employed to retrieve. Further, pre-trained models are used to enhance dense retrieval [28,41]. Differentiable search index. Differentiable Search Index (DSI) [52] is gaining increasing attention, which retrieves documents by generating their docid using a generative model. It presents an endto-end solution for document retrieval tasks and allows for better exploitation of the capabilities of pre-trained generative models. \n\nFor the docids, the original DSI proposed that the docid could be represented by a single token (atomic integers) or a string of tokens, which can be an arbitrary string or a semantic numeric string [52].",
            "score": 0.5550452913882975,
            "section_title": "RELATED WORK",
            "char_start_offset": 38325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1476
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "216144735"
                },
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "155300"
                },
                {
                    "start": 652,
                    "end": 656,
                    "matchedPaperCorpusId": "3002280"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "14286159"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "270664"
                },
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "13091446"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 934,
                    "end": 937,
                    "matchedPaperCorpusId": "211572791"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1471,
                    "end": 1475,
                    "matchedPaperCorpusId": "246863488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "259370750",
            "title": "On Complementarity Objectives for Hybrid Retrieval",
            "text": "Representing and matching queries and documents (or answers) is crucial for designing models for Information Retrieval (IR) and open-domain Question Answering (QA). Existing approaches have been categorized into sparse and dense retrieval. \n\nClassic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations. \n\nThese complementary advantages of each model have naturally motivated hybrid models (Gao et al., 2020;Yadav et al., 2020;Ma et al., 2021), which we denote as BM25+DPR, extracting scores from both models and selecting documents with the highest linearly combined scores. \n\nTo illustrate how we advance BM25+DPR baseline, Figure 1(a) shows Recall@10 of BM25+DPR on Natural Questions, where a yellow circle, represents questions answerable by BM25, or S, and a blue circle, represents those answerable by DPR, or D. Desirably, two retrievers together should cover all questions in the universe U, but failure is 46.5%, which corresponds to U \u2212 D \u222a S.",
            "score": 0.5549684016111387,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 886,
                    "end": 910,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9306640625
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "objective. It significantly outperforms DPR models initialized from BERT-base in the single-set training setting. We show that using coCondenser-Wiki for initialization is also beneficial for TASER under the multi-set setting, especially for SQuAD where R@20 and R@100 are improved by 3.2 and 2.2 points. Notably, SQuAD is the only dataset among the five where DPR underperforms BM25, due to its higher lexical overlap between questions and passages. TASER effectively narrows down the performance gap, suggesting that task-aware specialization indeed makes dense retrieval models more robust in terms of in-domain generalization. Further, TASER results surpass BM25 on all five datasets, and they are either on-par or better than state-of-the-art dense-only retriever models, demonstrating its superior parameter efficiency.\n\nConsistent with previous work, combining BM25 with dense models can further boost the performance, particularly on SQuAD. However, the improvement is more pronounced on DPR compared to TASER and TASER , indicating that TASER is able to capture more lexical overlap features. Finally, TASER + BM25 sets new state-ofthe-art performance on all five ODQA datasets.",
            "score": 0.5549275841565078,
            "section_title": "TriviaQA",
            "char_start_offset": 19709,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57177734375
        },
        {
            "corpus_id": "267751308",
            "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
            "text": "Dense retrieval. Dense retrieval (Lee, Chang, and Toutanova 2019;Karpukhin et al. 2020;Xiong et al. 2020;Khattab and Zaharia 2020) shows tremendous success in many downstream tasks (e.g. open-domain QA and web search) compared with the traditional sparse retrieval models (e.g. TF-IDF and BM25). The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dualencoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model finegrained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance. Besides, knowledge distillation has become a vital technique to enhance the capacity of the dual-encoder by distilling knowledge from a more powerful reader to a classical retriever (Qu et al. 2020;Ren et al. 2021b;Lin, Yang, and Lin 2020;Hofst\u00e4tter et al. 2021). \n\nRecently, massive works have investigated task-related pretraining methods for dense retrieval models (Gao and Callan 2021a,b;Wang, Reimers, and Gurevych 2021;Ren et al. 2021a;Oguz et al. 2021;Meng et al. 2021). Condenser (Gao and Callan 2021a) proposed the Condenser architecture to enforce the late backbone layers to aggregate the whole information. coCondenser (Gao and Callan 2021b) leveraged contrastive learning to incorporate a query-agnostic contrastive loss. PAIR (Ren et al. 2021a) and DPR-PAQ (Oguz et al. 2021) also designed special tasks in pre-training to enhance retrieval models. Additionally, jointly training retrieval models with the rerank model can bring about better performance.",
            "score": 0.5542065507739706,
            "section_title": "Related Work",
            "char_start_offset": 6374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 65,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 65,
                    "end": 87,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 557,
                    "end": 589,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 868,
                    "end": 886,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1146,
                    "end": 1162,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1203,
                    "end": 1226,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1389,
                    "end": 1406,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1704,
                    "end": 1721,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "248405719",
            "title": "A Thorough Examination on Zero-shot Dense Retrieval",
            "text": "Contrastive Pre-training (CP) With unsupervised contrastive learning showing great power in the field of NLP, researchers start to apply this approach to zero-shot DR. Contriever (Izacard et al., 2021) and LaPraDoR (Xu et al., 2022)  Model Size Scaling (MSS) For PLM based approaches, it becomes a broad consensus that scaling the model size can lead to substantial performance improvement (Brown et al., 2020;Fedus et al., 2021). Recently, MSS has shown the effectiveness in zero-shot DR. GTR (Ni et al., 2021) is a generalizable T5-based DR model that uses extremely large-scale training data and model parameters, which obtain performance gains. Moreover, large language models have attracted a lot of attention recently due to their excellent text generation capabilities (Zhao et al., 2023;Ren et al., 2023), they can also be applied to improve the zero-shot capabilities of dense retrieval. \n\nIntegrating Sparse Retrieval (ISR) It has been shown that both sparse and dense retrieval models have specific merits when dealing with different datasets. Thus, it is an important practical approach that integrates the two methods for enhancing the zero-shot retrieval capacities. SPAR (Chen et al., 2021) trains a student DR model that distills BM25 to the DR model, then concats the embeddings from the BM25-based DR model and a normal DR model. Moreover, LaPraDoR (Xu et al., 2022) enhances the DR model with BM25 by multiplying the BM25 score with the similarity score of DR model. We consider ISR as a special approach since it does not change the setup of source training set for DR model, but mainly combines the sparse and dense models in an ensemble way. Table 6: NDCG@10 results of different zero-shot methods on three target datasets.",
            "score": 0.5539724587713711,
            "section_title": "Reviewing Existing Solutions",
            "char_start_offset": 21498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 410,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447998046875
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "A retriever is a key component of the RAG pipeline, which finds relevant documents from a large database based on the user input, and provides contextual information for the large model. There are two main types of retrievers: Keyword-based search-sparse retrieval algorithms, which use keywords and their frequencies to compute the relevance between documents and queries. Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF method, which accounts for factors such as the length and position of words in the document. Dense retrieval algorithms, which use deep learning models to encode documents and queries into low-dimensional vectors, and then measure the cosine similarity between them. This method can capture the semantic and contextual information of words, and improve the retrieval performance. \n\nIn order to combine the advantages of both types of retrievers, we can fuse their retrieval results and randomly sample k from them as contexts for LLMs(Hybrid). Alternatively, we can also use a re-ranking model to re-rank the fused retrieval results, and then select the top-k ones as the context of LLMs(Hybrid+Rerank). In our experiments, we employ the bge-rank as the rerank model. \n\nText Continuation: As Table 5 displays, the performance of the dense retriever is roughly equivalent to that of BM25, except for the key information recall rate. Compared to the keywordbased algorithm, the modern vector search can capture the semantic and contextual information of words, so that more content that does not match keywords but is obviously semantically related can be retrieved. However, the RAG system using BM25 also performs well. In terms of the precision of key information, BM25 even exceeds the dense retriever. This suggests that in the continuation task, which is a creative task, BM25 can retrieve content that is highly relevant to the user's intention, but may overlook some details. \n\nOpen-Domain Multi-Document Summarization: On the overall semantic similarity metric, the performance of the dense retriever is roughly equivalent to that of BM25. On the QuestEval metric, BM25 surpasses dense retriever in terms of key information precision, but slightly trails behind in key information recall.",
            "score": 0.5537005768355983,
            "section_title": "Analyzing the Impact of Retriever on RAG Performance in Different Tasks",
            "char_start_offset": 48972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2256
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "268537038",
            "title": "DESIRE-ME: Domain-Enhanced Supervised Information Retrieval Using Mixture-of-Experts",
            "text": "2.1 Open Domain Q&A Models most commonly used for open-domain Q&A in IR can be broadly classified into five different families based on their architecture: Lexical models, Neural Sparse models, Late-interaction models, Re-ranking models, and Dense retrieval models.Lexical models include all adaptations to open-domain Q&A of classical IR models, such as BM25 [23], that do lexical matching.Neural Sparse models leverage deep neural networks to enhance and overcome some of the limitations of the lexical models, e.g.query-document vocabulary mismatch.They include models such as docT5query [21] that uses sequence-to-sequence models to expand document terms by generating possible queries for which the document would be relevant.Late-interaction models rely on a bi-encoder architecture to encode the query and documents at a token level.The relevance is assessed by computing the similarity between the representations of queries terms and document terms.Late-interaction models allow the pre-computation of documents' representation by delaying the interaction between the query and document representations.A notable example is ColBERT [16], which computes contextualized token-level embeddings for both documents and queries and uses them at retrieval and scoring time.Re-ranking models employ a computationally expensive neural model to re-rank documents retrieved by a fast first-stage ranker.The best-performing re-ranking model in a zero-shot retrieval scenario is currently based on a MonoT5 cross-encoder and utilizes BM25 as the first stage ranker.[24].Dense retrieval models project the query and the documents (or passages) in a common semantic dense vector space and leverage similarity functions to score the documents according to a given query.Many different dense models have been recently proposed because they empirically perform better than lexical and sparse models in many tasks while not being computationally expensive like cross-encoder re-ranking models.Two dense models, namely COCO-DR [29] and Contriever [13], are specifically attractive in this regard for open-domain Q&A as they generalize very well to new domains without the need for labeled data.They are currently among the best performing dense retrieval models on the BEIR benchmarks 5 .",
            "score": 0.553182008127252,
            "section_title": "Related Work",
            "char_start_offset": 4572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1401
                },
                {
                    "start": 1401,
                    "end": 1561
                },
                {
                    "start": 1561,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1983
                },
                {
                    "start": 1983,
                    "end": 2183
                },
                {
                    "start": 2183,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 2016,
                    "end": 2020,
                    "matchedPaperCorpusId": "260663530"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "277780712",
            "title": "A Survey of Personalization: From RAG to Agent",
            "text": "PersonaLM [77] employs group-wise contrastive learning, training its retrieval model to align user queries with domain-specific text fragments, thereby improving personalization. UIA [155] employs dual encoders to retrieve documents tailored to user preferences. XPERT [125] incorporates temporal events and user interactions into embeddings, enabling large-scale retrieval across millions of items. \n\nDense retrieval also enhances specific applications like e-commerce, medical assistance, and language models. DPSR [157] and RTM [11] encode user queries and product information to personalize product searches dynamically. Pearl [80] and MemPrompt [74] retrieve personalized content by leveraging historical user data and memory-assisted mechanisms. \n\nERRA [23] uses review embeddings as dense queries for recommendations. In medical assistance, MALP [160] and USER-LLM [84] integrate short-and long-term user interactions into embeddings for contextualized, personalized responses. \n\nFinally, PER-PCS [120] retrieves relevant information using individual user histories, enhancing the personalization capabilities of large language models. \n\n(2). Sparse Retrieval. Sparse retrieval methods often rely on term-based matching (e.g., BM25) and apply personalization by assigning higher weights to terms or keywords that are more relevant to the user. OPPU [121] uses the BM25 algorithm to select the k most relevant records from the user's historical data for the current query. Similarly, PAG [101] incorporates user input and profiles to enhance summarization and retrieval, aligning sparse representations with personalization objectives for large language models. Au et al. [5] uses BM25 search algorithms to find entries related to the target user or neighboring users through the graph structure. UniMS-RAG [128] combines sparse and dense retrieval by leveraging multi-source knowledge, such as dialogue context and user images, to refine personalized responses in dialogue systems. Lastly, Deng et al. [29] apply sparse retrieval to support fact-based queries, considering user queries and preferences to enhance answer generation for e-commerce applications. \n\n(3). Prompt-based Retrieval.",
            "score": 0.553018808198602,
            "section_title": "4.2.2",
            "char_start_offset": 15912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2166
                },
                {
                    "start": 2169,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "266176747"
                },
                {
                    "start": 183,
                    "end": 188,
                    "matchedPaperCorpusId": "258332095"
                },
                {
                    "start": 269,
                    "end": 274,
                    "matchedPaperCorpusId": "259949714"
                },
                {
                    "start": 517,
                    "end": 522,
                    "matchedPaperCorpusId": "219259903"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "234351913"
                },
                {
                    "start": 2009,
                    "end": 2013,
                    "matchedPaperCorpusId": "245502546"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "268667523",
            "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
            "text": "Existing retrieval augmented generation approaches adopts unsupervised sparse retrieval to collect relevant documents (Lewis et al., 2020b;Izacard and Grave, 2021;Izacard et al., 2022;Ram et al., 2023).An example of such algorithms include BM25, which computes query-document relevance by considering word frequency in both query and documents (Robertson et al., 1995).Although BM25 is widely used, it does not yield satisfactory ranking performance for knowledgeintensive tasks such as question answering.Therefore, dense retrieval methods are proposed for improved text understanding and relevance estimation (Karpukhin et al., 2020;Ren et al., 2021;Izacard et al., 2021;Wang et al., 2022Wang et al., , 2023b)).Nevertheless, dense retrieval is known to be inefficient for massive data quantity and require large amounts of annotated data for training.This limitation renders dense retrieval to be less effective in retrieval augmented response generation, where only limited annotated data is available for training.\n\nUnlike existing retrieval methods, we design a two-stage retrieval pipeline in RARG that performs coarse-to-fine ranking, which improves the computation efficiency and retrieval performance with limited data.Specifically, we include the following stages: (1) retrieve a smaller subset from the large collection of evidence documents via BM25; and (2) rerank the retrieved subset using a dense retriever fine-tuned on limited claim-evidence pairs.In the first retrieval stage, we adopt BM25 to efficiently generate a small subset {e i } m i=1 from a much large collection {e i } n i=1 .While BM25 may occasionally retrieve less or ir-relevant documents, we observe that in most cases, the desired evidence can still be found in {e i } m i=1 with proper selection of m.In our implementation, we adopt m = 20 as the subset size to balance the performance and efficiency of our retrieval pipeline.In the following stage, we fine-tune a dense retrieval model to perform fine-grained reranking and select the most relevant documents as supporting evidence.Compared to sparse retrieval, this additional stage yields notable performance improvements with minimal computational overhead.",
            "score": 0.5527386915472474,
            "section_title": "Two-Stage Evidence Retrieval",
            "char_start_offset": 12907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 713
                },
                {
                    "start": 713,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 1018
                },
                {
                    "start": 1020,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2070
                },
                {
                    "start": 2070,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 368,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 611,
                    "end": 635,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 635,
                    "end": 652,
                    "matchedPaperCorpusId": "236477844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "258887561",
            "title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
            "text": "Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead.",
            "score": 0.5526723314523481,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "247475919",
            "title": "Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data",
            "text": "Retrieval-based Methods Even a pre-trained model as large as GPT-3 (Brown et al., 2020) cannot remember everything, and it is important to leverage information retrieval to collect external knowledge to solve different NLP tasks. There are two types of representations for retriever: bag-ofword (BOW) based sparse representation (Chen et al., 2017) and dense representation from neural networks (Karpukhin et al., 2020).\n\nFor the sparse representation, as the method is based on BOW and usually rule-based score, such as BM25, is used for ranking, it can be easily adapted to a general large-scale search. This method has also been widely explored to solve open domain question answering (Chen et al., 2017;Lin et al., 2018) and Machine Translation (Gu et al., 2018).\n\nDense representation based retrieval (DPR) (Karpukhin et al., 2020) is the most widely explored area in recent years. Dense representations come from encoders, such as Transformer, trained with task-specific data. And these methods can achieve better recall performance than sparse representation on different tasks, such as open domain question answering (Karpukhin et al., 2020;Guu et al., 2020;Yu et al., 2021), knowledge-grounded generation (Zhang et al., 2021), and machine translation (Cai et al., 2021). One drawback of DPR is that it cannot process longer documents, usually less than 128 tokens (Karpukhin et al., 2020). Another drawback is that it needs parallel data for model training on specific tasks.\n\nConsidering the generalization and efficiency of sparse representation, in this paper, we use BM25 score (Robertson and Zaragoza, 2009;Sch\u00fctze et al., 2008) to retrieve from the training data, and our method is more flexible with no requirement of parallel data for model training. Compared to nonparametric systems guided by search engine (Gu et al., 2018;Khandelwal et al., 2020), our proposed method is based on supervised learning and is more general.  is related to our work by retrieving related questions from pre-built largescale question-answer pairs. However, our method doesn't need",
            "score": 0.5521988755950842,
            "section_title": "Related Work",
            "char_start_offset": 3901,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 419,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 707,
                    "end": 724,
                    "matchedPaperCorpusId": "51875405"
                },
                {
                    "start": 749,
                    "end": 766,
                    "matchedPaperCorpusId": "19206366"
                },
                {
                    "start": 812,
                    "end": 836,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1125,
                    "end": 1149,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1149,
                    "end": 1166,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1373,
                    "end": 1397,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86474609375
        },
        {
            "corpus_id": "248496840",
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "text": "There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval. After BERT [6] was proposed, most of the recent dense retrieval models are designed based on it and achieve significant improvement on retrieval quality, e.g., Sentence-BERT [34] and MarkedBERT [2]. But as a heavy model, BERT has high inference latency, which limits its application on online retrieval that requires low-latency serving. To solve this problem, following works have proposed several variants to reduce its complexity, including DistilBERT [36], ColBERT [15], TCT-ColBERT [22], etc. \n\nTo better support the low-latency online retrieval, in addition to deploying more lightweight neural models, most state-of-the-art end-to-end dense retrieval systems also arrange a high-performance ANN search module following the neural model to fast look up the closest documents to the queries based on their embeddings. ANN indexes include four major categories, i.e., hashing, graph, quantization and tree based indexes. Among them the tree based indexes are more suitable to low-dimensional space, so dense retrieval systems normally choose from the other three types of indexes. For example, DPR [14] utilizes a graph based index, HNSW [26], ColBERT deploys IVFADC index which is based on product quantization, BPR [43] integrates learning-to-hash technique into DPR [14], etc. FAISS [13] is one of the most popular ANN index libraries in today's dense retrieval, as it implements high-performance indexes of all the three classes.",
            "score": 0.5521530324396782,
            "section_title": "RELATED WORK",
            "char_start_offset": 10570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "277502276",
            "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
            "text": "We compare GRAM with several strong baseline models, including widely used sparse retrieval methods, dense retrieval methods, and generative retrieval methods. The detailed introductions are listed as follows: \n\n\u2022 BM25 [18]: It enhances relevance scoring by refining term weights based on the TF-IDF feature. \u2022 DocT5Query [14]: It utilizes T5 to generate a pseudo query for the document to expand document information and then applies BM25 for document retrieval. \u2022 DPR [7]: It uses BERT as an encoder to encode queries and documents into semantic space and train models with inbatch negatives. \n\n\u2022 SEAL [2]: It regards all n-grams contained in documents as their identifiers. \u2022 LC-Rec [31]: It utilizes the RQ-VAE [29] to generate semantic IDs for product indexing and proposes a series of semantic alignment tasks to align LLM with semantic IDs. \n\nTable 2: The experimental results are compared with sparse retrieval methods, dense retrieval methods, and generative retrieval methods.",
            "score": 0.5516373580016238,
            "section_title": "Baseline Models",
            "char_start_offset": 24375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 209
                },
                {
                    "start": 212,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 986
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "265213194"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "236149944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "259598351",
            "title": "DeBEIR: A Python Package for Dense Bi-Encoder Information Retrieval",
            "text": "Dense retrieval has been popular in Information Retrieval since 2015 (Guo et al., 2017;Hui et al., 2017;Yin et al., 2015). Retrieval effectiveness of these dense retrieval methods was often compared against weaker baselines and was not shown significantly stronger than statistical models (Yang et al., 2019), such as a well-tuned BM25 model while being considerably slower. This situation is similar to what happened in the early 2000s, where there was a slow down in retrieval effectiveness from the use of less robust baselines (Armstrong et al., 2009) when proposing new methods. \n\nHowever, attitudes on dense retrieval changed when transformer models were found to be effective once fine-tuned on Natural Language Inference tasks or Ms-Marco (T. Nguyen et al., 2016) as a cross-encoder (Lin, 2019), significantly overtaking even the best BM25 models. \n\nThere are generally two classes of dense retrieval models for IR: (1) the cross-encoder, which encodes queries and documents at query time and (2) the bi-encoder, which can encode documents at index time and queries at query time. The cross-encoder is generally more effective than the bi-encoder model for retrieval (Lin et al., 2021). However, this increased effectiveness requires a more substantial computation and can be a bottleneck in production systems. Therefore, a less expensive model such as BM25 is typically used to retrieve smaller candidate lists (first-stage retrieval) to be fed to second-stage retrieval re-ranking by a crossencoder. \n\nAlthough cross-encoders are more accurate than bi-encoders, bi-encoder are more effective than BM25 (V. Nguyen et al., 2022) and are faster than cross-encoders. Therefore, a gap in the literature in IR is to replace BM25 first-stage retrieval with a bi-encoder or otherwise used as the sole ranking system, without a second-stage re-ranker. However, current libraries do not address this use case because it requires integration with the indexing and querying pipeline of the search engine.",
            "score": 0.5515608510843073,
            "section_title": "Statement of Need",
            "char_start_offset": 3348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 87,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 87,
                    "end": 104,
                    "matchedPaperCorpusId": "307767"
                },
                {
                    "start": 104,
                    "end": 121,
                    "matchedPaperCorpusId": "5535381"
                },
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "126166925"
                },
                {
                    "start": 1175,
                    "end": 1193,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1617,
                    "end": 1637,
                    "matchedPaperCorpusId": "246749838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "266176289",
            "title": "Breaking Boundaries in Retrieval Systems: Unsupervised Domain Adaptation with Denoise-Finetuning",
            "text": "The goal of information retrieval (IR) is to enable users to input a query and retrieve relevant passages or documents from the retrieval system. A standard IR system (Matveeva et al., 2006;Liu et al., 2009;Wang et al., 2011;Yang et al., 2019) typically comprises two main stages (refer to Fig. 1): \n\n1. First-stage retrieval model: This model is designed to retrieve a small subset of relevant passages based on the given query. \n\n2. Rerank model: Responsible for reordering the retrieved passages, the rerank model aims to enhance the overall user experience. \n\nRecent advancements in contextualized word embeddings (Liu et al., 2019;Devlin et al., 2019;Lewis et al., 2020) have established dense retrieval Figure 1: Two-stage information retrieval involves using a retrieval model to select a small subset of passages, which are then reranked using a rerank model. (Karpukhin et al., 2020;Xiong et al., 2021) as the mainstream approach in information retrieval (IR). This approach effectively addresses the issue of missing words encountered in lexical methods like BM25 (Robertson et al., 1995) or TF-IDF (Christian et al., 2016) and has powerful retrieval ability. \n\nHowever, the introduction of the BEIR Benchmark (Thakur et al., 2021) has highlighted the limitations of dense models. Surprisingly, traditional lexical-based models like BM25 have exhibited superior performance compared to dense retrieval models in out-of-domain scenarios. Additionally, training dense retrieval models from scratch necessitates a significant amount of domain-specific training data, posing challenges in collecting largescale data for each specific domain. As a result, there is a growing demand for domain adaptation methods that can enhance model performance without relying on labeled data. \n\nPrevious unsupervised domain adaptation approaches, such as Qgen (Ma et al., 2021) and GPL (Wang et al., 2021b), have employed generative models to generate pseudo queries and augment the training data.",
            "score": 0.5514946999078496,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 190,
                    "matchedPaperCorpusId": "9346130"
                },
                {
                    "start": 190,
                    "end": 207,
                    "matchedPaperCorpusId": "28826624"
                },
                {
                    "start": 207,
                    "end": 225,
                    "matchedPaperCorpusId": "3357504"
                },
                {
                    "start": 225,
                    "end": 243,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 636,
                    "end": 656,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 656,
                    "end": 675,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 868,
                    "end": 892,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 892,
                    "end": 911,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1109,
                    "end": 1133,
                    "matchedPaperCorpusId": "51999160"
                },
                {
                    "start": 1220,
                    "end": 1241,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "248780080",
            "title": "R3 : Refined Retriever-Reader pipeline for Multidoc2dial",
            "text": "Our experimental setup refines both the retriever and reader components of the existing architecture. \n\nRetrieval We analyze the performance of different dense and sparse retrieval methods in a zeroshot setting on the MultiDoc2Dial dataset. For our dense retriever baselines, we conduct experiments with DPR, ANCE (Xiong et al., 2020) and TAS-B (Hofst\u00e4tter et al., 2021). For sparse retrieval methods, we experiment with SPLADE-max and DistilSPLDAE (Formal et al., 2021). During training, we label the retrieved passages (excluding the golden passage) from BM25 as hard negatives. We also experiment with the finetuned DPR model to mine harder negatives. \n\nReranker Following (Fajcik et al., 2021), we select the top 100 passages from the DistilSPLADE retriever to be reranked using RoBERTA as a cross encoder. We use this reranking only during validation time. The top 10 reranked documents are passed to the reader. \n\nReader We experiment with both T5 and BART models as the reader. We use the T5 based reader model to circumvent the limited tokens used for BART along with the FiD model pretrained on natural questions2 . We further experimented by placing the golden passage at the top-most position (Gold setting) in the retrieved passages before passing it to the reader during training. We also apply curriculum learning (CL) in the reader as per described in Section 4.",
            "score": 0.5514751428426798,
            "section_title": "Setup",
            "char_start_offset": 13127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1377
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 370,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 676,
                    "end": 697,
                    "matchedPaperCorpusId": "237439232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37548828125
        },
        {
            "corpus_id": "267750652",
            "title": "Dense Passage Retrieval: Is it Retrieving?",
            "text": "Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process to more knowledge so more can be decentralized, (2) inject facts as decentralized representations, (3) model and incorporate knowledge uncertainty in the retrieval process, and (4) directly map internal model knowledge to a knowledge base.",
            "score": 0.5506163609175283,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "258947132",
            "title": "Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering",
            "text": "Generation-Augmented Retrieval (GAR) (Mao et al., 2021a) aims to enhance sparse retrievers by query expansion with text generation from PLMs. Given the initial query, GAR generates relevant contexts including the answer, answer sentence, and title of answer passages, and then concatenates them to the initial query before performing retrieval with BM25. GAR achieves decent performance close to that of DPR while using the lightweight BM25 retriever. However, a limitation is that GAR is not aware of the existence of BM25, potentially generating suboptimal queries for retrieval. Additionally, GAR is only trained on in-domain data, limiting their ability to transfer to out-of-domain data.",
            "score": 0.550534990472836,
            "section_title": "Generation-Augmented Retrieval",
            "char_start_offset": 4538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 692
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 56,
                    "matchedPaperCorpusId": "221802772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "258887561",
            "title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
            "text": "In this paper, we study the source of performance gains in RETRO, which could be generalized to similar retrieval-augmented language models. After observing that the PPL drop correlates more strongly with surface-level overlap between the query and the retrieved text, we replace the retrieval method with BM25, and observe a significant drop in PPL, which confirms us in the findings of the correlation study. This is also an interesting insight as to how these models work, which could be lever-aged for performance gain in tasks like question answering where model relies on retrieving facts. In the end, we also conduct an analysis to find out how much BM25 neighbors overlap with those retrieved using ST. The results show that while faiss is able to find some of the neighbors with high token overlap, the majority of them remain unretrieved. This is however, enough to gain part of the loss reduction achieved with a pure BM25 retrieval system. \n\nThe proposed methods could also be used during training. By retrieving more overlapping neighbors during training, the process of guiding the model to use retrieved neighbors for language modeling could be done more efficiently. This is particularly relevant when augmenting an already trained language model with a retrieval mechanism. As reported by Borgeaud et al. (2022), retrieval augmentation results in a larger drop in BPB as the number of model parameters and the size of retrieval data grow. This calls for more efficient methods based on surface-level similarities if we wish to exploit this potential. Furthermore, the retrieval system in RETRO is based on semantic retrieval, the model seems to rely more on surface-level similarities. This could affect the generalizability capabilities of such models, which necessitates further investigations. Lastly, we only evaluate our modified RETRO model on language modeling. It would be interesting to know the impacts of BM25 retrieval on downstream tasks where retrieval is of use.",
            "score": 0.549156040882171,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 13879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 1306,
                    "end": 1328,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75146484375
        },
        {
            "corpus_id": "274150261",
            "title": "A Comparative Study of Text Retrieval Models on DaReCzech",
            "text": "Information retrieval (IR) is used in areas such as search engines and questionanswering systems. Lately, we've seen advancements in IR models [12,16,32, inter alia], but picking the right one for a non-English document collection can be challenging. We address this gap for Czech language by doing a comprehensive comparison in our study. In particular, we utilize DareCzech, a Czech retrieval and ranking dataset [14], for testing IR models to evaluate different IR models on Czech documents and queries. Our contributions are: (1) we analyze the index sizes to understand the storage requirements of various models, (2) we analyze the retrieval speed of such methods, to estimate how these models scale to large corpora in their default implementation, (3) we conduct ranking performance testing using multiple metrics on off-the-shelf models, and (4) we compare different model types, including those tested directly on the Czech dataset as well as on an English translation of the Czech dataset, keeping in mind the 3 Model Descriptions BM25 [24]. BM25 is a traditional lexical approach which has been widely used and had been the standard before the rise of neural models. It ranks documents based on a query's term frequency, inverse document frequency, and document length, meaning the importance of each term in the query and document is considered along with the document's length normalization, to produce relevance scores for each document. \n\nIn our study, we employed a BM25 baseline to assess the effectiveness of the other models. This model stands out as the only model with lemmatization applied to the query and document content, a distinction arising from the nature of BM25, which is not a neural model and relies on the precise lexical form of terms within the corpus. \n\nFor the Czech language, which features word inflection, lemmatization is essential for precise term matching and relevance ranking. Therefore, the lemmatized version of the corpus for BM25 is required. \n\nsplade-cocondenser-ensembledistil (SPLADE) [9]. (Sparse Lexical and Expansion Model) leverages sparse vocabulary-sized representations to leverage the advantages of BOW (bag-of-words).",
            "score": 0.5485163375764317,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 1993
                },
                {
                    "start": 1996,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417236328125
        },
        {
            "corpus_id": "257585074",
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "text": "Neural information retrieval has becoming increasingly common and effective with the introduction of transformers-based pre-trained language models [17]. Due to latency constraints, a pipeline is often split into two stages: first-stage retrieval and re-ranking. The former focuses on efficiently retrieving a set of candidates to re-rank, whereas the latter focuses on re-ranking using highly effective but inefficient methods. Neural first-stage retrieval approaches can be grouped into two categories: dense retrieval (e.g., [12,13,38]) and learned sparse retrieval (e.g., [7,40,44]). Learned sparse retrieval (LSR) methods transform an input text (i.e., a query or document) into sparse lexical vectors, with each dimension containing a term score analogous to TF. The sparsity of these vectors allows LSR methods to leverage an inverted index. Compared with dense retrieval, LSR has several attractive properties. Each dimension in the learned sparse vectors is usually tied to a term in vocabulary, which facilitates transparency. We can, for example, examine biases encoded by models by looking at the generated terms. Furthermore, LSR methods can re-use the inverted indexing infrastructure built and optimized for traditional lexical methods over decades. \n\nThe idea of using neural methods to learn weights for sparse retrieval predates transformers [40,42], but approaches' effectiveness with pre-BERT methods is limited. With the emergence of retrieval powered by transformer-based pre-trained language models [6,17,36], many LSR methods [5,7,8,16,20,23,41] have been introduced that leverage transformer architectures to substantially improve effectiveness. Among them, the Splade [7] family is a recent prominent approach that shows strong performance on the MSMarco [26] and BEIR benchmarks [35]. \n\nDespite their architectural similarities, different learned sparse retrieval methods exhibit very different behaviors regarding effectiveness and efficiency. The underlying reasons for these differences are often unclear. \n\nIn this work, we conceptually analyze existing LSR methods and identify key components in order to establish a comparative framework that unifies all methods under the same perspective. Under this framework, the key differences between existing LSR methods become apparent.",
            "score": 0.5485121519006388,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1364,
                    "end": 1367,
                    "matchedPaperCorpusId": "155300"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1525,
                    "end": 1528,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1550,
                    "end": 1553,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 1560,
                    "end": 1563,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 1563,
                    "end": 1566,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1781,
                    "end": 1785,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "265050994",
            "title": "Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning",
            "text": "LLMs can be augmented with various external modules such as document corpus [2], vector databases 5 , etc. Typically, the augmentation is accomplished in two ways, namely sparse (such as Bag of words) and dense where Neural network is used to encode documents. Dense retrievers are widely used in present time mainly due to the good representation capability of neural networks. \n\nRecent works suggest that the retrieval-augmented LLMs has significant improvement over the standard LLMs across various tasks especially with respect to scale [3,9]. REALM and RAG are the initial efforts where they train the retriever and language model by representing documents as latent variable, and minimizing the language model objective [3,9]. REALM leverages masked-language modeling as an objective to pretrain the model in end to end fashion. However, it is computationally very expensive to train these models that requires to retrain the entire index with new knowledge. Guu et al. [3] explored the concept of query-side finetuning that only refreshes the query encoder whereas document encoder remains frozen. Izacard et al. [4,5] proposed various ways to improve the retrieval augmented models, including novel learning objectives to align retriever with the language model [5]. Furthermore, RETRO [1] shows the benefits of scaling the retrieval memory to trillions of tokens. ATLAS [6] experiment with various design and training configurations for retrieval augmented models with a focus on few shot learning ability.",
            "score": 0.5484745240922717,
            "section_title": "Related Work",
            "char_start_offset": 13412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1515
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 544,
                    "end": 546,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 729,
                    "end": 731,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492919921875
        },
        {
            "corpus_id": "269448997",
            "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
            "text": "Sparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents. \n\nIn our experiments, we consider dense retrieval models at various scales for a comprehensive evaluation: (1) Base Size (<1B parameters), (2) Large Size (1B-5B), and (3) XL Size (>5B). \n\n\u2022 Contriever (Izacard et al., 2022) is a dense retrieval model (110M) pre-trained via contrastive learning on documents sampled from Wikipedia and CC-Net corpora. \n\n\u2022 Dragon (Lin et al., 2023) is a BERT-base-sized dense retrieval model (110M) that undergoes progressive training using a data augmentation approach, incorporating diverse queries and sources of supervision. \n\n\u2022 SPECTER 2.0 (Singh et al., 2023) is a scientific document representation model (110M) pretrained using multi-format representation learning, enabling tailored embeddings for a diverse range of task formats. \n\n\u2022 SciMult (Zhang et al., 2023) is a scientific dense retrieval model (110M) that employs a multitask contrastive learning framework with taskaware specialization and instruction tuning to enhance performance on scientific literature retrieval tasks. \n\n\u2022 COCO-DR (Yu et al., 2022) is a lightweight dense retrieval model (110M) pre-trained using continuous contrastive learning and implicit distributionally robust optimization on domainspecific corpora, enabling adaptation to various downstream tasks, including those in the biomedical domain. \n\n\u2022 QExt (Meng et al., 2022) is a data augmentation method that trains dense retrieval models by selecting salient spans from the original document, and generating pseudo queries using transferred language models. We report the performance of QExt (110M) in the unsupervised dense retrieval setting only.",
            "score": 0.5482787372362213,
            "section_title": "C.1 Baselines for Retrieval Tasks in Main Experiments",
            "char_start_offset": 42174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 172,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 736,
                    "end": 757,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 897,
                    "end": 915,
                    "matchedPaperCorpusId": "256868909"
                },
                {
                    "start": 1112,
                    "end": 1131,
                    "matchedPaperCorpusId": "254018137"
                },
                {
                    "start": 1319,
                    "end": 1338,
                    "matchedPaperCorpusId": "258841021"
                },
                {
                    "start": 1571,
                    "end": 1587,
                    "matchedPaperCorpusId": "253157773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "257495804",
            "title": "NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective Reasoning",
            "text": "The two methods (BM25 and DPR) provide a trade-off for our users between speed and accuracy, with BM25 being the fastest and DPR being the most accurate. \n\nDense Passage Retrieval: DPR (Dense Passage Retrieval) is a method that uses dense text embeddings for both query and documents that need to be matched. It is based on a BERT bi-encoder architecture that computes a dot product similarity between a document and a query. Our DPR is based on [21], using question & context encoders trained with the Natural Questions dataset [25,23]. This DPR is then used as a backend for of our neural_match primitive. The neural_match primitive receives a sub-query whose results are used to create a set of (id, document) pairs. It then tries to match the documents against a target query text, using our DPR to return the top_k results that matched (see full example of neural_match in Section 2.1.1). \n\nReader Model: A Reader or reading comprehension model is a neural network that can perform extractive Q&A by returning relevant text intervals of documents. In our work we use the MiniLM [49] model initially trained on the SQuAD 2.0 dataset [37] and further fine-tuned on the NeuroQL training set to improve its performance (as we describe in Section 2.4). This Reader is then used as a backend for of our neural_extract primitive. The neural_extract primitive receives the name of a new variable to bind for extracted answers, a query to create (id, document) pairs and finally a target query text. It then tries to extract relevant text intervals from our documents using the Reader to return the top_k results found. We fine-tuned our reader model for 3 epochs, using a 384-token sequence length with a 128-token document stride. We used a batch size of 16, learning rate of 1e-5 with a 0.2 warmup and included the ability to return no answers when predicting results. \n\nTranslation Model: A translation model is a sequence-to-sequence neural network trained over pairs of input and target sequences.",
            "score": 0.5473766544569548,
            "section_title": "Methods",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 533,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109]. \n\nQuery expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122]. Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114]. Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123]. Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105]. \n\nRetrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications. RAG capabilities are experienced in varied settings. In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114]. \n\nIn question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context. This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120]. RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124]. Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125]. Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].",
            "score": 0.547182168239349,
            "section_title": "Retrieval augmented generation",
            "char_start_offset": 50470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "265498312"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "Pre-trained Language Models (PLMs). Through pre-training, language models can achieve higher performance when transferred to specific tasks. Transformer [27], based on self-attention mechanism, is a mainstream architecture of PLMs, including encoderonly [11,14] model and decoder-only [2,29] model. To address the quadratic time scaling of Transformer architecture, some architectures broadly regarded as state space models have been proposed [8,20], especially high-performing models like sub-quadratic architecture M2-BERT [6], linear architecture Mamba [7] and Mamba-2 [5]. \n\nDense Retrieval Models. Transformer PLMs have been proven effective for dense retrieval. Initially, encoder-only models are adopted for retrieval tasks due to the bi-directional attention mechanisms [10,19]. Later, decoder-only models are adopted for retrieval tasks due to their effectiveness on larger model size [15,17]. \n\nLong-text Dense Retrieval Models. In long-text retrieval, early works use chunking strategies [4] due to the small context window. In order to help the model better understand complete and coherent semantics, some studies explore Transformer-based long-text retrieval models [9,15,30]. \n\nFaced with the quadratic time scaling of Transformer-based longtext retrieval models, the sub-quadratic M2-BERT model [6] has been utilized for long-text retrieval tasks [23].",
            "score": 0.5469462603230548,
            "section_title": "Related Work",
            "char_start_offset": 4479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "264288993"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "matchedPaperCorpusId": "264288993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "271915981",
            "title": "Mistral-SPLADE: LLMs for better Learned Sparse Retrieval",
            "text": "Learned Sparse Retrievers (LSR) have evolved into an effective retrieval strategy that can bridge the gap between traditional keyword-based sparse retrievers and embedding-based dense retrievers. At its core, learned sparse retrievers try to learn the most important semantic keyword expansions from a query and/or document which can facilitate better retrieval with overlapping keyword expansions. LSR like SPLADE has typically been using encoder only models with MLM (masked language modeling) style objective in conjunction with known ways of retrieval performance improvement such as hard negative mining, distillation, etc. In this work, we propose to use decoder-only model for learning semantic keyword expansion. We posit, decoder only models that have seen much higher magnitudes of data are better equipped to learn keyword expansions needed for improved retrieval. We use Mistral as the backbone to develop our Learned Sparse Retriever similar to SPLADE and train it on a subset of sentence-transformer data which is often used for training text embedding models. Our experiments support the hypothesis that a sparse retrieval model based on decoder only large language model (LLM) surpasses the performance of existing LSR systems, including SPLADE and all its variants. The LLM based model (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse retrieval model on the BEIR text retrieval benchmark.",
            "score": 0.5468506829925178,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "238259539",
            "title": "A proposed conceptual framework for a representational approach to information retrieval",
            "text": "The comparison function \u03c6 is, like DPR, defined in terms of the inner product. \n\nViewed in this manner, we can clearly see that BM25 and DPR have the same functional form, parameterized by \u03b7 q , \u03b7 d , and \u03c6, and in fact, \u03c6 is the inner product in both cases. Explained in terms of abstractions such as interfaces in programming languages, by analogy the logical scoring model defines the abstract methods (\u03b7 q , \u03b7 d , and \u03c6) that specific retrieval models override with custom implementations, and here I have demonstrated that the abstraction covers both BM25 and DPR. This framework can be applied to the recent panoply of proposed dense retrieval methods in the literature, as well as nearly all families of bag-of-words exact-match models beyond BM25's probabilistic formulation, e.g., tf-idf, query likelihood, divergence from randomness, etc. This conceptual framework allows us to draw a direct connection between dense retrieval and sparse retrieval as parametric variations of the same underlying logical scoring model. \n\nFinally, what about cross-encoders? Typical of this design is the monoBERT model [Nogueira andCho, 2019, Lin et al., 2021b], where a query and a document are fed into a pretrained transformer as part of an input template, and the contextual representation of the",
            "score": 0.546770737679565,
            "section_title": "Applications to Dense and Sparse Retrieval",
            "char_start_offset": 11388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1293
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "236957234",
            "title": "Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models",
            "text": "BM25 outperforms the biencoder consisting of a BERT model as question encoder and a TAPAS model as text and table encoder as well as the tri-encoder consisting of two BERT models serving as question encoder and text encoder, respectively, and a TAPAS model serving as table encoder. \n\nWhen it comes to the performance on the text modality, i.e., questions deriving from NQ whose gold-label answer resides in a text passage, four out of the six dense retrieval models outperform BM25. For this case, the best performing model is not a tri-encoder but the bi-encoder comprising two BERT models. This model outperforms BM25 by 16.8 percentage points with regard to recall@10. However, this bi-encoder exceeds the tri-encoder consisting of three BERT encoders only slightly by one percentage point. The sparse retrieval method BM25 beats the bi-encoder consisting of a BERT model as question encoder and a TAPAS model as text and table encoder as well as the tri-encoder consisting of two TAPAS models serving as question encoder and table encoder, respectively, and a BERT model serving as text encoder. \n\nIn summary, the best performance on the WIKI-SQL test set is achieved by the sparse retrieval method BM25. The tri-encoder consisting of three BERT encoders shows the best performance on the remaining two tabular datasets, OTT-QA and NQ-TABLES. On the NQ dataset, i.e., questions whose answers reside in the textual modality, the bi-encoder consisting of two BERT encoders performs best but is almost on par with the tri-encoder consisting of three BERT models. \n\nWe can conclude that, under the limited experimental conditions, in particular, on the datasets used in our study, models involving TAPAS as question, text, and/or table encoder perform worse than models that rely only on BERT language models.",
            "score": 0.5462391123441646,
            "section_title": "Experiments",
            "char_start_offset": 25548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "233219855",
            "title": "A Replication Study of Dense Passage Retriever",
            "text": "Text retrieval using learned dense representations has recently emerged as a promising alternative to\"traditional\"text retrieval using sparse bag-of-words representations. One recent work that has garnered much attention is the dense passage retriever (DPR) technique proposed by Karpukhin et al. (2020) for end-to-end open-domain question answering. We present a replication study of this work, starting with model checkpoints provided by the authors, but otherwise from an independent implementation in our group's Pyserini IR toolkit and PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that contribute to a better understanding of DPR: First, it appears that the original authors under-report the effectiveness of the BM25 baseline and hence also dense--sparse hybrid retrieval results. Second, by incorporating evidence from the retriever and an improved answer span scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the original work.",
            "score": 0.546167021279827,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "Baselines. We compare the proposed dense retrieval framework with the following baselines: \n\n\u2022 Sparse (Term Matching) Retrieval Models: We use two sparse retrieval baselines: (1) BM25: this baseline uses the BM25 formulation [47] with questions as queries, ignoring the images, and passages as documents. (2) BM25-Obj (CombMax): this approach extracts 36 objects from the image (objects are generated by a Faster R-CNN [45] model pre-trained on Visual Genome [2,27]) and concatenates each object's name to the question as the query and uses the BM25 formulation to retrieve passages. Then it uses CombMax [9,28] to aggregate these 36 ranked lists. [23]) with the same training objective as ours. We provide the results for two variations of this model, an image-independent approach whose query encoder only encodes the question, and an image-dependent approach whose query encoder takes the concatenation of the question and the image caption (captions are generated by Ex-pansionNet v2 [18]). The latter is the same as our   encoder. ( 5) Dense-LXMERT is a model that uses a multi-modal encoder, i.e., LXMERT, to encode queries and passages. It uses masked image tokens on the passage side. This approach is our   encoder. \u2022 Asymmetric Dual-Encoding Dense Retrieval Models: In this category, we use BERT-LXMERT, proposed in [40], that uses BERT for passage encoding and LXMERT for query encoding. 8 or fair comparison, we use the same training and evaluation process for all (our and baseline) models. To the best of our knowledge, our baseline results are the highest reported in the literature. \n\nComparison Against Retrieval Baselines. The passage retrieval results are reported in Table 1. We observe that dense retrieval models generally outperform sparse retrieval baselines, confirming our design choice to focus on dense retrieval for KI-VQA tasks.",
            "score": 0.5459317518685053,
            "section_title": "Passage Retrieval Results for KI-VQA Tasks",
            "char_start_offset": 30239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 419,
                    "end": 423,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 459,
                    "end": 462,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 462,
                    "end": 465,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 605,
                    "end": 608,
                    "matchedPaperCorpusId": "1309301"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "17427180"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "234337940"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "233219855",
            "title": "A Replication Study of Dense Passage Retriever",
            "text": "However, we noticed that the range of scores from DPR and BM25 can be quite different. To potentially address this issue, we tried the following normalization technique: If a document from sparse retrieval is not in the dense retrieval results, we assign to it the the minimum dense retrieval score among the retrieved documents as its dense retrieval score, and vice versa for the sparse retrieval score. \n\nTo arrive at a final top-k ranking, the original DPR paper generated top k \u2032 results from DPR and top k \u2032 results from BM25 (where k \u2032 > k), before considering the union of the two result sets and combining the scores to arrive at the final top k. Karpukhin et al. (2020) set k \u2032 = 2000, but after some preliminary experimentation, we decided to fix k \u2032 = 1000 in our experiments since it is a more common setting in information retrieval experiments (for example, k = 1000 is the default in most TREC evaluations).",
            "score": 0.5457154967295458,
            "section_title": "Retriever",
            "char_start_offset": 7270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 923
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 679,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "258967426",
            "title": "Graph Reasoning for Question Answering with Triplet Retrieval",
            "text": "We retrieve passages from corpus D for an MCQA example (q, C) with hybrid (i.e., both sparse and dense) retrievers since they are complementary (Karpukhin et al., 2020;Ma et al., 2021b). For sparse retriever, we utilize BM25 to index D and (q, c i ) to retrieve N passages from it. For dense retriever, we use DPR due to its strong performance in open domain question answering (Karpukhin et al., 2020). DPR embeds queries with question encoder and passages with passage encoder into low dimensional dense vectors, and retrieval can be efficiently done through FAISS library (Johnson et al., 2021) on GPUs. Similar to sparse retriever, we utilize (q, c i ) to retrieve N passages from corpus D for DPR. The total number of passages returned by BM25 and DPR is 2N .",
            "score": 0.5453303732821922,
            "section_title": "Hybrid passage retrieval",
            "char_start_offset": 4955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 764
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 168,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 378,
                    "end": 402,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 575,
                    "end": 597,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "271719872",
            "title": "Entity Retrieval for Answering Entity-Centric Questions",
            "text": "BM25 (Robertson et al., 1994(Robertson et al., , 2009) ) is a probabilistic retrieval method that ranks documents based on the frequency of query terms appearing in each document, adjusted by the length of the document and overall term frequency in the collection. It operates in the sparse vector space, relying on precomputed term frequencies and inverse document frequencies to retrieve documents based on keyword matching. \n\nDPR (Dense Passage Retrieval; Karpukhin et al., 2020) leverages a bi-encoder architecture, wherein the initial encoder processes the question and the subsequent encoder handles the passages to be retrieved. The similarity scores between the two encoded representations are computed using a dot product. Typically, the encoded representations of the second encoder are fixed and indexed in FAISS (Johnson et al., 2019), while the first encoder is optimized to maximize the dot-product scores based on positive and negative examples. \n\nANCE (Xiong et al., 2021) is another dense retrieval technique similar to DPR4 . It employs one encoder to transform both the questions and passages into dense representations. The key distinction from DPR is that ANCE uses hard negatives generated by periodically updating the passage embeddings during training, which helps the model learn more discriminative features, thereby enhancing retrieval performance over time.",
            "score": 0.544792569629288,
            "section_title": "Introduction",
            "char_start_offset": 3589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 28,
                    "matchedPaperCorpusId": "263608480"
                },
                {
                    "start": 28,
                    "end": 56,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 459,
                    "end": 482,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 824,
                    "end": 846,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 968,
                    "end": 988,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "234679223",
            "title": "QAConv: Question Answering on Informative Conversations",
            "text": "Two retrieval baselines are investigated in this paper: BM25 and DPR-wiki (Karpukhin et al., 2020). The BM25 retriever is a bag-of-words retrieval function weighted by term frequency and inverse document frequency. The DPR-wiki model is a BERT-based dense retriever model trained for opendomain QA tasks, learning to retrieve the most relevant Wikipedia passage.",
            "score": 0.5439764207123485,
            "section_title": "Retrieval Models",
            "char_start_offset": 16904,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "275921950",
            "title": "CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs",
            "text": "Information retrieval (IR) focuses on extracting relevant information from large corpora. Two primary retrieval techniques dominate the field: sparse retrieval and dense retrieval. Sparse retrieval methods, such as TF-IDF [32] and BM25 [31], rely on term-based representations to evaluate lexical matches between queries and documents. These approaches perform well in scenarios where exact term matching is essential, but they struggle with semantic meaning. In contrast, dense retrieval methods leverage pretrained language models such as BERT [5] to encode queries and documents as continuous, low-dimensional embeddings, capturing semantic similarity through maximum inner product search (MIPS) [16,30,43,44]. Dense retrieval effectively overcomes the lexical gap, retrieving semantically related results even when query terms differ from the document's terminology. Recently, hybrid retrieval techniques have also emerged to combine the strengths of sparse and dense methods while addressing their respective limitations [25,26,29]. By integrating sparse and dense signals, these approaches provide a robust solution for retrieving relevant information from long and complex documents.",
            "score": 0.5436072801255426,
            "section_title": "RELATED WORK 2.1 Information Retrieval",
            "char_start_offset": 6446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1190
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 699,
                    "end": 703,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "271114619"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "matchedPaperCorpusId": "252312510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "276107364",
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "text": "Retrieval models have evolved significantly, transitioning from traditional sparse retrievers to sophisticated dense and hybrid retrieval techniques. Sparse retrievers, such as BM25 [92], remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods [55], which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR [120] and ANCE [115], improve retrieval effectiveness by employing contrastive training with in-batch negatives [33] and hard-negative mining [46,116]. Multi-vector approaches like ColBERT [49] enable fine-grained interaction between queries and documents, albeit at the cost of increased computational overhead. To address the limitations of dense retrievers, hybrid models [30,67] combine sparse and dense approaches, utilizing both term matching and semantic understanding for enhanced retrieval performance. \n\nRecent research has also explored retrieval enhancements through knowledge distillation, curriculum learning, and pre-training. Distillation techniques transfer knowledge from cross-encoders or teacher retrievers to student models, improving dense retriever effectiveness [18,86]. Curriculum learning strategies, which progressively train retrievers from easy to hard samples [60,124], have been shown to enhance both supervised and zero-shot retrieval effectiveness. Meanwhile, pre-training methods such as contrastive learning [40] and masked autoencoding [29,63] have significantly improved the generalization of retrieval models. Additionally, approaches like query augmentation [66,70] generate synthetic training data to enhance retrieval robustness.",
            "score": 0.5434916377239832,
            "section_title": "Related Work 2.1 Retrievers",
            "char_start_offset": 5166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 519,
                    "end": 524,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 534,
                    "end": 539,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "202718954"
                },
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 898,
                    "end": 901,
                    "matchedPaperCorpusId": "248003306"
                },
                {
                    "start": 1309,
                    "end": 1312,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1720,
                    "end": 1723,
                    "matchedPaperCorpusId": "221802772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "273963180",
            "title": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking with Large Language Models",
            "text": "To evaluate the performance of KeyB2 models, we compare them against several baseline approaches across multiple datasets: \n\n\u2022 On TREC 2019 DL: Following [30], the baselines used for this dataset include traditional methods like BM25, neural IR models such as CO-PACRR [19], TK [15], and TKL [14], as well as transformer-based models like RoBERTa (FirstP and MaxP) [33] and PARADE [26]. Additionally, we compare with sparse attention models, including Sparse-Transformer [4], Longformer-QA [2], Transformer-XH [56], and QDS-Transformer [21]. Previous key block selection models like IDCM [13] and KeyB [30] are also included. \n\nThe LLM-based RankLLaMA [34] model serves as another important baseline. \u2022 On Robust04: The baselines for this dataset consist of BM25, as well as BERT-based KeyB models (using BM25 and BinB configurations). We also compare with the RankLLaMA model. \u2022 On MLDR-zh: the baselines include traditional IR methods such as BM25, along with KeyB models using BM25 and BinB configurations. For this dataset, the KeyB models utilize a Chinese version of BERT and a BM25 implementation adapted for Chinese text. The RankLLaMA model is also used as a baseline for multilingual retrieval.",
            "score": 0.5431643420456416,
            "section_title": "Baselines",
            "char_start_offset": 32201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1204
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "244346065"
                },
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "31963257"
                },
                {
                    "start": 278,
                    "end": 282,
                    "matchedPaperCorpusId": "211031834"
                },
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "218581396"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "212745235"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "225067971"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "234790128"
                },
                {
                    "start": 602,
                    "end": 606,
                    "matchedPaperCorpusId": "244346065"
                },
                {
                    "start": 652,
                    "end": 656,
                    "matchedPaperCorpusId": "263908865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1568603515625
        },
        {
            "corpus_id": "247450700",
            "title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation",
            "text": "One may have a concern that, as a sparse retrieval model -BM25 -outperforms all the other dense retrieval models on the TQA dataset in Table 1, TQA is not good enough to demonstrate the strength of our dense augmentation strategy. While we believe that sparse retrieval models are not our competitors as we aim to improve the dense retrieval models with data augmentation, in order to clear out such a concern, we additionally train and evaluate our DAR on the WebQuestions (WQ) dataset (Berant et al., 2013), following the data processing procedure from (Karpukhin et al., 2020). As Table 9 shows, our DAR outperforms both dense and sparse retrieval models. Thus, the best scheme  among sparse and dense retrievers still depends on the dataset, and combining sparse and dense models to complement each other will be a valuable research direction, which we leave as future work.",
            "score": 0.5427714572307015,
            "section_title": "B.3 Experiment on WebQuestions",
            "char_start_offset": 28729,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 508,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 555,
                    "end": 579,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "259298789",
            "title": "Copy is All You Need",
            "text": "Dense Retrieval The dense retrieval technique (Karpukhin et al., 2020) has been widely used in many downstream NLP tasks, such as open-domain question answering (Karpukhin et al., 2020;Lee et al., 2021), open-domain dialogue systems (Lan et al., 2021) and machine translation (Cai et al., 2021). Different from the traditional sparse retrieval system, such as BM25 and TF-IDF (Robertson & Zaragoza, 2009), dense retrieval learns a shared vector space for queries and documents, where relevant pairs of query and document have smaller distances (i.e., higher similarities) than the irrelevant pairs. \n\nThe most closely related work to our study is DensePhrase (Lee et al., 2021). DensePhrase reformulates the question-answering task as a phrase retrieval problem, where phrases are directly retrieved and returned as answers to factual questions. Differently, our work aims to generate coherent text continuations through multiple rounds of phrase retrieval. Since the connection between two adjacent phrases should be coherent and fluent in the text generation task, it is much more difficult. \n\nRetrieval-Augmented Text Generation (RAG) Retrieval-augmented text generation has gained increasing interest recently. Most prior work improves the generation quality (e.g., informativeness) of language models by grounding the generation on a set of retrieved materials (e.g., relevant documents) (Li et al., 2022;Guu et al., 2020;Hashimoto et al., 2018;Weston et al., 2018;Cai et al., 2019a;b;Khandelwal et al., 2020;Wu et al., 2019;Guu et al., 2020;Lewis et al., 2020;Borgeaud et al., 2022;Yang et al., 2023). Our work is on this line of research but takes a radical step forward. Unlike prior work that builds the combinations of retrieval and generation, retrieval is generation in COG. \n\nOne contemporary work to our work is Min et al. (2022), which shares the idea of replacing the fixed vocabulary with a nonparametric phrase table.",
            "score": 0.5427481919773143,
            "section_title": "RELATED WORK",
            "char_start_offset": 24161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 70,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 161,
                    "end": 185,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 376,
                    "end": 404,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1427,
                    "end": 1450,
                    "matchedPaperCorpusId": "54446010"
                },
                {
                    "start": 1450,
                    "end": 1470,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 1470,
                    "end": 1488,
                    "matchedPaperCorpusId": "52281331"
                },
                {
                    "start": 1488,
                    "end": 1490,
                    "matchedPaperCorpusId": "54446010"
                },
                {
                    "start": 1490,
                    "end": 1514,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1514,
                    "end": 1530,
                    "matchedPaperCorpusId": "49312395"
                },
                {
                    "start": 1566,
                    "end": 1588,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "277468629",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations",
            "text": "Recent studies have demonstrated that dense retrieval methods using pretrained language models to encode text into dense vectors outperform traditional lexical retrievers like TF-IDF and BM25. Following the introduction of DPR (Karpukhin et al., 2020), several approaches have been proposed to improve dense retrieval through advanced training strategies (e.g., ANCE (Xiong et al., 2021), Condenser (Gao & Callan, 2021)), data augmentation techniques (e.g., BGE (Xiao et al., 2023), GTE (Li et al., 2023), DRAMA (Ma et al., 2025)), and by leveraging large language models as backbones (e.g., LLM2Vec (BehnamGhader et al., 2024), RepLlama (Ma et al., 2024), Mistral-E5 (Wang et al., 2023)). Today, commercial embedding models (e.g., OpenAI (Neelakantan et al., 2022), GeminiEmbed (Lee et al., 2025)) are widely used in real-world retrieval systems. However, most existing methods are designed for single-turn retrieval with short queries for retrieval, which is not well suited for citation suggestions where paper contexts are used to retrieve the next relevant citation.",
            "score": 0.5423111329360738,
            "section_title": "Dense Retrieval",
            "char_start_offset": 4631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1071
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 251,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 638,
                    "end": 655,
                    "matchedPaperCorpusId": "263908865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "273901352",
            "title": "Language Bias in Multilingual Information Retrieval: The Nature of the Beast and Mitigation Methods",
            "text": "The MRC@5 results presented in Table 7 show more than a 20-fold disparity between BM25 and the neural retrieval ranker DPR, with scores of 0.6 and 11.7, respectively. To understand the underlying causes, we analyzed the top 100 ranking outputs from both methods. As shown in Figure 6, BM25's output document languages and query languages exhibit a strong correlation along the diagonal line, contributing to heightened language bias. Since BM25 is only able to retrieve documents containing keywords present within the query (Thakur et al., 2021) and suffers from lexical gap (Berger et al., 2000), resulting in high retrieval rates for documents in the same language as the query. Meanwhile, DPR retrieves documents across different languages more effectively, with substantial off-diagonal values and reflecting the skewness of the dataset (see Table 6). This suggests that neural retrieval approaches can mitigate language bias to leveraging multilingual pre-trained models that understand semantic content regardless of the language.",
            "score": 0.5409156346054091,
            "section_title": "Effect of Neural Retrieval Approaches",
            "char_start_offset": 16794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1037
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 597,
                    "matchedPaperCorpusId": "6749682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "Passage retrieval is an important component in applications like ad-hoc information retrieval, opendomain question answering (Karpukhin et al., 2020), retrieval-augmented generation (Lewis et al., 2020) and fact verification (Thorne et al., 2018). Sparse retrieval methods such as BM25 were the dominant approach for several decades, and still play a vital role nowadays. With the emergence of large-scale pre-trained language models (PLM) (Devlin et al., 2019), increasing attention is being paid to neural dense retrieval methods (Yates et al., 2021). Dense retrieval methods map both queries and passages into a low-dimensional vector space, where the relevance between the queries and passages are measured by the dot product or cosine similarity between their respective vectors. Like other NLP tasks, dense retrieval benefits greatly from a strong general-purpose pre-trained language model. However, general-purpose pretraining does not solve all the problems. As shown in Table 1, improved pre-training techniques that are verified by benchmarks like GLUE  do not result in consistent performance gain for retrieval tasks. Similar observations are also made by Lu et al. (2021). We hypothesize that, to perform robust retrieval, the [CLS] vector used for computing matching scores should encode all the essential information in the passage. The next-sentence prediction (NSP) task in BERT introduces some supervision signals for the [CLS] token, while RoBERTa  and ELECTRA do not have such sequence-level tasks.\n\nIn this paper, we propose SimLM to pre-train a representation bottleneck with replaced language modeling objective. SimLM consists of a deep encoder and a shallow decoder connected with a representation bottleneck, which is the [CLS] vector in our implementation. Given a randomly masked text segment, we first employ a generator to sample replaced tokens for masked positions, then use both the deep encoder and shallow decoder to predict the original tokens at all positions. Since the decoder only has limited modeling capacity, it must rely on the representation bottleneck to perform well on this pre-training task. As a result, the encoder will learn to compress important semantic information into the bottleneck, which would help train biencoder-based 1 dense retrievers. Our pretraining objective works",
            "score": 0.5401288181731789,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 149,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 225,
                    "end": 246,
                    "matchedPaperCorpusId": "53645946"
                },
                {
                    "start": 440,
                    "end": 461,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "252846763",
            "title": "RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media",
            "text": "BM25 A standard Bag-of-Words method for IR (Robertson et al., 1995). We form queries by concatenating the Reddit post with a single claim and its corresponding PIO frames. We used a publicly available BM25 implementation from the Rank-BM25 library. 10 ense Passage Retrieval (DPR) is a dense retrieval model trained to retrieve relevant context spans (\"paragraphs\") in an open domain questionanswering setting (Karpukhin et al., 2020). In general, such models map queries and candidates to embeddings, and then rank candidates with respect to a similarity measure (e.g., dot product) taken between these. While originally designed for opendomain question answering, use of DPR-inspired models has been extended to general retrieval tasks (Thai et al., 2022a). We use a DPR context encoder trained on Natural Questions (Kwiatkowski et al., 2019) with dot product similarity.11",
            "score": 0.5399141637100258,
            "section_title": "Baseline Models",
            "char_start_offset": 17025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 875
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 67,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 410,
                    "end": 434,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 738,
                    "end": 758,
                    "matchedPaperCorpusId": "247593935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337646484375
        },
        {
            "corpus_id": "267320756",
            "title": "Re3val: Reinforced and Reranked Generative Retrieval",
            "text": "The primary objective of retrieval models is to enhance the accuracy of answers by selecting the most relevant documents retrieved for a given query, ensuring models have sufficient information to help the downstream reasoning process. For instance, DRQA (Chen et al., 2017) introduces a \"retrieve and read\" pipeline using TF-IDF to return documents for a question answering model to achieve this goal. More recently, NLP researchers have studied neural retrieval models like Dense Passage Retrieval (DPR) (Karpukhin et al., 2020)    \u2020 Work performed while at KAIST AI. \n\nFigure 1: Re3val's Page Title Reranker (g \u03d5 ) enhances generated page titles (X) with DPR contextual information (Y ), producing reranked titles (Z). This is crucial when documents in X lack a suitable answer to a query (q), as depicted in the figure . with a seq2seq model to build retrieval augmented language models. \n\nRather than using inner-product-based retrieval, generative retrieval models such as GENRE (Cao et al., 2021) and CorpusBrain (Chen et al., 2022) generate page titles through constrained decoding, attaining higher R-Precision and Recall compared to DPR. In our work, we further evaluate how additional contextual information can benefit the generative retrieval models through reranking and how reinforcement learning can enhance relevance through reward signals. \n\nWe introduce Re3val: Reinforced and Reranked Generative Retrieval, a novel framework specifically designed to address the challenges in neural information retrieval. Our approach utilizes 500k pre-training data and 48k task-specific data for training. Despite the reduced data used in distant supervision, Re3val achieves exceptional performance. Our contributions are described as below: \n\n\u2022 We minimize the entropy of the initially retrieved page titles with contexts obtained from DPR, facilitating the novel generative reranking process. Through this reranking procedure, Re3val outperforms other generative retrieval models, including GENRE, Corpus-Brain, and SEAL (Bevilacqua et al., 2022) in terms of average R-Precision across five tasks, showcasing an average increase of 1.9%. \n\n\u2022 We incorporate REINFORCE (Williams, 1992) to integrate information during the decoding process of generative retrieval.",
            "score": 0.5389136692389512,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2146
                },
                {
                    "start": 2149,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 274,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1020,
                    "end": 1039,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 2030,
                    "end": 2055,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "273186680",
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "text": "Recent advancements in natural language processing have spurred the development of Retrieval-Augmented Generation (RAG) models, aimed at enhancing the quality and relevance of generated text by integrating external knowledge sources (Lewis et al., 2020;Guu et al., 2020;Izacard & Grave, 2021;Lin et al., 2024). These models leverage retrieved documents to provide contextually grounded responses, addressing inherent limitations in Large Language Models (LLMs) such as domain specificity (Siriwardhana et al., 2023;Xiong et al., 2024), and knowledge accuracy (Zhang et al., 2023;Kasai et al., 2023). In general, a retrieval system (Formal et al., 2022;Izacard et al., 2022;Wang et al., 2022a) first retrieves top-k related documents for a question from an external database, then LLMs read the question and these documents to generate an answer. \n\nThe alignment between generated text and human preference remains a significant challenge for RAG approaches, particularly evident in question-answering tasks. Retrieval mechanisms often struggle to retrieve relevant information essential for specific queries (Zhang et al., 2024). Stateof-the-art retrieval models can be categorized into dense retrieval and sparse retrieval (Luan et al., 2021). Sparse retrieval uses a sparse vector to represent statistical feature based on a vocabulary (Jones, 1972;Robertson & Zaragoza, 2009) which may fail to capture high level semantics, and suffer from the lexical gap (Berger et al., 2000;Izacard et al., 2022). On the other hand, dense retrieval leverages a pre-trained language model (PLM) to represent the input sequence by a fixed length vector (Reimers & Gurevych, 2019;Karpukhin et al., 2020) which may fail in specialized domains or with outdated data. Moreover, while PLMs excel in managing long-context windows (Su et al., 2024;Zhu et al., 2024;Ding et al., 2024), challenges arise with excessive retrieval context (Xu et al., 2024b;Liu et al., 2024a).",
            "score": 0.5379929750377174,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 253,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 253,
                    "end": 270,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 270,
                    "end": 292,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 488,
                    "end": 515,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 579,
                    "end": 598,
                    "matchedPaperCorpusId": "251105205"
                },
                {
                    "start": 631,
                    "end": 652,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 652,
                    "end": 673,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 673,
                    "end": 692,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1338,
                    "end": 1351,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 1351,
                    "end": 1378,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1459,
                    "end": 1480,
                    "matchedPaperCorpusId": "6749682"
                },
                {
                    "start": 1480,
                    "end": 1501,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1640,
                    "end": 1666,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1666,
                    "end": 1689,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "However, for error correction scenarios, the semantic consistency between chunks is relatively unimportant, and a smaller chunk overlap value can be considered. \n\nWhen choosing an embedding model, you can refer to the mteb leaderboard [38], which shows the performance of different embedding models on retrieval tasks. However, the actual performance of the RAG system may differ from the leaderboard, so you need to evaluate and adjust according to the specific scenario. \n\nWhen choosing a retrieval algorithm, BM25 has the advantage of saving computational resources compared to dense retrievers, and since it is a keyword-based algorithm, it can usually retrieve very relevant documents. However, keyword-based algorithms perform poorly in capturing semantics and may miss some relevant content. Therefore, we suggest using BM25 for tasks that require precise content generation, such as hallucination modification and summarization. \n\nHowever, BM25 may not be suitable for tasks that require semantic understanding, such as question answering and creative generation, and we recommend using dense algorithms based on deep learning embeddings instead. \n\nMoreover, the hybrid algorithm that combines dense and BM25 retriever has very limited improvement on the overall quality of the generated results. However, by using a rerank model to reorder the retrieval results and then inputting them into LLMs, the performance of almost all tasks improved, especially reasoning tasks. Therefore, we suggest trying to use the hybrid algorithm + rerank retrieval mode when the conditions permit, which can achieve better performance in the RAG system. \n\nWhen choosing a large language model, GPT-4 model is undoubtedly the most advanced model at present. However, due to the high cost of invoking GPT-4, we may need to consider some open-source alternatives. According to our experimental results, Qwen-14B model has shown similar performance to GPT-4 in the two tasks of text continuation and summary generation, and can generate high-quality creative and summarizing texts. In the QA task, Baichuan2-13B model also showed a level close to GPT-4, and can generate accurate and fluent answers. Therefore, we can choose a suitable LLM according to different tasks and cost requirements.",
            "score": 0.5378686278828068,
            "section_title": "Suggestions for Optimizing Your RAG System",
            "char_start_offset": 64934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2278
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "However, there are three major problems when directly adopting these methods to multilingual dense retrieval. First, zero-shot multilingual query generators suffer from grammatical adjustment and accidental translation problems (Xue et al., 2021). As a result, zero-shot query generators only provide little help in bridging the gap among different languages. Second, hybrid dense and sparse models such as LaPraDoR and SPAR get high latency in the inference stage1 . Finally, dense retrieval is different from other tasks, it not only needs positive query-passage pairs but also needs negative query-passage pairs (Xiong et al., 2021). However, previous methods such as the back-training focus on positive pairs and simply take the top passages of BM25 as negative passages. \n\nAlthough training data in target languages is very expensive, unlabeled queries and passages can be easily obtained from search engines such as Google and Bing. In this paper, we propose a novel method that augments data in target languages by combining sparse and dense models, namely LeSTM, which stands Lexicon-enhanced Self-supervised Training for Multilingual dense retrieval. First, as we mentioned above, sparse retrievers mainly depend on keyword matching between queries and passages and dense retrievers mainly depend on the language modeling ability of pre-trained models, which indicates the sparse and dense models perform retrieval in different aspects (Chen et al., 2021). In addition, the sparse-dense hybrid retriever is significantly better than both sparse and dense models (Zhang et al., 2021b;Ma et al., 2021b). Both can demonstrate that sparse and dense models notice different characteristics and are complementary. Therefore, we craft a lexiconenhanced retrieval module to mine positive and negative passages for each unlabeled query in target languages, which leverages the retrieval results of both sparse and dense models. We treat passages that both sparse and dense models regard are relevant as positive passages, and passages that one model regards are relevant but the other regards are irrelevant as negative passages. \n\nFurthermore, we employ a query generator to generate queries for passages in target languages due to the limited number of unlabeled queries.",
            "score": 0.537818316053448,
            "section_title": "Introduction",
            "char_start_offset": 1874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "229680000",
            "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes",
            "text": "For sparse, lexical retrieval, we use ElasticSearch, which is based on BM25. For dense retrieval, we use a DistilRoBERTa-base model (Sanh et al., 2020) as a bi-encoder: The query and the passage are passed independently to the transformer model and the output is averaged to create fixed-sized representations. We train this using InfoNCE loss (van den Oord et al., 2018):\n\nwith q the query, p + the relevant passage. We use in-batch negative sampling and use the other passages in a batch as negative examples. We found that \u03c4 = 20 performs well. We train the model in two setups: 1) only with random (in-batch) negatives, and 2) we provide for each query additionally one hard-negative passage. We use the hardnegative passages provided by the MS MARCO dataset, which were retrieved using lexical search. Models are trained with a batch size of 128 with Adam optimizer and a learning rate of 2e \u2212 5.\n\nDistilRoBERTa produces representations with 768 dimensions. We also experiment with lowerdimensional representations. There, we added a linear projection layer on-top of the mean pooling operation to down-project the representation to either 128 or 256 dimensions. Dense retrieval is performed using cosine similarity with exact search.\n\nModels were trained using the SBERT framework (Reimers and Gurevych, 2019). 4",
            "score": 0.5377824874801569,
            "section_title": "Model",
            "char_start_offset": 7837,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1287,
                    "end": 1315,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489501953125
        },
        {
            "corpus_id": "271161731",
            "title": "PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents",
            "text": "For a fair comparison and following the work of Mallen et al. [23] and Trivedi et al. [33], the same retriever, a term-based sparse retrieval model known as BM25 [26], is used across all different models.The retrieval model is implemented using the OpenMatch toolkit [38].For the external document corpus, the KILT-Wikipedia corpus preprocessed by Petroni et al. [25] is used, and the top-k relevant documents are retrieved.\n\nRegarding the LLMs used to generate answers, the Llama 3 model instruct (ref) with 70b parameters, Mixture of Experts (MoE) 8x7b (ref), and the GPT-3.5 model (gpt-3.5-turbo-0125)are employed.For the retrieval-augmented LLM design, the implementation details from Trivedi et al. [33] are followed, which include input prompts, instructions, and the number of test samples for evaluation (e.g., 500 samples per dataset).",
            "score": 0.5368059546505193,
            "section_title": "Implementation Details",
            "char_start_offset": 21056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 424
                },
                {
                    "start": 426,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 844
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "259949845"
                },
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 704,
                    "end": 708,
                    "matchedPaperCorpusId": "254877499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407958984375
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Existing dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
            "score": 0.5366907794599798,
            "section_title": "Related Work",
            "char_start_offset": 3117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 87,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 118,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 625,
                    "end": 642,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 661,
                    "end": 681,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 927,
                    "end": 946,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "The retrieval and reranking evaluation on Gemma-2-2B and Gemma-2-9B is presented in Table 8. The larger Gemma-2-9B model significantly outperforms its 2B counterpart across all benchmarks, demonstrating the benefits of model scaling in retrieval-augmented question answering. On Gemma-2-2B, DPR+R+G achieves 30.25% EM on NQ and 37.73% on TriviaQA, while the traditional BM25 retriever falls behind, achieving 14.02% EM on NQ and 43.28% on TriviaQA. The generative augmentation (G) alone leads to better recall but performs worse in terms of EM than retrieval-augmented methods, with 46.85% Recall on NQ and 72.99% on TriviaQA. \n\nThe Gemma-2-9B model exhibits substantial improvements, with DPR+R+G achieving 30.83% EM on NQ and 37.89% on TriviaQA, mirroring the performance trends observed with LLaMA-3.1 8B. Interestingly, BM25+G surpasses the retrievalonly (BM25 R) approach by achieving 63.02% Recall on TriviaQA, reinforcing that generative augmentation benefits sparse retrieval methods. However, the best-performing approach remains MSS+DPR+R+G, which achieves a Reciprocal Rank of 49.23% on NQ and 73.67% on TriviaQA, emphasizing the importance of hybrid search.",
            "score": 0.5365496241076035,
            "section_title": "A.3.1 Retrieval and RAG Performance on",
            "char_start_offset": 35901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "269762577",
            "title": "A Case Study of Enhancing Sparse Retrieval using LLMs",
            "text": "Although dense retrieval methods have shown to be highly effective across various information retrieval tasks [12,16,34], sparse retrieval still has advantages in many information retrieval (IR) scenarios. For example, many tasks apply sparse retrieval to produce the first ranking and then employ dense retrieval to re-rank the candidate documents. As a result, the quality of sparse retrieval directly affects the overall ranking performance [19,22]. In addition, sparse retrieval proves highly effective in scenarios where explicit feature weighting is critical. Such capabilities ensure that sparse retrieval maintains a significant role in the landscape of information retrieval technologies [10]. Finally, the agility and interpretability of sparse retrieval techniques should not be underestimated. Sparse retrieval can excel in specialized applications, offering nuanced insights and precision that dense methods may not always capture [16]. \n\nHowever, the critical issue of query-document mismatch renders sparse retrieval infeasible in many practical applications [10,20]. Query expansion or query rewriting are promising approaches to address this problem. The expanded or re-written query can enhance the original query with newly generated terms to recover relevant documents that had no lexical overlap with the original query. Traditional query expansion approaches are typically based on Pseudo-relevance Feedback (PRF) [3,[26][27][28]. They treat the set of retrieved documents from the original query as relevant documents and then use these documents to extract new terms. These approaches assume that the top retrieved documents are relevant to the query. As a result, PRF-based approaches may fail if the initial retrieved documents are not good enough [18,33]. \n\nTo address the issue of the PRF-based approaches, a rich body of research work develops pre-trained language models to conduct query expansion/query rewriting, in the belief that PLMs contain highly relevant information from large-scale unlabeled datasets [15,19,33]. For example, GAR uses a pre-trained language model called BART [17] to augment the query through text generation of heuristically discovered relevant contexts [19]. They demonstrate that the generated contexts can enrich the semantics of the queries. With the development of language modeling, LLMs have shown great performance in information retrieval tasks [36].",
            "score": 0.5364076241234599,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 1436,
                    "end": 1439,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 1439,
                    "end": 1443,
                    "matchedPaperCorpusId": "1418295"
                },
                {
                    "start": 1443,
                    "end": 1447,
                    "matchedPaperCorpusId": "45186038"
                },
                {
                    "start": 1774,
                    "end": 1778,
                    "matchedPaperCorpusId": "258332122"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "This model can capture more contextual information and generally performs better than the dual-encoder model. However, these dense retrieval methods still have 3 main problems: (1) The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert [15] further increases the latency to 458 milliseconds, nearly 13 times of the BM25. (3) Negative samples are typically randomly sampled from the current batch of data or the entire dataset, lacking true annotations. These samples may only be highly similar pseudo-negative ones to the query. Simply pushing away the query from them can impact the sentence representation and affect the overall model performance. \n\nCurrent research has focused on the combined use of crossencoder and dual-encoder models in dense passage retrieval. In this paper, we propose a multi-level distillation model called MD2PR, in which a dual-encoder model is utilized for large-scale candidate retrieval during the retrieval phase, while a cross-encoder model is employed for fine ranking of the retrieved results during the ranking phase. This method can not only improve the quality of the retrieval results but also decrease the computational cost. The main contributions are shown as follows. \n\n(1) We use a knowledge distillation method, by which the knowledge learned by the ranker is transferred to the retriever at both sentence and word levels.",
            "score": 0.5364048131380392,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "254854344",
            "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
            "text": "Retriever-based language modeling or question answering has been well studied in the literature, and different mechanisms have been proposed for integrating language models with retrieved statements. For instance, REALM (Retrieval-Augmented Language Model) is a masked language model that is augmented with a latent knowledge retriever, which allows the model to retrieve and attend over statements (Guu et al., 2020). The knowledge retriever employs a BERT encoder to achieve a dense representation for both the query and statements. The knowledge retriever selects the most similar statements among all based on a similarity metric (e.g. dense inner product) between the transformed query's and the statements' representations. In the language modeling task, the retrieved statements are appended to the query and passed to a trained BERT-based language model to solve the task. In the question answering task, however, a trained BERT-based reader extracts the more promising span from the statements as the answer.\n\nkNN-LM is another proposed model based on a decoder-only Transformer, integrated with a knearest neighbor module (Khandelwal et al., 2020). In this model, the most related token sequences from the statements are selected based on an L2 similarity metric between the representation of the query and all token sequences. The distribution over the next token in generation in this autoregressive model is subsequently computed as the interpolation between the Transformer's final distribution and the distribution of the next tokens over the retrieved statements.\n\nFiD (Fusion-in-Decoder) is a sequence-tosequence T5-based neural network which can work with any retriever (Izacard and Grave, 2021). Given the retrieved statements by the retriever, the encoder encodes the query and each retrieved statement separately. Afterwards, the decoder attends over the resulting representations of all the retrieved statements. In this paper, we investigate DPR (Dense Passage Retriever) as the retriever for FiD (Karpukhin et al., 2020). DPR retrieves the most similar documents based on the inner product of the representations of the query and the documents (i.e. embeddings of the [CLS] token) from two independent trained BERT encoder models.\n\nFinally, ATLAS is a pre-trained retrieval augmented language",
            "score": 0.5356213775324251,
            "section_title": "Retriever-augmented Language Models",
            "char_start_offset": 8151,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 417,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1688,
                    "end": 1713,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4384765625
        },
        {
            "corpus_id": "278129839",
            "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
            "text": "We compare PropRAG against a wide range of baseline methods, categorized by their retrieval approach. Baseline results are primarily taken from the comprehensive evaluation in [1], which used the same Llama-3.3-Instruct reader and NV-Embed-v2 retriever setup as our work, ensuring a fair comparison. \n\n\u2022 Classic Retrievers: Traditional IR methods based on lexical matching or early dense representations. \n\n-BM25 [32]: A sparse retrieval method based on term frequency and inverse document frequency. \n\n-Contriever [15]: An early dense retriever trained using unsupervised contrastive learning. \n\n-GTR (T5-base) [16]: A dense retriever based on the T5 architecture, known for strong performance on retrieval benchmarks. \n\n\u2022 Large Embedding Models: Recent, powerful dense retrieval models often based on large language model architectures, representing the state-of-the-art in single-vector passage retrieval. \n\n-GTE-Qwen2-7B-Instruct [17]: A strong embedding model based on the Qwen2 architecture. \n\n-GritLM-7B [18]: An embedding model trained using generative representational instruction tuning. \n\n-NV-Embed-v2 (7B) [7]: A state-of-the-art embedding model from NVIDIA, used as the primary retriever in our experiments and for HippoRAG 2 comparison. \n\n\u2022 Structure-Augmented RAG: Methods that explicitly build or leverage structure (summaries, graphs) beyond simple passage retrieval to improve context integration or reasoning. \n\n-RAPTOR [11]: Recursively clusters and summarizes passages to build a tree structure for retrieval. \n\n-GraphRAG [9]: Uses graph community detection and summarization over extracted entities and relationships. \n\n-LightRAG [10]: Employs a dual-level retrieval mechanism integrating graph structures and vector retrieval (results indicate potential issues with their implementation or general approach on these benchmarks). \n\n-HippoRAG [21]: The initial version using PPR over triples. \n\n-HippoRAG 2 [1]: The state-of-the-art structured RAG baseline prior to this work, using PPR over a triple-based KG with integrated passage information.",
            "score": 0.5353335253901801,
            "section_title": "Baselines",
            "char_start_offset": 26064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1439,
                    "end": 1443,
                    "matchedPaperCorpusId": "267334785"
                },
                {
                    "start": 1864,
                    "end": 1868,
                    "matchedPaperCorpusId": "269982289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al. 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al. 1995;Robertson and Zaragoza 2009). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models. We refer the reader to Thakur et al. (2021) for details on major retrieval methods. It is also reported that re-ranking the retrieved passages can improve recall in EQ dataset (e.g., Sachan et al. 2022), but note that re-rankers are outside the scope of this paper. \n\nIn-Domain Generalization of Retriever. Dense retrievers still have room for improvement of in-domain generalization, as well as out-of-distribution generalization (Thakur et al. 2021). To generalize in Wikipedia domain, Sciavolino et al. (2021) trained DPR on PAQ dataset (Lewis et al. 2021) which contains many of named entities in Wikipedia. However, the trained model still performed far worse than BM25 in EQ dataset. Sciavolino et al. (2021) also attempted to improve recalls for relational questions by training dedicated question encoders for each relation. While this approach somewhat improved recall, the dedicated retrievers did not reach the performance of BM25 on average and did not solve the poor generalization problem. In contrast, our proposed method employs a pretrained language model without finetuning for retrieval, allowing us to exploit the rich knowledge including named entities in Wikipedia learned by pretraining. \n\nDifference between Dense Retrieval and Sparse Retrieval. Ma et al. (2021) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020).",
            "score": 0.533664542506345,
            "section_title": "Related Work",
            "char_start_offset": 5041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 153,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 275,
                    "end": 298,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 298,
                    "end": 326,
                    "matchedPaperCorpusId": "3641284"
                },
                {
                    "start": 561,
                    "end": 581,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 969,
                    "end": 989,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1026,
                    "end": 1050,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1078,
                    "end": 1097,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 1228,
                    "end": 1252,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1808,
                    "end": 1824,
                    "matchedPaperCorpusId": "3641284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "272145432",
            "title": "LexBoost: Improving Lexical Document Retrieval with Nearest Neighbors",
            "text": "Traditionally, lexical methods based on word overlap were used [26]. Utilization of inverted index in this sparse retrieval leads to high efficiency due to the term to document mapping. Different methods of weighing and normalization [38] led to a range of Term Frequency Inverse Document Frequency (TFIDF) models. BM25 [32] is one of the most popular and effective formulation in sparse retrieval. BM25 is a bag-of-words retrieval function. Here the documents are ranked based on the query terms appearing in each document, regardless of their proximity within the document. BM25 can be viewed as a non-linear combination of three basic document attributes: term frequency, document frequency, and the length of the document [37]. Document length normalization and query term saturation are the key features in BM25 and hence it did not favor shorter or longer documents and mitigates the impact of excessively high term frequency unlike TFIDF. BM25 is also extended with prescription regarding how to combine additional fields in document description [31]. Divergence From Randomness (DFR) framework was proposed to build probabilistic term weighting schemes [3]. It consists of two divergence functions and one normalization function. Two of the best DFR framework models are PL2 (Poisson-Laplace with second normalization of term frequency) and DPH (hyper-geometric model Popper's normalization) [3]. These methods typically suffer from high vocabulary dependence [27]. Query Linear Combination and Relevant Documents (QLD) uses relevant documents of similar queries for expressing the query as a linear combination of existing queries [5]. Query expansion and pseudo relevance feedback offer resolve to some extent but come with certain latency overhead [8]. KL expansion [47], Rocchio [33], Relevance Modelling [25] and RM3 [1] are popular pseudo relevance feedback methods. Efficiency of lexical methods is exploited by using it as a first-stage document ranker for a short listed input to intricate dense retrieval and large language model based systems [18].",
            "score": 0.5335689292409411,
            "section_title": "Lexical Methods",
            "char_start_offset": 4174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "1500900"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "1661277"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "16628332"
                },
                {
                    "start": 1161,
                    "end": 1164,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 1400,
                    "end": 1403,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "57662599"
                },
                {
                    "start": 1640,
                    "end": 1643,
                    "matchedPaperCorpusId": "40045539"
                },
                {
                    "start": 1759,
                    "end": 1762,
                    "matchedPaperCorpusId": "10393627"
                },
                {
                    "start": 1777,
                    "end": 1781,
                    "matchedPaperCorpusId": "1043470"
                },
                {
                    "start": 1791,
                    "end": 1795,
                    "matchedPaperCorpusId": "61859400"
                },
                {
                    "start": 1817,
                    "end": 1821,
                    "matchedPaperCorpusId": "1118305"
                },
                {
                    "start": 2062,
                    "end": 2066,
                    "matchedPaperCorpusId": "260442139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "271244429",
            "title": "Optimizing Query Generation for Enhanced Document Retrieval in RAG",
            "text": "Dataset We evaluate on three retrieval datasets from BEIR (Thakur et al., 2021): SciFact (Wadden et al., 2020), Trec-Covid (Voorhees et al., 2021) and FiQA (Maia et al., 2018).We evaluated on fact checking task about scientific claims, Bio-medical information retrieval, and question answering task on financial domain, respectively.Baseline (1) Sparse Retrieval: (a) BM25 (Robertson and Zaragoza, 2009) model is a widelyused bag-of-words retrieval function that relies on token-matching between two high-dimensional sparse vectors, which use TF-IDF token weights.We used default setting from Pyserini (Lin et al., 2021).(b) BM25+RM3 (Robertson and Zaragoza, 2009;Lv and Zhai, 2009) is query expansion method using PRF.We also include (c) BM25+Q2D/PRF (Robertson and Zaragoza, 2009;Jagerman et al., 2023) that use both LLM-based and PRF query expansion methods.\n\n(2) Dense Retrieval: (a) BGE-base-en-v1.5 model is a stateof-the-art embedding model designed for various NLP tasks like retrieval, clustering, and classification.For dense retrieval tasks, we added 'Represent this sentence for searching relevant passages:' as a query prefix, following the default setting from Pyserini.(Lin et al., 2021).We also used CSQE (Lei et al., 2024) for both sparse retrieval and dense retrieval.\n\nImplementation details We utilize GPT-3.5-Turbo (OpenAI, 2024) as the LLM optimizer.The temperature is set to 1.0.We set the max optimization iteration as i = 1, 2, \u2022 \u2022 \u2022 , 50.We use N = 5, K = 3, R 0 = 3, and R i = 1.All hyperparameters of k 1 = 1.2, b = 0.75, and \u03b1 = 0.1 are set to default values from Pyserini (Lin et al., 2021).",
            "score": 0.5327808860447762,
            "section_title": "Results",
            "char_start_offset": 7178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 861
                },
                {
                    "start": 863,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1184
                },
                {
                    "start": 1184,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1286
                },
                {
                    "start": 1288,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1402
                },
                {
                    "start": 1402,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1621
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 110,
                    "matchedPaperCorpusId": "216867133"
                },
                {
                    "start": 156,
                    "end": 175,
                    "matchedPaperCorpusId": "13866508"
                },
                {
                    "start": 373,
                    "end": 403,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 602,
                    "end": 620,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 634,
                    "end": 664,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 664,
                    "end": 682,
                    "matchedPaperCorpusId": "12012494"
                },
                {
                    "start": 752,
                    "end": 782,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1184,
                    "end": 1202,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "249890267",
            "title": "Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation",
            "text": "We compare DSI-QG with the following baselines: \n\n\u2022 BM25 [41]: a classic sparse retrieval method based on inverted indexes. This method usually only works for monolingual retrieval tasks as it is a keyword-matching method. We use the Pyserini [27] implementation of BM25 for this baseline. \u2022 BM25 + docT5query [35]: a sparse retrieval method which also leverages query generation. It uses a T5 model to generate a set of queries and appends them to the original document. Then it uses an inverted index and BM25 to retrieve augmented documents. In the original study that investigated this method, only the mono-lingual retrieval task was considered [35]. For fair comparison with DSI-QG, we adapt this method to the cross-lingual retrieval setting by replacing the mono-lingual T5 query generation model with the same multi-lingual T5 generation model used in our DSI-QG. We also use the Pyserini implementation for this baseline. \n\n\u2022 SEAL [2]: an autoregressive generation model that is similar to DSI. It treats ngrams that appear in the collection as document identifiers; at retrieval time, it directly generates and scores distinctive ngrams that are mapped to the documents. Unlike DSI, which unifies the index into the model parameters, SEAL requires a separate index data structure to perform an efficient search. Note that no publicly available implementation of SEAL currently exists. Unlike for DSI below, the re-implementation of SEAL is outside the scope of our work, and thus we report the results obtained by Bevilacqua et al. [2] on the NQ 320k dataset. SEAL has not been devised for and experimented with the task of cross-lingual retrieval and thus no results for XOR QA 100k are reported. \n\n\u2022 mDPR [1,20]: a mBERT-based cross-lingual dense passage retrieval method trained with a contrastive loss and with hard negatives sampled from the top passages retrieved by BM25. mDPR relays on nearest neighbor index search (Faiss implementation [19]) to retrieve the passages that have the closest embeddings to the query embedding.",
            "score": 0.5321689383219678,
            "section_title": "Baselines",
            "char_start_offset": 17045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 50,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 1718,
                    "end": 1721,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "266693273",
            "title": "Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization",
            "text": "Text retrieval involves searching for relevant information in vast text collections based on user queries. Efficient and effective methods for this task have revolutionized how we interact with information systems. Recently, there has been growing interest in augmenting large language models (LLMs) with text retrieval for question answering (QA) (Lewis et al. 2020a;Guu et al. 2020;Glass et al. 2022;Borgeaud et al. 2022;Fu et al. 2022;Zhang et al. 2023). These approaches harness retrieval models to obtain external knowledge and ground LLM outputs, reducing hallucinations and the need for frequent LLM updates. Interestingly, augmenting an LM with a retrieval helps reduce the number of parameters required to achieve similar performance as larger LMs (Mialon et al. 2023). \n\nText retrieval methods can be broadly categorized into two main approaches: sparse and dense retrievals. Sparse methods, such as BM25, exploit the frequency of words to measure the relevance between a passage and a query. While efficient, these methods often fall short of capturing intricate relationships and contextual nuances of language. In contrast, dense retrieval methods aim to learn meaningful representations from the semantic content of passages and queries effectively. These models can be trained based on a pretraining model (e.g. BERT, RoBERTa) as well as fine-tuned for downstream QA tasks (Lewis et al. 2020b), offering easy integration. In addition, it is possible to apply approximate nearest neighbors (ANN) with dense retrieval (Xiong et al. 2021) for efficient retrieval. \n\nThis paper focuses on dense retrieval, where contrastive learning is often employed to train passage and query encoders. The core principle of contrastive learning is to encode passages and queries such that relevant passages are closer to their corresponding query in the embedding space, while irrelevant passages are farther away. To train such encoders, we need a labeled dataset with queries annotated with relevant passages (positive samples). However, due to the vast number of candidate passages and the complexity of questions, it is common for annotators to miss relevant information (texts) during data preparation, leading to unlabeled positive examples (false negatives) in the training set.",
            "score": 0.5318199555536276,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 368,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 368,
                    "end": 384,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 384,
                    "end": 402,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 402,
                    "end": 423,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 438,
                    "end": 455,
                    "matchedPaperCorpusId": "229349082"
                },
                {
                    "start": 757,
                    "end": 777,
                    "matchedPaperCorpusId": "391854"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1531,
                    "end": 1550,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "215737187",
            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
            "text": "In this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR), along with analysis on how its output differs from traditional retrieval methods, the effects of different training schemes and the run-time efficiency. \n\nThe DPR model used in our main experiments is trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional BM25 negative passage per question. We trained the question and passage encoders for up to 30 epochs for large datasets (NQ, Trivia, SQuAD) and 100 epochs for small datasets (TREC, WQ), with a learning rate of 10 \u22125 using Adam, linear scheduling with warm-up and dropout rate 0.1. \n\nWhile it is good to have the flexibility to adapt the retriever to each dataset, it would also be desirable to obtain a single retriever that works well across the board. To this end, we train a multidataset encoder by combining training data from all datasets excluding SQuAD. 7 In addition to DPR, we also present the results of BM25, the traditional retrieval method8 and BM25+DPR, using a linear combination of their scores as the new ranking function. Specifically, we re-rank top-2000 BM25 retrieved passages using BM25(q,p) + \u03bb \u2022 sim(q, p) as the ranking function. We used \u03bb = 1.1 based on the development set retrieval accuracy. Results can be improved further in some cases by combining DPR with BM25 in both single-and multi-dataset settings. We conjecture that the lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As a result, there is a high lexical overlap between passages and questions, which gives BM25 a clear advantage. Second, the data was collected from only 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as argued previously by Lee et al. (2019).",
            "score": 0.531788450204794,
            "section_title": "Experiments: Passage Retrieval",
            "char_start_offset": 16514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 1839,
                    "end": 1856,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "273901202",
            "title": "RAC: Retrieval-augmented Conversation Dataset for Open-domain Question Answering in Conversational Settings",
            "text": "Passage Retrieval The proposed reranking strategy significantly improved the first-stage retrieval results from the dense and BM25 retriever, reported in Table 2. As a result, more than half of the queries retrieved relevant passages within the top five results. Given that the query rewriting model trained to generate (pseudo) queries from passages is familiar to relevant query and passage pairs, the probability that a query generated from a relevant passage become higher than that generated from irrelevant passages. \n\nThe performance of BM25 generally exceeded that of the dense retriever. This can be attributed to the nature of human-written queries, which are constructed using a small number of terms derived from previous conversational histories or current questions. As a result, the model trained to generate such queries outputs that are well-suited to the BM25 retriever, which relies on the overlap of terms between a query and a passage. In contrast, the dense retriever, which is designed to capture the semantics of inputs, struggles to effectively capture context from those brief terms. \n\nFollowing the competitive query reformulation method Mo et al. (2023), we additionally trained response generation model that uses only a user question (not a query) as an input without passage retrieval. Then, generated responses were used for expanding queries to enhance the semantics of input queries for the dense retriever. With the expanded queries, the retrieval performance of the dense retriever is significantly improved as shown in Table 3. However, the performance is still lower than that of BM25 (i.e., first-stage retrieval). The result demonstrates that dense retrievers do not always guarantee superior performances compared to BM25 in line with the retrieval results on other CQA datasets, such as QReCC (Anantha et al., 2021). \n\nResponse Generation with FiD We generated responses with diverse retrieval results to understand the correlation between retrieval and response generation. Although the retrieval performance of the dense retriever and BM25 exhibited some differences, the final responses generated using the retrieved passages were almost identical, as shown in Table 4. Moreover, responses generated from passages retrieved by the dense retriever scored higher than those generated using BM25 results, despite BM25's higher retrieval performance. Specifically, response generation performance increased in line with significant improvements in retrieval performance.",
            "score": 0.5316676452867475,
            "section_title": "Main Results",
            "char_start_offset": 15018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2391
                },
                {
                    "start": 2392,
                    "end": 2511
                }
            ],
            "ref_mentions": [
                {
                    "start": 1165,
                    "end": 1181,
                    "matchedPaperCorpusId": "258887946"
                },
                {
                    "start": 1835,
                    "end": 1857,
                    "matchedPaperCorpusId": "222290679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "267782454",
            "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
            "text": "Effective table retrieval in the open-domain setting (Wang et al., 2022;Kweon et al., 2023) remains an open question (Ren et al., 2022).We select BM251 (Robertson et al., 2009b) as the table retriever in our framework for the following considerations.First, BM25, a probabilistic-based ranking function used in information retrieval systems to rank documents, is a standard sparse retrieval method that works efficiently on large corpora, considering term frequency and inverse document frequency with saturation.Second, it is easy to use without the necessary fine-tuning process of Dense Passage Retrieval (DPR) (Karpukhin et al., 2020).Third, empirically, we show that BM25 achieves competitive retrieval performance compared with dense methods such as DPR in Section 4.3.Given these, BM25 as RETRIEVER together with REASONER powered by LLMs make the OPENTAB system off-the-shelf usable without the necessity of fine-tuning on target databases, which is a significant advantage for industrial applications.",
            "score": 0.5316317381383696,
            "section_title": "TABLE RETRIEVER",
            "char_start_offset": 8734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 1009
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 72,
                    "matchedPaperCorpusId": "248965423"
                },
                {
                    "start": 72,
                    "end": 91,
                    "matchedPaperCorpusId": "258676308"
                },
                {
                    "start": 152,
                    "end": 177,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 614,
                    "end": 638,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "252683172",
            "title": "DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases",
            "text": "The retriever retrieves relevant passages from the linearized KB based on the input question. We consider two kinds of retrieval methods: sparse retrieval and dense retrieval. For sparse retrieval, we use BM25 (Robertson et al., 2009), which is based on TF-IDF scores of sparse word match between input questions and KB-linearized passages. For dense retrieval, we apply the DPR (Karpukhin et al., 2020) framework, which is based on similarity in the embedding space between input questions and passages from two fine-tuned BERTs (Devlin et al., 2019). We refer readers to the original paper for details of the fine-tuning process. During inference, suppose there are totally N passages in the knowledge source {p 1 , p 2 , . . . , p N }. DPR applies the passage encoder E P (\u2022) to encode all the passages and store embeddings in memory. For an input question q, DPR applies the question encoder E Q (\u2022) to obtain its representation, and then the passages are retrieved based on the dot-product similarity: \n\nThen it applies FAISS (Johnson et al., 2019) to conduct an efficient similarity search due to the large number of passages N . Through this step, we can retrieve |I retrieve | N passages which are potentially relevant to the input question.",
            "score": 0.5314976797089598,
            "section_title": "RETRIEVAL",
            "char_start_offset": 8706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1249
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 234,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 530,
                    "end": 551,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1031,
                    "end": 1053,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "248366550",
            "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
            "text": "The proposed conceptual framework by [30] argues for a categorization of retrieval models into two dimensions: supervised vs. unsupervised and dense vs. sparse representations 5 . An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods. \n\nA supervised sparse retrieval model such as COIL [12], SPLADE [8], TILDE [78] and DeepImpact [36] can take advantage of the effectiveness of transformer-based language models by changing the terms' weights from collection statistics to something that is learned. DeepCT [6] for example learns term weights with a transformerbased regression model from the supervision of the MSMarco dataset. Approaches that only modify non-zero weights however are not able to address the vocabulary mismatch problem [9], as non-zero terms will not be affected. One way to address such problem in sparse retrieval is by using query expansion methods. RM3 [1] has been shown to be a competitive query expansion technique that uses pseudo-relevance feedback to add new terms to the queries followed by another final retrieval step using the modified query. Document expansion has also been shown to be an effective technique to improve sparse retrieval, which is able to address the vocabulary mismatch problem. The core idea is to create pseudo documents that have expanded terms and use them instead when doing retrieval. Doc2query [41] is an effective approach to document expansion that uses a language model to predict the queries which might be issued to find the document. The predictions of this model are used to create the augmented pseudo documents. Expansion techniques are able to modify non-zero weights by adding terms that did not exist in the query or document. \n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics.",
            "score": 0.531353613509492,
            "section_title": "Dense and Sparse Retrieval",
            "char_start_offset": 8429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1031,
                    "end": 1034,
                    "matchedPaperCorpusId": "3002280"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "208612557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "267200117",
            "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
            "text": "Retrieval. We compare the performance of retrieval with different types of retrievers: \n\n\u2022 BM25 [41] is a type of sparse retriever, which takes into account term frequencies and inverse document frequency (TF-IDF) to score the relevance of documents to a query. \n\n\u2022 RocketQAv2 [40] is a type of dense retriever, which is a unified list-wise training approach for both the retriever and the re-ranker. \n\n\u2022 DPR [22] is a method that leverages dense vector representations for passages in a collection. It uses pre-trained language models to encode passages and queries into dense vectors independently, allowing for a more nuanced understanding of the content. \n\n\u2022 ChatGPT 6 We utilize the same instruction as shown in Figure 3 to prompt gpt-3.5-turbo-1106 to retrieve top-n evidence from corresponding source of knowledge. \n\nNLI(, ), if there is grounding  for  , and planning also uses  \u2032 . 0, if there is grounding  for  , and planning does not use  \u2032 . 0, if there is no grounding  for  , and planning uses  \u2032 . 1, if there is no grounding  for  , and planning does not use  \u2032 . \n\n(11) \n\nWe have chosen Recall@1 as our primary evaluation metric. This selection is motivated by the predominant use case scenario observed in the original datasets, where a single evidence from each knowledge source is typically sufficient.",
            "score": 0.5313179944536575,
            "section_title": "Knowledge",
            "char_start_offset": 29893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1323
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "224803983",
            "title": "Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently",
            "text": "Simple negative sampling methods, such as Rand Neg, NCE Neg, and BM25 Neg, cannot yield consistent or significant improvement compared with traditional retrievers. In general, they perform better on passage retrieval than document retrieval. \n\nMARCO Using BM25 Neg as warmup brings additional improvement. For example, BM25 \u2192 Rand significantly improves Rand Neg's top retrieval accuracy. Particularly, initialized with BM25 Neg, ANCE achieved previous state-of-the-art results. It outperforms all other sparse and dense retrieval baselines. \n\nSeveral results validate our training process investigation. Section 6.1 concludes that Rand Neg teaches the DR model to judge relevance roughly. In Table 1, Rand Neg performs the worst in top retrieval accuracy, but it significantly improves Recall@1k compared with other DR baselines, such as BM25 Neg, NCE Neg, and BM25 \u2192 NCE. Section 6.1 concludes that BM25 Neg is biased and mainly teaches the DR model how to rerank BM25 top candidates. In Table 1, BM25 Neg significantly improves reranking performance but performs the worst in Recall@1k. For example, on MSMARCO Dev passage retrieval, BM25 Neg significantly outperforms Rand Neg in MRR@10 (0.309 vs. 0.261), but underperforms it in Recall@1k (0.935 vs. 0.949).",
            "score": 0.530504564503098,
            "section_title": "Dense Retrieval Baselines",
            "char_start_offset": 25980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "276813494",
            "title": "HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using LLMs",
            "text": "For the evaluation of Information Retrieval, we compare the following baseline models: \n\n-DPR (Karpukhin et al., 2020): Learns dense embeddings for queries and passages using a dualencoder architecture for efficient retrieval. \n\n-SPLADE-v3 (Lassance et al., 2024): Employs sparse lexical representations for expansionbased retrieval, enhancing earlier SPLADE versions (Formal et al., 2021). \n\n-LLM2Vec (BehnamGhader et al., 2024): Creates dense vector representations of text using large language models to facilitate retrieval tasks. \n\nFor training-free models, we utilize the following in our comparison: \n\n-CoT Expansion (Jagerman et al., 2023): Expands queries using CoT prompting and uses both original and expanded queries for retrieval. \n\n-HyDE (Gao et al., 2022a): Generates hypothetical relevant documents using an LLM to enhance retrieval performance. \n\n-LameR (Shen et al., 2023a): Produces hypotheti-cal documents using BM25 initial results to improve retrieval effectiveness.",
            "score": 0.5302300817868434,
            "section_title": "Baseline",
            "char_start_offset": 13578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 988
                }
            ],
            "ref_mentions": [
                {
                    "start": 752,
                    "end": 771,
                    "matchedPaperCorpusId": "1849689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455322265625
        },
        {
            "corpus_id": "237562875",
            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
            "text": "Recent dense passage retrievers outperform traditional sparse retrieval methods like TF-IDF and BM25 (Robertson and Zaragoza, 2009) by a large margin on popular question answering datasets (Lee et al. 2019, Guu et al. 2020, Karpukhin et al. 2020, Xiong et al. 2021. These dense models are trained using supervised datasets and the dense passage retriever (DPR) model (Karpukhin et al., 2020)  In this work, we argue that dense retrieval models are not yet robust enough to replace sparse methods, and investigate some of the key shortcomings dense retrievers still face. We first construct EntityQuestions, an evaluation benchmark of simple, entity-centric questions like \"Where was Arve Furset born?\", and show dense retrieval methods generalize very poorly. As shown in Table 1, a DPR model trained on either a single dataset Natural Questions (NQ) (Kwiatkowski et al., 2019) or a combination of common QA datasets drastically underperforms the sparse BM25 baseline (49.7% vs 71.2% on average), with the gap on some question patterns reaching 60% absolute! Based on these results, we perform a deep dive into why a single dense model performs so poorly on these simple questions. We decouple the two distinct aspects of these questions: the entities and the question pattern, and identify what about these questions gives dense models such a hard time. We discover the dense model is only able to successfully answer questions based on common entities, quickly degrading on rarer entities. We also observe that dense models can generalize to unseen entities only when the question pattern is explicitly observed during training.\n\nWe end with two investigations of practical solutions towards addressing this crucial problem. First, we consider data augmentation and analyze the trade-off between single-and multi-task finetuning. Second, we consider a single fixed passage index and fine-tune specialized question encoders, leading to memory-efficient transfer to new questions.\n\nWe find that data augmentation, while able to close gaps on a single domain, is unable to consistently improve performance on unseen domains. We also find that building a robust passage encoder is crucial in order to successfully adapt to",
            "score": 0.5302222718521342,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "In this study, our model MD2PR utilized the Bert, which was also used in COIL, as the retriever and the DeBERTa [9] as the ranker. The models were trained and validated on both datasets with hyper-parameter tuning, and the results are summarized in Tables 2 and ??. \n\nMD2PR(full) represents the complete version of the MD2PR model, which includes both the sentence-level and word-level distillation as well as the dynamic negative sample filtering method. The baseline model metrics are obtained from the experimental results published in the existing papers. For the data of Natural Questions, the experiments primarily focused on dense retrieval models, so the models used for comparison are also dense retrieval models. \n\nAs can be seen, similar to other dense retrieval models, our MD2PR model proposed in this paper significantly outperforms traditional sparse retrieval models such as BM25 and doc2query, as well as enhanced sparse retrieval models like DocT5Query and DeepCT. Furthermore, compared to the dense retrieval models, MD2PR also demonstrates good performance. On the MS-MARCO dataset, the MD2PR model achieved a 1.4% higher @10 metric compared to the state-of-the-art COIL model, and a 1.1% higher @1000 metric than COIL. On the Natural Questions dataset, MD2PR achieved a 0.8% higher @5 metric than the dense retrieval model RocketQA v2, a 0.5% higher @20 metric than the dense retrieval model CoCondenser, and a 0.6% higher @100 metric than CoCondenser. \n\nThe experiments also indicates that when  is small, the difficulty of retrieving positive samples increases as fewer documents are screened out. This results in a greater differentiation in performance among different models, reflecting their varying capabilities. However, as  increases and more documents are considered as candidates, the probability of positive samples appearing in the top- also increases. In this case, the performance differences between models decrease.",
            "score": 0.5301644772105034,
            "section_title": "Evaluation with Baselines.",
            "char_start_offset": 30281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1953
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62841796875
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models. \n\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios. \n\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25. \n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]. \n\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness. \n\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment. \n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples.",
            "score": 0.5296931925363634,
            "section_title": "C. Retrieval",
            "char_start_offset": 21512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2329
                },
                {
                    "start": 2332,
                    "end": 2361
                },
                {
                    "start": 2362,
                    "end": 2466
                },
                {
                    "start": 2467,
                    "end": 2583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "259224595",
            "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective",
            "text": "Dense retrieval models. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and utilize embedding similarities as estimated relevance scores for effective retrieval [9]. Karpukhin et al. [12] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [32,35], late interaction [14]. Recently, researchers have also investigated pre-training tasks for dense retrieval [8,19]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually need considerable memory consumption and computational overhead. Generative retrieval models. Generative retrieval has recently garnered increasing interest [1,4,20,30], which retrieves documents by directly generating their identifiers based on the given query. It offers an end-to-end solution for document retrieval tasks [3,20] and allows for better exploitation of the capabilities of large generative language models. For example, De Cao et al. [7] proposed an autoregressive entity retrieval model and Tay et al. [28] introduced a differentiable search index (DSI) and represent documents as atomic ids, naive string, or semantic strings. Chen et al. [5] proposed a pre-trained generative retrieval model called Corpus-Brain to encode all information of the corpus within its parameters in a general way. However, the robustness of generative retrieval models has been overlooked by the community. Out-of-distribution in IR. Current studies on OOD robustness in IR have their own limitations. For example, Wu et al. [31] only explored the OOD generalization performance of neural ranking models. Some works have been devoted to alleviating the poor performance of dense retrieval in the scenarios of query variants [6,23,27,36] or zero/few-shot of corpus [17,29,34]. In this work, we focus on the OOD generalizability of generative retrieval models.",
            "score": 0.5296418547312608,
            "section_title": "RELATED WORK",
            "char_start_offset": 3158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 682,
                    "end": 685,
                    "matchedPaperCorpusId": "251718954"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1009,
                    "end": 1011,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "258418300"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1507,
                    "end": 1510,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "236976200"
                },
                {
                    "start": 2071,
                    "end": 2074,
                    "matchedPaperCorpusId": "250638661"
                },
                {
                    "start": 2074,
                    "end": 2077,
                    "matchedPaperCorpusId": "244709375"
                },
                {
                    "start": 2077,
                    "end": 2080,
                    "matchedPaperCorpusId": "248524694"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "237347134"
                },
                {
                    "start": 2115,
                    "end": 2118,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "We compare the selected retrieval models on the KILT benchmark. From Table 3, we can observe that the generative IR models significantly outperform sparse and dense retrieval models like BM25 and DPR across all the datasets, indicating that combining the retrieval components into a unified model benefits effective corpus indexing. CorpusBrain consistently outperforms BART on all five tasks, demonstrating that the adequately well-designed pre-training tasks for generative retrieval contribute to improving document understanding for generative IR models.",
            "score": 0.5292525197018807,
            "section_title": "Overall IID results",
            "char_start_offset": 16234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 558
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38525390625
        },
        {
            "corpus_id": "268032853",
            "title": "CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks",
            "text": "CorpusBrain has been shown to outperform traditional IR methods such as BM25 and DPR as well as generative IR methods such as GENRE and SEAL in the non-incremental retrieval scenario [8]. \n\nTo streamline our work, we do not repeat the experiments and refer, instead, to [8] for more empirical details. In this work, we solely evaluate the retrieval performance of distinct models in the incremental scenario. In the following, we first analyze the incremental effectiveness of distinct models on eleven individual downstream datasets. Additionally, we analyze the incremental effectiveness of distinct models on five individual downstream tasks, each of which contains one to four datasets. Finally, we analyze the overall incremental effectiveness of distinct models on all downstream datasets. 6.1.1 Incremental performance of individual downstream datasets. Table 3 provides an overview of the incremental retrieval performance of different models on specific KILT datasets. When we look at the retrieval results presented in Table 3, we observe the following: \n\n(1) DPR exhibits better performance than BM25 in the majority of downstream datasets, and the underlying reason may be that the supervised DPR method learns more semantic characteristics of downstream datasets than BM25, rendering it more adaptable in the face of newly arrived documents. However, the comparison results on the T-REx and zsRE datasets are opposite, which may be due to the significant difference between the input query format of these two datasets (phrases) and the query format learned by DPR (sentences). (2) Generative retrieval baselines, i.e., DSI++ and CLEVER, exhibit retrieval ability to some degree in non-incremental scenarios, especially on the AY2 and NQ datasets. Nevertheless, this retrieval ability fails to be effectively sustained in incremental scenarios. This may be attributed to the exclusive focus of these models on homogeneous downstream queries during continual learning, while the dynamic retrieval scenario for KILTs is dominated by heterogeneous downstream queries. The failure of previous generative retrieval models highlights that it is a non-trivial challenge to continually model the relevant relationship between heterogeneous downstream queries and the corresponding documents.",
            "score": 0.529058357884699,
            "section_title": "Baseline comparison",
            "char_start_offset": 48826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2296
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453369140625
        },
        {
            "corpus_id": "258840876",
            "title": "On the Risk of Misinformation Pollution with Large Language Models",
            "text": "We conduct experiments on four distinctive types of retrieve-and-read ODQA systems, classified based on the choice of the retrievers and the readers. \n\nRetrievers For retrievers, we use BM25 (Robertson and Zaragoza, 2009) and Dense Passage Retriever (DPR) (Karpukhin et al., 2020), representing sparse and dense retrieval mechanisms respectively, which are the mainstream of the current ODQA models. BM25 is a traditional probabilistic model for information retrieval that remains a robust baseline in retrieval tasks. Although sparse retrievers may fall short in capturing complex semantics, they excel at handling simple queries, thus forming the backbone of several contemporary retrieval systems (Formal et al., 2021). Conversely, DPR leverage learned embeddings to discern implicit semantics within sentences, outpacing sparse retrievers in most retrieval tasks. \n\nReaders For readers, we use Fusion-in-Decoder (FiD) (Izacard and Grave, 2021) and GPT-3.5 (text-davinci-003). FiD is a T5-based (Raffel et al., 2020) reader, which features utilizing multiple passages at once to predict answers compared to concurrent models, yielding outstanding performance. Considering that answering questions with conflicting information might diverge from the training objectives of current MRC models, we also experimented with GPT-3.5 as a reader to leverage its extensive training set and flexibility. Additional model configurations are in Appendix A.",
            "score": 0.5286435411934676,
            "section_title": "ODQA Systems",
            "char_start_offset": 15127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1447
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 221,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 256,
                    "end": 280,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "264110387",
            "title": "Hybrid Retrievers with Generative Re-Rankers",
            "text": "Traditionally, retrieval has been dominated by lexical approaches like TF-IDF + cosine similarity, and BM25. \n\nBM25 is a BOW retrieval function that ranks sets of documents based on the query terms that appear in them, regardless of their proximity [1]. BM25 is a retrieval model based on the probabilistic retrieval framework. BM25 often achieves better performance compared to TF-IDF, which rewards term frequency and penalizes document frequency. BM25 goes beyond this to consider document length and term frequency saturation. \n\nThe results [2] demonstrate that BM25 is a robust baseline. However, these approaches suffer from a lexical gap and are able to retrieve only documents that contain keywords present within the query. In addition, lexical approaches treat queries and documents as BOW by not considering word order. \n\nTo overcome this lexical gap, techniques to improve lexical retrieval systems using neural networks have been proposed. Sparse methods, such as docT5query [3], identified document expansion methods that use sequence-to-sequence models that generate possible queries for which a given document would be relevant. At base, it involves training a model that predicts questions for which the input document might contain answers. The generated questions are then appended to the original documents, which are indexed. The docT5query model takes its name from the generative model T5. The primary advantage of this approach is that expensive neural inference is pushed to indexing time.",
            "score": 0.5276913747097358,
            "section_title": "A. Sparse retrieval",
            "char_start_offset": 3896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 545,
                    "end": 548,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "208612557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "270710900",
            "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs",
            "text": "To Answer RQ1, we evaluate lexical, late-interaction, and diverse dense-retrieval models on the curated datasets in an open-domain setup.From Table 2, we observe that the lexical model BM25 is a strong baseline across all complex QA tasks, including reasoning over hybrid sources.We observe that DPR has the lowest performance, indicating that while this model is suited for simpler open-domain QA tasks, it falls short on complex QA tasks.We observe that though DPR was trained on Natural Questions it falls short on AmbigQA which is a derivative of natural questions.We posit that this is because the bi-encoder model was not trained to handle ambiguity, indicating that complex QA requires further care compared to traditional questions.They fall short on tasks that require multistep reasoning during retrieval or when retrieving hybrid evidence like tables and text from multiple sources.Surprisingly, we also observe that models like contriever that are pre-trained for retrieval underperform compared to lexical models like BM25 and other dense retrieval models like tas-b and MPNet.We also observe that neural sparse models like SPLADE are competitive or even outperform dense retrieval models and prove to be strong baselines following BM25.\n\nLate-Interaction model ColBERT (v2) [43] outperforms other methods on 5/7 datasets.We posit that cross-attention-based operation on tokenwise representations helps capture intricate relationships between query and context and also between different modalities of evidence such as table and text.This aids in higher retrieval performance on datasets like StrategyQA that require implicit reasoning and also on all table and text-based QA datasets compared to other dense retrieval models.We also find similar observations in individual works like [65] which employs cross-attention fusion between table and text evidence and [59] which employs cross-attention for multi-hop reasoning.The model used is gpt-3.5-turbo,unless otherwise specified.Insight 1: Pre-trained dense retrieval models fall short on retrieval performance for compositional questions and when retrieving from hybrid knowledge sources.In contrast, lexical models serve as strong baselines and late-interaction models demonstrate significant potential.",
            "score": 0.52757888672075,
            "section_title": "Comparison of Retrieval Models on Diverse Complex QA Tasks",
            "char_start_offset": 19839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1250
                },
                {
                    "start": 1252,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1547
                },
                {
                    "start": 1547,
                    "end": 1739
                },
                {
                    "start": 1739,
                    "end": 1935
                },
                {
                    "start": 1935,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2154
                },
                {
                    "start": 2154,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "244799249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "In this section ,we evaluate a Retrieval-Augmented Generation (RAG) setting, where retrieval-based models (BM25, MSS-DPR) are combined with generative models (e.g., InstructGPT, GenRead). The  In both subplots, language model perplexity is evaluated under several context strategies: 'No Context' (baseline), 'R' (Retrieval-only using BM25 from Wikipedia), 'G' (Generation-only, context from a generated document), 'R+G' (Retrieval followed by Generation), and 'G+R' (Generation followed by Retrieval). \n\nretrieved documents serve as input to a large language model, which generates the final response. Similar to prior works (Karpukhin et al., 2020b;Izacard and Grave, 2020), our setup involves retrieval (R), generation (G), and hybrid models (R+G, G+R). Furthermore, we implement reranking techniques such as UPR and RankGPT to refine the document selection process. Our reader model (LLama-3.3 8B) gets the question along with its corresponding retrieved documents and returns the answer. Reader models are simply a frozen large LM (not pre-trained, fine-tuned). Table 3, shows that hybrid models (R+G, G+R) when combined with reranking approaches such as RankGPT offer a more balanced approach across various datasets. For example, BM25 R+G achieves a score of 25.51 on NQ, in comparison with both retrieval-only (14.90) and generation-only (24.68) models in the LLama V3.3 8B setup. In particular, RankGPT consistently enhances performance, with BM25+RankGPT achieving 28.45 on NQ and 19.73 on WebQA, which highlights the effectiveness of reranking in refining document selection for question answering.",
            "score": 0.5274924125546316,
            "section_title": "In-Context for Open-Domain QA",
            "char_start_offset": 21405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 626,
                    "end": 651,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "277502276",
            "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
            "text": "The fundamental principle of dense retrieval is to transform documents and queries into vector representations. The introduction of pre-trained language models, particularly BERT [8], has revolutionized information retrieval, paving the way for dense retrieval methods such as Dense Passage Retrieval (DPR) [7], ColBERT [9], and GTR [13]. Techniques like SimCSE [5] leverage contrastive learning with models such as BERT and Roberta to optimize embeddings. Additionally, dense retrieval methods often employ Approximate Nearest Neighbor (ANN) search [6,25], Maximum Inner Product Search (MIPS) algorithms [21], and SimLM [24] to ensure efficient retrieval in sub-linear time. \n\nUnlike traditional sparse retrieval, these methods utilize transformer encoders to create dense vector representations for queries and documents, enhancing semantic understanding and retrieval accuracy. This combination of semantic depth and computational efficiency positions dense retrieval as a leading approach in modern information retrieval.",
            "score": 0.5269587901121893,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 307,
                    "end": 310,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "272146307",
            "title": "Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models",
            "text": "Recently, there has been growing interest in using dense retrieval methods that leverage dense vector representations generated by DNNs for image retrieval tasks [13,50,59,61]. However, sparse vector representations, typically in the form of lexical retrievers, have also been explored due to their enhanced interpretability and analytical capabilities [6,31,33,66]. To address the perceived trade-off between accuracy and interpretability, LexLIP [31] introduces a lexicon-weighting paradigm to significantly reduce retrieval latency while maintaining high performance with bag-of-words models. Similarly, STAIR [6] maps images and texts to a sparse token space to construct sparse text and image representations for improved retrieval accuracy. These studies demonstrate the potential of sparse retrievers to outperform dense retrievers. Our approach leverages a multi-modal language model to extract image features into textual data. We then utilize vectorization and retrieval algorithms in NLP tasks, such as BM25 [43], TF-IDF [44], and word2vec [32], for image retrieval tasks. Among these techniques, BM25 is considered an efficient sparse retrieval algorithm and is frequently used for benchmark evaluations in information retrieval tasks [31,49,66]. BM25 demonstrates better out-of-distribution generalization capabilities compared to dense retrievers [52], and outperforms them in retrieving named entities or words that were not seen during training [46]. Based on such potential capabilities, we employ the efficient and standard retrieval algorithm, BM25, for sparse lexical vectors directly converted from textual data. Our approach does not rely on dense latent representations extracted by DNNs, and eliminates the need for specialized vector space adaptation, enabling the application of key expansion techniques for enhanced performance. By transforming the image retrieval task into a sparse lexical retrieval task, we can rethink image retrieval from an NLP perspective.",
            "score": 0.5265610945845198,
            "section_title": "Sparse Retrieval",
            "char_start_offset": 9032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "49434640"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1361,
                    "end": 1365,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "237562875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81298828125
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Advancements in Open-Domain Question Answering (ODQA), Document Reranking, and Language Modeling (LM) have led to three main approaches: retriever-based, generator-based, and hybrid models, along with retrieval-augmented techniques for improving factual consistency in text generation. \n\nRetriever-based methods focus on identifying relevant documents before processing. Sparse models like BM25 (Robertson and Zaragoza, 2009) rely on lexical matching, while dense retrieval methods such as DPR (Karpukhin et al., 2020a) and Contriever (Izacard et al., 2021) encode queries and documents into dense vectors for improved retrieval. Further refinements include ANCE (Xiong et al., 2020), ColBERT (Khattab et al., 2021), andMSS-DPR (Sachan et al., 2021). Some models bypass full-document retrieval, using dense phrase retrieval (Lee et al., 2020;Seo et al., 2019) to extract answer spans directly. \n\nGenerator-based models generate responses rather than retrieving documents. LLMs like GPT-3.5 and InstructGPT (Brown et al., 2020;Ouyang et al., 2022) perform well in open-ended QA but struggle with factual accuracy, often hallucinating information (Huang et al., 2023). Models like GenRead (Yu et al., 2022) and DocGen (Askari et al., 2023) generate contextual documents before extracting answers. Pretrained models like T5-RC (Raffel et al., 2020) and BART-based retrieverreader architectures (Lewis et al., 2020) attempt to improve reliability, but consistency issues persist. \n\nHybrid approaches integrate retrieval and generation, balancing factual grounding with contextual flexibility. RAG (Lewis et al., 2020) conditions generation on retrieved documents, while FiD (Izacard and Grave, 2020) enhances multidocument conditioning. Merging Generator and Retriever (MGR) (Abdallah and Jatowt, 2023;Zhang et al., 2023) dynamically selects between retrieved and generated content.",
            "score": 0.5265081867040533,
            "section_title": "Related Work",
            "char_start_offset": 4492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 425,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 494,
                    "end": 519,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 535,
                    "end": 557,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1026,
                    "end": 1046,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1216,
                    "end": 1237,
                    "matchedPaperCorpusId": "266164016"
                },
                {
                    "start": 1324,
                    "end": 1345,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1391,
                    "end": 1411,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1593,
                    "end": 1613,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "259949923",
            "title": "SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval",
            "text": "Information Retrieval is a core task for many web search and NLP applications. Traditionally retrieval systems involved sparse bag-ofwords (BoW) models such as BM25 [38] or TF-IDF relied upon the exact lexical match and frequency-based term importance matching between the query and every document in a collection. However, these traditional lexical-based retrieval systems face challenges in vocabulary-mismatch [2] with searching semantically relevant documents. Recently, deep learning and in particular pre-trained Transformer models like BERT [7] have become popular in information retrieval [47]. Dense retrieval [16,19] method involves retrieving documents using semantic embedding similarities. Dense retrieval has been successfully applied across many retrieval-based tasks such as web search, question-answering [12,16,21,25,36,44]. However, dense retrieval systems are inefficient. For large-sized corpora, dense models produce bulky indexes and require approximate nearest neighbor (ANN) solutions such as faiss [15] for quick inference at scale. They are also computationally expensive to serve practically, as the index needs to be loaded completely within the RAM amounting to high hardware costs. On the other hand, there has been a growing interest in learning sparse representations for both queries and documents in information retrieval [48]. Neural sparse retrieval systems either assign scalar weights to word pieces or pool token weights on BERT's [7] vocabulary space. Similarity scores between query and document are computed either as exact token overlap or using max-pooling. Many recent works consider the sparse retrieval setup suitable for retrieval-based tasks [1, 8-10, 20, 22, 29, 33, 49-51]. Sparse retrievers have several noted advantages over their dense counterparts as summarized in Table 1. Overall, sparse retrievers have two major benefits: first, they are cost-effective requiring only CPU for inference. Second, lexical matching provides a boost in their effectiveness in contrast to dense retrievers. Despite previous work, there have been limited software tools that provide an easy out-of-box evaluation of different neural sparse retrievers. A unified toolkit would enable a fair comparison and better adoption of different sparse retrieval systems by practitioners.",
            "score": 0.5262690825089801,
            "section_title": "INTRODUCTION",
            "char_start_offset": 285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "6749682"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 822,
                    "end": 826,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 826,
                    "end": 829,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 832,
                    "end": 835,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1471,
                    "end": 1474,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "264425948",
            "title": "MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin",
            "text": "Single Modality Retrieval. In our experiments, we represent image documents using captions and use several text retrieval models as baselines. BM25 (Robertson et al., 2009) is widely used in text retrieval work, which conducts exact matches between queries and documents. DPR (Karpukhin et al., 2020) is trained using NQ dataset (Kwiatkowski et al., 2019) and uses a dual-encoder to encode queries and documents as dense vectors for retrieval. We start from vanilla BERT (Devlin et al., 2019) and DPR (Karpukhin et al., 2020) checkpoints and train the encoders using in-batch negatives to conduct BERT-DPR and NQ-DPR models. NQ-ANCE is also compared, which continuously trains NQ-DPR using hard negatives (Xiong et al., 2021a). Besides, T5-ANCE (Yu et al., 2023) and Anchor-DR (Xie et al., 2023) are dense retrieval models that are trained on MS MARCO and ClueWeb22, respectively. \n\nDivide-and-Conquer. The divide-and-conquer models retrieve image documents and text documents individually and then fuse the retrieval re-sults. Following previous work (Liu et al., 2023b), we use single modality retrievers, VinVL-DPR, CLIP-DPR and BM25, and fuse the retrieval results according to their unimodal rank reciprocals. \n\nUniversal Dense Retrieval. CLIP-DPR and VinVL-DPR employ the visual language models CLIP (Radford et al., 2021) and VinVL (Zhang et al., 2021) as image and text encoders and then are trained with in-batch negatives. UniVL-DR (Liu et al., 2023b) is our main baseline model, which further uses modality-balanced hard negative to train text and image encoders and also utilizes the image verbalization method to bridge the modality gap between images and texts. \n\nImplementation Details.",
            "score": 0.5256358021632264,
            "section_title": "Experimental Methodology",
            "char_start_offset": 14875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 300,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 329,
                    "end": 355,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 471,
                    "end": 492,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 501,
                    "end": 525,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 705,
                    "end": 726,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "259949845"
                },
                {
                    "start": 777,
                    "end": 795,
                    "matchedPaperCorpusId": "258588167"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "256615676"
                },
                {
                    "start": 1306,
                    "end": 1328,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1339,
                    "end": 1359,
                    "matchedPaperCorpusId": "235692795"
                },
                {
                    "start": 1442,
                    "end": 1461,
                    "matchedPaperCorpusId": "256615676"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341796875
        },
        {
            "corpus_id": "274822530",
            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
            "text": "Specifically, we explore the limitations of sparse retrieval, dense retrieval, and generative retrieval on Japanese datasets. \n\nSparse retrieval, which relies on term-matching methods like TF-IDF (Salton and Buckley, 1988), BM25 (Robertson et al., 2004), BM25+ (Robertson and Walker, 1994), struggles with Japanese due to its complex morphology, flexible syntax, and the nuances of word segmentation, which complicate accurate term matching. \n\nDense retrieval, which relies on embeddings for semantic similarity, has potential in Japanese but requires large, specialized datasets, a resource often lacking. The integration of large language models (LLMs) has propelled dense retrieval by improving both in-domain accuracy and cross-domain generalization, as well as enabling new capabilities like instruction-following and in-context learning. Early advances with models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) laid the groundwork, but recent LLMs such as LLaMA (Touvron et al., 2023a,b) and RepLLaMA (Ma et al., 2024) have set new benchmarks, outperforming traditional multi-stage methods on tasks like MS-MARCO (Bajaj et al., 2016) and BEIR (Thakur et al., 2021). Efforts such as Llama2Vec (Li et al., 2023) and NV-Embed (Lee et al., 2024) have further optimized these models, enhancing adaptability and performance. LLM-based dense retrieval models now offer advanced functionalities, handling diverse queries with better adaptability, flexibility, and efficiency than earlier methods. The overview of the multi-stage retrieval applies a two-phase approach for optimal performance. During the Preprocessing stage, unnecessary elements such as special characters, undesired symbols, and stop words are filtered out, ensuring the input text is cleaner and more accurate before passing through BM25+. This process allows for the retrieval of top-a 1 relevant documents related to the query.",
            "score": 0.5256212309669253,
            "section_title": "Introduction",
            "char_start_offset": 1993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 253,
                    "matchedPaperCorpusId": "16628332"
                },
                {
                    "start": 261,
                    "end": 289,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 910,
                    "end": 931,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1022,
                    "end": 1039,
                    "matchedPaperCorpusId": "263908865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "259145189",
            "title": "Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard",
            "text": "This work provides reproducible reference implementations of five different retrieval models for BEIR. These comprise a \"bag-of-words\" BM25 baseline, two learned dense retrieval models (TAS-B and Contriever), and two learned sparse retrieval models (uniCOIL without expansion and SPLADE). In this section, we provide an overview of these models as they are presented in the literature, but explore different model variants in Section 7.",
            "score": 0.5254449022634485,
            "section_title": "RETRIEVAL MODELS",
            "char_start_offset": 9646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "272827380",
            "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation",
            "text": "In naive RAG, retrieval is achieved by calculating the similarity (e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation capability of embedding models plays a key role. This mainly includes a sparse encoder (BM25) and a dense retriever (BERT architecture pre-training language models). Sparse and dense embedding approaches capture different relevance features and can benefit from each other by leveraging complementary relevance information. \n\nThe existing method of combining the two is generally to combine the scores of the two search methods in an ensemble, but in practice different search methods may be suitable for different questions, especially in questions requiring multi-hop reasoning. When query involves proper nouns, people, places, times, numbers, and coordinates, the representation ability of the pre-trained presentation model is limited, and more accurate text indexes are needed. For queries that are closer to the expression of a paragraph of text, such as scenes, behaviors, and abstract concepts, the two may be coupled in some questions. \n\nIn the design of logical form, it is feasible to effectively combine two retrieval methods. When keyword information is needed as explicit filtering criteria, conditions for selection can be specified within the retrieval function to achieve structured retrieval. \n\nFor example, for the query \"What documents are required to apply for a disability certificate at West Lake, Hangzhou?\", the retrieval function could be represented as: \"Retrieval(s=s1:Event[applying for a disability certificate], p=p1:support_chunks, o=o1:Chunk, s.location=West Lake, Hangzhou)\". This approach leverages the establishment of different indices (sparse or dense) to facilitate precise searches or fuzzy searches as needed. \n\nFurthermore, when structured knowledge in the form of SPO cannot be retrieved using logical functions, alternative approaches can be employed. These include semi-structured retrieval, which involves using logical functions to search through chunks of information, and unstructured retrieval. The latter encompasses methods such as Retrieval-Augmented Generation (RAG), where sub-problems expressed in natural language are used to retrieve relevant chunks of text. This highlights the adaptability of the system to leverage different retrieval strategies based on the availability and nature of the information.",
            "score": 0.5246610372450495,
            "section_title": "Logical Form for Retrieval",
            "char_start_offset": 28609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2302
                },
                {
                    "start": 2303,
                    "end": 2449
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "232147859",
            "title": "Semantic Models for the First-Stage Retrieval: A Comprehensive Review",
            "text": "Later, Lee et al. [122] proposed to learn contextual sparse representation for each phrase based on BERT to replace term-frequency-based sparse encodings in DenSPI [187]. This method leverages rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space, improving the quality of each phrase embedding by augmenting it with a contextualized sparse representation. Experimental results show that the OpenQA model that augments DenSPI with learned contextual sparse representations outperforms previous OpenQA models, including recent BERT-based pipeline models, with two orders of magnitude faster inference time. Luan et al. [137] proposed to linearly combine the term-based system (BM25-uni) and neural-based system (dual-encoder or multi-vector model) scores using a single trainable weight , tuned on a development set, which yields strong performance while maintaining the scalability. Gao et al. [78] proposed the CLEAR model, which uses a BERT-based embedding model to complement the term-based model (BM25). Experimental results show that retrieval from CLEAR without re-ranking is already almost as accurate as the BERT re-ranking pipeline. Similarly, Kuzi et al. [115] proposed a general hybrid approach for document retrieval that leverages both a semantic model (BERT) and a lexical retrieval model (BM25). An in-depth empirical analysis is performed, which demonstrates the effectiveness of the hybrid approach and also sheds some light on the complementary nature of the lexical and semantic models.",
            "score": 0.5246386167587093,
            "section_title": "ACM",
            "char_start_offset": 86654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1538
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 23,
                    "matchedPaperCorpusId": "219058995"
                },
                {
                    "start": 164,
                    "end": 169,
                    "matchedPaperCorpusId": "189762341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455810546875
        },
        {
            "corpus_id": "277620903",
            "title": "Causal Retrieval with Semantic Consideration",
            "text": "Traditional keyword-based information retrieval methods like BM25 (Robertson and Zaragoza, 2009) rely heavily on lexical overlap between queries and documents, which limits their ability to capture deeper semantic relationships. Karpukhin et al. (2020) address this limitation by proposing DPR that leverages a language model to convert queries and documents into dense vector representations, allowing them to incorporate semantic information beyond simple keyword matching. \n\nRecent studies on dense retrievers show that leveraging LLMs can improve retrieval accuracy. Lee et al. (2024) show the potential of distilling knowledge from LLMs to create compact yet versatile text embedding models. Luo et al. (2024) demonstrates that LLM-based dense retriever significantly outperforms traditional models through comprehensive experiments. Erker et al. (2024) introduce a Triple-Encoders to compute distributed utterance. The method encodes each sentence independently, and creates contextualized embeddings by linearly combining representations from multiple subspaces. Similarly, our method also uses three encoders, each specializing in capturing different aspects of causal relationships between sentences.",
            "score": 0.5241782749732221,
            "section_title": "Information Retrieval",
            "char_start_offset": 3464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 96,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 229,
                    "end": 252,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 839,
                    "end": 858,
                    "matchedPaperCorpusId": "267750126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "273233846",
            "title": "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities",
            "text": "Neural IR methods typically operate in two stages. Initially, a set of candidate documents is retrieved using a fast, computationally-efficient first-stage retrieval method that considers sparse or dense vector representations. These candidates are then re-ranked using more computationally-intensive scoring functions, such as those involving crossencoders (Nogueira and Cho, 2019;MacAvaney et al., 2019;Nogueira et al., 2020;Sun et al., 2023). \n\nLearned Sparse Retrieval (LSR) (Nguyen et al., 2023b;Formal et al., 2021Formal et al., , 2022) is a prominent neural method for first-stage retrieval. LSR encodes queries and documents into sparse, lexicallyaligned representations that can be stored in an inverted index for fast retrieval. LSR offers several advantages over Dense Retrieval (DR), another common approach for first-stage retrieval (Lin et al., 2020). LSR's lexically grounded representations are more transparent, making it easier for users to understand the model and inspect representations for biases (Abolghasemi et al., 2024). Furthermore, LSR's compatibility with an inverted index enables efficient and exact retrieval (Ding and Suel, 2011), while also simplifying the transition from existing lexical search infrastructure supporting methods like BM25. LSR not only performs competitively with DR in terms of performance within the same domain, but it also tends to generalize better across different domains and tasks (Formal et al., 2021). \n\nHowever, LSR models lack explicit representations for entities and concepts in their vocabulary. This can pose challenges due to the tokenization process, where words are segmented into subwords or wordpieces. For instance, a word like \"BioN-Tech\" might be tokenized into [bio, ##nte, ##ch]. Such fragmentation can lead to ambiguity, complicating the retrieval process by obscuring the full meaning and context of the original word, which in turn may affect the accuracy and relevance of search results.",
            "score": 0.5241234593995397,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 405,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 405,
                    "end": 427,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 427,
                    "end": 444,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 479,
                    "end": 501,
                    "matchedPaperCorpusId": "257585074"
                },
                {
                    "start": 501,
                    "end": 520,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 520,
                    "end": 541,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 1141,
                    "end": 1162,
                    "matchedPaperCorpusId": "216307"
                },
                {
                    "start": 1442,
                    "end": 1463,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "236956636",
            "title": "DoSSIER@COLIEE 2021: Leveraging dense retrieval and summarization-based re-ranking for case law retrieval",
            "text": "3.1.1 First stage retrieval. For the lexical retrieval in the first stage retrieval we use BM25 [17]. When querying the index with query , the BM25 model assigns each document  in the index a ranking score  25 (, ), the higher the score, the higher the relevance of the query and the indexed document. For semantic retrieval, we use a dense passage retrieval model [9] based on two Siamese BERT Encoders, one encodes the query passage, the other one the candidate passage. The encoder encodes the query and candidate passage into a vector and the dot-product between those two vectors denotes the relevance score   (, ) between the query  and the candidate passage . This dense retrieval model (lawDPR) is trained on the entailing query-paragraph pairs of Task 2, in order to align the vector representations of entailing paragraphs. As the maximum input length for the dense passage retrieval model [9] is limited, we employ this model on the full text of the case by splitting up the whole document  into its paragraphs  1 , ..,    and indexing each paragraph separately. When querying the paragraphlevel index, we also split up the query case  into its paragraphs  1 , ..,    and retrieve for each query paragraph   with  \u2208 [  ] the ranked list of paragraphs  1 , ..,    . The paragraph in the ranked lists   with  \u2208 [  ] are reduced to the paragraphs documents and  then the ranked lists of documents are aggregated to one ranked list of documents for the whole query case. This aggregation will be further investigated in the next section. Equivalently we also use this approach for retrieval with BM25 on the paragraph-level and refer to this approach to paragraph-level retrieval. \n\nIn order to analyze the impact of the paragraph-level retrieval compared to document-level retrieval on the retrieval effectiveness, we also conduct document-level retrieval with BM25 and lawDPR. \n\nHere all documents in the corpus are indexed based on their whole text and the query case is also encoded with all its text. As the dense passage retrieval model has a limited input length and therefore can only retrieve passages up to 512 tokens, the document-level retrieval for lawDPR is the retrieval based on the first 512 tokens of the query case and the candidate case.",
            "score": 0.5241036960142411,
            "section_title": "Task 1",
            "char_start_offset": 6525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 365,
                    "end": 368,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495849609375
        },
        {
            "corpus_id": "269950696",
            "title": "Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data",
            "text": "In the field of natural language processing, ranked information retrieval (IR), refers to retrieving information ordered by relevance from a large collection, in response to a query.Ranked IR remains important even with the emergence of advanced large language models (LLMs) as a means of greatly enriching their outputs.\n\nExisting retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson and Walker, 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large.\n\nIn an effort to balance the quality-cost trade-off, a two-stage pipeline is proposed where a quicker retriever first retrieves a smaller set of candidates and then a dense retriever re-ranks them in a second stage.Unfortunately, this approach suffers from two major problems.First, any semantically relevant information pruned due to lack of exact word matches in the first stage is not considered for further ranking.Second, the neural ranker in the last stage lacks interpretability because, for scoring, it uses the inner product of the latent representation of the text which is difficult to explain in human understandable terms.Recently proposed transformer (Vaswani et al., 2017) encoders have the potential to tackle these issues.By utilizing a pre-trained masked language model (MLM), SparTerm (Bai et al., 2020) and SPLADE (Formal et al., 2021) progressively improved the use of expansion-aware sparse lexical representation learners in mitigating vocabulary mismatch problems, while enhancing interpretability.",
            "score": 0.5238959772029215,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 321
                },
                {
                    "start": 323,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1158
                },
                {
                    "start": 1160,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1435
                },
                {
                    "start": 1435,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1898
                },
                {
                    "start": 1898,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 601,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 965,
                    "end": 986,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1824,
                    "end": 1846,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "268532051",
            "title": "Listwise Generative Retrieval Models via a Sequential Learning Process",
            "text": "In this section, we review related work, including the traditional document retrieval, pre-trained language models, and generative retrieval.\n\n6.1 Traditional document retrieval Document retrieval has traditionally followed an \"index-retrieve\" paradigm, where documents are indexed and then retrieved based on a query.This paradigm has resulted in two main approaches to document retrieval, namely sparse retrieval and dense retrieval.\n\n6.1.1Sparse retrieval.Sparse retrieval methods represent queries and documents using sparse vectors.These methods rely on exact matching to compute similarity scores between queries and documents.In sparse retrieval, the focus is on identifying the presence or absence of specific query terms within documents.Two typical methods in this category are BM25 [79] and the query likelihood model [50].BM25 takes into account factors such as document length, term frequency, and inverse document frequency to rank documents based on the occurrence of query terms within each document.The query likelihood model [50], on the other hand, leverages a generative model and estimates the probability of generating the query terms given a document.Documents are then ranked based on their likelihood of generating the query.However, these approaches solely consider statistical information and do not incorporate semantic information.To overcome this limitation, several studies [5-7, 20, 28, 104] have utilized word embeddings to reweight the importance of terms.For example, HDCT [21] focuses on long documents.It first utilizes BERT to generate contextual term representations, which are then used to estimate passage-level term weights.Subsequently, these passage-level term weights are aggregated using a weighted sum to obtain document-level term weights.DeepTR [104] constructs a feature vector for query terms and employs a regression model to map these feature vectors to the ground truth weights of terms.\n\nLimitations.Sparse retrieval methods offer computational efficiency due to their reliance on exact matching.They are particularly useful in large-scale retrieval scenarios where the number of documents is substantial.However, these methods often lack the ability to capture semantic relationships and contextual information between query terms and documents, which can limit their retrieval performance.\n\n6.1.2Dense retrieval.",
            "score": 0.5238253167935784,
            "section_title": "RELATED WORK",
            "char_start_offset": 47691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 143,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 747
                },
                {
                    "start": 747,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1539
                },
                {
                    "start": 1539,
                    "end": 1666
                },
                {
                    "start": 1666,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1941
                },
                {
                    "start": 1943,
                    "end": 1955
                },
                {
                    "start": 1955,
                    "end": 2051
                },
                {
                    "start": 2051,
                    "end": 2160
                },
                {
                    "start": 2160,
                    "end": 2346
                },
                {
                    "start": 2348,
                    "end": 2353
                },
                {
                    "start": 2353,
                    "end": 2369
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "14116318"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "14116318"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1794,
                    "end": 1799,
                    "matchedPaperCorpusId": "155300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "274822530",
            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
            "text": "In-Domain Evaluation Table 5 highlights the effectiveness of our proposed retrieval approach, which consistently outperforms existing methods across the training split of the MS MARCO passage dataset. \n\nSparse retrieval techniques like TF-IDF and BM25+ struggle significantly, achieving low recall and ranking scores due to their inability to capture semantic nuances. In comparison, our method demonstrates a clear advantage, particularly at lower recall levels. For example, while BM25+ achieves Recall@10 of 49.96, our model achieves 80.71, marking a substantial improvement. Dense retrieval models, such as Siamese Table 5: Illustrating the efficiency of our proposed approach for utilizing LLMs in retrieval systems, we employed QLoRA on the training split of the MS-MARCO passage dataset and evaluated its performance relative to current techniques.",
            "score": 0.5237557343244488,
            "section_title": "Benchmark Datasets Retrieval",
            "char_start_offset": 59205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 855
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "269009872",
            "title": "LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding",
            "text": "Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches. This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.",
            "score": 0.5237533080879075,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "255595937",
            "title": "Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering",
            "text": "Dense Retrieval. Dense Retrieval is a rapidly evolving field, surveyed in [36,11], with new pre-training tasks, optimizing methods, and variants of the Transformer architecture emerging [47,23,15,14]. [31] were the first to outperform sparse bag-of-words representations such as BM25 with dense representations for QA. Their approach relies on three components: (i) pre-trained language models such as BERT [10], which allow to encode the semantic of a sentence in a dense vector; (ii) a contrastive learning objective that optimizes the similarities between questions' and text passages' embeddings (see Section 3); (iii) an unsupervised training task, ICT (see Section 1). [27] criticize the latter for being computationally intensive3 and argue that regular sentences are not good surrogates of questions. Instead, they propose DPR, which takes advantage of (i) the heuristic of whether the passage contains the answer to the question to deem it relevant; (ii) unsupervised IR methods such as BM25 to mine hard negatives examples, which proved to be the key of their method's success. We aim at taking advantage of both approaches by (i) pre-training our model on text QA datasets like DPR; (ii) incorporating multimodality into this hopefully-well-initialized model by adapting the ICT of [31] to multimodal documents. \n\nMultimodal Fusion and Pre-Training. The success of BERT in NLP [10], which relies on the easily-parallelizable Transformer architecture [58], an unsupervised training objective, and a task-agnostic architecture, has concurrently inspired many works in the VQA and cross-modal retrieval fields [57,38,35,56,34,7]. These models are unified under a single framework in [5] and partly reviewed in [28]. All of these models rely on the Transformer architecture, often initialized with a pre-trained BERT, in order to fuse image and text.",
            "score": 0.523450068526141,
            "section_title": "Related Work",
            "char_start_offset": 3037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "245144844"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1293,
                    "end": 1297,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1628,
                    "end": 1631,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 1631,
                    "end": 1634,
                    "matchedPaperCorpusId": "201058752"
                },
                {
                    "start": 1634,
                    "end": 1636,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1691,
                    "end": 1694,
                    "matchedPaperCorpusId": "227238841"
                },
                {
                    "start": 1718,
                    "end": 1722,
                    "matchedPaperCorpusId": "6628106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "257045539",
            "title": "The detection of mental health conditions by incorporating external knowledge",
            "text": "In recent years, since the excellent feature extraction capabilities of the Pre-trained Language Models (PLMs), the Meta AI team developed a retrieval method based on the high-dimensional feature representation, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), which greatly increased the accuracy and reliability of the knowledge retrieval. The experiment results show that the retrieval accuracy of DPR not only exceeds the traditionally used BM25 (Robertson & Zaragoza, 2009), but also because it is based on PLMs, its effect can continue to improve with more training. Meta AI team also used DPR to achieve gratifying results in the field of open-domain question answering and generation (Lewis et al., 2020). \n\nBecause of DPR's outstanding retrieval ability, we use it to retrieve external knowledge, and then import relevant knowledge and posts into deep learning models for predicting mental health conditions. It is then tested on the SMHD dataset to show the performance of our model.",
            "score": 0.523396236068268,
            "section_title": "Detection of mental health conditions on social media",
            "char_start_offset": 10744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1000
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "262947262",
            "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
            "text": "Sparse retrieval models [8] usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning. We propose a fusion-based approach to overcome this issue, combining sparse and dense retrieval models to derive the final ranking. Hybrid retrieval methods have been studied to improve the effectiveness of a retrieval system via the fusion of different retrieval strategies [23]- [25]. Chen et al. [25] propose a simple yet effective zero-shot hybrid model that combines BM25 with NPR [26] to address the out-of-domain generalization problem. They also point out that the dense retrieval models could be incompetent in modeling long documents, and sparse retrieval models can compensate for these weaknesses. Computational notebooks and scientific queries are out-of-domain data for most large-scale pre-trained language models, e.g., SBERT [15] and computational notebook retrieval also faces the long document problem. Therefore, the combination of sparse and dense models can be a potential solution for out-ofdomain and long document problems in computational notebook retrieval. Hence, different from the previous studies [4], [5] that use only one type of model, we propose a fused sparsedense model for computational notebook retrieval.",
            "score": 0.5233039728961253,
            "section_title": "C. Hybrid Sparse-dense Retrieval",
            "char_start_offset": 7551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1795
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "246285831"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "246285831"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "matchedPaperCorpusId": "243865679"
                },
                {
                    "start": 1679,
                    "end": 1682,
                    "matchedPaperCorpusId": "231749675"
                },
                {
                    "start": 1684,
                    "end": 1687,
                    "matchedPaperCorpusId": "245144651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "252668606",
            "title": "Zero-Shot Retrieval with Search Agents and Hybrid Environments",
            "text": "Classic retrieval systems such as BM25 (Robertson & Zaragoza, 2009) use term frequency statistics to determine the relevancy of a document for a given query. Recently, neural retrieval models have become more popular and started to outperform classic systems on multiple search tasks. Karpukhin et al. (2020) use a dual-encoder setup based on BERT (Devlin et al., 2019), called DPR, to encode query and documents separately and use maximum inner product search (Shrivastava & Li, 2014) to find a match. They use this model to improve recall and answer quality for multiple open-domain question-answer datasets. Large encoder-decoder models such as T5 (Raffel et al., 2020) are now preferred as the basis for dual encoding as they outperform encoders-only retrievers (Ni et al., 2021). \n\nIt has been observed that dense retrievers can fail to catch trivial query-document syntactic matches involving n-grams or entities (Karpukhin et al., 2020;Xiong et al., 2021;Sciavolino et al., 2021). ColBERT (Khattab & Zaharia, 2020) gives more importance to individual terms by means of a late interaction multi-vector representation framework, in which individual term embeddings are accounted in the computation of the query-document relevance score. This is expensive as many more vectors need to be stored for each indexed object. ColBERTv2 (Santhanam et al., 2022) combines late interaction with more lightweight token representations. SPLADE (Formal et al., 2021b) is another approach that relies on sparse representations, this time induced from a transformer's masked heads. SPLADEv2 (Formal et al., 2021a;2022) further improves performance introducing hardnegative mining and distillation. Chen et al. (2021) propose to close the gap with sparse methods on phrase matching and better generalization by combining a dense retriever with a dense lexical model trained to mimic the output of a sparse retriever (BM25). Ma et al. (2021) combine single hybrid vectors and data augmentation via question generation.",
            "score": 0.5231600684019124,
            "section_title": "RELATED WORK",
            "char_start_offset": 3863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 67,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 285,
                    "end": 308,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 348,
                    "end": 369,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 461,
                    "end": 485,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 651,
                    "end": 672,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 919,
                    "end": 943,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 943,
                    "end": 962,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 962,
                    "end": 986,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 996,
                    "end": 1021,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1334,
                    "end": 1358,
                    "matchedPaperCorpusId": "244799249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "267334785",
            "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
            "text": "Research on the reader component includes Fusion-in-Decoder (FiD) (Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages independently in the encoder and RETRO (Borgeaud et al., 2022;Wang et al., 2023), which utilizes cross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context. End-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoderdecoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Generation) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural retriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a treedecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hierarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements in retrieval accuracy by combining document and passage level retrievals and integrating sparse and dense retrieval methods, respectively (Liu et al., 2021;Arivazhagan et al., 2023). Despite a diversity in methods, the retrieving components of models predominantly rely on standard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous segmentation might not capture the complete semantic depth of the text. Reading extracted snippets from technical or scientific documents may lack important context making them difficult to read or even misleading. (Cohan & Goharian, 2017;Newman et al., 2023;Zhang et al., 2023). \n\nRecursive summarization as Context Summarization techniques provide a condensed view of documents, enabling more focused engagement with the content (Angelidis & Lapata, 2018).",
            "score": 0.5231506040077317,
            "section_title": "Why Retrieval?",
            "char_start_offset": 4782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 222,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 506,
                    "end": 524,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 641,
                    "end": 661,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 1151,
                    "end": 1169,
                    "matchedPaperCorpusId": "240288895"
                },
                {
                    "start": 1169,
                    "end": 1194,
                    "matchedPaperCorpusId": "259858975"
                },
                {
                    "start": 1413,
                    "end": 1431,
                    "matchedPaperCorpusId": "258378282"
                },
                {
                    "start": 1694,
                    "end": 1718,
                    "matchedPaperCorpusId": "12665229"
                },
                {
                    "start": 1738,
                    "end": 1757,
                    "matchedPaperCorpusId": "252118883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Information Retrieval: Advancements in deep learning have revolutionized information retrieval systems, enhancing their personalization and accuracy in retrieving relevant documents. Early information retrieval frameworks employed sparse retrievers [6] or dense retrievers [4,8] to represent large corpora but struggled to capture deep semantic relationships [13]. LLM-based retrievers (generative retrieval) have since emerged as notable methods, leveraging the rich prior knowledge of LLMs to significantly improve performance by converting documents into parametric knowledge and generating them instead of computing similarity scores [38]. However, the frequent encoding and decoding processes in LLMs severely hinder efficiency [38]. To address the trade-off between effectiveness and efficiency, we propose invar-retrieval in our architecture, enabling the model to efficiently retrieve the most relevant documents without introducing variance. \n\nRetrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs [38]. However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process. Correspondingly, we introduce a representation learning method and invariance loss in our Invar-RAG architecture, which partially addresses these issues and explores a novel approach to using a single LLM for multiple roles within the RAG system.",
            "score": 0.5229247830996555,
            "section_title": "Related Work",
            "char_start_offset": 21968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "230433817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "266191420",
            "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "text": "Active RAG [60] regenerates sentences using retrieval if they contain low-probability tokens. \n\nRetrieval Methods RAG models connected to databases retrieve documents using fixed representations, such as Bag-of-Words or BM25 [61,62], pre-trained embeddings [63,61], or trainable encoders [19,64], which are trained offline [64] or online [65]. For models using open-ended sources such as the internet, phrase or keyword searches are used for retrieval [66,49]. Retrieved documents are typically passed to the model as input tokens, but some models separately process retrieved information with cross-attention [67,62]. Learning to rank and filter the retrieved documents [68] further improves the performance of RAG models. In our work, we evaluate Retrieval-Augmented Models that use keyword search systems and vector embedding retrieval. In particular, we focus on benchmarking against humans and uncover under what conditions LLMs can match human performance in information retrieval.",
            "score": 0.5229195363914674,
            "section_title": "RELATED WORKS",
            "char_start_offset": 7443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 987
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "267069067",
            "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
            "text": "Here, we summarize the advantages and disadvantages of various retriever models. The off-theshelf retrievers are easy to use without any downstream task finetuning and typically demonstrate stronger performance than random demonstrations. One exception is in commonsense reasoning tasks where Zhang et al. (2022b) and Ye et al. (2023a) found that for these tasks, random demonstrations are consistently better than retrieval-based method. Cheng et al. (2023) also show that retrieved demonstrations harm commonsense reasoning and coreference resolution tasks. Among the three categories of off-the-shelf retrievers, sparse retrievers such as BM25 are more index-efficient. This feature becomes particularly valuable when dealing with large volumes of demonstrations and limited hardware memory, making BM25 a preferable choice under such circumstances. In contrast, sentenceembedding similarity-based methods and dual-encoder-based retrieval systems, which are trained on language tasks, excel in capturing more semantically focused retrieval. Regarding performance, Luo et al. (2023) compared BM25 with dual encoder (GTR) across 5 tasks, and they found that the average performance of these two is very similar (within 0.5% difference), and BM25 outperformed the dual encoder in some tasks and vice versa. In another study, Ye et al. (2023a) observed a similar trend highlighting that no single retriever consistently outperforms others across different tasks. Both Rubin et al. (2022) and Li et al. (2023b) found that BM25 is better than SBERT on semantic parsing tasks, while Li et al. (2023b) found that SBERT is better than BM25 on sentiment analysis tasks. Nevertheless, retrievers that are fine-tuned demonstrate superior performance compared to their off-the-shelf counterparts. The main drawback of fine-tuned retrievers lies in the high cost of obtaining training data. Additionally, the common practice of employing task-specific retrievers complicates the system and limits its generalizability. Li et al. (2023b) proposed to train a universal retriever that shows stronger performance than task-specific demonstration retriever (e.g.",
            "score": 0.5228999388815415,
            "section_title": "Summary",
            "char_start_offset": 38485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 313,
                    "matchedPaperCorpusId": "252762275"
                },
                {
                    "start": 1467,
                    "end": 1486,
                    "matchedPaperCorpusId": "245218561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "265212781",
            "title": "Summarization-Based Document IDs for Generative Retrieval with Language Models",
            "text": "Wikipedia-based corpora have long been an important part of NLP research and form a natural benchmark for studying new techniques in textbased recommender and information retrieval systems. In this work, we examine how generative retrieval behaves on short-form and long-form documents drawn from Wikipedia and non-Wikipedia sources. We also propose a new type of document ID for generative retrieval based on document summarization, which demonstrably improves retrieval performance across the tasks that we examined. \n\nLarge language models (LMs) are now widely used across many NLP tasks, and extensions of generative models to document retrieval tasks have recently been proposed (Wang et al., 2022;Tay et al., 2022), in contrast to vector-based approaches like dense passage retrieval (DPR; Karpukhin et al., 2020). DPR is a widely-used technique for training document retrieval models, where queries and documents are mapped to dense vector representations with a transformer encoder (e.g., BERT; Devlin et al., 2019). By increasing the cosine similarity between positive query-document pairs and decreasing it between negative pairs, DPR performs metric learning over the space of queries and the set of documents to be indexed. \n\nGenerative alternatives to document retrieval address certain limitations of dense, vector-based approaches to retrieval. For example, query and document representations are constructed separately in DPR, which precludes complex query-document interactions. Using a single dense vector to represent an entire document limits the amount of information that can be stored; indeed, Tay et al. (2022) observed that increasing the number of parameters in the encoder does not significantly enhance DPR performance. Furthermore, the rich sequence generation capabilities of language models (LMs) cannot be used directly in dense retrieval. Tay et al. (2022) and Wang et al. (2022) therefore proposed a new direction called generative retrieval, where LMs learn to directly map queries to an identifier that is unique to each document. We illustrate the differences in Figure 1. \n\nInstead of retrieving documents based on cosine similarity, generative retrieval uses an LM to produce a sequence of tokens encoding the relevant document's ID, conditional on the query.",
            "score": 0.5228428483169039,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2109
                },
                {
                    "start": 2112,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 684,
                    "end": 703,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 703,
                    "end": 720,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 796,
                    "end": 819,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1003,
                    "end": 1023,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1617,
                    "end": 1634,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1872,
                    "end": 1889,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1894,
                    "end": 1912,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.467529296875
        },
        {
            "corpus_id": "238259539",
            "title": "A proposed conceptual framework for a representational approach to information retrieval",
            "text": "We can apply the conceptual framework proposed in this paper to organize various dense and sparse retrieval methods that have been proposed in the literature. This structure can facilitate comparisons across different classes of methods, and analyzing models in a common framework can perhaps help us better draw generalizations. Table 2 shows the effectiveness of various models on the development queries of the MS MARCO passage ranking test collection [Bajaj et al., 2018], which has emerged in recent years as the most prominent dataset for training and benchmarking retrieval models. \n\nAs a baseline, row (1) shows the effectiveness of BM25, which can be characterized as an unsupervised sparse retrieval method. Learned sparse retrieval methods are shown in the second main block of  document expansion components. BM25 with doc2query-T5 document expansions [Nogueira and Lin, 2019], row (2), can be understood as using a neural sequence-to-sequence model for expansion, but retaining the BM25 weighting scheme; thus, learning is only applied in the expansion component. \n\nDeepCT [Dai and Callan, 2019], row (3), uses a regression-based term weighting model without any expansion. SparTerm [Bai et al., 2020], row (4), uses the masked language model (MLM) layer of BERT to generate expansion terms on which term weights are learned. DeepImpact [Mallia et al., 2021], row (5), combines the use of doc2query-T5 for expansion with a term weighting model trained using pairwise loss. \n\nRows (6a) and (6b) present a contrastive condition comparing the same term weighting model-COIL [Gao et al., 2021a]-with and without an expansion model; adding document expansion yields a two-point gain in effectiveness. With uniCOIL [Lin and Ma, 2021], which builds on COIL, the literature reports three contrastive conditions: without expansion, row (7a), and with two different expansion methods, doc2query-T5 in row (7b) and TILDE [Zhuang and Zuccon, 2021b] in row (7c).",
            "score": 0.5226589449373329,
            "section_title": "Experimental Results",
            "char_start_offset": 39651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1371,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1584,
                    "end": 1603,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "273654410",
            "title": "Deep Learning Based Dense Retrieval: A Comparative Study",
            "text": "Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but their robustness against tokenizer poisoning remains underexplored. In this work, we assess the vulnerability of dense retrieval systems to poisoned tokenizers by evaluating models such as BERT, Dense Passage Retrieval (DPR), Contriever, SimCSE, and ANCE. We find that supervised models like BERT and DPR experience significant performance degradation when tokenizers are compromised, while unsupervised models like ANCE show greater resilience. Our experiments reveal that even small perturbations can severely impact retrieval accuracy, highlighting the need for robust defenses in critical applications.",
            "score": 0.5224838601438283,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328857421875
        },
        {
            "corpus_id": "270620264",
            "title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs",
            "text": "In the entity-node matching process mentioned in section 3.2.1, we used a dense retrieval method to link EMR's entities to KG's nodes. In order to better explore the appropriate retrieval method, we implemented three types of retrieval methods based on the retriv library2 : sparse retrieval, dense retrieval, and hybrid retrieval. \n\n\u2022 Sparse Retrieval: We evaluated two representative methods, namely bm25 and tf-idf. \n\n\u2022 Dense Retrieval: We evaluated several representative embedding models, namely m3elarge (Wang Yuxin, 2023), all-mpnet-base-v2, bge-large-zh-v1.5, and CoROM. \n\n\u2022 Hybrid Retrieval: We evaluated two combinations: \"bge + bm25\" and \"corom + bm25\". \n\nThe results are shown in Table 6. As we expected, the effect of dense retrieval is better than that of sparse retrieval and hybrid retrieval, because when the entity to be retrieved contains a large number of Chinese characters, sparse retrieval methods are prone to mismatching due to the lack of consideration of word order and semantics. According to the results, we choose the CoROM model as embedding model of the dense retrieval process. \n\nThe CoROM Chinese-medical text representation model we use in this paper is also released by Tongyi-Laboratory. It employs the classic dualencoder text representation model and is trained on medical domain data with Multi-CPR (Long et al., 2022). The training process is divided into two stages -in the first stage, negative sample data is randomly sampled from the official document set, and in the second stage, difficult negative samples are mined via Dense Retrieval to augment the training data for retraining.",
            "score": 0.5224205135047062,
            "section_title": "C.2 Details of Retrieval Method",
            "char_start_offset": 25116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1628
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "278664642",
            "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
            "text": "When it comes to the document retrieval, Sparse retrieval methods such as BM25 [Robertson and Zaragoza, 2009;Trotman et al., 2014] had been prevalent due to their straightforward approach to matching keywords. However, they can be limited by their inability to capture the deeper semantic meaning of text. Dense retrieval, on the other hand, showed many advantages over sparse retrieval [Lee et al., 2019;Karpukhin et al., 2020;Li et al., 2021]. Dense retrieval approaches go beyond mere keyword matching; they generate vector representations of documents and queries, facilitating the capture of deep semantic meanings. This is crucial in fields requiring high accuracy and contextual understanding, where the relevancy of documents cannot solely be determined by keyword frequency. Given the advantages of dense retrieval, this method was selected for our model.",
            "score": 0.5216832426102311,
            "section_title": "Document preprocessing & similarity search",
            "char_start_offset": 5487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 864
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 109,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 109,
                    "end": 130,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 405,
                    "end": 428,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 428,
                    "end": 444,
                    "matchedPaperCorpusId": "236034464"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "222125038",
            "title": "SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval",
            "text": "Table 2 shows the full ranking performances of our models and baselines on MSMARCO Passage Retrieval dataset. SparTerm (expansionenhanced) outperforms all baselines on MRR, achieving the stateof-the-art ranking performance among all sparse models, and outperforms all baselines except Doc2query-T5 on Recall. We find that SparTerm achieves more significant performance improvements on MRR and Recall@10-100, which illustrates that our model has a more significant ability on top ranking compared with previous sparse models. Further, pre-trained language model(PLM) based methods (DeepCT, Doc2query-T5, and SparTerm) perform better than those without PLM, demonstrating that PLM can facilitate the passage full ranking with better representation. Considering the improvements T5 brings to Doc2query, we believe that SparTerm can be further improved with more advanced PLM. Even without any expansion, SparTerm(literal-only) outperforms DeepCT on both MRR and Recall, demonstrating that SparTerm can produce more effective term weights thus facilitating the retrieval. We also analyze the difference between SparTerm and DeepCT on term weighting in Section 5.4. With only the expanded words, SparTerm achieves a definite improvement compared with BM25, especially on Recall. This improvement proves the effectiveness of passage expansion on improving the Recall for retrieval.",
            "score": 0.5216480062256207,
            "section_title": "Performance on Passage Full Ranking",
            "char_start_offset": 20386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "230435546",
            "title": "Multi-Task Retrieval for Knowledge-Intensive Tasks",
            "text": "128 samples, and in preliminary experiments we found no benefit in increasing this further. We use an additional \"hard\" confounder per batch, selected based on BM25 score as in .\n\nDownstream evaluation When evaluating our retriever within a larger architecture to perform a knowledge-intensive task, we replicate a setup analogous to DPR + BART of . This uses our multi-task model to retrieve and prepend the top 3 passages to the query, which is then processed by a task-specific fine-tuned BART model to generate the final answer for the end task.\n\nBaselines For our retrieval experiments, we include as baselines a BM25 model as well as a taskspecific DPR model for each of the training datasets. For the downstream evaluations, we compare against three strong representative models trained by : a task-specific DPR model combined with BART (Lewis et al., 2020a), RAG (Lewis et al., 2020b), and T5 (Raffel et al., 2020).",
            "score": 0.5214765614697504,
            "section_title": "Dataset",
            "char_start_offset": 13287,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 844,
                    "end": 865,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 871,
                    "end": 892,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 901,
                    "end": 922,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "252090145",
            "title": "CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval",
            "text": "We compare the following competitive retrieval systems using different retrieval components but with the same multilingual reader. \n\nMT + DPR A common approach to cross-lingual tasks is translating inputs from target languages to source languages of richer resources so that stronger monolingual models can be utilized (Ahmad et al., 2021;Asai et al., 2021a). We translate all claims and all passages into English with the HELSINKI-NLP neural machine translation models3 for its comprehensive language coverage and decent performance. For languages not covered by HELSINKI-NLP, we use Google Translate instead. Then, we use DPR (Karpukhin et al., 2020) to perform retrieval based on the translated claims and passages. \n\nBM25 BM25 (Robertson and Zaragoza, 2009) has demonstrated advantages over dense vector representation approaches in monolingual retrieval tasks (Lee et al., 2019). Since BM25 only works in a monolingual setup, we create dummy empty passages for claims whose languages are not presented in the passage collection P . Our implementation is based on the Rank-BM25 package4 . \n\nmDPR mDPR is a multilingual retriever based on DPR. It was trained on multilingual question answering datasets, as detailed in Section 3. \n\nGoogle Search As demonstrated in previous work (Augenstein et al., 2019;Gupta and Srikumar, 2021), snippets from Google Search results can serve as evidence for fact-checking. We directly take the snippets obtained by Gupta and Srikumar (2021) as inputs.",
            "score": 0.521328850568286,
            "section_title": "Baselines",
            "char_start_offset": 11248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1489
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 339,
                    "matchedPaperCorpusId": "222177188"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 628,
                    "end": 652,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 865,
                    "end": 883,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1282,
                    "end": 1307,
                    "matchedPaperCorpusId": "202541363"
                },
                {
                    "start": 1307,
                    "end": 1332,
                    "matchedPaperCorpusId": "235457983"
                },
                {
                    "start": 1453,
                    "end": 1478,
                    "matchedPaperCorpusId": "235457983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.172607421875
        },
        {
            "corpus_id": "225041183",
            "title": "Distilling Dense Representations for Ranking using Tightly-Coupled Teachers",
            "text": "Combined with BM25, our model already exhibits better retrieval effectiveness than CLEAR. In addition, the comparison between bi-encoder (PoolAvg) and bi-encoder (TCT-ColBERT) demonstrates that the gain from distilled dense representation is still present, even with the advanced sparse retrieval method doc2query-T5: +0.010 (+0.010) and +0.013 (+0.020) with BM25 (doc2query-T5) in MRR@10 for both the MS MARCO and TREC2019 DL tasks, respectively. The advanced hybrids (entries with doc2query-T5) reaches effectiveness even better than ColBERT and is almost on par with the cross-encoder reranker. It is also worth noting that our hybrid end-to-end retrieval method yields state-of-the-art recall in both tasks. More importantly, our proposed method is four times and thirty times more efficient than the multi-stage methods: ColBERT and the cross-encoder reranker, respectively. These results demonstrate that the dense-sparse hybrid is a promising solution for low latency end-to-end text retrieval. \n\nLatency. Table 2 shows the breakdown of end-toend retrieval latency into individual components. Specifically, we measure the system overhead of query embedding generation, dense retrieval with top 1000 passages, and dense-sparse score combination. To obtain the latency for dense retrieval, we run BERT query encoder and dot product search using a 32GB V100 GPU. Specifically, we conduct brute force dot product search in Faiss (indexing with IndexFlatIP). As for the dense-sparse hybrid, we assume sparse and dense retrieval can be run in parallel; this is a realistic assumption because sparse retrieval runs on CPUs. Thus, the total latency of the hybrid model (shown in Table 1) is bound by dense retrieval with additional 3ms for score combination (since sparse retrieval is faster than dense retrieval). \n\nAblation study. Finally, we study the effectiveness of our distilled dense representations on the MS MARCO development set under two settings, reranking and retrieval.",
            "score": 0.5207763858383417,
            "section_title": "Results",
            "char_start_offset": 15595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "267413218",
            "title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
            "text": "To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI-3), which was recently released by OpenAI 5 . \n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3-Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with E5 mistral-7b , which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef- fectively trained by M3-Embedding, as it outperforms the typical BM25 methods in all languages. We can also observe the additional improvement from multi-vector retrieval, which relies on finegrained interactions between query and passage's embeddings to compute the relevance score. Finally, the collaboration of dense and sparse method (Dense+Sparse) leads to a further improvement over each individual method, and the collaboration of all three methods (All) brings forth the best performance.",
            "score": 0.5204904093902791,
            "section_title": "Multi-Lingual Retrieval",
            "char_start_offset": 19563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1636
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435302734375
        },
        {
            "corpus_id": "252872857",
            "title": "Query Expansion Using Contextual Clue Sampling with Language Models",
            "text": "Retriever Retrieval in open-domain QA is traditionally implemented with sparse vector space model BM25 (Robertson and Zaragoza, 2009), based on exact term matching. DPR (Karpukhin et al., 2020) implements retrieval by representing questions and passages as dense vectors. GAR (Mao et al., 2021) proposes to expand the query by adding relevant answers, the title of a passage and the sentence where the answer belongs. It also fuse the results from its own and from DPR (GAR+DPR). To make a fair comparison, we extend our generation target from the answer context only (Ours-single) to include both the answer and the passage title (Ours-multi). We also report the fusion results with DPR. SEAL (Bevilacqua et al., 2022) use BART model to generate ngrams then map to full passage with FM index. \n\nReader DPR (Karpukhin et al., 2020) employs an extractive reader model based on BERT (Devlin et al., 2019) and predicts the answer span. RAG (Lewis et al., 2020b) combines the DPR dense retriever together with a BART answer generator, and jointly trains the two models end-to-end. FiD (Izacard and Grave, 2021) also uses DPR to retrieve relevant passages and the decoder attends over all the encoded passages to generate the final answer. For fair comparison, we evaluate the retrieval results of FiD, GAR, and SEAL on the same reader model, FiD-large which takes the question along with 100 top retrieved passages as input.",
            "score": 0.5204606859567076,
            "section_title": "Baselines",
            "char_start_offset": 8101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1420
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 133,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 169,
                    "end": 193,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 276,
                    "end": 294,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 807,
                    "end": 831,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 937,
                    "end": 958,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1081,
                    "end": 1105,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "268032392",
            "title": "Enhancing Retrieval Processes for Language Generation with Augmented Queries",
            "text": "Natural Language Processing (NLP) has witnessed significant advancements with the emergence of LLMs. Despite their prowess in downstream tasks, these models face challenges in accessing and manipulating knowledge, leading to suboptimal performance on knowledgeintensive tasks. A groundbreaking approach to address these limitations is Retrieval-Augmented Generation (RAG). In Lewis et al.'s seminal work (2020) [2], the authors introduce RAG models that combine pre-trained parametric and non-parametric memory for language generation. By leveraging a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory, RAG achieves superior performance on a spectrum of knowledge-intensive NLP tasks. It outshines parametric seq2seq models and task-specific architectures, setting new benchmarks in open-domain question answering (QA) tasks. Furthermore, RAG models exhibit enhanced language generation capabilities, producing more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines. \n\nIn the realm of information retrieval (IR), recent research has made remarkable strides. Hambarde and Proenca (2023) [7] provide an extensive overview of IR models, discussing the state-of-the-art methods, including those based on terms, semantic retrieval, and neural approaches. Karpukhin et al. (2020) [8] revolutionize open-domain QA by introducing dense passage retrieval, demonstrating its practical implementation using only dense representations. This approach outperforms traditional sparse vector space models, leading to state-of-the-art performance on multiple open-domain QA benchmarks. Qu et al. (2020) [9] further optimize dense passage retrieval with RocketQA, addressing challenges such as training-inference discrepancy and limited training data. RocketQA significantly outperforms previous state-of-theart models on well-established datasets, showcasing its effectiveness in improving end-to-end QA systems. Advancements in similarity search, as demonstrated by Johnson et al.'s (2019) [10], their work, optimizing k-selection and addressing memory hierarchy challenges, sets new benchmarks in various similarity search scenarios.",
            "score": 0.5198375768896519,
            "section_title": "Related works",
            "char_start_offset": 2269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480712890625
        },
        {
            "corpus_id": "267328301",
            "title": "A Quantitative Analysis of Noise Impact on Document Ranking",
            "text": "The aim of this study is to evaluate the performance document ranking models on the aforementioned noisy datasets.We use two classical ones, namely BM25 [5] and DistilBERT [6] on the passage ranking task.We use the Pyserini python toolkit to run our experiments [35].\n\nBM25 is a well-known model in the field of information retrieval that has been widely used for various retrieval tasks, such as web search and document ranking.BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks.DistilBERT is a pre-trained model that can be fine-tuned for various NLP tasks, including passage ranking.Precisely, it is a distilled version [36] of BERT [20], retaining 97% performance but being 60% faster and using only half the number of parameters.Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors.By comparing these two types of models in a noisy context, we can gain insight into the strengths and weaknesses of sparse and dense representation models, and how they perform under different levels and types of noise.This comparison is particularly interesting because the two models use fundamentally different approaches to represent text.Moreover, we believe that BM25 and DistilBERT are representative of their family of models and we would expect similar results if we conducted experiments with other models.",
            "score": 0.5195214984614263,
            "section_title": "D. Ranking models",
            "char_start_offset": 16399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 267
                },
                {
                    "start": 269,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1153
                },
                {
                    "start": 1155,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1586
                },
                {
                    "start": 1586,
                    "end": 1710
                },
                {
                    "start": 1710,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "233219855",
            "title": "A Replication Study of Dense Passage Retriever",
            "text": "As Karpukhin et al. (2020) noted, Retriever Multi was trained by combining training data from all datasets but excluding SQuAD; these poor results are expected, since SQuAD draws from a very small set of Wikipedia articles. Third, the effectiveness of hybrid dense-sparse fusion appears to be understated in the original DPR paper. Karpukhin et al. (2020) found that Condition k = 20 100 500 1000 NQ 6.1 5.2 4.4 4.2 TriviaQA 9.2 6.6 5.0 4.6 WQ 5.9 5.9 5.8 5.7 CuratedTrec 6.9 7.2 6.3 5.9 SQuAD 4.5 4.1 4.0 4.0 \n\nTable 2: The Jaccard overlap between sparse retrieval results and dense retrieval results. \n\nhybrid retrieval is less effective than dense retrieval in most settings, which is inconsistent with our experimental results. Instead, we found that dense-sparse retrieval consistently beats sparse retrieval across all settings. The gains from both hybrid scoring techniques are statistically significant, with the exception of top-20 for Curated-TREC. Our results might be due to better BM25 effectiveness, but we are unable to further diagnose these differences because, once again, the hybrid retrieval code is not provided in the DPR repo. Further testing also found that the differences between the two hybrid techniques are not significant. Thus, there does not appear to be a strong basis to prefer one hybrid technique over the other. \n\nIn Table 2, we report overlap when taking different top-k results from dense retrieval and sparse retrieval. Overlap is measured in terms of Jaccard overlap, which is computed by the intersection over the union. It is apparent that the overlap between dense and sparse results is quite small, which suggests that they are effective in very different ways. This provides an explanation of why hybrid retrieval is effective, i.e., they are exploiting very different signals. These results also justify the DPR design choice of retrieving k \u2032 > k results from dense and sparse retrieval and then rescoring the union to arrive at the final top-k.",
            "score": 0.5192335105422065,
            "section_title": "Retrieval",
            "char_start_offset": 17675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 26,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 332,
                    "end": 355,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428955078125
        },
        {
            "corpus_id": "268031876",
            "title": "Generative Retrieval with Large Language Models",
            "text": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
            "score": 0.5189432501811777,
            "section_title": "Related Work",
            "char_start_offset": 21339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 175,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "239016601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "270710900",
            "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs",
            "text": "We focus primarily on transformer based architectures and leverage publicly available checkpoints as shown in Table 5 in Appendix A.1.We evaluate retrieval models from several categories.However, DEXTER is model agnostic, and new models can be easily added because of the modular framework.For all datasets except FinQA we evaluate on dev set due to lack of publicly available test sets with gold labels.For FinQA we evaluate on test set.\n\nLexical retrieval: BM25 [40] computes token-matching between sparse representations using TF-IDF weights.We employ ElasticSearch's BM25 implementation due to its enhanced query speed and maintainability over other implementations [12].\n\nSparse retrieval: For sparse retrieval we employ SPLADE [14,13] which is a neural model employing query and document sparse expansions.\n\nDense retrieval: DPR [24] trains a dual encoder model with large datasets to maximize similarity between related queries and documents.We found the open source model trained on multiple datasets facebook-dpr-question-encoder-multiset-base to work better than the model trained on individual datasets like the bi-encoder trained on Natural Questions.We also include ANCE [57] in our evaluation which is a bi-encoder model that samples hard negatives through an Approximate Nearest Neighbour search over an index of the corpus which is also updated yielding better negatives.We employ the checkpoint msmarco-roberta-base-ance-firstp trained on MS-MARCO [1].Tas-b [19] is a bi-encoder model trained using supervision from a cross-encoder and ColBERT.",
            "score": 0.5188545524419417,
            "section_title": "Retrievers",
            "char_start_offset": 17277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 438
                },
                {
                    "start": 440,
                    "end": 545
                },
                {
                    "start": 545,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 812
                },
                {
                    "start": 814,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1163
                },
                {
                    "start": 1163,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1561
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "233231706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "270257714",
            "title": "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?",
            "text": "The above experiments have demonstrated the bias in LLMs against contexts retrieved by Contriever, a dense retrieval model.This section further explores whether such bias remains consistent across different retrieval models.To this end, we incorporate BM25 as a representative of sparse retrieval techniques, along with human-labeled golden passages, to mimic an ideal retrieval model (Gold).\n\nFor each retrieval model, we respectively construct AIR context-conflicting datasets based on the NQ dev sets from Karpukhin et al. (2020), which include the golden passages.As illustrated in Table 3, the results indicate that LLMs consistently display a pronounced bias in favor of generated contexts regardless of the retrieval model used.",
            "score": 0.5188331124618544,
            "section_title": "Consistent Preferences Across Retrievers",
            "char_start_offset": 17647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 735
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 532,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Dense retrieval models based on neural text representations have proven very effective Qu et al., 2021;Ram et al., 2022;Izacard et al., 2022a,b), improving upon strong traditional sparse models like BM25 (Robertson and Zaragoza, 2009). However, when applied off-theshelf (i.e., in out-of-domain settings) they often experience a severe drop in performance (Thakur et al., 2021;Sciavolino et al., 2021;Reddy et al., 2021). Moreover, the reasons for such failures are poorly understood, as the information captured in their representations remains under-investigated.  Figure 1: An example of our framework. We run the question \"Where was Michael Jack born?\" through the question encoder of DPR , and project the question representation e q to the vocabulary space using BERT's masked language modeling head . The result is a distribution over the vocabulary, Q. We apply the same procedure for passages as well. These projections enable reasoning about and improving retrieval representations.\n\nIn this work, we present a new approach for interpreting and reasoning about dense retrievers, through distributions induced by their query 2 and passage representations when projected to the vocabulary space, namely distributions over their vocabulary space (Figure 1). Such distributions enable a better understanding of the representational nature of dense models and their failures, which paves the way to simple solutions that improve their performance.  We begin by showing that dense retrieval representations can be projected to the vocabulary space, by feeding them through the masked language modeling (MLM) head of the pretrained model they were initialized from without any further training. This operation results in distributions over the vocabulary, which we refer to as query vocabulary projections and passage vocabulary projections.\n\nSurprisingly, we find these projections to be highly interpretable to humans ( Figure 2; Table 1). We analyze these projections and draw interesting connections between them and well-known concepts from sparse retrieval ( \u00a75). First, we highlight the high coverage of tokens shared by the query and the passage in the top-k of their projections. This obersvation suggests that the lexical overlap between query and passages plays an important role in the retrieval mechanism. Second, we show that vocabulary projections of passages they are likely to contain words that appear in",
            "score": 0.5188027654176375,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "231704318",
            "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation",
            "text": "Traditional term-based methods like BM25 (Robertson et al., 1995) are powerful zero-shot models and can outperform supervised neural models in many cases (Lin, 2019). Rescoring systems have shown that integrating BM25 into a neural model improves performance (McDonald et al., 2018). However, for first-stage retrieval most work focuses on approximations via re-ranking (Karpukhin et al., 2020;Luan et al., 2020). Here we present a technique for exact hybrid first-stage retrieval without the need for a re-ranking stage. Our method is motivated by the work of Seo et al. ( 2019) for sparse-dense QA. \n\nFor a query Q and a passage P , BM25 is computed as the following similarity score, \n\n, \n\nwhere k/b are BM25 hyperparameters, IDF is the term's inverse document frequency from the corpus, cnt is the term's frequency in a passage, n/m are the number of tokens in Q/P , and m avg is the collection's average passage length. Like most TF-IDF models, this can be written as a vector space model. Specifically, let q bm25 \u2208 [0, 1] |V | be a sparse binary encoding of a query of dimension |V |, where V is the term vocabulary. Specifically this vector is 1 at position i if v i \u2208 Q, here v i is the i-th entry in V . Furthermore, let p bm25 \u2208 R |V | be a sparse real-valued vector where, \n\nWe can see that, BM25(Q, P ) = q bm25 , p bm25",
            "score": 0.5187343542858452,
            "section_title": "Hybrid First-stage Retrieval",
            "char_start_offset": 16752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1333
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 65,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 154,
                    "end": 165,
                    "matchedPaperCorpusId": "58534751"
                },
                {
                    "start": 259,
                    "end": 282,
                    "matchedPaperCorpusId": "52169526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.472412109375
        },
        {
            "corpus_id": "265422486",
            "title": "Boosting legal case retrieval by query content selection with large language models",
            "text": "Model P@5(%) P@10(%) MAP(%) NDCG@10(%) NDCG@20(%) NDCG@30(%)  2, which reflects the same trend as the original paper [20]. The number in the bracket after a model name denotes the input format. For example, BERT-PLI(1*100,12*100) denotes that the input for BERT-PLI includes 1 segment of query whose length is 100 and 12 segments of documents whose length is 100. \n\nTraditional sparse retrieval models generally perform not as well as dense retrieval models as they are unsupervised. TF-IDF performs much worse as it is a bag-of-words model and only contains the word frequency information and does capture many matching signals. BM25 and QL perform better as they not only perceive word frequency but also perceive matching signals between a query and a document. \n\nDense retrieval models perform well on both precision and ranking metrics. Though BERT-CLS is a simple model, it performs significantly better than other models especially on the precision metrics. We find that when the input has a similar length, BERT-PLI does not perform as well as BERT-CLS. This could be the result of the separation of a document, which leads to the loss of interaction between passages. Despite we increase the inputs documents passages for BERT-PLI, we do not observe significant improvement. That may be the limited relevance labels provide insufficient guiding signals when BERT-PLI faces excessive textual inputs. This explains why truncating inputs of legal query is still prevalent in competitions.",
            "score": 0.5186610779898057,
            "section_title": "Model Type",
            "char_start_offset": 12705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1494
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "235792264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437744140625
        },
        {
            "corpus_id": "258332095",
            "title": "A Personalized Dense Retrieval Framework for Unified Information Access",
            "text": "Accurate contextual representation of text using large-scale pretrained language models has led to significant progress in various fields, including information retrieval. Combining these models with efficient approximate nearest neighbor search resulted in the development of dense retrieval models [17,34]. These models finetune the pre-trained language models on the downstream information retrieval task [4] and have shown significant performance improvement over strong lexical matching methods such as BM25 [26]. One line of methods focuses on the optimization of dense retrieval models by producing better negative samples for contrastive loss. For example, DPR [17] uses the BM25 negatives as the source for hard-negative sampling and ANCE [34] applies a self-sampling strategy for negative sampling. Models like RocketQA [24] and Condenser [8] use large batch sizes and have demonstrated that it would be conducive to the stability of training. The other line of methods applies the knowledge distillation techniques that distill the knowledge from re-ranking models to train the dense retrieval models [12,22,39]. Adapting dense retrieval models to unseen data is also an active area of research [29].",
            "score": 0.5182770943087054,
            "section_title": "Dense Retrieval",
            "char_start_offset": 7069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1211
                }
            ],
            "ref_mentions": [
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 849,
                    "end": 852,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1116,
                    "end": 1119,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1119,
                    "end": 1122,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77978515625
        },
        {
            "corpus_id": "271720020",
            "title": "Creating a Taxonomy for Retrieval Augmented Generation Applications",
            "text": "There are different types of retrieval augmentation methods (Li et al. 2022). In this dimension, three characteristics are considered i.e., sparse-vector retrieval, dense-vector retrieval, and task-specific retrieval (Li et al. 2022). Sparse-vector retrieval involves methods, e.g., TF-IDF, BM25, etc. \n\nDense-vector retrieval contains models based mostly on low-dimensional dense vectors, e.g., BERTencoders and relying often on vector databases. In the task-specific retrieval, the retrieval module is based on the task (Li et al. 2022) and might comprise a database (Radeva et al. 2024). Some research works \"directly use the edit distance between natural language texts (Hayati et al. 2018) or abstract syntax trees (AST) of code snippets (Poesia et al. 2021). D10 Application Architecture: When developing a RAG system, in addition to the RAG structure, we also need to consider the structure of the final application and the interaction of the components. Radeva et al. (2023) present a web application RAG system that consists of the \"local or server-based installation\", \"web application\", \"vector storage\" (database), as well as the testing and configurations. Therefore, this dimension comprises web app, local server, database, testing, and configuration characteristics.",
            "score": 0.5181766722004121,
            "section_title": "D 9 Retrieval Type:",
            "char_start_offset": 22351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1282
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 76,
                    "matchedPaperCorpusId": "262553219"
                },
                {
                    "start": 217,
                    "end": 232,
                    "matchedPaperCorpusId": "262553219"
                },
                {
                    "start": 522,
                    "end": 537,
                    "matchedPaperCorpusId": "262553219"
                },
                {
                    "start": 674,
                    "end": 694,
                    "matchedPaperCorpusId": "52136345"
                },
                {
                    "start": 743,
                    "end": 763,
                    "matchedPaperCorpusId": "246294475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "274626566",
            "title": "KG-EGV: A Framework for Question Answering with Integrated Knowledge Graphs and Large Language Models",
            "text": "For these open-domain QA tasks, we set up two categories of baseline models: \n\n1. \n\nRetriever-Reader Models without LLMs: \n\n\u2022 BM25 + BERT: Combines the sparse retrieval method BM25 with BERT for text representation. \u2022 REALM: Retrieves relevant documents from a knowledge base and integrates them into language model training. \u2022 DPR: Uses dense encoders to encode text passages and questions, retrieving relevant passages based on vector similarity. \u2022 RAG: Optimizes ODQA with a retrieval-augmented generation strategy. \u2022 FiD: Uses a multi-scale reader (770M and 3B) to process text.",
            "score": 0.5180815596091763,
            "section_title": "Baselines for NQ, WebQ, and TriviaQA",
            "char_start_offset": 26639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Retrieval-augmented generation (RAG) is a hybrid approach that integrates LLMs with a retrieval mechanism to fetch relevant information from external documents or knowledge in real-time during the text generation process. This technique allows LLMs to dynamically access vast external corpora (like knowledge graphs, databases or search engines) to retrieve the most relevant information [41,90,105,107]. RAG enables language models to retrieve factual information and generate more accurate and contextually aware outputs, especially in cases where a language model's training data may be outdated or incomplete [99]. This approach provides the LLM with an ''external memory'' to supplement its internal knowledge base, thus enhancing LLM quality and accuracy. \n\nRAG comprise of retrievers, generators, and knowledge bases [41]. The retriever dynamically fetches relevant information from external corpora, the generator uses this retrieved information to generate a response, and the knowledge base is a collection of text such as scientific articles, news articles, or Knowledge Graphs [95,119,120]. This architecture allows the LLM to access up-to-date knowledge beyond its static training data, alleviating hallucination issues by grounding the generated response in factual data. \n\nRAG enhances retrieval and generation through several key techniques which may include document chunking, embedding models, retrieval techniques, querying, knowledge graph integration, iterative retrieval and generation and self-reflection. Chunking breaks down large text into manageable sizes using a mix of static and semantic methods to maintain context [119,121]. Helps embedding models to represent information and queries in a way that maintains semantic meaning, improving retrieval accuracy [25]. RAG employs retrieval methods such as dense retrieval, which uses vector representations (cosine similarity) for semantic matching, and sparse encoding for keyword matches between queries and documents; with hybrid approaches combining the strengths of both [95,119]. Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109].",
            "score": 0.518042578491408,
            "section_title": "Retrieval augmented generation",
            "char_start_offset": 48408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "259145189",
            "title": "Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard",
            "text": "We examine two learned sparse retrieval models. Like their dense counterparts, these models rely on an approach to retrieval based on representation learning that exploits labeled datasets. However, these models generate sparse lexical representations instead of dense semantic ones. uniCOIL (noexp). This model, originally proposed by Lin and Ma [25], is a variant of COIL [12], where BERT is trained to assign Dataset nDCG@10 Recall@100 BM25 uniCOIL SPLADE TAS-B Contriever BM25 uniCOIL SPLADE TAS-B Contriever  2: Effectiveness results of five retrieval models across all 18 datasets in BEIR: nDCG@10 (left) and recall@100 (right). \n\nscalar weights to document tokens based on manually labeled relevance data (the MS MARCO passage dataset) to optimize retrieval effectiveness. In the full setting, uniCOIL depends on a separate document expansion model [28], but here we use the \"no expansion\" (noexp) variant, which allows us to examine the domain transfer capabilities of a \"basic\" learned term weighting function. \n\nSPLADE. This refers to a family of sparse retrieval models [9] that learns both document/query expansion and term weighting with the help of a regularization factor to induce sparsity. As far as we are aware, this represents the state of the art in sparse retrieval models, particularly in a zero-shot setting. More precisely, we use the SPLADE distil CoCondenser model, 3 which as the name implies, uses distillation and the pretrained CoCondenser model [11].",
            "score": 0.5179542860763593,
            "section_title": "Learned Sparse Retrieval Models",
            "char_start_offset": 13234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "249319719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Retrieval is to identify and obtain relevant information given an information need. Specifically, let's consider information resources that can be conceptualized as a key-value store, where each key corresponds to a value (keys and values can be identical). Given a query, the objective is to search for the top-k most similar keys using a similarity function, and obtain the paired values. Based on different similarity functions, existing retrieval methods can be categorized into sparse retrieval, dense retrieval, and others. In widely used sparse and dense retrieval, the entire process can be divided into two distinct phases: (i) each object is first encoded into a specific representation; and then (ii) an index is constructed to organize the data source for efficient search. \n\n1) Sparse Retriever: Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage Fig. 2: General architectures of several generators. term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics. \n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores. \n\n2) Dense Retriever: Unlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance.",
            "score": 0.517915879519205,
            "section_title": "C. Retriever",
            "char_start_offset": 12168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1627
                },
                {
                    "start": 1630,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 1050,
                    "end": 1054,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1942,
                    "end": 1946,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 2004,
                    "end": 2008,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 2142,
                    "end": 2146,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 2159,
                    "end": 2163,
                    "matchedPaperCorpusId": "8810481"
                },
                {
                    "start": 2176,
                    "end": 2180,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 2193,
                    "end": 2197,
                    "matchedPaperCorpusId": "81982618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "paperId": "63a1617af179ee8b5b096b3038913a19166168d4",
            "corpusId": 273026102,
            "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 16,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2232783785",
                    "name": "Shayekh Bin Islam"
                },
                {
                    "authorId": "2323863538",
                    "name": "Md Asib Rahman"
                },
                {
                    "authorId": "2323786676",
                    "name": "K. S. M. T. Hossain"
                },
                {
                    "authorId": "2274022429",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "3405393",
                    "name": "Md. Rizwan Parvez"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
            "corpus_id": "273026102",
            "text": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.43408203125
        },
        {
            "paperId": "66cbace0db91e9648e439d1530c021ae3199eb2e",
            "corpusId": 272310521,
            "title": "MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "referenceCount": 26,
            "citationCount": 2,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.17072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307559933",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2307454277",
                    "name": "Hainan Zhang"
                },
                {
                    "authorId": "2318979862",
                    "name": "Liang Pang"
                },
                {
                    "authorId": "2307558742",
                    "name": "Binghui Guo"
                },
                {
                    "authorId": "39133219",
                    "name": "Hongwei Zheng"
                },
                {
                    "authorId": "2319168951",
                    "name": "Zhiming Zheng"
                }
            ],
            "abstract": "In a real-world RAG system, the current query often involves spoken ellipses and ambiguous references from dialogue contexts, necessitating query rewriting to better describe user's information needs. However, traditional context-based rewriting has minimal enhancement on downstream generation tasks due to the lengthy process from query rewriting to response generation. Some researchers try to utilize reinforcement learning with generation feedback to assist the rewriter, but this sparse rewards provide little guidance in most cases, leading to unstable training and generation results.We find that user's needs are also reflected in the gold documents, retrieved documents and ground-truth. Therefore, by feeding back these multi-aspect dense rewards to query rewriting, more stable and satisfactory responses can be achieved. In this paper, we propose a novel query rewriting method MaFeRw, which improves RAG performance by integrating multi-aspect feedback from both the retrieval process and generated results. Specifically, we first use manual data to train a T5 model for the rewriter initialization. Next, we design three metrics as reinforcement learning feedback: the similarity between the rewritten query and the gold document, the ranking metrics, and ROUGE between the generation and the ground truth. Inspired by RLAIF, we train three kinds of reward models for the above metrics to achieve more efficient training. Finally, we combine the scores of these reward models as feedback, and use PPO algorithm to explore the optimal query rewriting strategy.Experimental results on two conversational RAG datasets demonstrate that MaFeRw achieves superior generation metrics and more stable training compared to baselines.",
            "corpus_id": "272310521",
            "text": "In a real-world RAG system, the current query often involves spoken ellipses and ambiguous references from dialogue contexts, necessitating query rewriting to better describe user's information needs. However, traditional context-based rewriting has minimal enhancement on downstream generation tasks due to the lengthy process from query rewriting to response generation. Some researchers try to utilize reinforcement learning with generation feedback to assist the rewriter, but this sparse rewards provide little guidance in most cases, leading to unstable training and generation results.We find that user's needs are also reflected in the gold documents, retrieved documents and ground-truth. Therefore, by feeding back these multi-aspect dense rewards to query rewriting, more stable and satisfactory responses can be achieved. In this paper, we propose a novel query rewriting method MaFeRw, which improves RAG performance by integrating multi-aspect feedback from both the retrieval process and generated results. Specifically, we first use manual data to train a T5 model for the rewriter initialization. Next, we design three metrics as reinforcement learning feedback: the similarity between the rewritten query and the gold document, the ranking metrics, and ROUGE between the generation and the ground truth. Inspired by RLAIF, we train three kinds of reward models for the above metrics to achieve more efficient training. Finally, we combine the scores of these reward models as feedback, and use PPO algorithm to explore the optimal query rewriting strategy.Experimental results on two conversational RAG datasets demonstrate that MaFeRw achieves superior generation metrics and more stable training compared to baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.228515625
        },
        {
            "paperId": "dfeaaf97e347b3317eba326ec8161fc6650fedc0",
            "corpusId": 273799974,
            "title": "Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 50,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2402.11035?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2402.11035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2216473237",
                    "name": "Benjamin Z. Reichman"
                },
                {
                    "authorId": "2290491646",
                    "name": "Larry Heck"
                }
            ],
            "abstract": "Dense passage retrieval (DPR) is the first step 001 in the retrieval augmented generation (RAG) 002 paradigm for improving the performance of 003 large language models (LLM). DPR fine-tunes 004 pre-trained networks to enhance the alignment 005 of the embeddings between queries and rele-006 vant textual data. A deeper understanding of 007 DPR fine-tuning will be required to fundamen-008 tally unlock the full potential of this approach. 009 In this work, we explore DPR-trained mod-010 els mechanistically by using a combination of 011 probing, layer activation analysis, and model 012 editing. Our experiments show that DPR train-013 ing decentralizes how knowledge is stored in 014 the network, creating multiple access pathways 015 to the same information. We also uncover a lim-016 itation in this training style: the internal knowl-017 edge of the pre-trained model bounds what the 018 retrieval model can retrieve. These findings 019 suggest a few possible directions for dense re-020 trieval: (1) expose the DPR training process to 021 more knowledge so more can be decentralized, 022 (2) inject facts as decentralized representations, 023 (3) model and incorporate knowledge uncer-024 tainty in the retrieval process, and (4) directly 025 map internal model knowledge to a knowledge 026 base. 027",
            "corpus_id": "273799974",
            "text": "Dense passage retrieval (DPR) is the first step 001 in the retrieval augmented generation (RAG) 002 paradigm for improving the performance of 003 large language models (LLM). DPR fine-tunes 004 pre-trained networks to enhance the alignment 005 of the embeddings between queries and rele-006 vant textual data. A deeper understanding of 007 DPR fine-tuning will be required to fundamen-008 tally unlock the full potential of this approach. 009 In this work, we explore DPR-trained mod-010 els mechanistically by using a combination of 011 probing, layer activation analysis, and model 012 editing. Our experiments show that DPR train-013 ing decentralizes how knowledge is stored in 014 the network, creating multiple access pathways 015 to the same information. We also uncover a lim-016 itation in this training style: the internal knowl-017 edge of the pre-trained model bounds what the 018 retrieval model can retrieve. These findings 019 suggest a few possible directions for dense re-020 trieval: (1) expose the DPR training process to 021 more knowledge so more can be decentralized, 022 (2) inject facts as decentralized representations, 023 (3) model and incorporate knowledge uncer-024 tainty in the retrieval process, and (4) directly 025 map internal model knowledge to a knowledge 026 base. 027",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.49365234375
        },
        {
            "paperId": "42d1dfab4a35583cac1e522a652800f0093285ff",
            "corpusId": 274510282,
            "title": "Enhancing Large Language Model Reliability: Minimizing Hallucinations with Dual Retrieval-Augmented Generation Based on the Latest Diabetes Guidelines",
            "venue": "Journal of Personalized Medicine",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 2,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11677479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108374658",
                    "name": "Jaedong Lee"
                },
                {
                    "authorId": "5318815",
                    "name": "H. Cha"
                },
                {
                    "authorId": "39464775",
                    "name": "Y. Hwangbo"
                },
                {
                    "authorId": "3801490",
                    "name": "W. Cheon"
                }
            ],
            "abstract": "Background/Objectives: Large language models (LLMs) show promise in healthcare but face challenges with hallucinations, particularly in rapidly evolving fields like diabetes management. Traditional LLM updating methods are resource-intensive, necessitating new approaches for delivering reliable, current medical information. This study aimed to develop and evaluate a novel retrieval system to enhance LLM reliability in diabetes management across different languages and guidelines. Methods: We developed a dual retrieval-augmented generation (RAG) system integrating both Korean Diabetes Association and American Diabetes Association 2023 guidelines. The system employed dense retrieval with 11 embedding models (including OpenAI, Upstage, and multilingual models) and sparse retrieval using BM25 algorithm with language-specific tokenizers. Performance was evaluated across different top-k values, leading to optimized ensemble retrievers for each guideline. Results: For dense retrievers, Upstage\u2019s Solar Embedding-1-large and OpenAI\u2019s text-embedding-3-large showed superior performance for Korean and English guidelines, respectively. Multilingual models outperformed language-specific models in both cases. For sparse retrievers, the ko_kiwi tokenizer demonstrated superior performance for Korean text, while both ko_kiwi and porter_stemmer showed comparable effectiveness for English text. The ensemble retrievers, combining optimal dense and sparse configurations, demonstrated enhanced coverage while maintaining precision. Conclusions: This study presents an effective dual RAG system that enhances LLM reliability in diabetes management across different languages. The successful implementation with both Korean and American guidelines demonstrates the system\u2019s cross-regional capability, laying a foundation for more trustworthy AI-assisted healthcare applications.",
            "corpus_id": "274510282",
            "text": "Background/Objectives: Large language models (LLMs) show promise in healthcare but face challenges with hallucinations, particularly in rapidly evolving fields like diabetes management. Traditional LLM updating methods are resource-intensive, necessitating new approaches for delivering reliable, current medical information. This study aimed to develop and evaluate a novel retrieval system to enhance LLM reliability in diabetes management across different languages and guidelines. Methods: We developed a dual retrieval-augmented generation (RAG) system integrating both Korean Diabetes Association and American Diabetes Association 2023 guidelines. The system employed dense retrieval with 11 embedding models (including OpenAI, Upstage, and multilingual models) and sparse retrieval using BM25 algorithm with language-specific tokenizers. Performance was evaluated across different top-k values, leading to optimized ensemble retrievers for each guideline. Results: For dense retrievers, Upstage\u2019s Solar Embedding-1-large and OpenAI\u2019s text-embedding-3-large showed superior performance for Korean and English guidelines, respectively. Multilingual models outperformed language-specific models in both cases. For sparse retrievers, the ko_kiwi tokenizer demonstrated superior performance for Korean text, while both ko_kiwi and porter_stemmer showed comparable effectiveness for English text. The ensemble retrievers, combining optimal dense and sparse configurations, demonstrated enhanced coverage while maintaining precision. Conclusions: This study presents an effective dual RAG system that enhances LLM reliability in diabetes management across different languages. The successful implementation with both Korean and American guidelines demonstrates the system\u2019s cross-regional capability, laying a foundation for more trustworthy AI-assisted healthcare applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.77783203125
        },
        {
            "paperId": "f21c2fe1519d5be5e005caa0b54c1f3be004841f",
            "corpusId": 279075388,
            "title": "Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval",
            "venue": "",
            "year": 2025,
            "referenceCount": 39,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.00041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352509944",
                    "name": "Seongwan Park"
                },
                {
                    "authorId": "2364814758",
                    "name": "Taeklim Kim"
                },
                {
                    "authorId": "2352338156",
                    "name": "Youngjoong Ko"
                }
            ],
            "abstract": "Despite their strong performance, Dense Passage Retrieval (DPR) models suffer from a lack of interpretability. In this work, we propose a novel interpretability framework that leverages Sparse Autoencoders (SAEs) to decompose previously uninterpretable dense embeddings from DPR models into distinct, interpretable latent concepts. We generate natural language descriptions for each latent concept, enabling human interpretations of both the dense embeddings and the query-document similarity scores of DPR models. We further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework that directly utilizes the extracted latent concepts as indexing units. CL-SR effectively combines the semantic expressiveness of dense embeddings with the transparency and efficiency of sparse representations. We show that CL-SR achieves high index-space and computational efficiency while maintaining robust performance across vocabulary and semantic mismatches.",
            "corpus_id": "279075388",
            "text": "Despite their strong performance, Dense Passage Retrieval (DPR) models suffer from a lack of interpretability. In this work, we propose a novel interpretability framework that leverages Sparse Autoencoders (SAEs) to decompose previously uninterpretable dense embeddings from DPR models into distinct, interpretable latent concepts. We generate natural language descriptions for each latent concept, enabling human interpretations of both the dense embeddings and the query-document similarity scores of DPR models. We further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework that directly utilizes the extracted latent concepts as indexing units. CL-SR effectively combines the semantic expressiveness of dense embeddings with the transparency and efficiency of sparse representations. We show that CL-SR achieves high index-space and computational efficiency while maintaining robust performance across vocabulary and semantic mismatches.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.86083984375
        },
        {
            "paperId": "a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
            "corpusId": 267212215,
            "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 18,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.14021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185953295",
                    "name": "Zhihao Zhang"
                },
                {
                    "authorId": "2231613073",
                    "name": "Alan Zhu"
                },
                {
                    "authorId": "2231664587",
                    "name": "Lijie Yang"
                },
                {
                    "authorId": "2281208130",
                    "name": "Yihua Xu"
                },
                {
                    "authorId": "2281325214",
                    "name": "Lanting Li"
                },
                {
                    "authorId": "2623548",
                    "name": "P. Phothilimthana"
                },
                {
                    "authorId": "2271058444",
                    "name": "Zhihao Jia"
                }
            ],
            "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline.",
            "corpus_id": "267212215",
            "text": "Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5361328125
        },
        {
            "paperId": "b4778c6945531373eb81db093d89f9f665214b71",
            "corpusId": 278936289,
            "title": "Evaluating Sparse and Dense Retrieval in Retrieval-Augmented Generation Systems: A Study",
            "venue": "International Conference on Critical Infrastructure Protection",
            "year": 2024,
            "referenceCount": 16,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3708657.3708747?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3708657.3708747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347750814",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2347743427",
                    "name": "Guanlin Dai"
                },
                {
                    "authorId": "2363736606",
                    "name": "Song Ke"
                },
                {
                    "authorId": "2349550661",
                    "name": "Chao Zheng"
                }
            ],
            "abstract": "With the rapid advancement of large-scale pre-trained models, their capabilities have found widespread applications across various fields, leading to increasing demand from individual users. However, personal users often face challenges such as concerns over data security and limitations in hardware resources, making the development and deployment of these models a complex task. Retrieval-Augmented Generation (RAG) systems offer a promising solution by enhancing a model\u2019s ability to acquire knowledge through the integration of external knowledge bases, without the need for extensive training. Nevertheless, RAG systems employ a variety of retrieval algorithms, and different configurations can significantly impact both resource consumption and performance. As a result, choosing the right retrieval algorithm has become a critical area of research. This study evaluates and compares sparse and dense retrieval algorithms, aiming to identify how RAG system performance can be optimized under varying resource constraints and user requirements. The experimental results demonstrate that a well-matched combination of Large-scale Language Model and retrieval algorithm can maximize system performance under specific hardware conditions, offering new strategies for individual users to efficiently leverage large models.",
            "corpus_id": "278936289",
            "text": "With the rapid advancement of large-scale pre-trained models, their capabilities have found widespread applications across various fields, leading to increasing demand from individual users. However, personal users often face challenges such as concerns over data security and limitations in hardware resources, making the development and deployment of these models a complex task. Retrieval-Augmented Generation (RAG) systems offer a promising solution by enhancing a model\u2019s ability to acquire knowledge through the integration of external knowledge bases, without the need for extensive training. Nevertheless, RAG systems employ a variety of retrieval algorithms, and different configurations can significantly impact both resource consumption and performance. As a result, choosing the right retrieval algorithm has become a critical area of research. This study evaluates and compares sparse and dense retrieval algorithms, aiming to identify how RAG system performance can be optimized under varying resource constraints and user requirements. The experimental results demonstrate that a well-matched combination of Large-scale Language Model and retrieval algorithm can maximize system performance under specific hardware conditions, offering new strategies for individual users to efficiently leverage large models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.79150390625
        },
        {
            "paperId": "a76f6feca7b1953c7bdbe5a8c7190d75ebc5529a",
            "corpusId": 279244766,
            "title": "Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion",
            "venue": "",
            "year": 2025,
            "referenceCount": 38,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.04760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366142077",
                    "name": "Lingyuan Liu"
                },
                {
                    "authorId": "2366090070",
                    "name": "Mengxiang Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.",
            "corpus_id": "279244766",
            "text": "Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.47314453125
        },
        {
            "paperId": "19edfb636a14f058f5d87db9f0519b393829ed6c",
            "corpusId": 272689590,
            "title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications",
            "venue": "CUSTOMNLP4U",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 7,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.09046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "80605886",
                    "name": "Rishi Kalra"
                },
                {
                    "authorId": "2284029866",
                    "name": "Zekun Wu"
                },
                {
                    "authorId": "2321025778",
                    "name": "Ayesha Gulley"
                },
                {
                    "authorId": "2128410682",
                    "name": "Airlie Hilliard"
                },
                {
                    "authorId": "2308070610",
                    "name": "Xin Guan"
                },
                {
                    "authorId": "2268316579",
                    "name": "A. Koshiyama"
                },
                {
                    "authorId": "2244619460",
                    "name": "Philip C. Treleaven"
                }
            ],
            "abstract": "While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy applications is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications.",
            "corpus_id": "272689590",
            "text": "While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy applications is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.407958984375
        },
        {
            "paperId": "0b374b43c171bc63935e2b01147dcb1857cb56d3",
            "corpusId": 277628313,
            "title": "Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 61,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302560355",
                    "name": "Chandana Sree Mala"
                },
                {
                    "authorId": "2302559951",
                    "name": "Gizem Gezici"
                },
                {
                    "authorId": "2294580587",
                    "name": "Fosca Giannotti"
                }
            ],
            "abstract": "Large Language Models (LLMs) excel in language comprehension and generation but are prone to hallucinations, producing factually incorrect or unsupported outputs. Retrieval Augmented Generation (RAG) systems address this issue by grounding LLM responses with external knowledge. This study evaluates the relationship between retriever effectiveness and hallucination reduction in LLMs using three retrieval approaches: sparse retrieval based on BM25 keyword search, dense retrieval using semantic search with Sentence Transformers, and a proposed hybrid retrieval module. The hybrid module incorporates query expansion and combines the results of sparse and dense retrievers through a dynamically weighted Reciprocal Rank Fusion score. Using the HaluBench dataset, a benchmark for hallucinations in question answering tasks, we assess retrieval performance with metrics such as mean average precision and normalised discounted cumulative gain, focusing on the relevance of the top three retrieved documents. Results show that the hybrid retriever achieves better relevance scores, outperforming both sparse and dense retrievers. Further evaluation of LLM-generated answers against ground truth using metrics such as accuracy, hallucination rate, and rejection rate reveals that the hybrid retriever achieves the highest accuracy on fails, the lowest hallucination rate, and the lowest rejection rate. These findings highlight the hybrid retriever's ability to enhance retrieval relevance, reduce hallucination rates, and improve LLM reliability, emphasising the importance of advanced retrieval techniques in mitigating hallucinations and improving response accuracy.",
            "corpus_id": "277628313",
            "text": "Large Language Models (LLMs) excel in language comprehension and generation but are prone to hallucinations, producing factually incorrect or unsupported outputs. Retrieval Augmented Generation (RAG) systems address this issue by grounding LLM responses with external knowledge. This study evaluates the relationship between retriever effectiveness and hallucination reduction in LLMs using three retrieval approaches: sparse retrieval based on BM25 keyword search, dense retrieval using semantic search with Sentence Transformers, and a proposed hybrid retrieval module. The hybrid module incorporates query expansion and combines the results of sparse and dense retrievers through a dynamically weighted Reciprocal Rank Fusion score. Using the HaluBench dataset, a benchmark for hallucinations in question answering tasks, we assess retrieval performance with metrics such as mean average precision and normalised discounted cumulative gain, focusing on the relevance of the top three retrieved documents. Results show that the hybrid retriever achieves better relevance scores, outperforming both sparse and dense retrievers. Further evaluation of LLM-generated answers against ground truth using metrics such as accuracy, hallucination rate, and rejection rate reveals that the hybrid retriever achieves the highest accuracy on fails, the lowest hallucination rate, and the lowest rejection rate. These findings highlight the hybrid retriever's ability to enhance retrieval relevance, reduce hallucination rates, and improve LLM reliability, emphasising the importance of advanced retrieval techniques in mitigating hallucinations and improving response accuracy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.88525390625
        },
        {
            "paperId": "cf3563b8d71a75cc86a82f8f6c12714d423736cb",
            "corpusId": 278904858,
            "title": "RaDeR: Reasoning-aware Dense Retrieval Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 47,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2363518947",
                    "name": "Debrup Das"
                },
                {
                    "authorId": "2363492916",
                    "name": "Sam O' Nuallain"
                },
                {
                    "authorId": "2363493475",
                    "name": "Razieh Rahimi"
                }
            ],
            "abstract": "We propose RaDeR, a set of reasoning-based dense retrieval models trained with data derived from mathematical problem solving using large language models (LLMs). Our method leverages retrieval-augmented reasoning trajectories of an LLM and self-reflective relevance evaluation, enabling the creation of both diverse and hard-negative samples for reasoning-intensive relevance. RaDeR retrievers, trained for mathematical reasoning, effectively generalize to diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently outperforming strong baselines in overall performance. Notably, RaDeR achieves significantly higher performance than baselines on the Math and Coding splits. In addition, RaDeR presents the first dense retriever that outperforms BM25 when queries are Chain-of-Thought reasoning steps, underscoring the critical role of reasoning-based retrieval to augment reasoning language models. Furthermore, RaDeR achieves comparable or superior performance while using only 2.5% of the training data used by the concurrent work REASONIR, highlighting the quality of our synthesized training data.",
            "corpus_id": "278904858",
            "text": "We propose RaDeR, a set of reasoning-based dense retrieval models trained with data derived from mathematical problem solving using large language models (LLMs). Our method leverages retrieval-augmented reasoning trajectories of an LLM and self-reflective relevance evaluation, enabling the creation of both diverse and hard-negative samples for reasoning-intensive relevance. RaDeR retrievers, trained for mathematical reasoning, effectively generalize to diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently outperforming strong baselines in overall performance. Notably, RaDeR achieves significantly higher performance than baselines on the Math and Coding splits. In addition, RaDeR presents the first dense retriever that outperforms BM25 when queries are Chain-of-Thought reasoning steps, underscoring the critical role of reasoning-based retrieval to augment reasoning language models. Furthermore, RaDeR achieves comparable or superior performance while using only 2.5% of the training data used by the concurrent work REASONIR, highlighting the quality of our synthesized training data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.43896484375
        },
        {
            "paperId": "c62686ad8abfe20148d3d94843dbddedfc02c078",
            "corpusId": 277550342,
            "title": "Enhanced Privacy Policy Comprehension via Pre-trained and Retrieval-Augmented Models",
            "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TrustCom63139.2024.00094?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TrustCom63139.2024.00094, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316805476",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2288179195",
                    "name": "Bingbing Zhang"
                },
                {
                    "authorId": "2261901004",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2118564",
                    "name": "Lingbo Wei"
                }
            ],
            "abstract": "Privacy policies are crucial for informing users about how their personal information is collected, stored, and used by organizations. However, privacy policies are lengthy, information-dense, and filled with legalese, making them difficult for users to comprehend. Although previous studies have attempted to improve the readability of privacy policies via traditional machine learning techniques, these methods overlook or oversimplify critical information or rely on predefined question-answers that cannot adapt to user personalized queries. Inspired by the fact that large language models perform well in terms of text comprehension and text question-answering, we propose a novel large language model-based privacy policy question-answering framework, which aims to help users understand privacy policies more intuitively and effectively. Specifically, our framework consists of two modules: the personal data practice disclosure module, which performs a pre-training and fine-tuning approach to extract structured information about data categories and corresponding data operations from privacy policies. The retrieval-augmented question-answering module integrates sparse and dense retrievers to find the most relevant evidence from the privacy policy and generate responses corresponding to user queries. The experimental results across two representative datasets demonstrate the superiority of our method over other prior methods.",
            "corpus_id": "277550342",
            "text": "Privacy policies are crucial for informing users about how their personal information is collected, stored, and used by organizations. However, privacy policies are lengthy, information-dense, and filled with legalese, making them difficult for users to comprehend. Although previous studies have attempted to improve the readability of privacy policies via traditional machine learning techniques, these methods overlook or oversimplify critical information or rely on predefined question-answers that cannot adapt to user personalized queries. Inspired by the fact that large language models perform well in terms of text comprehension and text question-answering, we propose a novel large language model-based privacy policy question-answering framework, which aims to help users understand privacy policies more intuitively and effectively. Specifically, our framework consists of two modules: the personal data practice disclosure module, which performs a pre-training and fine-tuning approach to extract structured information about data categories and corresponding data operations from privacy policies. The retrieval-augmented question-answering module integrates sparse and dense retrievers to find the most relevant evidence from the privacy policy and generate responses corresponding to user queries. The experimental results across two representative datasets demonstrate the superiority of our method over other prior methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.325439453125
        },
        {
            "paperId": "c3f0600830c75d363c89b916ef1f2a222e9b1483",
            "corpusId": 273654514,
            "title": "Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP Technical Specifications",
            "venue": "IEEE Wireless Communications and Networking Conference",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2410.19790",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.19790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1578253722",
                    "name": "Thaina Saraiva"
                },
                {
                    "authorId": "1399856637",
                    "name": "M. Sousa"
                },
                {
                    "authorId": "2238062459",
                    "name": "Pedro Vieira"
                },
                {
                    "authorId": "145636822",
                    "name": "Ant\u00f3nio Rodrigues"
                }
            ],
            "abstract": "This paper proposes a Question-Answering (QA) system for the telecom domain using 3rd Generation Partnership Project (3GPP) technical documents. Alongside, a hybrid dataset, Telco-DPR11Telco-DPR dataset presented in this paper is available open-source [1]., which consists of a curated 3GPP corpus in a hybrid format, combining text and tables, is presented. Additionally, the dataset includes a set of synthetic question/answer pairs designed to evaluate the retrieval performance of QA systems on this type of data. The retrieval models, including the sparse model, Best Matching 25 (BM25), as well as dense models, such as Dense Passage Retriever (DPR) and Dense Hierarchical Retrieval (DHR), are evaluated and compared using top-K accuracy and Mean Reciprocal Rank (MRR). The results show that DHR, a retriever model utilising hierarchical passage selection through fine-tuning at both the document and passage levels, outperforms traditional methods in retrieving relevant technical information, achieving a Top-10 accuracy of 86.2 %. Additionally, the Retriever-Augmented Generation (RAG) technique, used in the proposed QA system, is evaluated to demonstrate the benefits of using the hybrid dataset and the DHR. The proposed QA system, using the developed RAG model and the Generative Pre-trained Transformer (GPT)-4, achieves a 14% improvement in answer accuracy, when compared to a previous benchmark on the same dataset.",
            "corpus_id": "273654514",
            "text": "This paper proposes a Question-Answering (QA) system for the telecom domain using 3rd Generation Partnership Project (3GPP) technical documents. Alongside, a hybrid dataset, Telco-DPR11Telco-DPR dataset presented in this paper is available open-source [1]., which consists of a curated 3GPP corpus in a hybrid format, combining text and tables, is presented. Additionally, the dataset includes a set of synthetic question/answer pairs designed to evaluate the retrieval performance of QA systems on this type of data. The retrieval models, including the sparse model, Best Matching 25 (BM25), as well as dense models, such as Dense Passage Retriever (DPR) and Dense Hierarchical Retrieval (DHR), are evaluated and compared using top-K accuracy and Mean Reciprocal Rank (MRR). The results show that DHR, a retriever model utilising hierarchical passage selection through fine-tuning at both the document and passage levels, outperforms traditional methods in retrieving relevant technical information, achieving a Top-10 accuracy of 86.2 %. Additionally, the Retriever-Augmented Generation (RAG) technique, used in the proposed QA system, is evaluated to demonstrate the benefits of using the hybrid dataset and the DHR. The proposed QA system, using the developed RAG model and the Generative Pre-trained Transformer (GPT)-4, achieves a 14% improvement in answer accuracy, when compared to a previous benchmark on the same dataset.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.61669921875
        },
        {
            "paperId": "3d9ac85db7c2bdd79a9922695dbf0033275e4fd0",
            "corpusId": 275668365,
            "title": "A Retrieval-Augmented Generation Framework for Electric Power Industry Question Answering",
            "venue": "Proceedings of the 2024 2nd International Conference on Electronics, Computers and Communication Technology",
            "year": 2024,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3705754.3705771?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3705754.3705771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166274497",
                    "name": "Yanyan Lu"
                },
                {
                    "authorId": "2243329915",
                    "name": "Jiao Peng"
                },
                {
                    "authorId": "2328504535",
                    "name": "Xing Xu"
                },
                {
                    "authorId": "2324338573",
                    "name": "Yue He"
                },
                {
                    "authorId": "2341337003",
                    "name": "Tao Li"
                },
                {
                    "authorId": "2340836032",
                    "name": "Jie Wei"
                },
                {
                    "authorId": "2340793548",
                    "name": "Hongyu Jing"
                },
                {
                    "authorId": "2340967375",
                    "name": "Heqing Wang"
                },
                {
                    "authorId": "2172451435",
                    "name": "Bo Xu"
                },
                {
                    "authorId": "2309675817",
                    "name": "Hui Song"
                }
            ],
            "abstract": "Retrieval-augmented Generation has achieved significant success in improving the performance of large language models by utilizing external knowledge sources. However, despite its advancements, it still faces challenges in domain-specific structured documents such as the electric power industry, exhibiting low recall rates and response inaccuracies. To solve these issues, this paper designs a question-answering framework specifically for the electric power industry using the large language model. This study first analyzes the characteristics of documents in the electric power industry and proposes the Hierarchical Adaptive Semantic Segmentation(HASS) method, which improves the accuracy of responses and the precision of queries by subdividing knowledge points and integrating metadata. In the retrieval strategy, a Hybrid Search(HS) method is designed, combining the advantages of sparse and dense retrieval to improve the recall rate. To enable large language models to focus on relevant documents when generating responses, we propose the Irrelevant Document Filtering(IDF) method for minimizing the noise impact from irrelevant documents. Additionally, the model is designed with prompts and fine-tuned to further enhance its ability to utilize context and the accuracy of answer generation. Finally, due to the lack of publicly available datasets for document question-answering in the electricity sector, this work constructs a dataset manually for training and evaluation, comprising 1300 QA items covering several types of questions. Experimental results demonstrate that the methods proposed in this paper effectively improve the accuracy of LLMs in the electric power industry, with our method showing a 9% improvement in accuracy over the traditional RAG approach.",
            "corpus_id": "275668365",
            "text": "Retrieval-augmented Generation has achieved significant success in improving the performance of large language models by utilizing external knowledge sources. However, despite its advancements, it still faces challenges in domain-specific structured documents such as the electric power industry, exhibiting low recall rates and response inaccuracies. To solve these issues, this paper designs a question-answering framework specifically for the electric power industry using the large language model. This study first analyzes the characteristics of documents in the electric power industry and proposes the Hierarchical Adaptive Semantic Segmentation(HASS) method, which improves the accuracy of responses and the precision of queries by subdividing knowledge points and integrating metadata. In the retrieval strategy, a Hybrid Search(HS) method is designed, combining the advantages of sparse and dense retrieval to improve the recall rate. To enable large language models to focus on relevant documents when generating responses, we propose the Irrelevant Document Filtering(IDF) method for minimizing the noise impact from irrelevant documents. Additionally, the model is designed with prompts and fine-tuned to further enhance its ability to utilize context and the accuracy of answer generation. Finally, due to the lack of publicly available datasets for document question-answering in the electricity sector, this work constructs a dataset manually for training and evaluation, comprising 1300 QA items covering several types of questions. Experimental results demonstrate that the methods proposed in this paper effectively improve the accuracy of LLMs in the electric power industry, with our method showing a 9% improvement in accuracy over the traditional RAG approach.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.3935546875
        },
        {
            "paperId": "d47bb3997a81e904ec696aa05bf90bfd7c22f47c",
            "corpusId": 277382311,
            "title": "MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2401.06311?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2401.06311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279401048",
                    "name": "Le Zhang"
                },
                {
                    "authorId": "2261148799",
                    "name": "Yihong Wu"
                },
                {
                    "authorId": "2309219030",
                    "name": "Qian Yang"
                },
                {
                    "authorId": "2363348621",
                    "name": "Jian-Yun Nie"
                }
            ],
            "abstract": "Large Language Models (LLMs) have become a cornerstone in language technologies, notably in information retrieval (IR). Benefiting from vast training datasets, LLMs excel in developing nuanced understandings and processing of natural language queries, a crucial capability required by IR. Motivated by this, we propose a straightforward yet potent framework, Multi-Text Generation Integration (MuGI), designed to bolster IR systems. MuGI leverages LLMs to produce multiple pseudo references, integrating them with queries through a dynamic repetition strategy applicable to both sparse and dense retrievers. This training-free approach demonstrates superior performance over traditional query expansion methods. Specifically, BM25 enhanced with MuGI outperforms advanced dense retrievers like ANCE, showing a remarkable improvement of over 18% on the TREC DL dataset and 7.5% on BEIR. MuGI establishes a swift and high-fidelity information retrieval pipeline, enabling a compact 110M model to exceed the performance of larger 7B models in in-domain tests and effectively narrowing the gap in out-of-distribution scenarios. Our code and all generated references are made available at https://github.com/lezhang7/ Retrieval_MuGI",
            "corpus_id": "277382311",
            "text": "Large Language Models (LLMs) have become a cornerstone in language technologies, notably in information retrieval (IR). Benefiting from vast training datasets, LLMs excel in developing nuanced understandings and processing of natural language queries, a crucial capability required by IR. Motivated by this, we propose a straightforward yet potent framework, Multi-Text Generation Integration (MuGI), designed to bolster IR systems. MuGI leverages LLMs to produce multiple pseudo references, integrating them with queries through a dynamic repetition strategy applicable to both sparse and dense retrievers. This training-free approach demonstrates superior performance over traditional query expansion methods. Specifically, BM25 enhanced with MuGI outperforms advanced dense retrievers like ANCE, showing a remarkable improvement of over 18% on the TREC DL dataset and 7.5% on BEIR. MuGI establishes a swift and high-fidelity information retrieval pipeline, enabling a compact 110M model to exceed the performance of larger 7B models in in-domain tests and effectively narrowing the gap in out-of-distribution scenarios. Our code and all generated references are made available at https://github.com/lezhang7/ Retrieval_MuGI",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.373046875
        },
        {
            "paperId": "696f8823161606bee7e54fb23cb8f5fab7ec8438",
            "corpusId": 276820391,
            "title": "Efficient and verifiable responses using Retrieval Augmented Generation (RAG)",
            "venue": "International Conference on AI-ML-Systems",
            "year": 2024,
            "referenceCount": 12,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3703412.3703431?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3703412.3703431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349311416",
                    "name": "Henry Liang"
                },
                {
                    "authorId": "2343698112",
                    "name": "Yu Zhou"
                },
                {
                    "authorId": "1734558",
                    "name": "V. Gurbani"
                }
            ],
            "abstract": "The rise of large language models (LLMs) like ChatGPT has greatly enhanced the efficiency of everyday tasks through automation. However, the deployment of LLMs for tasks such as responding to Request-for-Proposals (RFPs) is hindered by deficiencies like hallucinations and lack of response provenance. For such tasks, the aim of an automated response is to generate precise answers that can still be quickly reviewed and corrected by a human; therefore it is critical to optimize the system such that relevant source document sections are identified for as many questions as possible, and all relevant contexts are attributed correctly; this makes LLMs alone insufficient for this task. We present an improved Retrieval Augmented Generation (RAG) architecture for automated RFP completion that enhances relevant content generation and significantly reduces manual effort in drafting responses. The proposed improvements are two-fold: we present a novel text embedding scheme that combines a dense contextual embedding with a sparse statistical embedding for document retrieval, and we improve on the provenance of the generated response by presenting an algorithm that accurately provides the document page numbers as references when generating the answers. The practical deployment of this solution highlights its potential for automatic RFP completion, as well as its ability to act as an architecture for applications in various domains with differing complexity levels, especially when efficiency, accuracy, and verifiable responses are paramount.",
            "corpus_id": "276820391",
            "text": "The rise of large language models (LLMs) like ChatGPT has greatly enhanced the efficiency of everyday tasks through automation. However, the deployment of LLMs for tasks such as responding to Request-for-Proposals (RFPs) is hindered by deficiencies like hallucinations and lack of response provenance. For such tasks, the aim of an automated response is to generate precise answers that can still be quickly reviewed and corrected by a human; therefore it is critical to optimize the system such that relevant source document sections are identified for as many questions as possible, and all relevant contexts are attributed correctly; this makes LLMs alone insufficient for this task. We present an improved Retrieval Augmented Generation (RAG) architecture for automated RFP completion that enhances relevant content generation and significantly reduces manual effort in drafting responses. The proposed improvements are two-fold: we present a novel text embedding scheme that combines a dense contextual embedding with a sparse statistical embedding for document retrieval, and we improve on the provenance of the generated response by presenting an algorithm that accurately provides the document page numbers as references when generating the answers. The practical deployment of this solution highlights its potential for automatic RFP completion, as well as its ability to act as an architecture for applications in various domains with differing complexity levels, especially when efficiency, accuracy, and verifiable responses are paramount.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.40576171875
        }
    ],
    "quotes": {
        "cost": 0.29531099999999993,
        "quotes": [
            {
                "idx": 0,
                "key": "[237304122 | Parvez et al. | 2021 | Citations: 189]",
                "snippets": "Term-based retrieval methods, a.k.a. sparse retrieval models, such as TF-IDF or BM25 (Robertson and Zaragoza, 2009) use sparse vector representations to perform lexical matching and compute relevance scores to rank the documents based on a query. On the other hand, dense retrieval methods encode documents into a fixed-size representations and retrieve documents via maximum inner product search (Sutskever et al., 2014)Guo et al., 2016).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7961699 | Sutskever et al. | 2014 | Citations: 20596]": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
                },
                "metadata": [
                    {
                        "section_title": "Retriever: DPR",
                        "pdf_hash": "",
                        "start": 186,
                        "end": 625,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "7961699"
                        ],
                        "quote": "Term-based retrieval methods, a.k.a. sparse retrieval models, such as TF-IDF or BM25 (Robertson and Zaragoza, 2009) use sparse vector representations to perform lexical matching and compute relevance scores to rank the documents based on a query. On the other hand, dense retrieval methods encode documents into a fixed-size representations and retrieve documents via maximum inner product search (Sutskever et al., 2014)Guo et al., 2016)."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[237562875 | Sciavolino et al. | 2021 | Citations: 167]",
                "snippets": "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background and Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 714,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[238259539 | Lin | 2021 | Citations: 53]",
                "snippets": "Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as (Karpukhin et al., 2020) did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Generalization of Logical Scoring Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1315,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 174
                            },
                            {
                                "start": 175,
                                "end": 262
                            },
                            {
                                "start": 265,
                                "end": 305
                            },
                            {
                                "start": 306,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 567
                            },
                            {
                                "start": 570,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 790
                            },
                            {
                                "start": 793,
                                "end": 1014
                            },
                            {
                                "start": 1015,
                                "end": 1112
                            },
                            {
                                "start": 1113,
                                "end": 1147
                            },
                            {
                                "start": 1148,
                                "end": 1253
                            },
                            {
                                "start": 1254,
                                "end": 1315
                            }
                        ],
                        "ref_mentions": [
                            "215737187"
                        ],
                        "quote": "Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as (Karpukhin et al., 2020) did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[238744204 | Chen et al. | 2021 | Citations: 69]",
                "snippets": "On the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, (Karpukhin et al., 2020) show that DPR, unlike a sparse BM25 retriever (Robertson et al., 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, (Sciavolino et al., 2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and (Thakur et al., 2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237562875 | Sciavolino et al. | 2021 | Citations: 167]": "Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., \u201cWhere was Arve Furset born?\u201d), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[233296016 | Thakur et al. | 2021 | Citations: 1055]": "Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1017,
                        "end": 1826,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "215737187",
                            "2218552",
                            "237562875",
                            "233296016"
                        ],
                        "quote": "On the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, (Karpukhin et al., 2020) show that DPR, unlike a sparse BM25 retriever (Robertson et al., 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, (Sciavolino et al., 2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and (Thakur et al., 2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[245334864 | Piktus et al. | 2021 | Citations: 66]",
                "snippets": "We consider two retrieval architectures. BM25 (Robertson et al., 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 778,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 40
                            },
                            {
                                "start": 41,
                                "end": 261
                            },
                            {
                                "start": 262,
                                "end": 494
                            },
                            {
                                "start": 495,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 778
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "215737187"
                        ],
                        "quote": "We consider two retrieval architectures. BM25 (Robertson et al., 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[247292113 | Long et al. | 2022 | Citations: 23]",
                "snippets": "Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1031,
                        "end": 1377,
                        "sentence_offsets": [
                            {
                                "start": 1031,
                                "end": 1377
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[248366550 | Penha et al. | 2022 | Citations: 0]",
                "snippets": "An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods.\n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods.\n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[248496043 | Li et al. | 2022 | Citations: 30]",
                "snippets": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220302524 | Xiong et al. | 2020 | Citations: 1234]": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                    "[231815627 | Qu et al. | 2020 | Citations: 617]": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
                    "[233231706 | Hofstatter et al. | 2021 | Citations: 402]": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                    "[235720578 | Lin et al. | 2021 | Citations: 179]": "We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the ColBERT late-interaction ranking model. Specifically, we propose to transfer the knowledge from a bi-encoder teacher to a student by distilling knowledge from ColBERT\u2019s expressive MaxSim operator into a simple dot product. The advantage of the bi-encoder teacher\u2013student setup is that we can efficiently add in-batch negatives during knowledge distillation, enabling richer interactions between teacher and student models. In addition, using ColBERT as the teacher reduces training cost compared to a full cross-encoder. Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently.",
                    "[235792476 | Zhuang et al. | 2021 | Citations: 76]": "Deep language models (deep LMs) are increasingly being used for full text retrieval or within cascade retrieval pipelines as later-stage re-rankers. A problem with using deep LMs is that, at query time, a slow inference step needs to be performed -- this hinders the practical adoption of these powerful retrieval models, or limits sensibly how many documents can be considered for re-ranking. We propose the novel, BERT-based, Term Independent Likelihood moDEl (TILDE), which ranks documents by both query and document likelihood. At query time, our model does not require the inference step of deep language models based retrieval approaches, thus providing consistent time-savings, as the prediction of query terms' likelihood can be pre-computed and stored during index creation. This is achieved by relaxing the term dependence assumption made by the deep LMs. In addition, we have devised a novel bi-directional training loss which allows TILDE to maximise both query and document likelihood at the same time during training. At query time, TILDE can rely on its query likelihood component (TILDE-QL) solely, or the combination of TILDE-QL and its document likelihood component (TILDE-DL), thus providing a flexible trade-off between efficiency and effectiveness. Exploiting both components provide the highest effectiveness at a higher computational cost while relying only on TILDE-QL trades off effectiveness for faster response time due to no inference being required. TILDE is evaluated on the MS MARCO and TREC Deep Learning 2019 and 2020 passage ranking datasets. Empirical results show that, compared to other approaches that aim to make deep language models viable operationally, TILDE achieves competitive effectiveness coupled with low query latency.",
                    "[237366133 | Wang et al. | 2021 | Citations: 83]": "The integration of pre-trained deep language models, such as BERT, into retrieval and ranking pipelines has shown to provide large effectiveness gains over traditional bag-of-words models in the passage retrieval task. However, the best setup for integrating such deep language models is still unclear. When BERT is used to re-rank passages (i.e., BERT re-ranker), previous work has empirically shown that, while in practice BERT re-ranker cannot act as initial retriever due to BERT's high query time costs, and thus a bag-of-words model such as BM25 is required. It is not necessary to interpolate BERT re-ranker and bag-of-words scores to generate the final ranking. In fact, the BERT re-ranker scores alone can be used by the re-ranker: the BERT re-ranker score appears to already capture the relevance signal provided by BM25. In this paper, we further investigate the topic of interpolating BM25 and BERT-based rankers. Unlike previous work that considered the BERT re-ranker, however, here we consider BERT-based dense retrievers (RepBERT and ANCE). Dense retrievers encode queries and documents into low dimensional BERT-based embeddings. These methods overcome BERT's high computational costs at query time, and can thus be feasibly used in practice as whole-collection retrievers, rather than just as re-rankers. Our novel empirical findings suggest that, unlike for BERT re-ranker, interpolation with BM25 is necessary for BERT-based dense retrievers to perform effectively; and the gains provided by the interpolation are significant. Further analysis reveals why this is so: dense retrievers are very effective at encoding strong relevance signals, but they fail in identifying weaker relevance signals -- a task that the interpolation with BM25 is able to make up for.",
                    "[238857121 | Ren et al. | 2021 | Citations: 255]": "In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other\u2019s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1763,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 159,
                                "end": 308
                            },
                            {
                                "start": 309,
                                "end": 431
                            },
                            {
                                "start": 432,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 984
                            },
                            {
                                "start": 985,
                                "end": 1182
                            },
                            {
                                "start": 1183,
                                "end": 1342
                            },
                            {
                                "start": 1343,
                                "end": 1549
                            },
                            {
                                "start": 1550,
                                "end": 1657
                            },
                            {
                                "start": 1658,
                                "end": 1763
                            }
                        ],
                        "ref_mentions": [
                            "237366133",
                            "233231706",
                            "235720578",
                            "231815627",
                            "238857121",
                            "220302524",
                            "235792476",
                            "237366133"
                        ],
                        "quote": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[248496840 | Wang et al. | 2022 | Citations: 13]",
                "snippets": "There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 602,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 80
                            },
                            {
                                "start": 81,
                                "end": 297
                            },
                            {
                                "start": 298,
                                "end": 469
                            },
                            {
                                "start": 470,
                                "end": 602
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[257427642 | Hoshi et al. | 2023 | Citations: 0]",
                "snippets": "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models....Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                    "[3641284 | McInnes et al. | 2018 | Citations: 9476]": "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 539,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 18
                            },
                            {
                                "start": 19,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 422
                            },
                            {
                                "start": 423,
                                "end": 537
                            },
                            {
                                "start": 538,
                                "end": 621
                            }
                        ],
                        "ref_mentions": [
                            "3618568",
                            "41563977",
                            "3641284"
                        ],
                        "quote": "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1751,
                        "end": 2068,
                        "sentence_offsets": [
                            {
                                "start": 1751,
                                "end": 1807
                            },
                            {
                                "start": 1808,
                                "end": 1911
                            },
                            {
                                "start": 1912,
                                "end": 2066
                            }
                        ],
                        "ref_mentions": [
                            "3641284"
                        ],
                        "quote": ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[257585074 | Nguyen et al. | 2023 | Citations: 29]",
                "snippets": "The third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies.\n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8]35].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                },
                "metadata": [
                    {
                        "section_title": "Unified learned sparse retrieval framework",
                        "pdf_hash": "",
                        "start": 1185,
                        "end": 1867,
                        "sentence_offsets": [
                            {
                                "start": 1134,
                                "end": 1358
                            },
                            {
                                "start": 1359,
                                "end": 1444
                            },
                            {
                                "start": 1447,
                                "end": 1587
                            },
                            {
                                "start": 1588,
                                "end": 1827
                            },
                            {
                                "start": 1830,
                                "end": 1958
                            }
                        ],
                        "ref_mentions": [
                            "216553223"
                        ],
                        "quote": "The third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies.\n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8]35]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[258041275 | Xie et al. | 2023 | Citations: 48]",
                "snippets": "Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval Performance",
                        "pdf_hash": "",
                        "start": 11,
                        "end": 445,
                        "sentence_offsets": [
                            {
                                "start": 11,
                                "end": 116
                            },
                            {
                                "start": 117,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 445
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[258048596 | Sun et al. | 2023 | Citations: 75]",
                "snippets": "Sparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24].\n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[201646309 | Reimers et al. | 2019 | Citations: 12316]": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[220302524 | Xiong et al. | 2020 | Citations: 1234]": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                    "[231815627 | Qu et al. | 2020 | Citations: 617]": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
                    "[233231706 | Hofstatter et al. | 2021 | Citations: 402]": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                    "[244799249 | Santhanam et al. | 2021 | Citations: 415]": "Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce Maize, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate Maize across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6\u201310x.",
                    "[248366293 | Bevilacqua et al. | 2022 | Citations: 165]": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",
                    "[249097975 | Izacard et al. | 2021 | Citations: 924]": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",
                    "[3666085 | Dehghani et al. | 2017 | Citations: 421]": "Despite the impressive improvements achieved by unsupervised deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval. The reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available. Hence, in this paper, we propose to train a neural ranking model using weak supervision, where labels are obtained automatically without human annotators or any external resources (e.g., click data). To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal. We further train a set of simple yet effective ranking models based on feed-forward neural networks. We study their effectiveness under various learning scenarios (point-wise and pair-wise models) and using different input representations (i.e., from encoding query-document pairs into dense/sparse vectors to using word embedding representation). We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection (Robust) and a heterogeneous large-scale web collection (ClueWeb). Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections. Our findings also suggest that supervised neural ranking models can greatly benefit from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.",
                    "[5688521 | Guo et al. | 2016 | Citations: 880]": "In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1703,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 17
                            },
                            {
                                "start": 18,
                                "end": 161
                            },
                            {
                                "start": 162,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 346
                            },
                            {
                                "start": 347,
                                "end": 498
                            },
                            {
                                "start": 499,
                                "end": 708
                            },
                            {
                                "start": 711,
                                "end": 727
                            },
                            {
                                "start": 728,
                                "end": 876
                            },
                            {
                                "start": 877,
                                "end": 963
                            },
                            {
                                "start": 964,
                                "end": 1123
                            },
                            {
                                "start": 1124,
                                "end": 1255
                            },
                            {
                                "start": 1256,
                                "end": 1474
                            },
                            {
                                "start": 1475,
                                "end": 1704
                            }
                        ],
                        "ref_mentions": [
                            "16829071",
                            "1183147",
                            "207178704",
                            "3666085",
                            "5688521",
                            "155300",
                            "215737187",
                            "231815627",
                            "220302524",
                            "216553223",
                            "244799249",
                            "233231706",
                            "249097975",
                            "201646309",
                            "215737187",
                            "218971783",
                            "248366293"
                        ],
                        "quote": "Sparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24].\n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1]."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[258714822 | Ziems et al. | 2023 | Citations: 42]",
                "snippets": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson et al., 2009)(Chen et al., 2017)Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25....However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020)(Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[220302524 | Xiong et al. | 2020 | Citations: 1234]": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                    "[220302658 | Khattab et al. | 2020 | Citations: 100]": "Abstract Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system attains state-of-the-art extractive OpenQA performance on all three datasets.",
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task."
                },
                "metadata": [
                    {
                        "section_title": "Traditional Document Retrievers",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 494,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 215
                            },
                            {
                                "start": 216,
                                "end": 333
                            },
                            {
                                "start": 334,
                                "end": 493
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "3618568",
                            "215737187"
                        ],
                        "quote": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson et al., 2009)(Chen et al., 2017)Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25"
                    },
                    {
                        "section_title": "Traditional Document Retrievers",
                        "pdf_hash": "",
                        "start": 689,
                        "end": 945,
                        "sentence_offsets": [
                            {
                                "start": 689,
                                "end": 944
                            }
                        ],
                        "ref_mentions": [
                            "215737187",
                            "220302524",
                            "220302658"
                        ],
                        "quote": ".However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020)(Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2020)."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[258840999 | Cho et al. | 2023 | Citations: 24]",
                "snippets": "We use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson et al., 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "A.3 Retrievers",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 702,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 297
                            },
                            {
                                "start": 298,
                                "end": 380
                            },
                            {
                                "start": 383,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 635
                            },
                            {
                                "start": 636,
                                "end": 702
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "215737187",
                            "52967399"
                        ],
                        "quote": "We use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson et al., 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[258865792 | Tang et al. | 2023 | Citations: 41]",
                "snippets": "Sparse retrieval. The key idea of sparse retrieval methods is to utilize exact matching signals to design a relevance scoring function. Specifically, these models consider easily computed statistics (e.g., term frequency, document length, and inverse document frequency) of normalized terms matched exactly between the query and document. Among these models, BM25 (Robertson et al., 1994) is shown to be effective and is still regarded as a strong baseline of many retrieval models nowadays. To enhance the semantic relationships, several works utilize word embeddings as term weights (Frej et al., 2020)(Zheng et al., 2015). Dense retrieval. To solve the vocabulary mismatch problem in sparse retrieval (Furnas et al., 1987)(Zhao et al., 2010), many researchers turn to dense retrieval models (Luan et al., 2020)[59], which first learn dense representations of both queries and documents, and then approximate nearest neighbor search (Beis et al., 1997)(Bentley, 1975) is employed to retrieve. Further, pre-trained models are used to enhance dense retrieval (Khattab et al., 2020)(Zhang et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211572791 | Zhang et al. | 2020 | Citations: 61]": "Recent studies on open-domain question answering have achieved prominent performance improvement using pre-trained language models such as BERT. State-of-the-art approaches typically follow the \"retrieve and read\" pipeline and employ BERT-based reranker to filter retrieved documents before feeding them into the reader module. The BERT retriever takes as input the concatenation of question and each retrieved document. Despite the success of these approaches in terms of QA accuracy, due to the concatenation, they can barely handle high-throughput of incoming questions each with a large collection of retrieved documents. To address the efficiency problem, we propose DC-BERT, a decoupled contextual encoding framework that has dual BERT models: an online BERT which encodes the question only once, and an offline BERT which pre-encodes all the documents and caches their encodings. On SQuAD Open and Natural Questions Open datasets, DC-BERT achieves 10x speedup on document retrieval, while retaining most (about 98%) of the QA performance compared to state-of-the-art approaches for open-domain question answering.",
                    "[216144735 | Frej et al. | 2020 | Citations: 11]": "Document indexing is a key component for efficient information retrieval (IR). After preprocessing steps such as stemming and stop-word removal, document indexes usually store term-frequencies (tf). Along with tf (that only reflects the importance of a term in a document), traditional IR models use term discrimination values (TDVs) such as inverse document frequency (idf) to favor discriminative terms during retrieval. In this work, we propose to learn TDVs for document indexing with shallow neural networks that approximate traditional IR ranking functions such as TF-IDF and BM25. Our proposal outperforms, both in terms of nDCG and recall, traditional approaches, even with few positively labelled query-document pairs as learning data. Our learned TDVs, when used to filter out terms of the vocabulary that have zero discrimination value, allow to both significantly lower the memory footprint of the inverted index and speed up the retrieval process (BM25 is up to 3~times faster), without degrading retrieval quality.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[218470027 | Luan et al. | 2020 | Citations: 408]": "Abstract Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 938,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 17
                            },
                            {
                                "start": 18,
                                "end": 135
                            },
                            {
                                "start": 136,
                                "end": 338
                            },
                            {
                                "start": 339,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 573
                            },
                            {
                                "start": 574,
                                "end": 590
                            },
                            {
                                "start": 591,
                                "end": 865
                            },
                            {
                                "start": 866,
                                "end": 938
                            }
                        ],
                        "ref_mentions": [
                            "41563977",
                            "216144735",
                            "155300",
                            "3002280",
                            "14286159",
                            "218470027",
                            "270664",
                            "13091446",
                            "216553223",
                            "211572791"
                        ],
                        "quote": "Sparse retrieval. The key idea of sparse retrieval methods is to utilize exact matching signals to design a relevance scoring function. Specifically, these models consider easily computed statistics (e.g., term frequency, document length, and inverse document frequency) of normalized terms matched exactly between the query and document. Among these models, BM25 (Robertson et al., 1994) is shown to be effective and is still regarded as a strong baseline of many retrieval models nowadays. To enhance the semantic relationships, several works utilize word embeddings as term weights (Frej et al., 2020)(Zheng et al., 2015). Dense retrieval. To solve the vocabulary mismatch problem in sparse retrieval (Furnas et al., 1987)(Zhao et al., 2010), many researchers turn to dense retrieval models (Luan et al., 2020)[59], which first learn dense representations of both queries and documents, and then approximate nearest neighbor search (Beis et al., 1997)(Bentley, 1975) is employed to retrieve. Further, pre-trained models are used to enhance dense retrieval (Khattab et al., 2020)(Zhang et al., 2020)."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[259370750 | Lee et al. | 2023 | Citations: 5]",
                "snippets": "Classic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 242,
                        "end": 1215,
                        "sentence_offsets": [
                            {
                                "start": 242,
                                "end": 478
                            },
                            {
                                "start": 479,
                                "end": 626
                            },
                            {
                                "start": 627,
                                "end": 807
                            },
                            {
                                "start": 810,
                                "end": 1063
                            },
                            {
                                "start": 1064,
                                "end": 1215
                            }
                        ],
                        "ref_mentions": [
                            "215737187"
                        ],
                        "quote": "Classic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[259924840 | Wang et al. | 2023 | Citations: 43]",
                "snippets": "Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 262,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 262
                            }
                        ],
                        "ref_mentions": [
                            "52967399"
                        ],
                        "quote": "Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[262066417 | Pirozelli et al. | 2023 | Citations: 4]",
                "snippets": "Two types of IR models were investigated for this benchmark: BM25 (Robertson et al., 2009), a sparse retrieval, and Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Benchmark Setup",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 738,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 144
                            },
                            {
                                "start": 145,
                                "end": 288
                            },
                            {
                                "start": 289,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 484
                            },
                            {
                                "start": 485,
                                "end": 642
                            },
                            {
                                "start": 643,
                                "end": 738
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "215737187"
                        ],
                        "quote": "Two types of IR models were investigated for this benchmark: BM25 (Robertson et al., 2009), a sparse retrieval, and Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[262947262 | Li et al. | 2023 | Citations: 1]",
                "snippets": "Sparse retrieval models (Robertson et al., 1994) usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Hybrid Sparse-dense Retrieval",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 650,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 134
                            },
                            {
                                "start": 135,
                                "end": 355
                            },
                            {
                                "start": 356,
                                "end": 394
                            },
                            {
                                "start": 395,
                                "end": 491
                            },
                            {
                                "start": 492,
                                "end": 650
                            }
                        ],
                        "ref_mentions": [
                            "41563977"
                        ],
                        "quote": "Sparse retrieval models (Robertson et al., 1994) usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[264147039 | Dominguez et al. | 2023 | Citations: 0]",
                "snippets": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson et al., 2009)) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers et al., 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[201646309 | Reimers et al. | 2019 | Citations: 12316]": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1106,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 336
                            },
                            {
                                "start": 337,
                                "end": 417
                            },
                            {
                                "start": 418,
                                "end": 609
                            },
                            {
                                "start": 610,
                                "end": 785
                            },
                            {
                                "start": 786,
                                "end": 876
                            },
                            {
                                "start": 877,
                                "end": 1106
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "215737187",
                            "201646309"
                        ],
                        "quote": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson et al., 2009)) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers et al., 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[266230831 | Lin et al. | 2023 | Citations: 5]",
                "snippets": "Recently, Lin (Lin, 2021) made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[238259539 | Lin | 2021 | Citations: 53]": "This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access \"technologies\" prior to the dawn of computing."
                },
                "metadata": [
                    {
                        "section_title": "MONO-LINGUAL RETRIEVAL OVERVIEW",
                        "pdf_hash": "",
                        "start": 638,
                        "end": 1448,
                        "sentence_offsets": [
                            {
                                "start": 638,
                                "end": 871
                            },
                            {
                                "start": 872,
                                "end": 977
                            },
                            {
                                "start": 978,
                                "end": 1259
                            },
                            {
                                "start": 1260,
                                "end": 1448
                            }
                        ],
                        "ref_mentions": [
                            "238259539"
                        ],
                        "quote": "Recently, Lin (Lin, 2021) made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[267320876 | Lyu et al. | 2024 | Citations: 40]",
                "snippets": "Keyword-based search-sparse retrieval algorithms, which use keywords and their frequencies to compute the relevance between documents and queries. Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF method, which accounts for factors such as the length and position of words in the document. Dense retrieval algorithms, which use deep learning models to encode documents and queries into low-dimensional vectors, and then measure the cosine similarity between them. This method can capture the semantic and contextual information of words, and improve the retrieval performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Analyzing the Impact of Retriever on RAG Performance in Different Tasks",
                        "pdf_hash": "",
                        "start": 227,
                        "end": 840,
                        "sentence_offsets": [
                            {
                                "start": 187,
                                "end": 373
                            },
                            {
                                "start": 374,
                                "end": 433
                            },
                            {
                                "start": 434,
                                "end": 553
                            },
                            {
                                "start": 554,
                                "end": 727
                            },
                            {
                                "start": 728,
                                "end": 840
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Keyword-based search-sparse retrieval algorithms, which use keywords and their frequencies to compute the relevance between documents and queries. Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF method, which accounts for factors such as the length and position of words in the document. Dense retrieval algorithms, which use deep learning models to encode documents and queries into low-dimensional vectors, and then measure the cosine similarity between them. This method can capture the semantic and contextual information of words, and improve the retrieval performance."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[267328301 | Giamphy et al. | 2023 | Citations: 2]",
                "snippets": "BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks...Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D. Ranking models",
                        "pdf_hash": "",
                        "start": 429,
                        "end": 730,
                        "sentence_offsets": [
                            {
                                "start": 429,
                                "end": 592
                            },
                            {
                                "start": 592,
                                "end": 731
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks"
                    },
                    {
                        "section_title": "D. Ranking models",
                        "pdf_hash": "",
                        "start": 985,
                        "end": 1367,
                        "sentence_offsets": [
                            {
                                "start": 985,
                                "end": 1153
                            },
                            {
                                "start": 1155,
                                "end": 1245
                            },
                            {
                                "start": 1245,
                                "end": 1286
                            },
                            {
                                "start": 1286,
                                "end": 1352
                            },
                            {
                                "start": 1352,
                                "end": 1367
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[268091298 | Zhao et al. | 2024 | Citations: 282]",
                "snippets": "Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage...term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.\n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.\n\nUnlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Retriever",
                        "pdf_hash": "",
                        "start": 809,
                        "end": 960,
                        "sentence_offsets": [
                            {
                                "start": 788,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 1012
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage"
                    },
                    {
                        "quote": "term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.\n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.\n\nUnlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[268554288 | Mao et al. | 2024 | Citations: 10]",
                "snippets": "Sparse retrievers, such as TF-IDF [37] and BM25 [35], predominantly rely on keyword matching for document retrieval. These methods determine the relevance between queries and documents by analyzing the occurrence and distribution of keywords within the documents. Dense retrievers employ dual-encoders to generate dense vector representations of text for more accurate semantic matching. Consequently, dense retrievers are considered more suitable for retrieval-augmented applications. Some techniques like vector quantization [25,47] and embedding optimization [48] also improves the efficiency of dense retrievers. Common dense retrievers include DPR [20], ANCE [49] and Contriever [13].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245986472 | Xiao et al. | 2022 | Citations: 18]": "Ad-hoc search calls for the selection of appropriate answers from a massive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a promising solution, where deep learning based document representation and ANN search techniques are allied to handle this task. However, a major challenge is that the ANN index can be too large to fit into memory, given the considerable size of answer corpus. In this work, we tackle this problem with Bi-Granular Document Representation, where the lightweight sparse embeddings are indexed and standby in memory for coarse-grained candidate search, and the heavyweight dense embeddings are hosted in disk for fine-grained post verification. For the best of retrieval accuracy, a Progressive Optimization framework is designed. The sparse embeddings are learned ahead for high-quality search of candidates. Conditioned on the candidate distribution induced by the sparse embeddings, the dense embeddings are continuously learned to optimize the discrimination of ground-truth from the shortlisted candidates. Besides, two techniques: the contrastive quantization and the locality-centric sampling are introduced for the learning of sparse and dense embeddings, which substantially contribute to their performances. Thanks to the above features, our method effectively handles massive-scale EBR with strong advantages in accuracy: with up to recall gain on million-scale corpus, and up to recall gain on billion-scale corpus. Besides, Our method is applied to a major sponsored search platform with substantial gains on revenue (), Recall () and CTR (). Our code is available at https://github.com/microsoft/BiDR.",
                    "[247922621 | Xiao et al. | 2022 | Citations: 33]": "Vector quantization (VQ) based ANN indexes, such as Inverted File System (IVF) and Product Quantization (PQ), have been widely applied to embedding based document retrieval thanks to the competitive time and memory efficiency. Originally, VQ is learned to minimize the reconstruction loss, i.e., the distortions between the original dense embeddings and the reconstructed embeddings after quantization. Unfortunately, such an objective is inconsistent with the goal of selecting ground-truth documents for the input query, which may cause severe loss of retrieval quality. Recent works identify such a defect, and propose to minimize the retrieval loss through contrastive learning. However, these methods intensively rely on queries with ground-truth documents, whose performance is limited by the insufficiency of labeled data. In this paper, we propose Distill-VQ, which unifies the learning of IVF and PQ within a knowledge distillation framework. In Distill-VQ, the dense embeddings are leveraged as \"teachers'', which predict the query's relevance to the sampled documents. The VQ modules are treated as the \"students'', which are learned to reproduce the predicted relevance, such that the reconstructed embeddings may fully preserve the retrieval result of the dense embeddings. By doing so, Distill-VQ is able to derive substantial training signals from the massive unlabeled data, which significantly contributes to the retrieval quality. We perform comprehensive explorations for the optimal conduct of knowledge distillation, which may provide useful insights for the learning of VQ based ANN index. We also experimentally show that the labeled data is no longer a necessity for high-quality vector quantization, which indicates Distill-VQ's strong applicability in practice. The evaluations are performed on MS MARCO and Natural Questions benchmarks, where Distill-VQ notably outperforms the SOTA VQ methods in Recall and MRR. Our code is avaliable at https://github.com/staoxiao/LibVQ.",
                    "[259949894 | Li et al. | 2023 | Citations: 2]": "Vector quantization is one of the critical techniques which enables dense retrieval for realtime applications. The recent study shows that vanilla vector quantization methods, like those implemented by FAISS [8], are lossy and prone to limited retrieval performances when large acceleration ratios are needed [14, 16, 18]. Besides, there have also been multiple algorithms which make the retriever and VQ better collaborated to alleviate such a loss. On top of these progresses, we develop LibVQ, which optimizes vector quantization for efficient dense retrieval. Our toolkit is highlighted for three advantages. 1. Effectiveness. The retrieval quality can be substantially improved over the vanilla implementations of VQ. 2. Simplicity. The optimization can be conducted in a lowcode fashion, and the optimization results can be easily loaded to ANN indexes to support downstream applications. 3. Universality. The optimization is agnostic to the embedding's learning process, and may accommodate different input conditions and ANN back-ends with little modification of the workflow. LibVQ may also support rich applications beyond dense retrieval, e.g., embedding compression, topic modeling, and de-duplication. In this demo, we provide comprehensive hand-on examples and evaluations for LibVQ. The toolkit is publicly released at: https://github.com/staoxiao/LibVQ/tree/demo.",
                    "[2996187 | Jones | 2021 | Citations: 4816]": "The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently\u2010occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 540,
                        "end": 1229,
                        "sentence_offsets": [
                            {
                                "start": 466,
                                "end": 544
                            },
                            {
                                "start": 544,
                                "end": 655
                            },
                            {
                                "start": 655,
                                "end": 771
                            },
                            {
                                "start": 771,
                                "end": 917
                            },
                            {
                                "start": 917,
                                "end": 1040
                            },
                            {
                                "start": 1040,
                                "end": 1137
                            },
                            {
                                "start": 1137,
                                "end": 1267
                            }
                        ],
                        "ref_mentions": [
                            "2996187",
                            "41563977",
                            "259949894",
                            "247922621",
                            "245986472"
                        ],
                        "quote": "Sparse retrievers, such as TF-IDF [37] and BM25 [35], predominantly rely on keyword matching for document retrieval. These methods determine the relevance between queries and documents by analyzing the occurrence and distribution of keywords within the documents. Dense retrievers employ dual-encoders to generate dense vector representations of text for more accurate semantic matching. Consequently, dense retrievers are considered more suitable for retrieval-augmented applications. Some techniques like vector quantization [25,47] and embedding optimization [48] also improves the efficiency of dense retrievers. Common dense retrievers include DPR [20], ANCE [49] and Contriever [13]."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[268724187 | Chen et al. | 2024 | Citations: 5]",
                "snippets": "Traditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46]. These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears. However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context. These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.e., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text. The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space. Ranking models based on dense vectors is referred to as dense retrieval models. Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth. Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[201646309 | Reimers et al. | 2019 | Citations: 12316]": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[220302524 | Xiong et al. | 2020 | Citations: 1234]": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Ranking Models in Information Retrieval",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1770,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 140
                            },
                            {
                                "start": 140,
                                "end": 443
                            },
                            {
                                "start": 443,
                                "end": 587
                            },
                            {
                                "start": 587,
                                "end": 686
                            },
                            {
                                "start": 688,
                                "end": 759
                            },
                            {
                                "start": 761,
                                "end": 978
                            },
                            {
                                "start": 978,
                                "end": 1130
                            },
                            {
                                "start": 1130,
                                "end": 1209
                            },
                            {
                                "start": 1209,
                                "end": 1421
                            },
                            {
                                "start": 1421,
                                "end": 1766
                            },
                            {
                                "start": 1768,
                                "end": 1981
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "2218552",
                            "1183147",
                            "215737187",
                            "220302524",
                            "216553223",
                            "201646309",
                            "52967399",
                            "215737187",
                            "216553223",
                            "201646309",
                            "220302524",
                            "215737187",
                            "220302524",
                            "201646309"
                        ],
                        "quote": "Traditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46]. These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears. However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context. These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.e., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text. The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space. Ranking models based on dense vectors is referred to as dense retrieval models. Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth. Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[269302715 | Dong et al. | 2024 | Citations: 3]",
                "snippets": "Current approaches to content retrieval are primarily classified into sparse and dense retrieval. There are two widely-used sparse retrieval methods, namely TF-IDF (Salton et al., 1983) and BM25 (Robertson et al., 1995). TF-IDF calculates the relevance of a word to a document in the corpus by multiplying the word frequency with the inverse document frequency. BM25 is an advancement of TF-IDF that introduces nonlinear word frequency saturation and length normalization to improve retrieval accuracy. Recently, dense retrieval methods have shown promising results, by encoding content into high-dimensional representations. DPR (Karpukhin et al., 2020) is the pioneering work of dense vector representations for QA tasks. Similarly, Col-BERT (Khattab and Zaharia, 2020b) introduces an efficient question-document interaction model, enhancing retrieval accuracy by allowing fine-grained term matching. Contriever (Izacard et al., 2022) further leverages contrastive learning to improve content dense encoding. E5 (Wang et al., 2022) and BGE (Xiao et al., 2023) propose novel training and data preparation techniques to enhance retrieval performance, e.g., consistency-filtering of noisy web data in E5 and the usage of RetroMAE (Xiao et al., 2022) pre-training paradigm in BGE. Moreover, GTE (Li et al., 2023) integrates graph-based techniques to enhance dense embedding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[263831502 | Chen et al. | 2023 | Citations: 86]": "Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 16,
                        "end": 1388,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 18
                            },
                            {
                                "start": 18,
                                "end": 115
                            },
                            {
                                "start": 115,
                                "end": 237
                            },
                            {
                                "start": 237,
                                "end": 377
                            },
                            {
                                "start": 377,
                                "end": 517
                            },
                            {
                                "start": 517,
                                "end": 638
                            },
                            {
                                "start": 638,
                                "end": 735
                            },
                            {
                                "start": 735,
                                "end": 913
                            },
                            {
                                "start": 913,
                                "end": 1020
                            },
                            {
                                "start": 1020,
                                "end": 1287
                            },
                            {
                                "start": 1287,
                                "end": 1380
                            },
                            {
                                "start": 1382,
                                "end": 1493
                            }
                        ],
                        "ref_mentions": [
                            "207180535",
                            "41563977",
                            "215737187",
                            "216553223",
                            "263831502"
                        ],
                        "quote": "Current approaches to content retrieval are primarily classified into sparse and dense retrieval. There are two widely-used sparse retrieval methods, namely TF-IDF (Salton et al., 1983) and BM25 (Robertson et al., 1995). TF-IDF calculates the relevance of a word to a document in the corpus by multiplying the word frequency with the inverse document frequency. BM25 is an advancement of TF-IDF that introduces nonlinear word frequency saturation and length normalization to improve retrieval accuracy. Recently, dense retrieval methods have shown promising results, by encoding content into high-dimensional representations. DPR (Karpukhin et al., 2020) is the pioneering work of dense vector representations for QA tasks. Similarly, Col-BERT (Khattab and Zaharia, 2020b) introduces an efficient question-document interaction model, enhancing retrieval accuracy by allowing fine-grained term matching. Contriever (Izacard et al., 2022) further leverages contrastive learning to improve content dense encoding. E5 (Wang et al., 2022) and BGE (Xiao et al., 2023) propose novel training and data preparation techniques to enhance retrieval performance, e.g., consistency-filtering of noisy web data in E5 and the usage of RetroMAE (Xiao et al., 2022) pre-training paradigm in BGE. Moreover, GTE (Li et al., 2023) integrates graph-based techniques to enhance dense embedding."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[269448997 | Xu et al. | 2024 | Citations: 25]",
                "snippets": "Sparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."
                },
                "metadata": [
                    {
                        "section_title": "C.1 Baselines for Retrieval Tasks in Main Experiments",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 534,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 24
                            },
                            {
                                "start": 25,
                                "end": 138
                            },
                            {
                                "start": 141,
                                "end": 386
                            },
                            {
                                "start": 389,
                                "end": 412
                            },
                            {
                                "start": 413,
                                "end": 534
                            }
                        ],
                        "ref_mentions": [
                            "207178704"
                        ],
                        "quote": "Sparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[269605025 | Zhang et al. | 2024 | Citations: 0]",
                "snippets": "Sparse retrievers, such as TF-IDF (Ramos, 2003) and BM25 (Robertson et al., 2009), are grounded in token frequency-centric metrics, contrasting with dense retrievers represented by Spider (Ram et al., 2021), Contriever (Izacard et al., 2021), and DPR (Karpukhin et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[245144844 | Ram et al. | 2021 | Citations: 62]": "Dense retrievers for open-domain question answering (ODQA) have been shown to achieve impressive performance by training on large datasets of question-passage pairs. In this work we ask whether this dependence on labeled data can be reduced via unsupervised pretraining that is geared towards ODQA. We show this is in fact possible, via a novel pretraining scheme designed for retrieval. Our \u201crecurring span retrieval\u201d approach uses recurring spans across passages in a document to create pseudo examples for contrastive learning. Our pretraining scheme directly controls for term overlap across pseudo queries and relevant passages, thus allowing to model both lexical and semantic relations between them. The resulting model, named Spider, performs surprisingly well without any labeled training examples on a wide range of ODQA datasets. Specifically, it significantly outperforms all other pretrained baselines in a zero-shot setting, and is competitive with BM25, a strong sparse baseline. Moreover, a hybrid retriever over Spider and BM25 improves over both, and is often competitive with DPR models, which are trained on tens of thousands of examples. Last, notable gains are observed when using Spider as an initialization for supervised training.",
                    "[249097975 | Izacard et al. | 2021 | Citations: 924]": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods."
                },
                "metadata": [
                    {
                        "section_title": "The Influence of Different Retrievers",
                        "pdf_hash": "",
                        "start": 114,
                        "end": 309,
                        "sentence_offsets": [
                            {
                                "start": 114,
                                "end": 309
                            }
                        ],
                        "ref_mentions": [
                            "14638345",
                            "207178704",
                            "245144844",
                            "249097975",
                            "215737187"
                        ],
                        "quote": "Sparse retrievers, such as TF-IDF (Ramos, 2003) and BM25 (Robertson et al., 2009), are grounded in token frequency-centric metrics, contrasting with dense retrievers represented by Spider (Ram et al., 2021), Contriever (Izacard et al., 2021), and DPR (Karpukhin et al., 2020)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[269950696 | Biswas et al. | 2024 | Citations: 1]",
                "snippets": "Existing retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson et al., 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 323,
                        "end": 1158,
                        "sentence_offsets": [
                            {
                                "start": 323,
                                "end": 406
                            },
                            {
                                "start": 406,
                                "end": 646
                            },
                            {
                                "start": 646,
                                "end": 837
                            },
                            {
                                "start": 837,
                                "end": 987
                            },
                            {
                                "start": 987,
                                "end": 1158
                            }
                        ],
                        "ref_mentions": [
                            "2218552",
                            "52967399"
                        ],
                        "quote": "Existing retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson et al., 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[270702658 | Jeong et al. | 2024 | Citations: 2]",
                "snippets": "Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021). Typically, it can be performed with two types of models: sparse and dense retrievers. Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020; Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it. For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents. Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[247839624 | Pan et al. | 2022 | Citations: 15]": "Most existing end-to-end Table Question Answering (Table QA) models consist of a two-stage framework with a retriever to select relevant table candidates from a corpus and a reader to locate the correct answers from table candidates. Even though the accuracy of the reader models is significantly improved with the recent transformer-based approaches, the overall performance of such frameworks still suffers from the poor accuracy of using traditional information retrieval techniques as retrievers. To alleviate this problem, we introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence model to generate answer tokens. Given any natural language question, T-RAG utilizes a unified pipeline to automatically search through a table corpus to directly locate the correct answer from the table cells. We apply T-RAG to recent open-domain Table QA benchmarks and demonstrate that the fine-tuned T-RAG model is able to achieve state-of-the-art performance in both the end-to-end Table QA and the table retrieval tasks.",
                    "[249097975 | Izacard et al. | 2021 | Citations: 924]": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1444,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 169
                            },
                            {
                                "start": 169,
                                "end": 254
                            },
                            {
                                "start": 254,
                                "end": 597
                            },
                            {
                                "start": 599,
                                "end": 901
                            },
                            {
                                "start": 901,
                                "end": 1183
                            },
                            {
                                "start": 1183,
                                "end": 1439
                            },
                            {
                                "start": 1439,
                                "end": 1560
                            }
                        ],
                        "ref_mentions": [
                            "247839624",
                            "215737187",
                            "249097975",
                            "215737187",
                            "249097975"
                        ],
                        "quote": "Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021). Typically, it can be performed with two types of models: sparse and dense retrievers. Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020; Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it. For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents. Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[271571401 | Gao et al. | 2024 | Citations: 20]",
                "snippets": "Sparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25.\n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Retrieval",
                        "pdf_hash": "",
                        "start": 779,
                        "end": 1435,
                        "sentence_offsets": [
                            {
                                "start": 743,
                                "end": 843
                            },
                            {
                                "start": 844,
                                "end": 939
                            },
                            {
                                "start": 942,
                                "end": 1037
                            },
                            {
                                "start": 1038,
                                "end": 1138
                            },
                            {
                                "start": 1139,
                                "end": 1223
                            },
                            {
                                "start": 1224,
                                "end": 1263
                            },
                            {
                                "start": 1266,
                                "end": 1383
                            },
                            {
                                "start": 1384,
                                "end": 1480
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25.\n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[273234268 | Hu et al. | 2024 | Citations: 5]",
                "snippets": "From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded (Fan et al., 2024). Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 (Robertson et al., 2009) rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats (Zhao et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[254044526 | Zhao et al. | 2022 | Citations: 192]": "Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user\u2019s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models (PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.",
                    "[269740933 | Fan et al. | 2024 | Citations: 248]": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/"
                },
                "metadata": [
                    {
                        "section_title": "A More details of Retrieval and Generation",
                        "pdf_hash": "",
                        "start": 441,
                        "end": 1491,
                        "sentence_offsets": [
                            {
                                "start": 441,
                                "end": 613
                            },
                            {
                                "start": 614,
                                "end": 766
                            },
                            {
                                "start": 767,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 966
                            },
                            {
                                "start": 967,
                                "end": 1110
                            },
                            {
                                "start": 1111,
                                "end": 1326
                            },
                            {
                                "start": 1327,
                                "end": 1491
                            }
                        ],
                        "ref_mentions": [
                            "269740933",
                            "207178704",
                            "254044526"
                        ],
                        "quote": "From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded (Fan et al., 2024). Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 (Robertson et al., 2009) rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats (Zhao et al., 2022)."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[273346484 | Zhao et al. | 2024 | Citations: 0]",
                "snippets": "These metrics can be classified into sparse-vector retrieval, which involves methods like TF-IDF and BM25 (Robertson et al., 2009), enabling efficient keyword matching using inverted indexes. On the other hand, dense-vector retrieval leverages pre-trained language models, such as those based on BERT, to encode text into low-dimensional dense vectors [9].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 486,
                        "end": 821,
                        "sentence_offsets": [
                            {
                                "start": 486,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 821
                            }
                        ],
                        "ref_mentions": [
                            "207178704"
                        ],
                        "quote": "These metrics can be classified into sparse-vector retrieval, which involves methods like TF-IDF and BM25 (Robertson et al., 2009), enabling efficient keyword matching using inverted indexes. On the other hand, dense-vector retrieval leverages pre-trained language models, such as those based on BERT, to encode text into low-dimensional dense vectors [9]."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[273549875 | Saberi et al. | 2024 | Citations: 1]",
                "snippets": "To analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data.\n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods.\n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RQ1: Does PKG improve code generation?",
                        "pdf_hash": "",
                        "start": 91,
                        "end": 1379,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 122
                            },
                            {
                                "start": 125,
                                "end": 452
                            },
                            {
                                "start": 453,
                                "end": 717
                            },
                            {
                                "start": 720,
                                "end": 905
                            },
                            {
                                "start": 906,
                                "end": 1070
                            },
                            {
                                "start": 1073,
                                "end": 1287
                            },
                            {
                                "start": 1288,
                                "end": 1415
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data.\n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods.\n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[275119098 | Liu et al. | 2024 | Citations: 4]",
                "snippets": "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], to probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance.\n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[232147859 | Cai et al. | 2021 | Citations: 131]": "Multi-stage ranking pipelines have been a practical solution in modern search systems, where the first-stage retrieval is to return a subset of candidate documents and latter stages attempt to re-rank those candidates. Unlike re-ranking stages going through quick technique shifts over the past decades, the first-stage retrieval has long been dominated by classical term-based models. Unfortunately, these models suffer from the vocabulary mismatch problem, which may block re-ranking stages from relevant documents at the very beginning. Therefore, it has been a long-term desire to build semantic models for the first-stage retrieval that can achieve high recall efficiently. Recently, we have witnessed an explosive growth of research interests on the first-stage semantic retrieval models. We believe it is the right time to survey current status, learn from existing methods, and gain some insights for future development. In this article, we describe the current landscape of the first-stage retrieval models under a unified framework to clarify the connection between classical term-based retrieval methods, early semantic retrieval methods, and neural semantic retrieval methods. Moreover, we identify some open challenges and envision some future directions, with the hope of inspiring more research on these important yet less investigated topics.",
                    "[254044526 | Zhao et al. | 2022 | Citations: 192]": "Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user\u2019s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models (PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval."
                },
                "metadata": [
                    {
                        "section_title": "Sparse and dense retrieval models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1155,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 215
                            },
                            {
                                "start": 216,
                                "end": 381
                            },
                            {
                                "start": 384,
                                "end": 416
                            },
                            {
                                "start": 417,
                                "end": 548
                            },
                            {
                                "start": 551,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 978
                            },
                            {
                                "start": 979,
                                "end": 1158
                            }
                        ],
                        "ref_mentions": [
                            "12255087",
                            "232147859",
                            "2218552",
                            "6473756",
                            "6473756",
                            "2218552",
                            "207178704",
                            "215737187",
                            "216553223",
                            "254044526",
                            "232147859",
                            "215737187"
                        ],
                        "quote": "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], to probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance.\n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[275906690 | Some et al. | 2025 | Citations: 0]",
                "snippets": "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82]109].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval augmented generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 296,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 296
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82]109]."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[275921747 | Tang et al. | 2024 | Citations: 1]",
                "snippets": "The retrieval methods can be divided into two categories: (1) Sparse retrieval. Sparse retrieval retrieves the most similar items from an index based on query keywords or feature vectors, such as BM25. (2) Dense retrieval. Dense retrieval methods (Xiao et al., 2023;Reimers and Gurevych, 2019;(Jiang et al., 2023)G\u00fcnther et al., 2023) involve training retrieval models on large corpora, generating dense vector representations for queries and demonstrations through model inference, and computing similarities based on these vector representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259858947 | Jiang et al. | 2023 | Citations: 68]": ","
                },
                "metadata": [
                    {
                        "section_title": "Retrieval Methods",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 548,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 79
                            },
                            {
                                "start": 80,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 222
                            },
                            {
                                "start": 223,
                                "end": 548
                            }
                        ],
                        "ref_mentions": [
                            "259858947"
                        ],
                        "quote": "The retrieval methods can be divided into two categories: (1) Sparse retrieval. Sparse retrieval retrieves the most similar items from an index based on query keywords or feature vectors, such as BM25. (2) Dense retrieval. Dense retrieval methods (Xiao et al., 2023;Reimers and Gurevych, 2019;(Jiang et al., 2023)G\u00fcnther et al., 2023) involve training retrieval models on large corpora, generating dense vector representations for queries and demonstrations through model inference, and computing similarities based on these vector representations."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[275921950 | Hu et al. | 2025 | Citations: 2]",
                "snippets": "Sparse retrieval methods, such as TF-IDF (Salton et al., 1975) and BM25 (Robertson et al., 2009), rely on term-based representations to evaluate lexical matches between queries and documents. These approaches perform well in scenarios where exact term matching is essential, but they struggle with semantic meaning. In contrast, dense retrieval methods leverage pretrained language models such as BERT [5] to encode queries and documents as continuous, low-dimensional embeddings, capturing semantic similarity through maximum inner product search (MIPS) (Karpukhin et al., 2020)30,(Xiao et al., 2023)[44]. Dense retrieval effectively overcomes the lexical gap, retrieving semantically related results even when query terms differ from the document's terminology.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[271114619 | Xiao et al. | 2023 | Citations: 160]": "We introduce C-Pack, a package of resources that significantly advances the field of general text embeddings for Chinese. C-Pack includes three critical resources. 1) C-MTP is a massive training dataset for text embedding, which is based on the curation of vast unlabeled corpora and the integration of high-quality labeled corpora. 2) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 3) BGE is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by more than +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for BGE. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models also achieve state-of-the-art performance on the MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. Both Chinese and English datasets are the largest public release of training data for text embeddings. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK 2.1 Information Retrieval",
                        "pdf_hash": "",
                        "start": 181,
                        "end": 870,
                        "sentence_offsets": [
                            {
                                "start": 181,
                                "end": 335
                            },
                            {
                                "start": 336,
                                "end": 459
                            },
                            {
                                "start": 460,
                                "end": 713
                            },
                            {
                                "start": 714,
                                "end": 870
                            }
                        ],
                        "ref_mentions": [
                            "6473756",
                            "207178704",
                            "215737187",
                            "271114619"
                        ],
                        "quote": "Sparse retrieval methods, such as TF-IDF (Salton et al., 1975) and BM25 (Robertson et al., 2009), rely on term-based representations to evaluate lexical matches between queries and documents. These approaches perform well in scenarios where exact term matching is essential, but they struggle with semantic meaning. In contrast, dense retrieval methods leverage pretrained language models such as BERT [5] to encode queries and documents as continuous, low-dimensional embeddings, capturing semantic similarity through maximum inner product search (MIPS) (Karpukhin et al., 2020)30,(Xiao et al., 2023)[44]. Dense retrieval effectively overcomes the lexical gap, retrieving semantically related results even when query terms differ from the document's terminology."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[276107364 | Abdallah et al. | 2025 | Citations: 1]",
                "snippets": "Sparse retrievers, such as BM25 (Robertson et al., 1994), remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods (Lee et al., 2019), which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR (Lin et al., 2020) and ANCE (Xiong et al., 2020), improve retrieval effectiveness by employing contrastive training with in-batch negatives (Gillick et al., 2019) and hard-negative mining (Karpukhin et al., 2020)(Xiong et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[173990818 | Lee et al. | 2019 | Citations: 1018]": "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",
                    "[202718954 | Gillick et al. | 2019 | Citations: 208]": "We show that it is feasible to perform entity linking by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-text links in Wikipedia, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, it can retrieve candidates extremely fast, and generalizes well to a new dataset derived from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised negative mining algorithm for this task.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[220302524 | Xiong et al. | 2020 | Citations: 1234]": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.",
                    "[222310837 | Lin et al. | 2020 | Citations: 627]": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly."
                },
                "metadata": [
                    {
                        "section_title": "Related Work 2.1 Retrievers",
                        "pdf_hash": "",
                        "start": 150,
                        "end": 670,
                        "sentence_offsets": [
                            {
                                "start": 150,
                                "end": 276
                            },
                            {
                                "start": 277,
                                "end": 484
                            },
                            {
                                "start": 485,
                                "end": 670
                            }
                        ],
                        "ref_mentions": [
                            "41563977",
                            "173990818",
                            "222310837",
                            "220302524",
                            "202718954",
                            "215737187",
                            "220302524"
                        ],
                        "quote": "Sparse retrievers, such as BM25 (Robertson et al., 1994), remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods (Lee et al., 2019), which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR (Lin et al., 2020) and ANCE (Xiong et al., 2020), improve retrieval effectiveness by employing contrastive training with in-batch negatives (Gillick et al., 2019) and hard-negative mining (Karpukhin et al., 2020)(Xiong et al., 2020)."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[276647280 | Abdallah et al. | 2025 | Citations: 0]",
                "snippets": "Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth....BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively....The results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 215,
                        "end": 363,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth"
                    },
                    {
                        "section_title": "Retriever Models",
                        "pdf_hash": "",
                        "start": 567,
                        "end": 1013,
                        "sentence_offsets": [
                            {
                                "start": 567,
                                "end": 687
                            },
                            {
                                "start": 688,
                                "end": 787
                            },
                            {
                                "start": 788,
                                "end": 1011
                            },
                            {
                                "start": 1012,
                                "end": 1179
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively"
                    },
                    {
                        "section_title": "A.1 Retrieval Performance on TriviaQA",
                        "pdf_hash": "",
                        "start": 562,
                        "end": 930,
                        "sentence_offsets": [
                            {
                                "start": 562,
                                "end": 694
                            },
                            {
                                "start": 695,
                                "end": 929
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".The results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness."
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[276928453 | Lu et al. | 2025 | Citations: 0]",
                "snippets": "Traditional sparse retrieval methods are based on BM25 (Robertson et al., 2009), TF-IDF (Ramos, 2003), etc., rely on keyword matching and statistical weighting to evaluate relevance, which suffers from the well-known issue of lexical gap (Berger et al., 2000), restricting their ability to effectively capture semantic relationships (Luan et al., 2020)Nian et al., 2024). \n\nDense retrieval addresses this limitation by encoding both queries and documents as embeddings within a joint latent space, where the semantic relationship is captured through the similarity scores between their embeddings (Li et al., 2023a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[218470027 | Luan et al. | 2020 | Citations: 408]": "Abstract Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 32,
                        "end": 661,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 416
                            },
                            {
                                "start": 419,
                                "end": 661
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "14638345",
                            "6749682",
                            "218470027"
                        ],
                        "quote": "Traditional sparse retrieval methods are based on BM25 (Robertson et al., 2009), TF-IDF (Ramos, 2003), etc., rely on keyword matching and statistical weighting to evaluate relevance, which suffers from the well-known issue of lexical gap (Berger et al., 2000), restricting their ability to effectively capture semantic relationships (Luan et al., 2020)Nian et al., 2024). \n\nDense retrieval addresses this limitation by encoding both queries and documents as embeddings within a joint latent space, where the semantic relationship is captured through the similarity scores between their embeddings (Li et al., 2023a)."
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[277113527 | Kim et al. | 2025 | Citations: 3]",
                "snippets": "Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers et al., 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[201646309 | Reimers et al. | 2019 | Citations: 12316]": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
                    "[237366133 | Wang et al. | 2021 | Citations: 83]": "The integration of pre-trained deep language models, such as BERT, into retrieval and ranking pipelines has shown to provide large effectiveness gains over traditional bag-of-words models in the passage retrieval task. However, the best setup for integrating such deep language models is still unclear. When BERT is used to re-rank passages (i.e., BERT re-ranker), previous work has empirically shown that, while in practice BERT re-ranker cannot act as initial retriever due to BERT's high query time costs, and thus a bag-of-words model such as BM25 is required. It is not necessary to interpolate BERT re-ranker and bag-of-words scores to generate the final ranking. In fact, the BERT re-ranker scores alone can be used by the re-ranker: the BERT re-ranker score appears to already capture the relevance signal provided by BM25. In this paper, we further investigate the topic of interpolating BM25 and BERT-based rankers. Unlike previous work that considered the BERT re-ranker, however, here we consider BERT-based dense retrievers (RepBERT and ANCE). Dense retrievers encode queries and documents into low dimensional BERT-based embeddings. These methods overcome BERT's high computational costs at query time, and can thus be feasibly used in practice as whole-collection retrievers, rather than just as re-rankers. Our novel empirical findings suggest that, unlike for BERT re-ranker, interpolation with BM25 is necessary for BERT-based dense retrievers to perform effectively; and the gains provided by the interpolation are significant. Further analysis reveals why this is so: dense retrievers are very effective at encoding strong relevance signals, but they fail in identifying weaker relevance signals -- a task that the interpolation with BM25 is able to make up for.",
                    "[266999115 | Sengupta et al. | 2024 | Citations: 2]": "In Natural Language Processing (NLP), Machine Reading Comprehension (MRC) is the task of answering a question based on a given context. To handle questions in the medical domain, modern language models such as BioBERT, SciBERT and even ChatGPT are trained on vast amounts of in-domain medical corpora. However, in-domain pre-training is expensive in terms of time and resources. In this paper, we propose a resource-efficient approach for injecting domain knowledge into a model without relying on such domain-specific pre-training.(p)(/p)Knowledge graphs are powerful resources for accessing medical information. Building on existing work, we introduce a method using Multi-Layer Perceptrons (MLPs) for aligning and integrating embeddings extracted from medical knowledge graphs with the embedding spaces of pre-trained language models (LMs). The aligned embeddings are fused with open-domain LMs BERT and RoBERTa that are fine-tuned for two MRC tasks, span detection (COVID-QA) and multiple-choice questions (PubMedQA). We compare our method to prior techniques that rely on a vocabulary overlap for embedding alignment and show how our method circumvents this requirement to deliver better performance. On both datasets, our method allows BERT/RoBERTa to either perform on par (occasionally exceeding) with stronger domain-specific models or show improvements in general over prior techniques. With the proposed approach, we signal an alternative method to in-domain pre-training to achieve domain proficiency. Our code is available here 1.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "HYBRID RETRIEVAL",
                        "pdf_hash": "",
                        "start": 232,
                        "end": 861,
                        "sentence_offsets": [
                            {
                                "start": 232,
                                "end": 478
                            },
                            {
                                "start": 479,
                                "end": 593
                            },
                            {
                                "start": 594,
                                "end": 837
                            },
                            {
                                "start": 838,
                                "end": 949
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "201646309",
                            "237366133",
                            "266999115"
                        ],
                        "quote": "Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers et al., 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024)"
                    }
                ]
            },
            {
                "idx": 44,
                "key": "[277451883 | Hsu et al. | 2025 | Citations: 1]",
                "snippets": "Hybrid retrieval (Ma et al., 2020)(Sawarkar et al., 2024)Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[269043117 | Sawarkar et al. | 2024 | Citations: 57]": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 564,
                        "end": 992,
                        "sentence_offsets": [
                            {
                                "start": 564,
                                "end": 766
                            },
                            {
                                "start": 767,
                                "end": 992
                            }
                        ],
                        "ref_mentions": [
                            "221668044",
                            "269043117",
                            "215737187"
                        ],
                        "quote": "Hybrid retrieval (Ma et al., 2020)(Sawarkar et al., 2024)Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap."
                    }
                ]
            },
            {
                "idx": 45,
                "key": "[278664642 | Kim et al. | 2024 | Citations: 0]",
                "snippets": "When it comes to the document retrieval, Sparse retrieval methods such as BM25 (Robertson et al., 2009)(Robertson et al., 2009) had been prevalent due to their straightforward approach to matching keywords. However, they can be limited by their inability to capture the deeper semantic meaning of text. Dense retrieval, on the other hand, showed many advantages over sparse retrieval [Lee et al., 2019;(Karpukhin et al., 2020)(Li et al., 2021). Dense retrieval approaches go beyond mere keyword matching; they generate vector representations of documents and queries, facilitating the capture of deep semantic meanings. This is crucial in fields requiring high accuracy and contextual understanding, where the relevancy of documents cannot solely be determined by keyword frequency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[236034464 | Li et al. | 2021 | Citations: 38]": "Dense retrieval conducts text retrieval in the embedding space and has shown many advantages compared to sparse retrieval. Existing dense retrievers optimize representations of queries and documents with contrastive training and map them to the embedding space. The embedding space is optimized by aligning the matched query-document pairs and pushing the negative documents away from the query. However, in such training paradigm, the queries are only optimized to align to the documents and are coarsely positioned, leading to an anisotropic query embedding space. In this paper, we analyze the embedding space distributions and propose an effective training paradigm, Contrastive Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained query representations for dense retrieval. DANCE incorporates an additional dual training object of query retrieval, inspired by the classic information retrieval training axiom, query likelihood. With contrastive learning, the dual training object of DANCE learns more tailored representations for queries and documents to keep the embedding space smooth and uniform, thriving on the ranking performance of DANCE on the MS MARCO document retrieval task. Different from ANCE that only optimized with the document retrieval task, DANCE concentrates the query embeddings closer to document representations while making the document distribution more discriminative. Such concentrated query embedding distribution assigns more uniform negative sampling probabilities to queries and helps to sufficiently optimize query representations in the query retrieval task. Our codes are released at https://github.com/thunlp/DANCE."
                },
                "metadata": [
                    {
                        "section_title": "Document preprocessing & similarity search",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 783,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 209
                            },
                            {
                                "start": 210,
                                "end": 305
                            },
                            {
                                "start": 306,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 620
                            },
                            {
                                "start": 621,
                                "end": 783
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "207178704",
                            "215737187",
                            "236034464"
                        ],
                        "quote": "When it comes to the document retrieval, Sparse retrieval methods such as BM25 (Robertson et al., 2009)(Robertson et al., 2009) had been prevalent due to their straightforward approach to matching keywords. However, they can be limited by their inability to capture the deeper semantic meaning of text. Dense retrieval, on the other hand, showed many advantages over sparse retrieval [Lee et al., 2019;(Karpukhin et al., 2020)(Li et al., 2021). Dense retrieval approaches go beyond mere keyword matching; they generate vector representations of documents and queries, facilitating the capture of deep semantic meanings. This is crucial in fields requiring high accuracy and contextual understanding, where the relevancy of documents cannot solely be determined by keyword frequency."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.044382,
        "cot": "For addressing the question about primary differences between sparse retrieval models like BM25 and dense retrieval models like DPR or SentenceTransformer, I'll organize the quotes into meaningful dimensions that comprehensively answer the query. After reviewing the content, I identified several key dimensions that capture the essential differences:\n\n1. Introduction/Background: This section will provide context about retrieval models and their importance in retrieval-augmented language models. This is necessary to establish basic understanding before diving into specific differences.\n\n2. Representation Approach: A fundamental difference between sparse and dense retrieval is how they represent documents and queries. This section will explain vector representation differences.\n\n3. Matching Mechanism: The way sparse and dense models match queries to documents differs significantly. This dimension captures how each approach determines relevance.\n\n4. Training Requirements: An important distinction is whether these models require training data. This dimension will contrast supervised vs unsupervised approaches.\n\n5. Performance Characteristics: This dimension will cover the relative strengths and weaknesses of each approach in different scenarios.\n\n6. Implementation and Efficiency: This dimension will explain differences in how these models are implemented and their computational considerations.\n\n7. Complementary Nature: Many quotes suggest these approaches can be complementary rather than just competitive alternatives. This dimension will explain how they can work together.\n\nFor each dimension, I'll choose the format that best serves the information:\n- Introduction/Background: Synthesis format to provide coherent context\n- Representation Approach: Synthesis format to explain fundamental differences\n- Matching Mechanism: Synthesis format to compare mechanisms\n- Training Requirements: Synthesis format to explain supervised vs unsupervised approaches\n- Performance Characteristics: List format to clearly enumerate strengths and weaknesses\n- Implementation and Efficiency: Synthesis format to explain technical differences\n- Complementary Nature: Synthesis format to explain how they can work together\n\nThis organization should provide a comprehensive and structured answer to the query.",
        "plan": {
            "Introduction: Retrieval Models in RAG Systems (synthesis)": [],
            "Representation Approach: Sparse vs. Dense Vectors (synthesis)": [
                0,
                4,
                5,
                6,
                10,
                11,
                20,
                21,
                22,
                23,
                24,
                26,
                30,
                32,
                34,
                36,
                38,
                39
            ],
            "Matching Mechanism: Lexical vs. Semantic Matching (synthesis)": [
                1,
                7,
                9,
                12,
                15,
                16,
                17,
                18,
                19,
                25,
                27,
                28,
                29,
                31,
                33,
                42,
                43
            ],
            "Training Requirements: Supervised vs. Unsupervised (synthesis)": [
                2,
                3,
                6,
                7,
                13,
                14,
                21,
                35,
                40
            ],
            "Performance Characteristics: Strengths and Weaknesses (list)": [
                3,
                7,
                8,
                9,
                12,
                18,
                19,
                26,
                31,
                35,
                37,
                41,
                44,
                45
            ],
            "Implementation and Efficiency Considerations (synthesis)": [
                4,
                5,
                7,
                10,
                24,
                30,
                36
            ],
            "Complementary Nature and Hybrid Approaches (synthesis)": [
                9,
                37,
                44
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction: Retrieval Models in RAG Systems",
                "tldr": "Retrieval-Augmented Generation (RAG) systems use different retrieval models to fetch relevant information before generating responses. Sparse retrieval models like BM25 and dense retrieval models like DPR represent fundamentally different approaches to finding relevant context for language models. (LLM Memory)",
                "text": "\nRetrieval-Augmented Generation (RAG) systems enhance language model outputs by retrieving relevant information from external knowledge sources before generating responses. At the core of these systems are retrieval models that determine which pieces of information are most relevant to a given query. These retrieval models generally fall into two main categories: sparse retrieval models and dense retrieval models. Sparse retrieval models, such as BM25, rely on term-based matching and represent documents as high-dimensional, sparse vectors where most elements are zero. In contrast, dense retrieval models like Dense Passage Retrieval (DPR) and SentenceTransformer encode semantic meaning into dense vector representations where all dimensions potentially contain information. Understanding these different retrieval approaches is crucial for developing effective RAG systems, as the choice of retrieval model significantly impacts the quality and relevance of information provided to the language model for response generation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Representation Approach: Sparse vs. Dense Vectors",
                "tldr": "Sparse retrieval models like BM25 represent documents and queries as high-dimensional vectors with mostly zero values corresponding to vocabulary terms, while dense retrieval models like DPR use lower-dimensional continuous vectors that encode semantic meaning. (11 sources)",
                "text": "\nAt their core, sparse and dense retrieval models differ fundamentally in how they represent textual information. Sparse retrieval models, such as BM25, represent documents and queries as high-dimensional, sparse vectors where dimensions correspond to the vocabulary terms in the collection <Paper corpusId=\"245334864\" paperTitle=\"(Piktus et al., 2021)\" isShortName></Paper> <Paper corpusId=\"207178704\" paperTitle=\"(Robertson et al., 2009)\" isShortName></Paper>. In these representations, most elements are zero (due to non-occurring terms), and the non-zero weights indicate the importance of each term, typically calculated using statistical methods <Paper corpusId=\"248366550\" paperTitle=\"(Penha et al., 2022)\" isShortName></Paper>. This approach follows the bag-of-words assumption, treating text as a multiset of words while ignoring grammar and word order <Paper corpusId=\"275119098\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nIn contrast, dense retrieval models like Dense Passage Retrieval (DPR) and SentenceTransformer encode text into fixed-size, low-dimensional continuous vector representations <Paper corpusId=\"245334864\" paperTitle=\"(Piktus et al., 2021)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. These dense vectors typically range from hundreds to thousands of dimensions\u2014significantly fewer than sparse representations\u2014where all dimensions potentially contain information <Paper corpusId=\"267328301\" paperTitle=\"(Giamphy et al., 2023)\" isShortName></Paper>. As Lin points out, both sparse and dense retrieval models can be viewed as parametric variations of a bi-encoder architecture, with the primary difference being the basis of the representation vector: lexical space versus semantic space <Paper corpusId=\"266230831\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper>.\n\nAnother important distinction is that encoders in sparse retrieval only produce non-negative weights, whereas dense encoders have no such constraint <Paper corpusId=\"257585074\" paperTitle=\"(Nguyen et al., 2023)\" isShortName></Paper>. This constraint in sparse models arises from their reliance on inverted indexing software stacks built for traditional lexical search, where weights are always non-negative term frequencies.\n\nThe representation difference leads to different indexing approaches. Sparse retrieval typically leverages inverted indices, where each term maps to a list of documents containing that term <Paper corpusId=\"268091298\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Dense retrieval, however, requires approximate nearest neighbor (ANN) search algorithms to efficiently find similar vectors in the embedding space <Paper corpusId=\"247292113\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>. This architectural difference affects not only how information is represented but also how it is stored, indexed, and retrieved in practical applications.",
                "citations": [
                    {
                        "id": "(Piktus et al., 2021)",
                        "snippets": [
                            "We consider two retrieval architectures. BM25 (Robertson et al., 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval."
                        ],
                        "paper": {
                            "corpus_id": 245334864,
                            "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
                            "authors": [
                                {
                                    "authorId": "120174856",
                                    "name": "Aleksandra Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "113568063",
                                    "name": "Dmytro Okhonko"
                                },
                                {
                                    "authorId": "2966239",
                                    "name": "Samuel Broscheit"
                                },
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "1628391446",
                                    "name": "Barlas Ouguz"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 66
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Robertson et al., 2009)",
                        "snippets": [
                            "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."
                        ],
                        "paper": {
                            "corpus_id": 207178704,
                            "title": "The Probabilistic Relevance Framework: BM25 and Beyond",
                            "authors": [
                                {
                                    "authorId": "144430625",
                                    "name": "S. Robertson"
                                },
                                {
                                    "authorId": "2833561",
                                    "name": "H. Zaragoza"
                                }
                            ],
                            "year": 2009,
                            "venue": "Foundations and Trends in Information Retrieval",
                            "n_citations": 3760
                        },
                        "score": 0
                    },
                    {
                        "id": "(Penha et al., 2022)",
                        "snippets": [
                            "An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods.\n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario."
                        ],
                        "paper": {
                            "corpus_id": 248366550,
                            "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
                            "authors": [
                                {
                                    "authorId": "145579682",
                                    "name": "Gustavo Penha"
                                },
                                {
                                    "authorId": "2731925",
                                    "name": "C. Hauff"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9521484375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], to probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance.\n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25."
                        ],
                        "paper": {
                            "corpus_id": 275119098,
                            "title": "On the Robustness of Generative Information Retrieval Models",
                            "authors": [
                                {
                                    "authorId": "2143860482",
                                    "name": "Yuansan Liu"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "70414094",
                                    "name": "Jiafeng Guo"
                                },
                                {
                                    "authorId": "2287815279",
                                    "name": "Changjiang Zhou"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2244825947",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Information Retrieval",
                            "n_citations": 4
                        },
                        "score": 0.96630859375
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Giamphy et al., 2023)",
                        "snippets": [
                            "BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks",
                            "Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors."
                        ],
                        "paper": {
                            "corpus_id": 267328301,
                            "title": "A Quantitative Analysis of Noise Impact on Document Ranking",
                            "authors": [
                                {
                                    "authorId": "2181131501",
                                    "name": "Edward Giamphy"
                                },
                                {
                                    "authorId": "2212071788",
                                    "name": "K\u00e9vin Sanchis"
                                },
                                {
                                    "authorId": "144101598",
                                    "name": "G. Dashyan"
                                },
                                {
                                    "authorId": "2281888390",
                                    "name": "Jean-Loup Guillaume"
                                },
                                {
                                    "authorId": "2924500",
                                    "name": "Ahmed Hamdi"
                                },
                                {
                                    "authorId": "2281878315",
                                    "name": "Lilian Sanselme"
                                },
                                {
                                    "authorId": "2241553562",
                                    "name": "Antoine Doucet"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Systems, Man and Cybernetics",
                            "n_citations": 2
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "Recently, Lin (Lin, 2021) made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models."
                        ],
                        "paper": {
                            "corpus_id": 266230831,
                            "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1419474794",
                                    "name": "David Alfonso-Hermelo"
                                },
                                {
                                    "authorId": "2274328833",
                                    "name": "Vitor Jeronymo"
                                },
                                {
                                    "authorId": "2023642",
                                    "name": "Ehsan Kamalloo"
                                },
                                {
                                    "authorId": "2131640257",
                                    "name": "Carlos Lassance"
                                },
                                {
                                    "authorId": "2274330487",
                                    "name": "Rodrigo Nogueira"
                                },
                                {
                                    "authorId": "2166106776",
                                    "name": "Odunayo Ogundepo"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "47583894",
                                    "name": "Nandan Thakur"
                                },
                                {
                                    "authorId": "2109723027",
                                    "name": "Jheng-Hong Yang"
                                },
                                {
                                    "authorId": "2118895402",
                                    "name": "Xinyu Crystina Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Text Retrieval Conference",
                            "n_citations": 5
                        },
                        "score": 0.9072265625
                    },
                    {
                        "id": "(Lin, 2021)",
                        "snippets": [
                            "Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as (Karpukhin et al., 2020) did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation."
                        ],
                        "paper": {
                            "corpus_id": 238259539,
                            "title": "A proposed conceptual framework for a representational approach to information retrieval",
                            "authors": [
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                }
                            ],
                            "year": 2021,
                            "venue": "SIGIR Forum",
                            "n_citations": 53
                        },
                        "score": 0.935546875
                    },
                    {
                        "id": "(Nguyen et al., 2023)",
                        "snippets": [
                            "The third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies.\n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8]35]."
                        ],
                        "paper": {
                            "corpus_id": 257585074,
                            "title": "A Unified Framework for Learned Sparse Retrieval",
                            "authors": [
                                {
                                    "authorId": "2116028119",
                                    "name": "Thong Nguyen"
                                },
                                {
                                    "authorId": "22214396",
                                    "name": "Sean MacAvaney"
                                },
                                {
                                    "authorId": "2136074457",
                                    "name": "Andrew Yates"
                                }
                            ],
                            "year": 2023,
                            "venue": "European Conference on Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage",
                            "term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.\n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.\n\nUnlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance."
                        ],
                        "paper": {
                            "corpus_id": 268091298,
                            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                            "authors": [
                                {
                                    "authorId": "2268718776",
                                    "name": "Penghao Zhao"
                                },
                                {
                                    "authorId": "2288557803",
                                    "name": "Hailin Zhang"
                                },
                                {
                                    "authorId": "2289597580",
                                    "name": "Qinhan Yu"
                                },
                                {
                                    "authorId": "2288675277",
                                    "name": "Zhengren Wang"
                                },
                                {
                                    "authorId": "2288532368",
                                    "name": "Yunteng Geng"
                                },
                                {
                                    "authorId": "46182701",
                                    "name": "Fangcheng Fu"
                                },
                                {
                                    "authorId": "2249513224",
                                    "name": "Ling Yang"
                                },
                                {
                                    "authorId": "2277807793",
                                    "name": "Wentao Zhang"
                                },
                                {
                                    "authorId": "2277742543",
                                    "name": "Bin Cui"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 282
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Long et al., 2022)",
                        "snippets": [
                            "Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]."
                        ],
                        "paper": {
                            "corpus_id": 247292113,
                            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "2117205083",
                                    "name": "Qiong Gao"
                                },
                                {
                                    "authorId": "3271753",
                                    "name": "Kuan-sheng Zou"
                                },
                                {
                                    "authorId": "2149131512",
                                    "name": "Guangwei Xu"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                },
                                {
                                    "authorId": "2114009505",
                                    "name": "Rui Guo"
                                },
                                {
                                    "authorId": "2146054545",
                                    "name": "Jianfeng Xu"
                                },
                                {
                                    "authorId": "2084534507",
                                    "name": "Guanjun Jiang"
                                },
                                {
                                    "authorId": "30967877",
                                    "name": "Luxi Xing"
                                },
                                {
                                    "authorId": "2327836457",
                                    "name": "Ping Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 23
                        },
                        "score": 0.92431640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Matching Mechanism: Lexical vs. Semantic Matching",
                "tldr": "Sparse retrieval models like BM25 perform lexical matching by comparing exact terms between queries and documents, which excels at finding keyword matches but struggles with synonyms. Dense retrieval models like DPR enable semantic matching through learned representations, capturing meaning relationships even when exact words differ. (12 sources)",
                "text": "\nThe fundamental difference in how sparse and dense retrieval models evaluate relevance lies in their matching mechanisms. Sparse retrieval models like BM25 rely on *lexical matching*, which measures similarity based on the weighted occurrence of exact terms shared between queries and documents <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper>. This approach works by calculating statistical metrics such as term frequency and inverse document frequency to determine relevance <Paper corpusId=\"269448997\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. While effective at identifying documents with exact keyword matches, sparse models \"fail to capture synonyms and paraphrases\" <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper> and are \"vulnerable to the vocabulary mismatch problem\" where retrieval fails when query and document terms differ lexically <Paper corpusId=\"270702658\" paperTitle=\"(Jeong et al., 2024)\" isShortName></Paper>.\n\nIn contrast, dense retrieval models like DPR and SentenceTransformer enable *semantic matching* by representing text in a continuous vector space where similar meanings cluster together, regardless of the exact words used <Paper corpusId=\"259370750\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. These models are trained to encode semantic relationships, allowing them to recognize that terms like \"password\" and \"passwd\" or phrases like \"facebook change password\" and \"fb modify passwd\" are similar in meaning despite having no lexical overlap <Paper corpusId=\"259370750\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. This capability helps dense retrievers overcome the vocabulary mismatch problem that plagues sparse retrieval methods <Paper corpusId=\"259924840\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nThe distinct matching mechanisms lead to different strengths and weaknesses. Sparse retrieval models like BM25 excel at finding documents with precise term matches and are particularly effective at identifying passages with lower relevance values <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. They often provide high recall due to their ability to capture partial matches between queries and documents <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. Dense retrievers, on the other hand, are \"very effective at encoding passages characterized by high relevance labels\" <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> but may struggle with identifying passages of lower relevance value <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237366133\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\nEmpirical studies have shown that the results produced by sparse and dense retrievers have relatively little overlap <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3641284\" paperTitle=\"(McInnes et al., 2018)\" isShortName></Paper>, suggesting they capture complementary aspects of relevance. This complementary nature explains why hybrid approaches that combine both lexical and semantic matching often outperform either method alone <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"277113527\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sciavolino et al., 2021)",
                        "snippets": [
                            "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."
                        ],
                        "paper": {
                            "corpus_id": 237562875,
                            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
                            "authors": [
                                {
                                    "authorId": "2112021127",
                                    "name": "Christopher Sciavolino"
                                },
                                {
                                    "authorId": "49164966",
                                    "name": "Zexuan Zhong"
                                },
                                {
                                    "authorId": "46664096",
                                    "name": "Jinhyuk Lee"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 167
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Sparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents."
                        ],
                        "paper": {
                            "corpus_id": 269448997,
                            "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
                            "authors": [
                                {
                                    "authorId": "2265148831",
                                    "name": "Ran Xu"
                                },
                                {
                                    "authorId": "2263890944",
                                    "name": "Wenqi Shi"
                                },
                                {
                                    "authorId": "2218865512",
                                    "name": "Yue Yu"
                                },
                                {
                                    "authorId": "8103389",
                                    "name": "Yuchen Zhuang"
                                },
                                {
                                    "authorId": "2653121",
                                    "name": "Yanqiao Zhu"
                                },
                                {
                                    "authorId": "2237844925",
                                    "name": "M. D. Wang"
                                },
                                {
                                    "authorId": "2263536473",
                                    "name": "Joyce C. Ho"
                                },
                                {
                                    "authorId": "2256776233",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "2237940940",
                                    "name": "Carl Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 25
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Jeong et al., 2024)",
                        "snippets": [
                            "Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021). Typically, it can be performed with two types of models: sparse and dense retrievers. Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020; Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it. For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents. Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data."
                        ],
                        "paper": {
                            "corpus_id": 270702658,
                            "title": "Database-Augmented Query Representation for Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "8599185",
                                    "name": "Soyeong Jeong"
                                },
                                {
                                    "authorId": "90765684",
                                    "name": "Jinheon Baek"
                                },
                                {
                                    "authorId": "2158892171",
                                    "name": "Sukmin Cho"
                                },
                                {
                                    "authorId": "2260611009",
                                    "name": "Sung Ju Hwang"
                                },
                                {
                                    "authorId": "2109285560",
                                    "name": "Jong C. Park"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Lee et al., 2023)",
                        "snippets": [
                            "Classic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations."
                        ],
                        "paper": {
                            "corpus_id": 259370750,
                            "title": "On Complementarity Objectives for Hybrid Retrieval",
                            "authors": [
                                {
                                    "authorId": "2135607329",
                                    "name": "Dohyeon Lee"
                                },
                                {
                                    "authorId": "2153642272",
                                    "name": "Seung-won Hwang"
                                },
                                {
                                    "authorId": "79733119",
                                    "name": "Kyungjae Lee"
                                },
                                {
                                    "authorId": "5841595",
                                    "name": "Seungtaek Choi"
                                },
                                {
                                    "authorId": "2108106092",
                                    "name": "Sunghyun Park"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 5
                        },
                        "score": 0.9306640625
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem."
                        ],
                        "paper": {
                            "corpus_id": 259924840,
                            "title": "Learning to Retrieve In-Context Examples for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "145769448",
                                    "name": "Liang Wang"
                                },
                                {
                                    "authorId": "144610884",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 43
                        },
                        "score": 0.94873046875
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 248496043,
                            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
                            "authors": [
                                {
                                    "authorId": "2118384241",
                                    "name": "Hang Li"
                                },
                                {
                                    "authorId": "2146514461",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "143832672",
                                    "name": "Ahmed Mourad"
                                },
                                {
                                    "authorId": "2461713",
                                    "name": "Xueguang Ma"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 30
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "The integration of pre-trained deep language models, such as BERT, into retrieval and ranking pipelines has shown to provide large effectiveness gains over traditional bag-of-words models in the passage retrieval task. However, the best setup for integrating such deep language models is still unclear. When BERT is used to re-rank passages (i.e., BERT re-ranker), previous work has empirically shown that, while in practice BERT re-ranker cannot act as initial retriever due to BERT's high query time costs, and thus a bag-of-words model such as BM25 is required. It is not necessary to interpolate BERT re-ranker and bag-of-words scores to generate the final ranking. In fact, the BERT re-ranker scores alone can be used by the re-ranker: the BERT re-ranker score appears to already capture the relevance signal provided by BM25. In this paper, we further investigate the topic of interpolating BM25 and BERT-based rankers. Unlike previous work that considered the BERT re-ranker, however, here we consider BERT-based dense retrievers (RepBERT and ANCE). Dense retrievers encode queries and documents into low dimensional BERT-based embeddings. These methods overcome BERT's high computational costs at query time, and can thus be feasibly used in practice as whole-collection retrievers, rather than just as re-rankers. Our novel empirical findings suggest that, unlike for BERT re-ranker, interpolation with BM25 is necessary for BERT-based dense retrievers to perform effectively; and the gains provided by the interpolation are significant. Further analysis reveals why this is so: dense retrievers are very effective at encoding strong relevance signals, but they fail in identifying weaker relevance signals -- a task that the interpolation with BM25 is able to make up for."
                        ],
                        "paper": {
                            "corpus_id": 237366133,
                            "title": "BERT-based Dense Retrievers Require Interpolation with BM25 for Effective Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2146514461",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on the Theory of Information Retrieval",
                            "n_citations": 83
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hoshi et al., 2023)",
                        "snippets": [
                            "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models",
                            ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."
                        ],
                        "paper": {
                            "corpus_id": 257427642,
                            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
                            "authors": [
                                {
                                    "authorId": "2211101536",
                                    "name": "Yasuto Hoshi"
                                },
                                {
                                    "authorId": "2441156",
                                    "name": "D. Miyashita"
                                },
                                {
                                    "authorId": "51194024",
                                    "name": "Yasuhiro Morioka"
                                },
                                {
                                    "authorId": "20556792",
                                    "name": "Youyang Ng"
                                },
                                {
                                    "authorId": "2422593",
                                    "name": "Osamu Torii"
                                },
                                {
                                    "authorId": "49192096",
                                    "name": "J. Deguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(McInnes et al., 2018)",
                        "snippets": [
                            "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning."
                        ],
                        "paper": {
                            "corpus_id": 3641284,
                            "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
                            "authors": [
                                {
                                    "authorId": "31785573",
                                    "name": "Leland McInnes"
                                },
                                {
                                    "authorId": "2062756303",
                                    "name": "John Healy"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 9476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kim et al., 2025)",
                        "snippets": [
                            "Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers et al., 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024)"
                        ],
                        "paper": {
                            "corpus_id": 277113527,
                            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
                            "authors": [
                                {
                                    "authorId": "2350956983",
                                    "name": "Sejong Kim"
                                },
                                {
                                    "authorId": "2350857179",
                                    "name": "Hyunseo Song"
                                },
                                {
                                    "authorId": "2351317331",
                                    "name": "Hyunwoo Seo"
                                },
                                {
                                    "authorId": "2350867590",
                                    "name": "Hyunjun Kim"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9287109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Training Requirements: Supervised vs. Unsupervised",
                "tldr": "Sparse retrievers like BM25 are unsupervised methods that don't require training data, while dense retrievers like DPR need substantial supervised training. This fundamental difference affects generalization capabilities, with sparse retrievers showing stronger zero-shot performance across domains despite lower overall effectiveness when training data is available. (13 sources)",
                "text": "\nA key differentiator between sparse and dense retrieval models is their training requirements. Sparse retrieval models like BM25 are fundamentally unsupervised methods that don't require any training data <Paper corpusId=\"248366550\" paperTitle=\"(Penha et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. They calculate term weights using statistical methods based on term frequency and document frequency, operating effectively without learning from labeled examples. In contrast, dense retrieval models like DPR and SentenceTransformer are supervised approaches that require substantial training data to learn effective representations <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>.\n\nLin argues that comparing supervised dense retrievers to unsupervised sparse retrievers like BM25 represents an \"apples-to-oranges juxtaposition,\" and it should not be surprising that supervised techniques show higher effectiveness in domains where training data is available <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper>. This perspective is supported by organizing retrieval models along two dimensions: dense vs. sparse representations and supervised vs. unsupervised approaches <Paper corpusId=\"266230831\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper>.\n\nThe training differences significantly impact generalization capabilities. While dense retrievers consistently outperform BM25 when evaluated on in-domain data, their performance often deteriorates in zero-shot scenarios where the test data differs from the training distribution <Paper corpusId=\"248366550\" paperTitle=\"(Penha et al., 2022)\" isShortName></Paper>. Studies show that BM25 outperforms dense retrievers on most tasks in zero-shot evaluation settings <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"233296016\" paperTitle=\"(Thakur et al., 2021)\" isShortName></Paper>. This generalization gap is particularly evident for entity-centric questions and out-of-domain queries <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper>.\n\nThe supervised nature of dense retrievers brings additional training complexities. Modern approaches employ sophisticated techniques such as contrastive training with in-batch negatives and hard-negative mining to improve effectiveness <Paper corpusId=\"276107364\" paperTitle=\"(Abdallah et al., 2025)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220302524\" paperTitle=\"(Xiong et al., 2020)\" isShortName></Paper>. Many dense retrieval methods are initialized with pre-trained language models like BERT and then fine-tuned on domain-specific data <Paper corpusId=\"258840999\" paperTitle=\"(Cho et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nDespite the superior performance of dense retrievers in tasks with sufficient training data, the training-free nature of BM25 gives it practical advantages in some settings. BM25 remains widely used due to its generalization capabilities and strong lexical matching <Paper corpusId=\"258840999\" paperTitle=\"(Cho et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276107364\" paperTitle=\"(Abdallah et al., 2025)\" isShortName></Paper>. The generalization gap between dense and sparse retrievers has motivated research into models that can better transfer across domains and tasks without requiring domain-specific supervised data <Paper corpusId=\"273549875\" paperTitle=\"(Saberi et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Penha et al., 2022)",
                        "snippets": [
                            "An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods.\n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario."
                        ],
                        "paper": {
                            "corpus_id": 248366550,
                            "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
                            "authors": [
                                {
                                    "authorId": "145579682",
                                    "name": "Gustavo Penha"
                                },
                                {
                                    "authorId": "2731925",
                                    "name": "C. Hauff"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9521484375
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 248496043,
                            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
                            "authors": [
                                {
                                    "authorId": "2118384241",
                                    "name": "Hang Li"
                                },
                                {
                                    "authorId": "2146514461",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "143832672",
                                    "name": "Ahmed Mourad"
                                },
                                {
                                    "authorId": "2461713",
                                    "name": "Xueguang Ma"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 30
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Lin, 2021)",
                        "snippets": [
                            "Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as (Karpukhin et al., 2020) did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation."
                        ],
                        "paper": {
                            "corpus_id": 238259539,
                            "title": "A proposed conceptual framework for a representational approach to information retrieval",
                            "authors": [
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                }
                            ],
                            "year": 2021,
                            "venue": "SIGIR Forum",
                            "n_citations": 53
                        },
                        "score": 0.935546875
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "Recently, Lin (Lin, 2021) made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models."
                        ],
                        "paper": {
                            "corpus_id": 266230831,
                            "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1419474794",
                                    "name": "David Alfonso-Hermelo"
                                },
                                {
                                    "authorId": "2274328833",
                                    "name": "Vitor Jeronymo"
                                },
                                {
                                    "authorId": "2023642",
                                    "name": "Ehsan Kamalloo"
                                },
                                {
                                    "authorId": "2131640257",
                                    "name": "Carlos Lassance"
                                },
                                {
                                    "authorId": "2274330487",
                                    "name": "Rodrigo Nogueira"
                                },
                                {
                                    "authorId": "2166106776",
                                    "name": "Odunayo Ogundepo"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "47583894",
                                    "name": "Nandan Thakur"
                                },
                                {
                                    "authorId": "2109723027",
                                    "name": "Jheng-Hong Yang"
                                },
                                {
                                    "authorId": "2118895402",
                                    "name": "Xinyu Crystina Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Text Retrieval Conference",
                            "n_citations": 5
                        },
                        "score": 0.9072265625
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "On the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, (Karpukhin et al., 2020) show that DPR, unlike a sparse BM25 retriever (Robertson et al., 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, (Sciavolino et al., 2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and (Thakur et al., 2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks."
                        ],
                        "paper": {
                            "corpus_id": 238744204,
                            "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?",
                            "authors": [
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "1410624139",
                                    "name": "Kushal Lakhotia"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "3377939",
                                    "name": "Anchit Gupta"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3139260",
                                    "name": "Stanislav Peshterliev"
                                },
                                {
                                    "authorId": "2121361882",
                                    "name": "Yashar Mehdad"
                                },
                                {
                                    "authorId": "2118343423",
                                    "name": "Sonal Gupta"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 69
                        },
                        "score": 0.96240234375
                    },
                    {
                        "id": "(Thakur et al., 2021)",
                        "snippets": [
                            "Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir."
                        ],
                        "paper": {
                            "corpus_id": 233296016,
                            "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
                            "authors": [
                                {
                                    "authorId": "47583894",
                                    "name": "Nandan Thakur"
                                },
                                {
                                    "authorId": "2959414",
                                    "name": "Nils Reimers"
                                },
                                {
                                    "authorId": "1404060894",
                                    "name": "Andreas Ruckl'e"
                                },
                                {
                                    "authorId": "153257123",
                                    "name": "Abhishek Srivastava"
                                },
                                {
                                    "authorId": "69033154",
                                    "name": "Iryna Gurevych"
                                }
                            ],
                            "year": 2021,
                            "venue": "NeurIPS Datasets and Benchmarks",
                            "n_citations": 1055
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sciavolino et al., 2021)",
                        "snippets": [
                            "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."
                        ],
                        "paper": {
                            "corpus_id": 237562875,
                            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
                            "authors": [
                                {
                                    "authorId": "2112021127",
                                    "name": "Christopher Sciavolino"
                                },
                                {
                                    "authorId": "49164966",
                                    "name": "Zexuan Zhong"
                                },
                                {
                                    "authorId": "46664096",
                                    "name": "Jinhyuk Lee"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 167
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Abdallah et al., 2025)",
                        "snippets": [
                            "Sparse retrievers, such as BM25 (Robertson et al., 1994), remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods (Lee et al., 2019), which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR (Lin et al., 2020) and ANCE (Xiong et al., 2020), improve retrieval effectiveness by employing contrastive training with in-batch negatives (Gillick et al., 2019) and hard-negative mining (Karpukhin et al., 2020)(Xiong et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 276107364,
                            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2238699512",
                                    "name": "Abdelrahman Abdallah"
                                },
                                {
                                    "authorId": "1935823995",
                                    "name": "Bhawna Piryani"
                                },
                                {
                                    "authorId": "1389526186",
                                    "name": "Jamshid Mozafari"
                                },
                                {
                                    "authorId": "2343952059",
                                    "name": "Mohammed Ali"
                                },
                                {
                                    "authorId": "2261673463",
                                    "name": "Adam Jatowt"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Xiong et al., 2020)",
                        "snippets": [
                            "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up."
                        ],
                        "paper": {
                            "corpus_id": 220302524,
                            "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
                            "authors": [
                                {
                                    "authorId": "101655391",
                                    "name": "Lee Xiong"
                                },
                                {
                                    "authorId": "144628574",
                                    "name": "Chenyan Xiong"
                                },
                                {
                                    "authorId": "2110766301",
                                    "name": "Ye Li"
                                },
                                {
                                    "authorId": "1785396874",
                                    "name": "Kwok-Fung Tang"
                                },
                                {
                                    "authorId": "2108415378",
                                    "name": "Jialin Liu"
                                },
                                {
                                    "authorId": "144609235",
                                    "name": "Paul N. Bennett"
                                },
                                {
                                    "authorId": "144643947",
                                    "name": "Junaid Ahmed"
                                },
                                {
                                    "authorId": "2734525",
                                    "name": "Arnold Overwijk"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1234
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cho et al., 2023)",
                        "snippets": [
                            "We use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson et al., 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers."
                        ],
                        "paper": {
                            "corpus_id": 258840999,
                            "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
                            "authors": [
                                {
                                    "authorId": "2158892171",
                                    "name": "Sukmin Cho"
                                },
                                {
                                    "authorId": "8599185",
                                    "name": "Soyeong Jeong"
                                },
                                {
                                    "authorId": "2148402840",
                                    "name": "Jeong-yeon Seo"
                                },
                                {
                                    "authorId": "2109285560",
                                    "name": "Jong C. Park"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 24
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saberi et al., 2024)",
                        "snippets": [
                            "To analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data.\n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods.\n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data."
                        ],
                        "paper": {
                            "corpus_id": 273549875,
                            "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
                            "authors": [
                                {
                                    "authorId": "2085891275",
                                    "name": "Iman Saberi"
                                },
                                {
                                    "authorId": "2283136625",
                                    "name": "Fatemeh Fard"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8857421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Characteristics: Strengths and Weaknesses",
                "tldr": "Sparse retrievers like BM25 excel at precise keyword matching and generalization to new domains, while dense retrievers like DPR better capture semantic relationships but struggle with entity-centric queries without training. Both approaches have complementary strengths that make hybrid solutions often the most effective approach. (19 sources)",
                "text": "\n## Strengths of Sparse Retrieval (BM25)\n- **High recall capability**: BM25 excels at retrieving documents that partially match the query, offering strong recall even when documents are only somewhat relevant <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Zero-shot generalization**: Consistently outperforms dense retrievers on most tasks in zero-shot evaluation settings where no domain-specific training is available <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"233296016\" paperTitle=\"(Thakur et al., 2021)\" isShortName></Paper>\n- **No training required**: Functions effectively without any training data or parameters to tune <Paper corpusId=\"262066417\" paperTitle=\"(Pirozelli et al., 2023)\" isShortName></Paper>\n- **Computational efficiency**: Offers highly efficient search through inverted indices <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Precise term matching**: Particularly effective at capturing salient phrases and exact matches in queries <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>\n\n## Weaknesses of Sparse Retrieval (BM25)\n- **Vocabulary mismatch problem**: Fails when query and document use different terms to express the same concept <Paper corpusId=\"270702658\" paperTitle=\"(Jeong et al., 2024)\" isShortName></Paper>\n- **Limited semantic understanding**: Cannot capture synonyms, paraphrases, or deeper semantic relationships <Paper corpusId=\"268724187\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>\n- **Lower precision for highly relevant content**: Less effective at pushing the most relevant results to the top of rankings <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Bag-of-words limitation**: Treats text as unordered collections of terms, ignoring grammar, word order, and context <Paper corpusId=\"248496840\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>\n\n## Strengths of Dense Retrieval (DPR, SentenceTransformer)\n- **Semantic matching**: Captures meaningful relationships between concepts even when exact terms differ <Paper corpusId=\"262066417\" paperTitle=\"(Pirozelli et al., 2023)\" isShortName></Paper>\n- **Superior performance on highly relevant passages**: Very effective at encoding and retrieving passages with high relevance labels <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Better ranking precision**: Pushes the most relevant documents to the top of search results <Paper corpusId=\"276647280\" paperTitle=\"(Abdallah et al._1, 2025)\" isShortName></Paper>\n- **Cross-lingual capabilities**: Can perform retrieval across different languages and even different scripts, which is impossible with term-matching methods <Paper corpusId=\"258048596\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249097975\" paperTitle=\"(Izacard et al., 2021)\" isShortName></Paper>\n- **Efficient inference**: Once trained, dense retrievers can pre-compute document embeddings offline and perform efficient similarity calculations at query time <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235792476\" paperTitle=\"(Zhuang et al., 2021)\" isShortName></Paper>\n\n## Weaknesses of Dense Retrieval (DPR, SentenceTransformer)\n- **Training dependency**: Requires substantial training data to learn effective representations <Paper corpusId=\"262947262\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>\n- **Poor generalization to new domains**: Performs worse than BM25 on out-of-domain data without specific training <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>\n- **Struggles with entity-centric queries**: Shows poor performance compared to BM25 on simple entity-centric questions <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper>\n- **Misses salient phrases**: Sometimes fails to match important unique phrases in queries that BM25 can easily capture <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>\n- **Less effective for passages with lower relevance**: Struggles to identify passages that are only somewhat relevant to the query <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237366133\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>\n\n## Complementary Nature\n- **Little result overlap**: The results produced by sparse and dense retrievers have relatively little overlap, suggesting they capture different aspects of relevance <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3641284\" paperTitle=\"(McInnes et al., 2018)\" isShortName></Paper>\n- **Hybrid approaches excel**: Combining sparse and dense methods consistently outperforms either method alone <Paper corpusId=\"277451883\" paperTitle=\"(Hsu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"275906690\" paperTitle=\"(Some et al., 2025)\" isShortName></Paper> <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>\n- **Score interpolation benefits**: Interpolating BM25 scores with dense retriever scores significantly improves performance, as dense retrievers alone may miss weaker relevance signals that BM25 captures <Paper corpusId=\"237366133\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 248496043,
                            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
                            "authors": [
                                {
                                    "authorId": "2118384241",
                                    "name": "Hang Li"
                                },
                                {
                                    "authorId": "2146514461",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "143832672",
                                    "name": "Ahmed Mourad"
                                },
                                {
                                    "authorId": "2461713",
                                    "name": "Xueguang Ma"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 30
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "On the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, (Karpukhin et al., 2020) show that DPR, unlike a sparse BM25 retriever (Robertson et al., 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, (Sciavolino et al., 2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and (Thakur et al., 2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks."
                        ],
                        "paper": {
                            "corpus_id": 238744204,
                            "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?",
                            "authors": [
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "1410624139",
                                    "name": "Kushal Lakhotia"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "3377939",
                                    "name": "Anchit Gupta"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3139260",
                                    "name": "Stanislav Peshterliev"
                                },
                                {
                                    "authorId": "2121361882",
                                    "name": "Yashar Mehdad"
                                },
                                {
                                    "authorId": "2118343423",
                                    "name": "Sonal Gupta"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 69
                        },
                        "score": 0.96240234375
                    },
                    {
                        "id": "(Thakur et al., 2021)",
                        "snippets": [
                            "Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir."
                        ],
                        "paper": {
                            "corpus_id": 233296016,
                            "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
                            "authors": [
                                {
                                    "authorId": "47583894",
                                    "name": "Nandan Thakur"
                                },
                                {
                                    "authorId": "2959414",
                                    "name": "Nils Reimers"
                                },
                                {
                                    "authorId": "1404060894",
                                    "name": "Andreas Ruckl'e"
                                },
                                {
                                    "authorId": "153257123",
                                    "name": "Abhishek Srivastava"
                                },
                                {
                                    "authorId": "69033154",
                                    "name": "Iryna Gurevych"
                                }
                            ],
                            "year": 2021,
                            "venue": "NeurIPS Datasets and Benchmarks",
                            "n_citations": 1055
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pirozelli et al., 2023)",
                        "snippets": [
                            "Two types of IR models were investigated for this benchmark: BM25 (Robertson et al., 2009), a sparse retrieval, and Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack."
                        ],
                        "paper": {
                            "corpus_id": 262066417,
                            "title": "Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change",
                            "authors": [
                                {
                                    "authorId": "2243283631",
                                    "name": "Paulo Pirozelli"
                                },
                                {
                                    "authorId": "2140512811",
                                    "name": "M. M. Jos'e"
                                },
                                {
                                    "authorId": "2185349318",
                                    "name": "I. Silveira"
                                },
                                {
                                    "authorId": "2135817708",
                                    "name": "Fl'avio Nakasato"
                                },
                                {
                                    "authorId": "2125066",
                                    "name": "S. M. Peres"
                                },
                                {
                                    "authorId": "119984906",
                                    "name": "A. Brand\u00e3o"
                                },
                                {
                                    "authorId": "2243884199",
                                    "name": "Anna H. R. Costa"
                                },
                                {
                                    "authorId": "70089890",
                                    "name": "F. G. Cozman"
                                }
                            ],
                            "year": 2023,
                            "venue": "Data Intelligence",
                            "n_citations": 4
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Jeong et al., 2024)",
                        "snippets": [
                            "Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021). Typically, it can be performed with two types of models: sparse and dense retrievers. Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020; Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it. For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents. Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data."
                        ],
                        "paper": {
                            "corpus_id": 270702658,
                            "title": "Database-Augmented Query Representation for Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "8599185",
                                    "name": "Soyeong Jeong"
                                },
                                {
                                    "authorId": "90765684",
                                    "name": "Jinheon Baek"
                                },
                                {
                                    "authorId": "2158892171",
                                    "name": "Sukmin Cho"
                                },
                                {
                                    "authorId": "2260611009",
                                    "name": "Sung Ju Hwang"
                                },
                                {
                                    "authorId": "2109285560",
                                    "name": "Jong C. Park"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Traditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46]. These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears. However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context. These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.e., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text. The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space. Ranking models based on dense vectors is referred to as dense retrieval models. Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth. Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2."
                        ],
                        "paper": {
                            "corpus_id": 268724187,
                            "title": "Decoy Effect in Search Interaction: Understanding User Behavior and Measuring System Vulnerability",
                            "authors": [
                                {
                                    "authorId": "2257286538",
                                    "name": "Nuo Chen"
                                },
                                {
                                    "authorId": "2265515693",
                                    "name": "Jiqun Liu"
                                },
                                {
                                    "authorId": "2293660445",
                                    "name": "Hanpei Fang"
                                },
                                {
                                    "authorId": "2293898339",
                                    "name": "Yuankai Luo"
                                },
                                {
                                    "authorId": "2257233277",
                                    "name": "Tetsuya Sakai"
                                },
                                {
                                    "authorId": "2265517306",
                                    "name": "Xiao-Ming Wu"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 5
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval."
                        ],
                        "paper": {
                            "corpus_id": 248496840,
                            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2115569215",
                                    "name": "Yifan Wang"
                                },
                                {
                                    "authorId": "2110816708",
                                    "name": "Haodi Ma"
                                },
                                {
                                    "authorId": "2111220343",
                                    "name": "D. Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Proceedings of the VLDB Endowment",
                            "n_citations": 13
                        },
                        "score": 0.91455078125
                    },
                    {
                        "id": "(Abdallah et al._1, 2025)",
                        "snippets": [
                            "Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth",
                            ".BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively",
                            ".The results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness."
                        ],
                        "paper": {
                            "corpus_id": 276647280,
                            "title": "From Retrieval to Generation: Comparing Different Approaches",
                            "authors": [
                                {
                                    "authorId": "2238699512",
                                    "name": "Abdelrahman Abdallah"
                                },
                                {
                                    "authorId": "1389526186",
                                    "name": "Jamshid Mozafari"
                                },
                                {
                                    "authorId": "1935823995",
                                    "name": "Bhawna Piryani"
                                },
                                {
                                    "authorId": "2343952059",
                                    "name": "Mohammed Ali"
                                },
                                {
                                    "authorId": "2261673463",
                                    "name": "Adam Jatowt"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.912109375
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "Sparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24].\n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1]."
                        ],
                        "paper": {
                            "corpus_id": 258048596,
                            "title": "Learning to Tokenize for Generative Retrieval",
                            "authors": [
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2117203270",
                                    "name": "Zheng Chen"
                                },
                                {
                                    "authorId": "2386396",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2387872",
                                    "name": "Haichao Zhu"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "2136400100",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "1696030",
                                    "name": "M. de Rijke"
                                },
                                {
                                    "authorId": "2780667",
                                    "name": "Z. Ren"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 75
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Izacard et al., 2021)",
                        "snippets": [
                            "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods."
                        ],
                        "paper": {
                            "corpus_id": 249097975,
                            "title": "Unsupervised Dense Information Retrieval with Contrastive Learning",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "2062862676",
                                    "name": "Mathilde Caron"
                                },
                                {
                                    "authorId": "26360550",
                                    "name": "Lucas Hosseini"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "2329288",
                                    "name": "Piotr Bojanowski"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2021,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 924
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhuang et al., 2021)",
                        "snippets": [
                            "Deep language models (deep LMs) are increasingly being used for full text retrieval or within cascade retrieval pipelines as later-stage re-rankers. A problem with using deep LMs is that, at query time, a slow inference step needs to be performed -- this hinders the practical adoption of these powerful retrieval models, or limits sensibly how many documents can be considered for re-ranking. We propose the novel, BERT-based, Term Independent Likelihood moDEl (TILDE), which ranks documents by both query and document likelihood. At query time, our model does not require the inference step of deep language models based retrieval approaches, thus providing consistent time-savings, as the prediction of query terms' likelihood can be pre-computed and stored during index creation. This is achieved by relaxing the term dependence assumption made by the deep LMs. In addition, we have devised a novel bi-directional training loss which allows TILDE to maximise both query and document likelihood at the same time during training. At query time, TILDE can rely on its query likelihood component (TILDE-QL) solely, or the combination of TILDE-QL and its document likelihood component (TILDE-DL), thus providing a flexible trade-off between efficiency and effectiveness. Exploiting both components provide the highest effectiveness at a higher computational cost while relying only on TILDE-QL trades off effectiveness for faster response time due to no inference being required. TILDE is evaluated on the MS MARCO and TREC Deep Learning 2019 and 2020 passage ranking datasets. Empirical results show that, compared to other approaches that aim to make deep language models viable operationally, TILDE achieves competitive effectiveness coupled with low query latency."
                        ],
                        "paper": {
                            "corpus_id": 235792476,
                            "title": "TILDE: Term Independent Likelihood moDEl for Passage Re-ranking",
                            "authors": [
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 76
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Sparse retrieval models (Robertson et al., 1994) usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 262947262,
                            "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks",
                            "authors": [
                                {
                                    "authorId": "2085129",
                                    "name": "N. Li"
                                },
                                {
                                    "authorId": "2247665349",
                                    "name": "Yangjun Zhang"
                                },
                                {
                                    "authorId": "2247992622",
                                    "name": "Zhiming Zhao"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on e-Science",
                            "n_citations": 1
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Sciavolino et al., 2021)",
                        "snippets": [
                            "Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."
                        ],
                        "paper": {
                            "corpus_id": 237562875,
                            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
                            "authors": [
                                {
                                    "authorId": "2112021127",
                                    "name": "Christopher Sciavolino"
                                },
                                {
                                    "authorId": "49164966",
                                    "name": "Zexuan Zhong"
                                },
                                {
                                    "authorId": "46664096",
                                    "name": "Jinhyuk Lee"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 167
                        },
                        "score": 0.953125
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "The integration of pre-trained deep language models, such as BERT, into retrieval and ranking pipelines has shown to provide large effectiveness gains over traditional bag-of-words models in the passage retrieval task. However, the best setup for integrating such deep language models is still unclear. When BERT is used to re-rank passages (i.e., BERT re-ranker), previous work has empirically shown that, while in practice BERT re-ranker cannot act as initial retriever due to BERT's high query time costs, and thus a bag-of-words model such as BM25 is required. It is not necessary to interpolate BERT re-ranker and bag-of-words scores to generate the final ranking. In fact, the BERT re-ranker scores alone can be used by the re-ranker: the BERT re-ranker score appears to already capture the relevance signal provided by BM25. In this paper, we further investigate the topic of interpolating BM25 and BERT-based rankers. Unlike previous work that considered the BERT re-ranker, however, here we consider BERT-based dense retrievers (RepBERT and ANCE). Dense retrievers encode queries and documents into low dimensional BERT-based embeddings. These methods overcome BERT's high computational costs at query time, and can thus be feasibly used in practice as whole-collection retrievers, rather than just as re-rankers. Our novel empirical findings suggest that, unlike for BERT re-ranker, interpolation with BM25 is necessary for BERT-based dense retrievers to perform effectively; and the gains provided by the interpolation are significant. Further analysis reveals why this is so: dense retrievers are very effective at encoding strong relevance signals, but they fail in identifying weaker relevance signals -- a task that the interpolation with BM25 is able to make up for."
                        ],
                        "paper": {
                            "corpus_id": 237366133,
                            "title": "BERT-based Dense Retrievers Require Interpolation with BM25 for Effective Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2146514461",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on the Theory of Information Retrieval",
                            "n_citations": 83
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hoshi et al., 2023)",
                        "snippets": [
                            "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models",
                            ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."
                        ],
                        "paper": {
                            "corpus_id": 257427642,
                            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
                            "authors": [
                                {
                                    "authorId": "2211101536",
                                    "name": "Yasuto Hoshi"
                                },
                                {
                                    "authorId": "2441156",
                                    "name": "D. Miyashita"
                                },
                                {
                                    "authorId": "51194024",
                                    "name": "Yasuhiro Morioka"
                                },
                                {
                                    "authorId": "20556792",
                                    "name": "Youyang Ng"
                                },
                                {
                                    "authorId": "2422593",
                                    "name": "Osamu Torii"
                                },
                                {
                                    "authorId": "49192096",
                                    "name": "J. Deguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(McInnes et al., 2018)",
                        "snippets": [
                            "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning."
                        ],
                        "paper": {
                            "corpus_id": 3641284,
                            "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
                            "authors": [
                                {
                                    "authorId": "31785573",
                                    "name": "Leland McInnes"
                                },
                                {
                                    "authorId": "2062756303",
                                    "name": "John Healy"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 9476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hsu et al., 2025)",
                        "snippets": [
                            "Hybrid retrieval (Ma et al., 2020)(Sawarkar et al., 2024)Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap."
                        ],
                        "paper": {
                            "corpus_id": 277451883,
                            "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2349603255",
                                    "name": "Hsin-Ling Hsu"
                                },
                                {
                                    "authorId": "145168633",
                                    "name": "Jengnan Tzeng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.90478515625
                    },
                    {
                        "id": "(Some et al., 2025)",
                        "snippets": [
                            "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82]109]."
                        ],
                        "paper": {
                            "corpus_id": 275906690,
                            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                            "authors": [
                                {
                                    "authorId": "2342276561",
                                    "name": "Lilian Some"
                                },
                                {
                                    "authorId": "2341600949",
                                    "name": "Wenli Yang"
                                },
                                {
                                    "authorId": "2342277330",
                                    "name": "Michael Bain"
                                },
                                {
                                    "authorId": "2341910700",
                                    "name": "Byeong Kang"
                                }
                            ],
                            "year": 2025,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 0
                        },
                        "score": 0.92578125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implementation and Efficiency Considerations",
                "tldr": "Sparse and dense retrieval models use fundamentally different indexing and search architectures that impact their computational efficiency. Sparse retrievers like BM25 rely on inverted indices for fast term-based lookups, while dense retrievers require approximate nearest neighbor search algorithms to efficiently find similar vectors. (7 sources)",
                "text": "\nThe implementation architectures of sparse and dense retrieval models differ significantly, resulting in distinct efficiency characteristics that impact their practical deployment in RAG systems. Sparse retrieval models like BM25 leverage inverted indices, where each term maps to a list of documents containing that term <Paper corpusId=\"245334864\" paperTitle=\"(Piktus et al., 2021)\" isShortName></Paper>. This enables efficient query processing by retrieving only documents that contain at least one query term <Paper corpusId=\"269950696\" paperTitle=\"(Biswas et al., 2024)\" isShortName></Paper>. The constraint that sparse retrievers only produce non-negative weights comes from their reliance on these inverted indexing software stacks built for traditional lexical search <Paper corpusId=\"257585074\" paperTitle=\"(Nguyen et al., 2023)\" isShortName></Paper>.\n\nIn contrast, dense retrieval models encode queries and documents into continuous vector representations and require approximate nearest neighbor (ANN) search algorithms for efficient retrieval <Paper corpusId=\"247292113\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268091298\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. This architectural difference creates a tradeoff: while dense retrievers offer better semantic matching, they introduce computational complexities that sparse retrievers avoid. As Biswas et al. note, \"While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large\" <Paper corpusId=\"269950696\" paperTitle=\"(Biswas et al., 2024)\" isShortName></Paper>.\n\nDense retrievers do offer an important efficiency advantage in their workflow structure. They can pre-compute document embeddings offline during indexing, allowing efficient similarity calculations at query time <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. This approach enables \"retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness\" <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. The ColBERT model exemplifies this advantage, as it \"can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing\" <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>.\n\nThe implementation differences between sparse and dense retrievers create distinct scaling characteristics. While sparse retrievers like BM25 maintain consistent performance as corpus size increases, dense retrievers may face challenges with very large collections due to the computational demands of ANN search. However, advancements in vector database technology and ANN algorithms continue to improve the scalability of dense retrieval methods <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThese architectural differences ultimately influence the choice of retrieval model in practical RAG systems. Organizations with limited computational resources might prefer sparse retrievers for their efficiency, while those prioritizing retrieval quality might opt for dense retrievers despite their higher computational requirements. The complementary strengths of both approaches have also led to hybrid systems that combine their indexing and search capabilities to achieve both efficiency and effectiveness <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Piktus et al., 2021)",
                        "snippets": [
                            "We consider two retrieval architectures. BM25 (Robertson et al., 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval."
                        ],
                        "paper": {
                            "corpus_id": 245334864,
                            "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus",
                            "authors": [
                                {
                                    "authorId": "120174856",
                                    "name": "Aleksandra Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "113568063",
                                    "name": "Dmytro Okhonko"
                                },
                                {
                                    "authorId": "2966239",
                                    "name": "Samuel Broscheit"
                                },
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "1628391446",
                                    "name": "Barlas Ouguz"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 66
                        },
                        "score": 0.93408203125
                    },
                    {
                        "id": "(Biswas et al., 2024)",
                        "snippets": [
                            "Existing retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson et al., 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large."
                        ],
                        "paper": {
                            "corpus_id": 269950696,
                            "title": "Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data",
                            "authors": [
                                {
                                    "authorId": "2073867373",
                                    "name": "Biplob Biswas"
                                },
                                {
                                    "authorId": "2253261",
                                    "name": "R. Ramnath"
                                }
                            ],
                            "year": 2024,
                            "venue": "ECNLP",
                            "n_citations": 1
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Nguyen et al., 2023)",
                        "snippets": [
                            "The third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies.\n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8]35]."
                        ],
                        "paper": {
                            "corpus_id": 257585074,
                            "title": "A Unified Framework for Learned Sparse Retrieval",
                            "authors": [
                                {
                                    "authorId": "2116028119",
                                    "name": "Thong Nguyen"
                                },
                                {
                                    "authorId": "22214396",
                                    "name": "Sean MacAvaney"
                                },
                                {
                                    "authorId": "2136074457",
                                    "name": "Andrew Yates"
                                }
                            ],
                            "year": 2023,
                            "venue": "European Conference on Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Long et al., 2022)",
                        "snippets": [
                            "Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]."
                        ],
                        "paper": {
                            "corpus_id": 247292113,
                            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "2117205083",
                                    "name": "Qiong Gao"
                                },
                                {
                                    "authorId": "3271753",
                                    "name": "Kuan-sheng Zou"
                                },
                                {
                                    "authorId": "2149131512",
                                    "name": "Guangwei Xu"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                },
                                {
                                    "authorId": "2114009505",
                                    "name": "Rui Guo"
                                },
                                {
                                    "authorId": "2146054545",
                                    "name": "Jianfeng Xu"
                                },
                                {
                                    "authorId": "2084534507",
                                    "name": "Guanjun Jiang"
                                },
                                {
                                    "authorId": "30967877",
                                    "name": "Luxi Xing"
                                },
                                {
                                    "authorId": "2327836457",
                                    "name": "Ping Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 23
                        },
                        "score": 0.92431640625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage",
                            "term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.\n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.\n\nUnlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance."
                        ],
                        "paper": {
                            "corpus_id": 268091298,
                            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                            "authors": [
                                {
                                    "authorId": "2268718776",
                                    "name": "Penghao Zhao"
                                },
                                {
                                    "authorId": "2288557803",
                                    "name": "Hailin Zhang"
                                },
                                {
                                    "authorId": "2289597580",
                                    "name": "Qinhan Yu"
                                },
                                {
                                    "authorId": "2288675277",
                                    "name": "Zhengren Wang"
                                },
                                {
                                    "authorId": "2288532368",
                                    "name": "Yunteng Geng"
                                },
                                {
                                    "authorId": "46182701",
                                    "name": "Fangcheng Fu"
                                },
                                {
                                    "authorId": "2249513224",
                                    "name": "Ling Yang"
                                },
                                {
                                    "authorId": "2277807793",
                                    "name": "Wentao Zhang"
                                },
                                {
                                    "authorId": "2277742543",
                                    "name": "Bin Cui"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 282
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 248496043,
                            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
                            "authors": [
                                {
                                    "authorId": "2118384241",
                                    "name": "Hang Li"
                                },
                                {
                                    "authorId": "2146514461",
                                    "name": "Shuai Wang"
                                },
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "143832672",
                                    "name": "Ahmed Mourad"
                                },
                                {
                                    "authorId": "2461713",
                                    "name": "Xueguang Ma"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 30
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Complementary Nature and Hybrid Approaches",
                "tldr": "Sparse retrieval methods like BM25 and dense retrieval methods like DPR capture different aspects of relevance with minimal results overlap, making hybrid approaches that combine both methods consistently outperform either method alone. (5 sources)",
                "text": "\nDespite their fundamental differences, sparse and dense retrieval models aren't inherently competing approaches but rather complementary techniques that excel in different scenarios. Studies have shown that the overlap of results between dense and sparse retrieval methods is quite small, indicating that they capture different aspects of relevance <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper>. This limited overlap suggests that each retrieval approach identifies different relevant documents that the other might miss.\n\nThis complementary nature has led to the development of hybrid retrieval approaches that combine the strengths of both sparse and dense methods. These hybrid systems have consistently demonstrated superior performance compared to either method used in isolation <Paper corpusId=\"277451883\" paperTitle=\"(Hsu et al., 2025)\" isShortName></Paper>. By combining BM25's precise keyword matching capabilities with dense retrieval's semantic understanding, hybrid approaches can address the weaknesses of each individual method while leveraging their respective strengths.\n\nThe effectiveness of hybrid approaches has been well-documented across various retrieval tasks. For example, the combination of Dense Passage Retrieval (DPR) with traditional sparse retrieval methods like BM25 has been shown to enhance retrieval precision, particularly in complex or high-precision tasks <Paper corpusId=\"275906690\" paperTitle=\"(Some et al., 2025)\" isShortName></Paper>. This improvement stems from BM25's ability to capture exact lexical matches that dense retrievers might miss, while dense methods contribute semantic matching capabilities that identify relevant passages without lexical overlap.\n\nEmpirically, researchers have demonstrated that ensemble results combining the relevance scores from both sparse and dense retrievers can exceed the performance of each approach used alone <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. More recently, advanced hybrid approaches like \"Blended RAG\" have leveraged both dense vector indexes and sparse encoder indexes with hybrid query strategies to achieve superior retrieval results across benchmark datasets <Paper corpusId=\"277451883\" paperTitle=\"(Hsu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269043117\" paperTitle=\"(Sawarkar et al., 2024)\" isShortName></Paper>.\n\nIn practical RAG applications, this complementary relationship suggests that rather than choosing between sparse and dense retrieval, the most effective approach often involves integrating both methods. Such integration allows RAG systems to benefit from BM25's robust keyword matching and zero-shot capabilities while also leveraging the semantic understanding provided by dense retrievers, resulting in more comprehensive and accurate information retrieval.",
                "citations": [
                    {
                        "id": "(Hoshi et al., 2023)",
                        "snippets": [
                            "Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models",
                            ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."
                        ],
                        "paper": {
                            "corpus_id": 257427642,
                            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
                            "authors": [
                                {
                                    "authorId": "2211101536",
                                    "name": "Yasuto Hoshi"
                                },
                                {
                                    "authorId": "2441156",
                                    "name": "D. Miyashita"
                                },
                                {
                                    "authorId": "51194024",
                                    "name": "Yasuhiro Morioka"
                                },
                                {
                                    "authorId": "20556792",
                                    "name": "Youyang Ng"
                                },
                                {
                                    "authorId": "2422593",
                                    "name": "Osamu Torii"
                                },
                                {
                                    "authorId": "49192096",
                                    "name": "J. Deguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(Hsu et al., 2025)",
                        "snippets": [
                            "Hybrid retrieval (Ma et al., 2020)(Sawarkar et al., 2024)Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap."
                        ],
                        "paper": {
                            "corpus_id": 277451883,
                            "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2349603255",
                                    "name": "Hsin-Ling Hsu"
                                },
                                {
                                    "authorId": "145168633",
                                    "name": "Jengnan Tzeng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.90478515625
                    },
                    {
                        "id": "(Some et al., 2025)",
                        "snippets": [
                            "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82]109]."
                        ],
                        "paper": {
                            "corpus_id": 275906690,
                            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                            "authors": [
                                {
                                    "authorId": "2342276561",
                                    "name": "Lilian Some"
                                },
                                {
                                    "authorId": "2341600949",
                                    "name": "Wenli Yang"
                                },
                                {
                                    "authorId": "2342277330",
                                    "name": "Michael Bain"
                                },
                                {
                                    "authorId": "2341910700",
                                    "name": "Byeong Kang"
                                }
                            ],
                            "year": 2025,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 0
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sawarkar et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."
                        ],
                        "paper": {
                            "corpus_id": 269043117,
                            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
                            "authors": [
                                {
                                    "authorId": "2003089508",
                                    "name": "Kunal Sawarkar"
                                },
                                {
                                    "authorId": "2295990127",
                                    "name": "Abhilasha Mangal"
                                },
                                {
                                    "authorId": "2295990033",
                                    "name": "S. R. Solanki"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Multimedia Information Processing and Retrieval",
                            "n_citations": 57
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.323718
    }
}