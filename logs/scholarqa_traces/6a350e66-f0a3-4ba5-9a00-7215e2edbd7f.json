{
    "query": "key advantages challenges of building multilingual evaluation datasets translating existing English datasets into other target languages how datasets the quality reliability of cross lingual evaluations what potential pitfalls when translations as primary method non English benchmarks",
    "user_id": "lib_user",
    "task_id": "6a350e66-f0a3-4ba5-9a00-7215e2edbd7f",
    "timestamp": "2025-06-23T22:10:00.391839",
    "n_retrieval": 256,
    "n_retrieved": 269,
    "n_candidates": 41,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.4935330000000001,
    "decomposed_query": {
        "rewritten_query": "Key advantages and challenges of building multilingual evaluation datasets by translating existing English datasets into other target languages, how these datasets affect the quality and reliability of cross-lingual evaluations, and what potential pitfalls exist when using translations as the primary method for non-English benchmarks.",
        "keyword_query": "multilingual evaluation datasets translating English datasets target languages quality reliability cross-lingual evaluations pitfalls translations non-English benchmarks",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.0,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282539942",
                    "name": "Gonccalo Gomes"
                },
                {
                    "authorId": "36259430",
                    "name": "Chrysoula Zerva"
                },
                {
                    "authorId": "2344751390",
                    "name": "Bruno Martins"
                }
            ],
            "abstract": "The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.",
            "corpus_id": 276249513,
            "sentences": [
                {
                    "corpus_id": "276249513",
                    "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
                    "text": "This study highlights the importance of expanding image captioning evaluation to include multilingual and multicultural research, encouraging more inclusive frameworks in this field. Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models. \n\nWe also propose a finetuning strategy to better leverage and learn from both multi-cultural data and human preferences, and test our models on a set of different datasets and tasks. Our findings show that multilingual models trained with the same amount but with less English-specific data perform equally well on English tasks, while excelling in multilingual and multicultural ones. This reveals the potential of multilingual models to generalize across languages, making them more versatile assets. Additionally, our finetuning approach significantly boosted the model's ability to handle complex linguistic challenges, such as quantifiers, plurality, and numeric inconsistencies, highlighting its adaptability to more intricate language patterns. \n\nFurther to machine translation of English data, we also propose a strategy to adapt multilingual datasets from other tasks to support captioning evaluation. The integration of natively multilingual and multicultural datasets into both training and evaluation processes mitigates cultural information loss, reinforcing the reliability of our proposed pipeline for training and evaluating multilingual CLIP models, and making them effective tools for real-world multilingual and multicultural evaluation. \n\nOverall, our work contributes to multilingual captioning evaluation, both in terms of modelling and benchmarking. We hope it will inspire and support further work in this under-researched field.",
                    "score": 0.6035584650444683,
                    "section_title": "Conclusions",
                    "char_start_offset": 30484,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 494
                        },
                        {
                            "start": 497,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 998
                        },
                        {
                            "start": 999,
                            "end": 1247
                        },
                        {
                            "start": 1250,
                            "end": 1406
                        },
                        {
                            "start": 1407,
                            "end": 1752
                        },
                        {
                            "start": 1755,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 1949
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Gonccalo Gomes, Chrysoula Zerva, Bruno Martins\n## Abstract\nThe evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.\n## Conclusions\nThis study highlights the importance of expanding image captioning evaluation to include multilingual and multicultural research, encouraging more inclusive frameworks in this field. Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models. \n\nWe also propose a finetuning strategy to better leverage and learn from both multi-cultural data and human preferences, and test our models on a set of different datasets and tasks. Our findings show that multilingual models trained with the same amount but with less English-specific data perform equally well on English tasks, while excelling in multilingual and multicultural ones. This reveals the potential of multilingual models to generalize across languages, making them more versatile assets. Additionally, our finetuning approach significantly boosted the model's ability to handle complex linguistic challenges, such as quantifiers, plurality, and numeric inconsistencies, highlighting its adaptability to more intricate language patterns. \n\nFurther to machine translation of English data, we also propose a strategy to adapt multilingual datasets from other tasks to support captioning evaluation. The integration of natively multilingual and multicultural datasets into both training and evaluation processes mitigates cultural information loss, reinforcing the reliability of our proposed pipeline for training and evaluating multilingual CLIP models, and making them effective tools for real-world multilingual and multicultural evaluation. \n\nOverall, our work contributes to multilingual captioning evaluation, both in terms of modelling and benchmarking. We hope it will inspire and support further work in this under-researched field.",
            "reference_string": "[276249513 | Gomes et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 81,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327246234",
                    "name": "Junhua Liu"
                },
                {
                    "authorId": "2325881142",
                    "name": "Bin Fu"
                }
            ],
            "abstract": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.",
            "corpus_id": 273532366,
            "sentences": [
                {
                    "corpus_id": "273532366",
                    "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                    "text": "The datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2. \n\n1) Datasets: Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets Fig. 3: Evaluation methods for MLLMs XTREME, XTREME-R, and XGLUE based on them. The goal of multilingual evaluation datasets is to cover more diverse tasks and languages so that we can better evaluate the generalization performance of MLLMs. At the same time, some researchers are also dedicated to building evaluation datasets for specific languages, such as IndicXTREME for Indian languages. \n\nIn general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation. \n\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks.",
                    "score": 0.7367452113483335,
                    "section_title": "B. Evaluation Tasks",
                    "char_start_offset": 28760,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 383
                        },
                        {
                            "start": 386,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1128
                        },
                        {
                            "start": 1131,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 2242
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90771484375
                }
            ],
            "relevance_judgement": 0.90771484375,
            "relevance_judgment_input_expanded": "# Title: Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact\n# Venue: arXiv.org\n# Authors: Junhua Liu, Bin Fu\n## Abstract\nMultilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.\n## B. Evaluation Tasks\nThe datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2. \n\n1) Datasets: Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets Fig. 3: Evaluation methods for MLLMs XTREME, XTREME-R, and XGLUE based on them. The goal of multilingual evaluation datasets is to cover more diverse tasks and languages so that we can better evaluate the generalization performance of MLLMs. At the same time, some researchers are also dedicated to building evaluation datasets for specific languages, such as IndicXTREME for Indian languages. \n\nIn general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation. \n\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks.",
            "reference_string": "[273532366 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.05049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304752238",
                    "name": "Israel Abebe Azime"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "1484858320",
                    "name": "Tadesse Destaw Belay"
                },
                {
                    "authorId": "2238004186",
                    "name": "Yonas Chanie"
                },
                {
                    "authorId": "2305622331",
                    "name": "Bontu Fufa Balcha"
                },
                {
                    "authorId": "2330082951",
                    "name": "Negasi Haile Abadi"
                },
                {
                    "authorId": "2305619369",
                    "name": "Henok Biadglign Ademtew"
                },
                {
                    "authorId": "2330082798",
                    "name": "Mulubrhan Abebe Nerea"
                },
                {
                    "authorId": "1665980742",
                    "name": "D. Yadeta"
                },
                {
                    "authorId": "2330083054",
                    "name": "Derartu Dagne Geremew"
                },
                {
                    "authorId": "2330083089",
                    "name": "Assefa Atsbiha tesfau"
                },
                {
                    "authorId": "2292260327",
                    "name": "Philipp Slusallek"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "2561225",
                    "name": "D. Klakow"
                }
            ],
            "abstract": "With the rapid development of evaluation datasets to assess LLMs understanding across a wide range of subjects and domains, identifying a suitable language understanding benchmark has become increasingly challenging. In this work, we explore LLM evaluation challenges for low-resource language understanding and introduce \\proverbeval, LLM evaluation benchmark for low-resource languages, focusing on low-resource language understanding in culture-specific scenarios. We benchmark various LLMs and explore factors that create variability in the benchmarking process. We observed performance variances of up to 50\\%, depending on the order in which answer choices were presented in multiple-choice tasks. Native language proverb descriptions significantly improve tasks such as proverb generation, contributing to improved outcomes. Additionally, monolingual evaluations consistently outperformed their cross-lingual counterparts in generation tasks. We argue that special attention must be given to the order of choices, the choice of prompt language, task variability, and generation tasks when creating LLM evaluation benchmarks. Evaluation data available at https://huggingface.co/datasets/israel/ProverbEval, evaluation code https://github.com/EthioNLP/EthioProverbEval.",
            "corpus_id": 273950544,
            "sentences": [
                {
                    "corpus_id": "273950544",
                    "title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding",
                    "text": "Large language models (LLMs) evaluation is gaining increasing attention as these models are typically trained on general-domain datasets while demonstrating notable performance on tasks out of their training domains (Mosbach et al., 2023). The creation of evaluation datasets helps to identify the capabilities of LLMs, pinpoint shortcomings, and establish a measurable path for improvement. Based on Chang et al. (2024), LLM evaluation addresses questions such as what to evaluate (subjects and topics), where to evaluate (selecting appropriate datasets), and how to evaluate (the evaluation process). \n\nTo improve LLMs' capabilities and effectively assess their performance, researchers are creating benchmark datasets using a diverse range of domains and languages. This inclusive methodology allows for a more comprehensive evaluation of LLMs' performance across various domains and languages. Popular benchmark datasets like MMLU (Hendrycks et al., 2020) and MEGA-VERSE (Ahuja et al., 2023) cover a wide range of extensive world knowledge tasks and subjects. \n\nTo create evaluation benchmarks that are multilingual, researchers Koto et al. (2024); Li et al. (2023); Son et al. (2024) introduced benchmark datasets for different languages by translating a subset of the MMLU dataset. Beyond research efforts, translating existing benchmarks into different languages is an effective strategy to evaluate the multilingual capabilities of closed-source LLMs. These benchmarks evaluate multilingual understanding of models by presenting a range of extensive world knowledge tasks in the language of interest. While combining different subjects in a benchmark dataset may seem beneficial, it does not always provide a clear picture of the model's shortcomings. For example, using MMLU in different languages tests language and subject understanding simultaneously (Hendrycks et al., 2020). There should be evaluation benchmarks that disentangle language understanding and specific subject knowledge. \n\nLanguage understanding of LLM can be measured in numerous ways, and it is crucial to introduce benchmarks that evaluate complex text comprehension while considering each language's specific linguistic, cultural, and contextual nuances.",
                    "score": 0.7219752042114613,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 602
                        },
                        {
                            "start": 605,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1063
                        },
                        {
                            "start": 1066,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1888
                        },
                        {
                            "start": 1889,
                            "end": 1998
                        },
                        {
                            "start": 2001,
                            "end": 2236
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 401,
                            "end": 420,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 935,
                            "end": 959,
                            "matchedPaperCorpusId": "267760288"
                        },
                        {
                            "start": 1133,
                            "end": 1151,
                            "matchedPaperCorpusId": "267760288"
                        },
                        {
                            "start": 1863,
                            "end": 1887,
                            "matchedPaperCorpusId": "267760288"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Israel Abebe Azime, A. Tonja, Tadesse Destaw Belay, Yonas Chanie, Bontu Fufa Balcha, Negasi Haile Abadi, Henok Biadglign Ademtew, Mulubrhan Abebe Nerea, D. Yadeta, Derartu Dagne Geremew, Assefa Atsbiha tesfau, Philipp Slusallek, T. Solorio, D. Klakow\n## Abstract\nWith the rapid development of evaluation datasets to assess LLMs understanding across a wide range of subjects and domains, identifying a suitable language understanding benchmark has become increasingly challenging. In this work, we explore LLM evaluation challenges for low-resource language understanding and introduce \\proverbeval, LLM evaluation benchmark for low-resource languages, focusing on low-resource language understanding in culture-specific scenarios. We benchmark various LLMs and explore factors that create variability in the benchmarking process. We observed performance variances of up to 50\\%, depending on the order in which answer choices were presented in multiple-choice tasks. Native language proverb descriptions significantly improve tasks such as proverb generation, contributing to improved outcomes. Additionally, monolingual evaluations consistently outperformed their cross-lingual counterparts in generation tasks. We argue that special attention must be given to the order of choices, the choice of prompt language, task variability, and generation tasks when creating LLM evaluation benchmarks. Evaluation data available at https://huggingface.co/datasets/israel/ProverbEval, evaluation code https://github.com/EthioNLP/EthioProverbEval.\n## Introduction\nLarge language models (LLMs) evaluation is gaining increasing attention as these models are typically trained on general-domain datasets while demonstrating notable performance on tasks out of their training domains (Mosbach et al., 2023). The creation of evaluation datasets helps to identify the capabilities of LLMs, pinpoint shortcomings, and establish a measurable path for improvement. Based on Chang et al. (2024), LLM evaluation addresses questions such as what to evaluate (subjects and topics), where to evaluate (selecting appropriate datasets), and how to evaluate (the evaluation process). \n\nTo improve LLMs' capabilities and effectively assess their performance, researchers are creating benchmark datasets using a diverse range of domains and languages. This inclusive methodology allows for a more comprehensive evaluation of LLMs' performance across various domains and languages. Popular benchmark datasets like MMLU (Hendrycks et al., 2020) and MEGA-VERSE (Ahuja et al., 2023) cover a wide range of extensive world knowledge tasks and subjects. \n\nTo create evaluation benchmarks that are multilingual, researchers Koto et al. (2024); Li et al. (2023); Son et al. (2024) introduced benchmark datasets for different languages by translating a subset of the MMLU dataset. Beyond research efforts, translating existing benchmarks into different languages is an effective strategy to evaluate the multilingual capabilities of closed-source LLMs. These benchmarks evaluate multilingual understanding of models by presenting a range of extensive world knowledge tasks in the language of interest. While combining different subjects in a benchmark dataset may seem beneficial, it does not always provide a clear picture of the model's shortcomings. For example, using MMLU in different languages tests language and subject understanding simultaneously (Hendrycks et al., 2020). There should be evaluation benchmarks that disentangle language understanding and specific subject knowledge. \n\nLanguage understanding of LLM can be measured in numerous ways, and it is crucial to introduce benchmarks that evaluate complex text comprehension while considering each language's specific linguistic, cultural, and contextual nuances.",
            "reference_string": "[273950544 | Azime et al. | 2024 | Citations: 2]"
        },
        {
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 60,
            "citation_count": 509,
            "influential_citation_count": 125,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.653.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.07475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "1905713",
                    "name": "Ruty Rinott"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                },
                {
                    "authorId": "144518416",
                    "name": "Holger Schwenk"
                }
            ],
            "abstract": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
            "corpus_id": 204734128,
            "sentences": [
                {
                    "corpus_id": "204734128",
                    "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                    "text": "First, we state our desired properties for a crosslingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section 3). We then describe our annotation protocol, which seeks to fulfil these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \n\nDiverse Languages A primary goal of crosslingual research is to develop systems that work well in many languages. The dataset should enable quantitative performance comparison across languages with different linguistic resources, language families and scripts. \n\nExtractive QA Cross-lingual understanding benchmarks are typically based on classification (Conneau et al., 2018). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extractive QA datasets in a number of languages (see Section 3), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \n\nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desirable to select a textual domain that matches existing extractive QA training resources, in order to isolate the change in performance due to language transfer. \n\nTo satisfy these desiderata, we identified the method described below and illustrated in Figure 1. Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset.",
                    "score": 0.6366745478205195,
                    "section_title": "The MLQA corpus",
                    "char_start_offset": 4847,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 285
                        },
                        {
                            "start": 288,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 682
                        },
                        {
                            "start": 685,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1336
                        },
                        {
                            "start": 1339,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1768
                        },
                        {
                            "start": 1771,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 2026
                        },
                        {
                            "start": 2029,
                            "end": 2127
                        },
                        {
                            "start": 2128,
                            "end": 2273
                        },
                        {
                            "start": 2274,
                            "end": 2437
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                },
                {
                    "corpus_id": "204734128",
                    "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                    "text": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
                    "score": 0.6149261334429743,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7900390625
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: MLQA: Evaluating Cross-lingual Extractive Question Answering\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Patrick Lewis, Barlas O\u011fuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk\n## Abstract\nQuestion answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.\n## The MLQA corpus\nFirst, we state our desired properties for a crosslingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section 3). We then describe our annotation protocol, which seeks to fulfil these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \n\nDiverse Languages A primary goal of crosslingual research is to develop systems that work well in many languages. The dataset should enable quantitative performance comparison across languages with different linguistic resources, language families and scripts. \n\nExtractive QA Cross-lingual understanding benchmarks are typically based on classification (Conneau et al., 2018). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extractive QA datasets in a number of languages (see Section 3), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \n\nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desirable to select a textual domain that matches existing extractive QA training resources, in order to isolate the change in performance due to language transfer. \n\nTo satisfy these desiderata, we identified the method described below and illustrated in Figure 1. Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset.",
            "reference_string": "[204734128 | Lewis et al. | 2019 | Citations: 509]"
        },
        {
            "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 52,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345018196",
                    "name": "Xu Huang"
                },
                {
                    "authorId": "2279840735",
                    "name": "Wenhao Zhu"
                },
                {
                    "authorId": "2345123408",
                    "name": "Hanxu Hu"
                },
                {
                    "authorId": "2351236188",
                    "name": "Conghui He"
                },
                {
                    "authorId": "2151532469",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2339084258",
                    "name": "Shujian Huang"
                },
                {
                    "authorId": "2279544481",
                    "name": "Fei Yuan"
                }
            ],
            "abstract": "Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.",
            "corpus_id": 276259236,
            "sentences": [
                {
                    "corpus_id": "276259236",
                    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
                    "text": "To ensure high quality, we devise an annotation framework to optimize the dataset quality with human effort and LLM feedback. The process involves translating data from English to selected non-English languages using machine translation systems, post-editing each sample by three nativespeaking annotators with multiple iterations across most tasks, and picking the final translation version using a strong LLM that involves swapping sample positions for debiasing (Wang et al., 2024b;Li et al., 2024). \n\nLeading multilingual LLMs are evaluated on BenchMAX, revealing that language significantly influences language-agnostic capabilities of existing LLMs. Interestingly, simply increasing the parameters can boost average performance on these tasks but does not universally reduce the performance gap across languages. Moreover, compared to general translation, domain translation not only poses new challenges for LLMs but also requires new evaluation metrics. The main contributions can be summarized as follows: \n\n\u2022 We develop a comprehensive, multi-way multilingual benchmark across 17 languages for evaluating 6 crucial capabilities on 10 diverse tasks. \u2022 We propose a pipeline for curating high-quality mutlilingual datasets, involving both human annotation and LLM-as-a-judge.",
                    "score": 0.6445967921292717,
                    "section_title": "Introduction",
                    "char_start_offset": 3804,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 502
                        },
                        {
                            "start": 505,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1014
                        },
                        {
                            "start": 1017,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1283
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 465,
                            "end": 485,
                            "matchedPaperCorpusId": "258960339"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.865234375
                },
                {
                    "corpus_id": "276259236",
                    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
                    "text": "Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.",
                    "score": 0.5986215589582607,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83740234375
                }
            ],
            "relevance_judgement": 0.865234375,
            "relevance_judgment_input_expanded": "# Title: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\n# Venue: arXiv.org\n# Authors: Xu Huang, Wenhao Zhu, Hanxu Hu, Conghui He, Lei Li, Shujian Huang, Fei Yuan\n## Abstract\nPrevious multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.\n## Introduction\nTo ensure high quality, we devise an annotation framework to optimize the dataset quality with human effort and LLM feedback. The process involves translating data from English to selected non-English languages using machine translation systems, post-editing each sample by three nativespeaking annotators with multiple iterations across most tasks, and picking the final translation version using a strong LLM that involves swapping sample positions for debiasing (Wang et al., 2024b;Li et al., 2024). \n\nLeading multilingual LLMs are evaluated on BenchMAX, revealing that language significantly influences language-agnostic capabilities of existing LLMs. Interestingly, simply increasing the parameters can boost average performance on these tasks but does not universally reduce the performance gap across languages. Moreover, compared to general translation, domain translation not only poses new challenges for LLMs but also requires new evaluation metrics. The main contributions can be summarized as follows: \n\n\u2022 We develop a comprehensive, multi-way multilingual benchmark across 17 languages for evaluating 6 crucial capabilities on 10 diverse tasks. \u2022 We propose a pipeline for curating high-quality mutlilingual datasets, involving both human annotation and LLM-as-a-judge.",
            "reference_string": "[276259236 | Huang et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 46,
            "citation_count": 120,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "2064469779",
                    "name": "Gorka Labaka"
                },
                {
                    "authorId": "1733049",
                    "name": "Eneko Agirre"
                }
            ],
            "abstract": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",
            "corpus_id": 215548041,
            "sentences": [
                {
                    "corpus_id": "215548041",
                    "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                    "text": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018;Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. \n\nClosely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (TRANSLATE-TEST), machine translating the training set into each target language and training the models on their respective languages (TRANSLATE-TRAIN), or using English data to finetune a multilingual model that is then transferred to the rest of languages (ZERO-SHOT). \n\nThe dataset creation and transfer procedures described above result in a mixture of original, 1 human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: TRANSLATE-TEST systems are trained on original data and evaluated on machine translated test sets, ZERO-SHOT systems are trained on original data and evaluated on human translated test sets, and TRANSLATE-TRAIN systems are trained on machine translated data and evaluated on human translated test sets. \n\nDespite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI).",
                    "score": 0.6890037379174337,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1362
                        },
                        {
                            "start": 1365,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1970
                        },
                        {
                            "start": 1973,
                            "end": 2101
                        },
                        {
                            "start": 2102,
                            "end": 2413
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 288,
                            "end": 310,
                            "matchedPaperCorpusId": "52271711"
                        },
                        {
                            "start": 310,
                            "end": 331,
                            "matchedPaperCorpusId": "204901567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85693359375
                }
            ],
            "relevance_judgement": 0.85693359375,
            "relevance_judgment_input_expanded": "# Title: Translation Artifacts in Cross-lingual Transfer Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mikel Artetxe, Gorka Labaka, Eneko Agirre\n## Abstract\nBoth human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.\n## Introduction\nWhile most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018;Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. \n\nClosely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (TRANSLATE-TEST), machine translating the training set into each target language and training the models on their respective languages (TRANSLATE-TRAIN), or using English data to finetune a multilingual model that is then transferred to the rest of languages (ZERO-SHOT). \n\nThe dataset creation and transfer procedures described above result in a mixture of original, 1 human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: TRANSLATE-TEST systems are trained on original data and evaluated on machine translated test sets, ZERO-SHOT systems are trained on original data and evaluated on human translated test sets, and TRANSLATE-TRAIN systems are trained on machine translated data and evaluated on human translated test sets. \n\nDespite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI).",
            "reference_string": "[215548041 | Artetxe et al. | 2020 | Citations: 120]"
        },
        {
            "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 80,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.07445",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.07445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "2240783638",
                    "name": "Hannah Liu"
                },
                {
                    "authorId": "2240598941",
                    "name": "Xiaoyu Shen"
                },
                {
                    "authorId": "2240523498",
                    "name": "Nikita Vassilyev"
                },
                {
                    "authorId": "122367036",
                    "name": "Jesujoba Oluwadara Alabi"
                },
                {
                    "authorId": "2240607447",
                    "name": "Yanke Mao"
                },
                {
                    "authorId": "2240717465",
                    "name": "Haonan Gao"
                },
                {
                    "authorId": "2240524331",
                    "name": "Annie Lee"
                }
            ],
            "abstract": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200\u2014a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.",
            "corpus_id": 261822560,
            "sentences": [
                {
                    "corpus_id": "261822560",
                    "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
                    "text": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200\u2014a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.",
                    "score": 0.6210946866274704,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8505859375
                }
            ],
            "relevance_judgement": 0.8505859375,
            "relevance_judgment_input_expanded": "# Title: SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Oluwadara Alabi, Yanke Mao, Haonan Gao, Annie Lee\n## Abstract\nDespite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200\u2014a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.\n",
            "reference_string": "[261822560 | Adelani et al. | 2023 | Citations: 80]"
        },
        {
            "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 47,
            "citation_count": 160,
            "influential_citation_count": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00433/1976187/tacl_a_00433.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.15207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "29909347",
                    "name": "S. Longpre"
                },
                {
                    "authorId": "2143774328",
                    "name": "Yi Lu"
                },
                {
                    "authorId": "1696803",
                    "name": "Joachim Daiber"
                }
            ],
            "abstract": "Abstract Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1",
            "corpus_id": 220871404,
            "sentences": [
                {
                    "corpus_id": "220871404",
                    "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
                    "text": "Training and evaluation data for question answering (QA) is severely lacking outside of highresource languages like English. As unsupervised, transfer learning and zero/few-shot methods narrow the multilingual performance gap with English (Conneau et al., 2020;Lee and Lee, 2019;Cui et al., 2019a;Lewis et al., 2020), their real progress is hard to measure without challenging, realistic, and linguistically diverse evaluation sets. Existing multilingual QA datasets are realistic and challenging, but they lack linguistic diversity, comparable evaluation between languages, and are often limited to passages provided with the dataset (see Table 2). \n\nWe introduce Multilingual Knowledge Questions and Answers (MKQA) for evaluation of open-1 MKQA data and evaluation scripts are available at https://github.com/apple/ml-mkqa. domain question answering. MKQA selects 10k realistic English queries from the Natural Questions dataset (NQ, Kwiatkowski et al., 2019) and human translates them into 25 additional languages and dialects. Accompanying these query translations we replace NQ's passage embedded answer spans with high-quality, language-and retrieval-independent answer annotations, linked directly against Wikidata entities and a limited set of well-defined value types (numbers, dates, strings, etc.). 2  See one full example in Table 1. More flexible than existing multilingual datasets, MKQA's grading procedure ensures these labels are sufficient to evaluate any QA method, including knowledge graph and generative approaches. The objective of this evaluation set is to facilitate fair comparison between languages, without imposing assumptions on the underlying QA approach. We see MKQA as a useful tool enabling practitioners to benchmark a variety of multilingual open domain question answering methods against the widest range of available languages yet. Below, we discuss its central properties as an evaluation benchmark. \n\nRealistic & Reliable Annotations Of crucial importance to any evaluation set is (a) how well it reflects realistic, real-world settings, and (b) the reliability of its annotations.",
                    "score": 0.6023461616388932,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 649
                        },
                        {
                            "start": 652,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1686
                        },
                        {
                            "start": 1687,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1938
                        },
                        {
                            "start": 1941,
                            "end": 2121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 239,
                            "end": 261,
                            "matchedPaperCorpusId": "207880568"
                        },
                        {
                            "start": 297,
                            "end": 316,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 936,
                            "end": 961,
                            "matchedPaperCorpusId": "86611921"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8408203125
                }
            ],
            "relevance_judgement": 0.8408203125,
            "relevance_judgment_input_expanded": "# Title: MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: S. Longpre, Yi Lu, Joachim Daiber\n## Abstract\nAbstract Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1\n## Introduction\nTraining and evaluation data for question answering (QA) is severely lacking outside of highresource languages like English. As unsupervised, transfer learning and zero/few-shot methods narrow the multilingual performance gap with English (Conneau et al., 2020;Lee and Lee, 2019;Cui et al., 2019a;Lewis et al., 2020), their real progress is hard to measure without challenging, realistic, and linguistically diverse evaluation sets. Existing multilingual QA datasets are realistic and challenging, but they lack linguistic diversity, comparable evaluation between languages, and are often limited to passages provided with the dataset (see Table 2). \n\nWe introduce Multilingual Knowledge Questions and Answers (MKQA) for evaluation of open-1 MKQA data and evaluation scripts are available at https://github.com/apple/ml-mkqa. domain question answering. MKQA selects 10k realistic English queries from the Natural Questions dataset (NQ, Kwiatkowski et al., 2019) and human translates them into 25 additional languages and dialects. Accompanying these query translations we replace NQ's passage embedded answer spans with high-quality, language-and retrieval-independent answer annotations, linked directly against Wikidata entities and a limited set of well-defined value types (numbers, dates, strings, etc.). 2  See one full example in Table 1. More flexible than existing multilingual datasets, MKQA's grading procedure ensures these labels are sufficient to evaluate any QA method, including knowledge graph and generative approaches. The objective of this evaluation set is to facilitate fair comparison between languages, without imposing assumptions on the underlying QA approach. We see MKQA as a useful tool enabling practitioners to benchmark a variety of multilingual open domain question answering methods against the widest range of available languages yet. Below, we discuss its central properties as an evaluation benchmark. \n\nRealistic & Reliable Annotations Of crucial importance to any evaluation set is (a) how well it reflects realistic, real-world settings, and (b) the reliability of its annotations.",
            "reference_string": "[220871404 | Longpre et al. | 2020 | Citations: 160]"
        },
        {
            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 77,
            "citation_count": 327,
            "influential_citation_count": 61,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2005.00333",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.00333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3381663",
                    "name": "E. Ponti"
                },
                {
                    "authorId": "1666177566",
                    "name": "Goran Glavavs"
                },
                {
                    "authorId": "46963731",
                    "name": "Olga Majewska"
                },
                {
                    "authorId": "50383712",
                    "name": "Qianchu Liu"
                },
                {
                    "authorId": "1747849",
                    "name": "Ivan Vulic"
                },
                {
                    "authorId": "145762466",
                    "name": "A. Korhonen"
                }
            ],
            "abstract": "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL.",
            "corpus_id": 218470125,
            "sentences": [
                {
                    "corpus_id": "218470125",
                    "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                    "text": "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language,3 and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation). Detailed translation guidelines are available in Appendix A. \n\nLanguage Sampling. Multilingual evaluation benchmarks assess the expected performance of a model across languages. However, should such languages be randomly sampled from the distribution of digital texts? Or rather, should the sample represent the distribution over the languages spoken around the world? Resource-rich languages tend to belong to the same families and areas, which facilitates knowledge transfer and leads to an overestimation of the expected performance in the second sense (Gerz et al., 2018;Ponti et al., 2019a). Moreover, rather than samples that account for independent and identically distributed draws from the 'true' language distribution (known as probability sampling), we opt for a uniform distribution of linguistic phenomena, which encourages the inclusion of outliers (known as variety sampling; Rijkhoff et al., 1993;Dryer, 1989). Thus, the performance on XCOPA also reflects the robustness of a model, i.e. its resilience to phenomena that are unlikely to be observed in the training data.",
                    "score": 0.5869212259470584,
                    "section_title": "Annotation Design",
                    "char_start_offset": 4684,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 18
                        },
                        {
                            "start": 19,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 673
                        },
                        {
                            "start": 676,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1092
                        },
                        {
                            "start": 1095,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2118
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 604,
                            "end": 628,
                            "matchedPaperCorpusId": "9497990"
                        },
                        {
                            "start": 1588,
                            "end": 1607,
                            "matchedPaperCorpusId": "52229163"
                        },
                        {
                            "start": 1607,
                            "end": 1627,
                            "matchedPaperCorpusId": "49564714"
                        },
                        {
                            "start": 1923,
                            "end": 1945,
                            "matchedPaperCorpusId": "170125293"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83740234375
                }
            ],
            "relevance_judgement": 0.83740234375,
            "relevance_judgment_input_expanded": "# Title: XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, A. Korhonen\n## Abstract\nIn order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL.\n## Annotation Design\nDesign Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language,3 and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation). Detailed translation guidelines are available in Appendix A. \n\nLanguage Sampling. Multilingual evaluation benchmarks assess the expected performance of a model across languages. However, should such languages be randomly sampled from the distribution of digital texts? Or rather, should the sample represent the distribution over the languages spoken around the world? Resource-rich languages tend to belong to the same families and areas, which facilitates knowledge transfer and leads to an overestimation of the expected performance in the second sense (Gerz et al., 2018;Ponti et al., 2019a). Moreover, rather than samples that account for independent and identically distributed draws from the 'true' language distribution (known as probability sampling), we opt for a uniform distribution of linguistic phenomena, which encourages the inclusion of outliers (known as variety sampling; Rijkhoff et al., 1993;Dryer, 1989). Thus, the performance on XCOPA also reflects the robustness of a model, i.e. its resilience to phenomena that are unlikely to be observed in the training data.",
            "reference_string": "[218470125 | Ponti et al. | 2020 | Citations: 327]"
        },
        {
            "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 108,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "98623604",
                    "name": "Amir Hossein Kargaran"
                },
                {
                    "authorId": "2054744",
                    "name": "Ali Modarressi"
                },
                {
                    "authorId": "2210982230",
                    "name": "Nafiseh Nikeghbal"
                },
                {
                    "authorId": "2261085110",
                    "name": "Jana Diesner"
                },
                {
                    "authorId": "2302797923",
                    "name": "Franccois Yvon"
                },
                {
                    "authorId": "2130001188",
                    "name": "Hinrich Schutze"
                }
            ],
            "abstract": "English-centric large language models (LLMs) often show strong multilingual capabilities. However, their multilingual performance remains unclear and is under-evaluated for many other languages. Most benchmarks for multilinguality focus on classic NLP tasks or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers. MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in different languages. We conduct controlled experiments using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://cis-lmu-mexa.hf.space, Code: https://github.com/cisnlp/MEXA.",
            "corpus_id": 273229002,
            "sentences": [
                {
                    "corpus_id": "273229002",
                    "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
                    "text": "Multilingual evaluation methods and the development of benchmarks not only facilitate the assessment of diverse language representations in LLMs but also help in monitoring cross-lingual generalization, to assess the effect of quantization across multiple languages (Marchisio et al., 2024), the development of language-specific models (Tejaswi et al., 2024), and the optimization of safety preferences (Li et al., 2024e), among others. In Table 4, we list benchmarks with the largest language coverage. This list includes benchmarks referenced by MEGA (Ahuja et al., 2023), MEGA-VERSE (Ahuja et al., 2024), xP3 (Muennighoff et al., 2023), the Aya collection (Singh et al., 2024), the lm-evaluation-harness framework (Gao et al., 2023;Biderman et al., 2024), and inter alia. These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation Lai et al. (2023b). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages. Despite the efforts reflected in Table 4, the community is still lacking highly multilingual benchmarks for tasks such as natural language understanding or text generation.",
                    "score": 0.7331026145784374,
                    "section_title": "A.2 MULTILINGUAL EVALUATION BENCHMARKS",
                    "char_start_offset": 36079,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 503
                        },
                        {
                            "start": 504,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1351
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 553,
                            "end": 573,
                            "matchedPaperCorpusId": "257663467"
                        },
                        {
                            "start": 586,
                            "end": 605,
                            "matchedPaperCorpusId": "265150423"
                        },
                        {
                            "start": 967,
                            "end": 985,
                            "matchedPaperCorpusId": "260334562"
                        },
                        {
                            "start": 1075,
                            "end": 1095,
                            "matchedPaperCorpusId": "235658519"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8271484375
                }
            ],
            "relevance_judgement": 0.8271484375,
            "relevance_judgment_input_expanded": "# Title: MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment\n# Venue: arXiv.org\n# Authors: Amir Hossein Kargaran, Ali Modarressi, Nafiseh Nikeghbal, Jana Diesner, Franccois Yvon, Hinrich Schutze\n## Abstract\nEnglish-centric large language models (LLMs) often show strong multilingual capabilities. However, their multilingual performance remains unclear and is under-evaluated for many other languages. Most benchmarks for multilinguality focus on classic NLP tasks or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers. MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in different languages. We conduct controlled experiments using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://cis-lmu-mexa.hf.space, Code: https://github.com/cisnlp/MEXA.\n## A.2 MULTILINGUAL EVALUATION BENCHMARKS\nMultilingual evaluation methods and the development of benchmarks not only facilitate the assessment of diverse language representations in LLMs but also help in monitoring cross-lingual generalization, to assess the effect of quantization across multiple languages (Marchisio et al., 2024), the development of language-specific models (Tejaswi et al., 2024), and the optimization of safety preferences (Li et al., 2024e), among others. In Table 4, we list benchmarks with the largest language coverage. This list includes benchmarks referenced by MEGA (Ahuja et al., 2023), MEGA-VERSE (Ahuja et al., 2024), xP3 (Muennighoff et al., 2023), the Aya collection (Singh et al., 2024), the lm-evaluation-harness framework (Gao et al., 2023;Biderman et al., 2024), and inter alia. These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation Lai et al. (2023b). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages. Despite the efforts reflected in Table 4, the community is still lacking highly multilingual benchmarks for tasks such as natural language understanding or text generation.",
            "reference_string": "[273229002 | Kargaran et al. | 2024 | Citations: 7]"
        },
        {
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327995656",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2258789284",
                    "name": "Weixuan Wang"
                },
                {
                    "authorId": "2349802912",
                    "name": "Sinuo Liu"
                },
                {
                    "authorId": "2331767275",
                    "name": "Huifeng Yin"
                },
                {
                    "authorId": "2356794252",
                    "name": "Xintong Wang"
                },
                {
                    "authorId": "2331686159",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2302342302",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "corpus_id": 277993848,
            "sentences": [
                {
                    "corpus_id": "277993848",
                    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                    "text": "The domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
                    "score": 0.7362914146724128,
                    "section_title": "Domain",
                    "char_start_offset": 9088,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 63,
                            "end": 115
                        },
                        {
                            "start": 118,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1581
                        },
                        {
                            "start": 1584,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 511,
                            "end": 528,
                            "matchedPaperCorpusId": "252735112"
                        },
                        {
                            "start": 528,
                            "end": 546,
                            "matchedPaperCorpusId": "260334562"
                        },
                        {
                            "start": 690,
                            "end": 713,
                            "matchedPaperCorpusId": "221516475"
                        },
                        {
                            "start": 854,
                            "end": 872,
                            "matchedPaperCorpusId": "263829211"
                        },
                        {
                            "start": 889,
                            "end": 911,
                            "matchedPaperCorpusId": "271245129"
                        },
                        {
                            "start": 1177,
                            "end": 1198,
                            "matchedPaperCorpusId": "273850461"
                        },
                        {
                            "start": 1395,
                            "end": 1415,
                            "matchedPaperCorpusId": "270371898"
                        },
                        {
                            "start": 2088,
                            "end": 2104,
                            "matchedPaperCorpusId": "254043519"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                },
                {
                    "corpus_id": "277993848",
                    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                    "text": "Most benchmarks (61.4%) use original language content rather than translations, with human translations accounting for just 13.2%. Discriminative tasks (66.5%) significantly outnumber generative ones (23.5%), with text classification remaining dominant while question answering shows dramatic growth following the emergence of LLMs in 2023. Dataset sizes have consistently expanded across all categories over the years, with an estimated cost of $11 million USD. Most content is sourced from publicly accessible domains, such as news (17%) and social media (13.3%), while high-value domains like healthcare and law remain underrepresented. evelopment efforts are concentrated in China, India, Germany, the UK, and the USA, with Europe emphasizing academic research and China and the USA demonstrating stronger academia-industry collaborations. \n\n2. PRESENT: What is the current status of multilingual evaluation? (Section 5) We present two key insights about multilingual LLM evaluation. First, users across different languages (English, Chinese, French, German, Spanish, and Russian) share remarkably similar interests, with writing tasks dominating (30-45%), followed by commonsense reasoning and programming tasks, suggesting universal priorities despite linguistic differences. Second, when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources.",
                    "score": 0.6029165550626578,
                    "section_title": "Introduction",
                    "char_start_offset": 2166,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 843
                        },
                        {
                            "start": 846,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1859
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.767578125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n# Venue: arXiv.org\n# Authors: Minghao Wu, Weixuan Wang, Sinuo Liu, Huifeng Yin, Xintong Wang, Yu Zhao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang\n## Abstract\nAs large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.\n## Introduction\nMost benchmarks (61.4%) use original language content rather than translations, with human translations accounting for just 13.2%. Discriminative tasks (66.5%) significantly outnumber generative ones (23.5%), with text classification remaining dominant while question answering shows dramatic growth following the emergence of LLMs in 2023. Dataset sizes have consistently expanded across all categories over the years, with an estimated cost of $11 million USD. Most content is sourced from publicly accessible domains, such as news (17%) and social media (13.3%), while high-value domains like healthcare and law remain underrepresented. evelopment efforts are concentrated in China, India, Germany, the UK, and the USA, with Europe emphasizing academic research and China and the USA demonstrating stronger academia-industry collaborations. \n\n2. PRESENT: What is the current status of multilingual evaluation? (Section 5) We present two key insights about multilingual LLM evaluation. First, users across different languages (English, Chinese, French, German, Spanish, and Russian) share remarkably similar interests, with writing tasks dominating (30-45%), followed by commonsense reasoning and programming tasks, suggesting universal priorities despite linguistic differences. Second, when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources.\n\n## Domain\nThe domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
            "reference_string": "[277993848 | Wu et al. | 2025 | Citations: 5]"
        },
        {
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "venue": "Frontiers Comput. Sci.",
            "year": 2024,
            "reference_count": 185,
            "citation_count": 43,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257136845",
                    "name": "Yuemei Xu"
                },
                {
                    "authorId": "2258334185",
                    "name": "Ling Hu"
                },
                {
                    "authorId": "2294513520",
                    "name": "Jiayi Zhao"
                },
                {
                    "authorId": "2294361104",
                    "name": "Zihan Qiu"
                },
                {
                    "authorId": "2294363807",
                    "name": "Yuqi Ye"
                },
                {
                    "authorId": "2294933103",
                    "name": "Hanwen Gu"
                }
            ],
            "abstract": "\n Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.",
            "corpus_id": 268819377,
            "sentences": [
                {
                    "corpus_id": "268819377",
                    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                    "text": "This section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
                    "score": 0.6020732988246955,
                    "section_title": "B. Bias Benchmark",
                    "char_start_offset": 48672,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 552
                        },
                        {
                            "start": 555,
                            "end": 727
                        },
                        {
                            "start": 730,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1241
                        },
                        {
                            "start": 1244,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1549
                        },
                        {
                            "start": 1552,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2150
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 404,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 406,
                            "end": 411,
                            "matchedPaperCorpusId": "173991101"
                        },
                        {
                            "start": 448,
                            "end": 453,
                            "matchedPaperCorpusId": "21670658"
                        },
                        {
                            "start": 455,
                            "end": 460,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 540,
                            "end": 545,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 546,
                            "end": 551,
                            "matchedPaperCorpusId": "252547725"
                        },
                        {
                            "start": 970,
                            "end": 975,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 977,
                            "end": 982,
                            "matchedPaperCorpusId": "219530686"
                        },
                        {
                            "start": 1076,
                            "end": 1081,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 1364,
                            "end": 1369,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 1383,
                            "end": 1388,
                            "matchedPaperCorpusId": "4952494"
                        },
                        {
                            "start": 1455,
                            "end": 1460,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 1649,
                            "end": 1654,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 1666,
                            "end": 1671,
                            "matchedPaperCorpusId": "252547725"
                        },
                        {
                            "start": 1988,
                            "end": 1993,
                            "matchedPaperCorpusId": "135465247"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias\n# Venue: Frontiers Comput. Sci.\n# Authors: Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu\n## Abstract\n\n Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.\n## B. Bias Benchmark\nThis section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
            "reference_string": "[268819377 | Xu et al. | 2024 | Citations: 43]"
        },
        {
            "title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 22,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.13738",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.13738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2167031295",
                    "name": "Vitor Jeronymo"
                },
                {
                    "authorId": "2186302660",
                    "name": "Mauricio Nascimento"
                },
                {
                    "authorId": "2066179820",
                    "name": "R. Lotufo"
                },
                {
                    "authorId": "143744603",
                    "name": "Rodrigo Nogueira"
                }
            ],
            "abstract": "Robust 2004 is an information retrieval benchmark whose large number of judgments per query make it a reliable evaluation dataset. In this paper, we present mRobust04, a multilingual version of Robust04 that was translated to 8 languages using Google Translate. We also provide results of three different multilingual retrievers on this dataset. The dataset is available at https://huggingface.co/datasets/unicamp-dl/mrobust",
            "corpus_id": 252568105,
            "sentences": [
                {
                    "corpus_id": "252568105",
                    "title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
                    "text": "A key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL. \n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04. \n\nIn Table 1 we compare mRobust04 with other multilingual IR datasets. Despite having a modest number of queries and documents, mRobust04 has much more annotations per query, which, we conjecture, makes it a reliable benchmark for evaluating future multilingual retrieval models. We also evaluate on this dataset two multilingual models that are close to the state of the art. dently to the API. The resulting translations of these chunks were then concatenated to form the translated document. The annotated query-document pairs are the same for all languages.",
                    "score": 0.5852733588282164,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1012
                        },
                        {
                            "start": 1015,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1297
                        },
                        {
                            "start": 1300,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1859
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 105,
                            "end": 108,
                            "matchedPaperCorpusId": "233296851"
                        },
                        {
                            "start": 108,
                            "end": 111,
                            "matchedPaperCorpusId": "248817938"
                        },
                        {
                            "start": 235,
                            "end": 239,
                            "matchedPaperCorpusId": "42162974"
                        },
                        {
                            "start": 239,
                            "end": 242,
                            "matchedPaperCorpusId": "38347697"
                        },
                        {
                            "start": 242,
                            "end": 245,
                            "matchedPaperCorpusId": "262629903"
                        },
                        {
                            "start": 245,
                            "end": 248,
                            "matchedPaperCorpusId": "1720123"
                        },
                        {
                            "start": 297,
                            "end": 300,
                            "matchedPaperCorpusId": "246275873"
                        },
                        {
                            "start": 543,
                            "end": 547,
                            "matchedPaperCorpusId": "237213465"
                        },
                        {
                            "start": 876,
                            "end": 880,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 1150,
                            "end": 1154,
                            "matchedPaperCorpusId": "59910946"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80810546875
                }
            ],
            "relevance_judgement": 0.80810546875,
            "relevance_judgment_input_expanded": "# Title: mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark\n# Venue: arXiv.org\n# Authors: Vitor Jeronymo, Mauricio Nascimento, R. Lotufo, Rodrigo Nogueira\n## Abstract\nRobust 2004 is an information retrieval benchmark whose large number of judgments per query make it a reliable evaluation dataset. In this paper, we present mRobust04, a multilingual version of Robust04 that was translated to 8 languages using Google Translate. We also provide results of three different multilingual retrievers on this dataset. The dataset is available at https://huggingface.co/datasets/unicamp-dl/mrobust\n## Introduction\nA key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL. \n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04. \n\nIn Table 1 we compare mRobust04 with other multilingual IR datasets. Despite having a modest number of queries and documents, mRobust04 has much more annotations per query, which, we conjecture, makes it a reliable benchmark for evaluating future multilingual retrieval models. We also evaluate on this dataset two multilingual models that are close to the state of the art. dently to the API. The resulting translations of these chunks were then concatenated to form the translated document. The annotated query-document pairs are the same for all languages.",
            "reference_string": "[252568105 | Jeronymo et al. | 2022 | Citations: 3]"
        },
        {
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2226458991",
                    "name": "Julen Etxaniz"
                },
                {
                    "authorId": "2481918",
                    "name": "Gorka Azkune"
                },
                {
                    "authorId": "2260104163",
                    "name": "A. Soroa"
                },
                {
                    "authorId": "2251043402",
                    "name": "Oier L\u00f3pez de Lacalle"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                }
            ],
            "abstract": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.",
            "corpus_id": 270380088,
            "sentences": [
                {
                    "corpus_id": "270380088",
                    "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                    "text": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
                    "score": 0.7185046265406501,
                    "section_title": "Related Work",
                    "char_start_offset": 20594,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 569
                        },
                        {
                            "start": 572,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1520
                        },
                        {
                            "start": 1523,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1998
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1248,
                            "end": 1268,
                            "matchedPaperCorpusId": "258685666"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8037109375
                }
            ],
            "relevance_judgement": 0.8037109375,
            "relevance_judgment_input_expanded": "# Title: BertaQA: How Much Do Language Models Know About Local Culture?\n# Venue: Neural Information Processing Systems\n# Authors: Julen Etxaniz, Gorka Azkune, A. Soroa, Oier L\u00f3pez de Lacalle, Mikel Artetxe\n## Abstract\nLarge Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.\n## Related Work\nResearch in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
            "reference_string": "[270380088 | Etxaniz et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Towards Zero-shot Cross-lingual Image Retrieval and Tagging",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.07622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "30141435",
                    "name": "Pranav Aggarwal"
                },
                {
                    "authorId": "51251519",
                    "name": "Ritiz Tambi"
                },
                {
                    "authorId": "37493415",
                    "name": "Ajinkya Kale"
                }
            ],
            "abstract": "There has been a recent spike in interest in multi-modal Language and Vision problems. On the language side, most of these models primarily focus on English since most multi-modal datasets are monolingual. We try to bridge this gap with a zero-shot approach for learning multi-modal representations using cross-lingual pre-training on the text side. We present a simple yet practical approach for building a cross-lingual image retrieval model which trains on a monolingual training dataset but can be used in a zero-shot cross-lingual fashion during inference. We also introduce a new objective function which tightens the text embedding clusters by pushing dissimilar texts away from each other. For evaluation, we introduce a new 1K multi-lingual MSCOCO2014 caption test dataset (XTD10) in 7 languages that we collected using a crowdsourcing platform. We use this as the test set for zero-shot model performance across languages. We also demonstrate how a cross-lingual model can be used for downstream tasks like multi-lingual image tagging in a zero shot manner. XTD10 dataset is made publicly available here: https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10.",
            "corpus_id": 237532506,
            "sentences": [
                {
                    "corpus_id": "237532506",
                    "title": "Towards Zero-shot Cross-lingual Image Retrieval and Tagging",
                    "text": "Image retrieval is a well studied problem in both academia and industry [8,21,32,37,40]. Most research looks at image retrieval in a monolingual setup for a couple of reasons: \n\n\u2022 Lack of multi-lingual Vision-Language datasets supporting a wide range of languages \u2022 Extensibility towards new and low-resource language support \n\nMulti-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28,31] rely on direct translations of English captions while others [12,20] have used independent image and language text pairs. Based on previous research learning we try to explore following ideas in this paper: \n\n\u2022 One-model-fits-all: Can we use pre-trained cross-lingual embeddings with monolingual image-text training data to learn representations in a common embedding space for image retrieval and tagging? \n\nFigure 1: Overview of our approach. \n\n\u2022 Multi-lingual Eval Dataset: Build an evaluation set for multilingual image retrieval to test in a zero-shot retrieval setup. \n\nIn our approach we try to take advantage of the recent developments in cross-lingual sentence embeddings [3,38] which are effective in aligning multiple languages in a common embedding space. Due to scarcity of multi-lingual sentence test datasets, for evaluation we combine 10 non-English language annotations to create a Cross-lingual Test Dataset called XTD10. \n\nAs an extension of [1], in this paper we also try to take advantage of the One-model-fits-all approach on multi-lingual image tagging and tackle it as a zero-shot problem. Monolingual image tagging models, in languages other than English face extensibility issues and training data constraints similar to the image retrieval problem. Using an English image tagger and direct translation at tag level is a straightforward solution but its error prone because of word ambiguity which word-level translation systems lack the context to capture. Spring in French has two translations: Printemps(Season Spring) and Ressort(Bouncy Spring).",
                    "score": 0.5720163014247617,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 175
                        },
                        {
                            "start": 178,
                            "end": 325
                        },
                        {
                            "start": 328,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 762
                        },
                        {
                            "start": 765,
                            "end": 962
                        },
                        {
                            "start": 965,
                            "end": 1000
                        },
                        {
                            "start": 1003,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1495
                        },
                        {
                            "start": 1498,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2039
                        },
                        {
                            "start": 2040,
                            "end": 2131
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 72,
                            "end": 75,
                            "matchedPaperCorpusId": "7060187"
                        },
                        {
                            "start": 75,
                            "end": 78,
                            "matchedPaperCorpusId": "1153609"
                        },
                        {
                            "start": 81,
                            "end": 84,
                            "matchedPaperCorpusId": "22367645"
                        },
                        {
                            "start": 84,
                            "end": 87,
                            "matchedPaperCorpusId": "50776405"
                        },
                        {
                            "start": 552,
                            "end": 555,
                            "matchedPaperCorpusId": "21688999"
                        },
                        {
                            "start": 617,
                            "end": 621,
                            "matchedPaperCorpusId": "19396711"
                        },
                        {
                            "start": 621,
                            "end": 624,
                            "matchedPaperCorpusId": "202772083"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7958984375
                }
            ],
            "relevance_judgement": 0.7958984375,
            "relevance_judgment_input_expanded": "# Title: Towards Zero-shot Cross-lingual Image Retrieval and Tagging\n# Venue: arXiv.org\n# Authors: Pranav Aggarwal, Ritiz Tambi, Ajinkya Kale\n## Abstract\nThere has been a recent spike in interest in multi-modal Language and Vision problems. On the language side, most of these models primarily focus on English since most multi-modal datasets are monolingual. We try to bridge this gap with a zero-shot approach for learning multi-modal representations using cross-lingual pre-training on the text side. We present a simple yet practical approach for building a cross-lingual image retrieval model which trains on a monolingual training dataset but can be used in a zero-shot cross-lingual fashion during inference. We also introduce a new objective function which tightens the text embedding clusters by pushing dissimilar texts away from each other. For evaluation, we introduce a new 1K multi-lingual MSCOCO2014 caption test dataset (XTD10) in 7 languages that we collected using a crowdsourcing platform. We use this as the test set for zero-shot model performance across languages. We also demonstrate how a cross-lingual model can be used for downstream tasks like multi-lingual image tagging in a zero shot manner. XTD10 dataset is made publicly available here: https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10.\n## INTRODUCTION\nImage retrieval is a well studied problem in both academia and industry [8,21,32,37,40]. Most research looks at image retrieval in a monolingual setup for a couple of reasons: \n\n\u2022 Lack of multi-lingual Vision-Language datasets supporting a wide range of languages \u2022 Extensibility towards new and low-resource language support \n\nMulti-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28,31] rely on direct translations of English captions while others [12,20] have used independent image and language text pairs. Based on previous research learning we try to explore following ideas in this paper: \n\n\u2022 One-model-fits-all: Can we use pre-trained cross-lingual embeddings with monolingual image-text training data to learn representations in a common embedding space for image retrieval and tagging? \n\nFigure 1: Overview of our approach. \n\n\u2022 Multi-lingual Eval Dataset: Build an evaluation set for multilingual image retrieval to test in a zero-shot retrieval setup. \n\nIn our approach we try to take advantage of the recent developments in cross-lingual sentence embeddings [3,38] which are effective in aligning multiple languages in a common embedding space. Due to scarcity of multi-lingual sentence test datasets, for evaluation we combine 10 non-English language annotations to create a Cross-lingual Test Dataset called XTD10. \n\nAs an extension of [1], in this paper we also try to take advantage of the One-model-fits-all approach on multi-lingual image tagging and tackle it as a zero-shot problem. Monolingual image tagging models, in languages other than English face extensibility issues and training data constraints similar to the image retrieval problem. Using an English image tagger and direct translation at tag level is a straightforward solution but its error prone because of word ambiguity which word-level translation systems lack the context to capture. Spring in French has two translations: Printemps(Season Spring) and Ressort(Bouncy Spring).",
            "reference_string": "[237532506 | Aggarwal et al. | 2021 | Citations: 6]"
        },
        {
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "venue": "",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 113,
            "influential_citation_count": 17,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.13897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003019597",
                    "name": "L. Bonifacio"
                },
                {
                    "authorId": "1472908948",
                    "name": "Israel Campiotti"
                },
                {
                    "authorId": "1809633",
                    "name": "R. Lotufo"
                },
                {
                    "authorId": "143744603",
                    "name": "Rodrigo Nogueira"
                }
            ],
            "abstract": "The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by finetuning monolingual and multilingual reranking models, as well as a multilingual dense retrieval model on this dataset. We also evaluated models finetuned using the mMARCO dataset in a zero-shot scenario on Mr. TyDi dataset, demonstrating that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Our experiments also show that a distilled multilingual reranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and finetuned models are available at https://github.com/unicamp-dl/mMARCO.",
            "corpus_id": 237364084,
            "sentences": [
                {
                    "corpus_id": "237364084",
                    "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
                    "text": "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages. \n\nMore recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO. This dataset is a suitable candidate as it shows good transfer learning capabilities, as well as being a popular choice for evaluating deep learning models (Craswell et al., 2020(Craswell et al., , 2021)). By translating the dataset we are able to leverages extensive manually annotated data. Lastly, as far as we know, no previous work has translated a large IR dataset such as the MS MARCO dataset to multiple languages. \n\nOur contributions are translating the MS MARCO (both training and evaluation sets) to 13 different languages such as Portuguese, Spanish, German, and Chinese. We finetuned mono and multilingual deep learning-based models both considering dense retrieval and reranking approaches using our translated dataset. Moreover, we evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Additionally, we made available the mMARCO dataset with 13 languages and all models finetuned on our translated dataset.",
                    "score": 0.6407717231553907,
                    "section_title": "Introduction",
                    "char_start_offset": 1757,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 181,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1390
                        },
                        {
                            "start": 1391,
                            "end": 1520
                        },
                        {
                            "start": 1523,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78759765625
                }
            ],
            "relevance_judgement": 0.78759765625,
            "relevance_judgment_input_expanded": "# Title: mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset\n# Venue: \n# Authors: L. Bonifacio, Israel Campiotti, R. Lotufo, Rodrigo Nogueira\n## Abstract\nThe MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by finetuning monolingual and multilingual reranking models, as well as a multilingual dense retrieval model on this dataset. We also evaluated models finetuned using the mMARCO dataset in a zero-shot scenario on Mr. TyDi dataset, demonstrating that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Our experiments also show that a distilled multilingual reranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and finetuned models are available at https://github.com/unicamp-dl/mMARCO.\n## Introduction\nAs most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages. \n\nMore recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO. This dataset is a suitable candidate as it shows good transfer learning capabilities, as well as being a popular choice for evaluating deep learning models (Craswell et al., 2020(Craswell et al., , 2021)). By translating the dataset we are able to leverages extensive manually annotated data. Lastly, as far as we know, no previous work has translated a large IR dataset such as the MS MARCO dataset to multiple languages. \n\nOur contributions are translating the MS MARCO (both training and evaluation sets) to 13 different languages such as Portuguese, Spanish, German, and Chinese. We finetuned mono and multilingual deep learning-based models both considering dense retrieval and reranking approaches using our translated dataset. Moreover, we evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Additionally, we made available the mMARCO dataset with 13 languages and all models finetuned on our translated dataset.",
            "reference_string": "[237364084 | Bonifacio et al. | 2021 | Citations: 113]"
        },
        {
            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 108,
            "citation_count": 64,
            "influential_citation_count": 15,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.11732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "83574123",
                    "name": "Emanuele Bugliarello"
                },
                {
                    "authorId": "144097210",
                    "name": "Fangyu Liu"
                },
                {
                    "authorId": "153733568",
                    "name": "Jonas Pfeiffer"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "50369944",
                    "name": "Desmond Elliott"
                },
                {
                    "authorId": "3381663",
                    "name": "E. Ponti"
                },
                {
                    "authorId": "1747849",
                    "name": "Ivan Vulic"
                }
            ],
            "abstract": "Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together - by both aggregating pre-existing datasets and creating new ones - visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target-source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.",
            "corpus_id": 246294502,
            "sentences": [
                {
                    "corpus_id": "246294502",
                    "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
                    "text": "2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research. 4) Our results and additional analyses take stock of the current gaps and challenges in cross-lingual V&L research. 5) To further facilitate replicable research in this area, we re-implement the existing multilingual V&L pretrained encoders in a unified framework (VOLTA; Bugliarello et al. 2021), which also provides access to five English V&L BERTs and 12 V&L tasks. We provide data and code for the evaluation of multilingual V&L models at https://iglue-benchmark.github.io/.",
                    "score": 0.6074718635293049,
                    "section_title": "Introduction",
                    "char_start_offset": 4870,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 954
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77392578125
                },
                {
                    "corpus_id": "246294502",
                    "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
                    "text": "We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer. \n\nBy virtue of the newly created IGLUE benchmark, we also run the first systematic comparative evaluation of cuttingedge multilingually pretrained V&L architectures (Ni et al., 2021;Zhou et al., 2021;Liu et al., 2021), as well as a series of representative monolingual V&L models combined with machine translation (Lu et al., 2019;Chen et al., 2020, inter alia) across a range of diverse V&L tasks and languages. \n\nThis evaluation offers new insights on the models' core strengths and current limitations. Foreshadowing, we showcase large gaps between performance in English and transfer performance, where (to a limited extent) the gaps are more prominent in lower-resource languages and languages more distant from English. In addition, leveraging target-language in-task few shots is remarkably arduous: whereas in previous text-only experiments (Lauscher et al., 2020), ceteris paribus there are huge gains of few-shot scenarios over their zero-shot counterparts, we demonstrate that current V&L models often require thousands of examples before showing signs of improvement. Finally, performance also seems correlated with task difficulty: for instance, NLI shows the smallest gaps between English and other languages and QA benefits the most from few-shot adaptation. On the other hand, visually grounded reasoning and cross-modal retrieval appear to be harder in both respects. \n\nContributions. 1) In order to guide and inspire more work in the area of multilingual V&L research, we present a first evaluation benchmark for cross-lingual transfer learning for V&L tasks, spanning 20 languages, 5 datasets, and 4 different tasks. 2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research.",
                    "score": 0.5821292463007914,
                    "section_title": "Introduction",
                    "char_start_offset": 3104,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 132,
                            "end": 542
                        },
                        {
                            "start": 545,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1514
                        },
                        {
                            "start": 1517,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 2241
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 312,
                            "end": 330,
                            "matchedPaperCorpusId": "232478575"
                        },
                        {
                            "start": 330,
                            "end": 347,
                            "matchedPaperCorpusId": "238198104"
                        },
                        {
                            "start": 444,
                            "end": 461,
                            "matchedPaperCorpusId": "199453025"
                        },
                        {
                            "start": 461,
                            "end": 478,
                            "matchedPaperCorpusId": "216080982"
                        },
                        {
                            "start": 979,
                            "end": 1002,
                            "matchedPaperCorpusId": "226262344"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7451171875
                }
            ],
            "relevance_judgement": 0.77392578125,
            "relevance_judgment_input_expanded": "# Title: IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages\n# Venue: International Conference on Machine Learning\n# Authors: Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, E. Ponti, Ivan Vulic\n## Abstract\nReliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together - by both aggregating pre-existing datasets and creating new ones - visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target-source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.\n## Introduction\nWe also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer. \n\nBy virtue of the newly created IGLUE benchmark, we also run the first systematic comparative evaluation of cuttingedge multilingually pretrained V&L architectures (Ni et al., 2021;Zhou et al., 2021;Liu et al., 2021), as well as a series of representative monolingual V&L models combined with machine translation (Lu et al., 2019;Chen et al., 2020, inter alia) across a range of diverse V&L tasks and languages. \n\nThis evaluation offers new insights on the models' core strengths and current limitations. Foreshadowing, we showcase large gaps between performance in English and transfer performance, where (to a limited extent) the gaps are more prominent in lower-resource languages and languages more distant from English. In addition, leveraging target-language in-task few shots is remarkably arduous: whereas in previous text-only experiments (Lauscher et al., 2020), ceteris paribus there are huge gains of few-shot scenarios over their zero-shot counterparts, we demonstrate that current V&L models often require thousands of examples before showing signs of improvement. Finally, performance also seems correlated with task difficulty: for instance, NLI shows the smallest gaps between English and other languages and QA benefits the most from few-shot adaptation. On the other hand, visually grounded reasoning and cross-modal retrieval appear to be harder in both respects. \n\nContributions. 1) In order to guide and inspire more work in the area of multilingual V&L research, we present a first evaluation benchmark for cross-lingual transfer learning for V&L tasks, spanning 20 languages, 5 datasets, and 4 different tasks. 2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research.\n...\n2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research. 4) Our results and additional analyses take stock of the current gaps and challenges in cross-lingual V&L research. 5) To further facilitate replicable research in this area, we re-implement the existing multilingual V&L pretrained encoders in a unified framework (VOLTA; Bugliarello et al. 2021), which also provides access to five English V&L BERTs and 12 V&L tasks. We provide data and code for the evaluation of multilingual V&L models at https://iglue-benchmark.github.io/.",
            "reference_string": "[246294502 | Bugliarello et al. | 2022 | Citations: 64]"
        },
        {
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325726142",
                    "name": "Fabio Barth"
                },
                {
                    "authorId": "2302558975",
                    "name": "Georg Rehm"
                }
            ],
            "abstract": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
            "corpus_id": 276421738,
            "sentences": [
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
                    "score": 0.575056623651157,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77392578125
                },
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
                    "score": 0.6769687993119665,
                    "section_title": "Challenge 3: Cultural Bias",
                    "char_start_offset": 13020,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 61,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1304
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 149,
                            "matchedPaperCorpusId": "265445838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7255859375
                }
            ],
            "relevance_judgement": 0.77392578125,
            "relevance_judgment_input_expanded": "# Title: Multilingual European Language Models: Benchmarking Approaches and Challenges\n# Venue: arXiv.org\n# Authors: Fabio Barth, Georg Rehm\n## Abstract\nThe breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.\n## Challenge 3: Cultural Bias\nMost benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
            "reference_string": "[276421738 | Barth et al. | 2025 | Citations: 0]"
        },
        {
            "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.19737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257338995",
                    "name": "Linyang He"
                },
                {
                    "authorId": "2197254657",
                    "name": "Ercong Nie"
                },
                {
                    "authorId": "2294424733",
                    "name": "Sukru Samet Dindar"
                },
                {
                    "authorId": "2321723374",
                    "name": "Arsalan Firoozi"
                },
                {
                    "authorId": "2311441706",
                    "name": "Adrian Florea"
                },
                {
                    "authorId": "2349460444",
                    "name": "Van Nguyen"
                },
                {
                    "authorId": "2174811177",
                    "name": "Corentin Puffay"
                },
                {
                    "authorId": "2347533359",
                    "name": "Riki Shimizu"
                },
                {
                    "authorId": "2278833873",
                    "name": "Haotian Ye"
                },
                {
                    "authorId": "2293392156",
                    "name": "Jonathan Brennan"
                },
                {
                    "authorId": "2256998691",
                    "name": "Helmut Schmid"
                },
                {
                    "authorId": "2261745622",
                    "name": "Hinrich Schutze"
                },
                {
                    "authorId": "1686269",
                    "name": "N. Mesgarani"
                }
            ],
            "abstract": "We introduce XCOMPS in this work, a multilingual conceptual minimal pair dataset covering 17 languages. Using this dataset, we evaluate LLMs' multilingual conceptual understanding through metalinguistic prompting, direct probability measurement, and neurolinguistic probing. By comparing base, instruction-tuned, and knowledge-distilled models, we find that: 1) LLMs exhibit weaker conceptual understanding for low-resource languages, and accuracy varies across languages despite being tested on the same concept sets. 2) LLMs excel at distinguishing concept-property pairs that are visibly different but exhibit a marked performance drop when negative pairs share subtle semantic similarities. 3) Instruction tuning improves performance in concept understanding but does not enhance internal competence; knowledge distillation can enhance internal competence in conceptual understanding for low-resource languages with limited gains in explicit task performance. 4) More morphologically complex languages yield lower concept understanding scores and require deeper layers for conceptual reasoning.",
            "corpus_id": 276647924,
            "sentences": [
                {
                    "corpus_id": "276647924",
                    "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs",
                    "text": "To construct XCOMPS, a multilingual benchmark covering 17 languages (Table 3), we adopted a human-LLM interactive translation pipeline, leveraging both human expertise and the multilingual generation capabilities of large language models (LLMs). The language set for XCOMPS aligns with the prior knowledge probing benchmark BMLAMA-17 (Qi et al., 2023), ensuring consistency in multilingual evaluation. The highly structured nature of conceptual minimal pair datasets, where positive and negative sentences primarily consist of two components-concepts and properties-enabled us to design a multi-step translation process that ensures high-quality multilingual data. \n\nThe construction process consists of four stages, as displayed in Figure 7. We use the GPT-4o model (GPT-4o-2024-08-06) via the OpenAI API as the translation assistant in the pipeline. In the first stage, we manually translated the original concepts and properties from English into German and Chinese using language experts. We used German and Chinese as additional seed languages to further reduce ambiguity, This multilingual seed data helped disambiguate concepts that might otherwise be unclear in translation. For example, the English word \"bat\" could refer to either the flying animal or the sports equipment. By including the German term \"Schl\u00e4ger\" and the Chinese term \"\u7403\u62cd\", which both unambiguously refer to the sports equipment, we ensured that the intended concept was accurately captured during translation. \n\nIn the second stage, we used LLMs to expand the seed data into the remaining 15 languages. LLMs were tasked with translating the concepts and properties, leveraging their multilingual machine translation capabilities. By providing seed data in three languages (English, German, and Chinese), we enhanced the LLMs' ability to generate accurate translations, as the additional context reduced the likelihood of semantic errors. \n\nIn the third stage, human experts for each target language manually reviewed and corrected the translated concepts and properties. This step ensured that the translations were accurate, culturally appropriate, and semantically aligned with the original dataset. Human intervention was particularly critical for low-resource languages, where LLMs often struggle with semantic precision in translation tasks.",
                    "score": 0.5691334537700856,
                    "section_title": "A Details of Multilingual Data Construction",
                    "char_start_offset": 28009,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 664
                        },
                        {
                            "start": 667,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1487
                        },
                        {
                            "start": 1490,
                            "end": 1580
                        },
                        {
                            "start": 1581,
                            "end": 1707
                        },
                        {
                            "start": 1708,
                            "end": 1915
                        },
                        {
                            "start": 1918,
                            "end": 2048
                        },
                        {
                            "start": 2049,
                            "end": 2179
                        },
                        {
                            "start": 2180,
                            "end": 2324
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 334,
                            "end": 351,
                            "matchedPaperCorpusId": "264145744"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.767578125
                },
                {
                    "corpus_id": "276647924",
                    "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs",
                    "text": "Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, Zhang et al. (2024) introduced MELA to assess multilingual linguistic acceptability, while Mueller et al. (2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021b), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020b) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018b), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2020). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020;Ladhak et al., 2020). \n\nWith the rise of multitask instruction finetuning, multilingual instruction datasets like Supernatural Instructions (Wang et al., 2022) andxP3 (Muennighoff et al., 2023) have further expanded the scope of multilingual evaluation. Despite these advancements, a critical gap remains in evaluating multilingual conceptual understanding, particularly in the format of conceptual minimal pairs. Our work addresses this gap by introducing XCOMPS, a multilingual benchmark specifically designed to evaluate conceptual understanding across languages.",
                    "score": 0.6141693729926606,
                    "section_title": "Multilingual Evaluation Benchmark",
                    "char_start_offset": 10381,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 739
                        },
                        {
                            "start": 742,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1319
                        },
                        {
                            "start": 1322,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1864
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 278,
                            "end": 297,
                            "matchedPaperCorpusId": "265213331"
                        },
                        {
                            "start": 369,
                            "end": 390,
                            "matchedPaperCorpusId": "218470403"
                        },
                        {
                            "start": 532,
                            "end": 555,
                            "matchedPaperCorpusId": "231740666"
                        },
                        {
                            "start": 564,
                            "end": 581,
                            "matchedPaperCorpusId": "264145744"
                        },
                        {
                            "start": 831,
                            "end": 849,
                            "matchedPaperCorpusId": "214641214"
                        },
                        {
                            "start": 860,
                            "end": 880,
                            "matchedPaperCorpusId": "214794966"
                        },
                        {
                            "start": 1059,
                            "end": 1082,
                            "matchedPaperCorpusId": "52271711"
                        },
                        {
                            "start": 1091,
                            "end": 1110,
                            "matchedPaperCorpusId": "201698093"
                        },
                        {
                            "start": 1121,
                            "end": 1141,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 1276,
                            "end": 1298,
                            "matchedPaperCorpusId": "216868259"
                        },
                        {
                            "start": 1298,
                            "end": 1318,
                            "matchedPaperCorpusId": "222177239"
                        },
                        {
                            "start": 1438,
                            "end": 1461,
                            "matchedPaperCorpusId": "253098274"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.703125
                }
            ],
            "relevance_judgement": 0.767578125,
            "relevance_judgment_input_expanded": "# Title: XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs\n# Venue: arXiv.org\n# Authors: Linyang He, Ercong Nie, Sukru Samet Dindar, Arsalan Firoozi, Adrian Florea, Van Nguyen, Corentin Puffay, Riki Shimizu, Haotian Ye, Jonathan Brennan, Helmut Schmid, Hinrich Schutze, N. Mesgarani\n## Abstract\nWe introduce XCOMPS in this work, a multilingual conceptual minimal pair dataset covering 17 languages. Using this dataset, we evaluate LLMs' multilingual conceptual understanding through metalinguistic prompting, direct probability measurement, and neurolinguistic probing. By comparing base, instruction-tuned, and knowledge-distilled models, we find that: 1) LLMs exhibit weaker conceptual understanding for low-resource languages, and accuracy varies across languages despite being tested on the same concept sets. 2) LLMs excel at distinguishing concept-property pairs that are visibly different but exhibit a marked performance drop when negative pairs share subtle semantic similarities. 3) Instruction tuning improves performance in concept understanding but does not enhance internal competence; knowledge distillation can enhance internal competence in conceptual understanding for low-resource languages with limited gains in explicit task performance. 4) More morphologically complex languages yield lower concept understanding scores and require deeper layers for conceptual reasoning.\n## Multilingual Evaluation Benchmark\nMultilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, Zhang et al. (2024) introduced MELA to assess multilingual linguistic acceptability, while Mueller et al. (2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021b), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020b) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018b), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2020). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020;Ladhak et al., 2020). \n\nWith the rise of multitask instruction finetuning, multilingual instruction datasets like Supernatural Instructions (Wang et al., 2022) andxP3 (Muennighoff et al., 2023) have further expanded the scope of multilingual evaluation. Despite these advancements, a critical gap remains in evaluating multilingual conceptual understanding, particularly in the format of conceptual minimal pairs. Our work addresses this gap by introducing XCOMPS, a multilingual benchmark specifically designed to evaluate conceptual understanding across languages.\n\n## A Details of Multilingual Data Construction\nTo construct XCOMPS, a multilingual benchmark covering 17 languages (Table 3), we adopted a human-LLM interactive translation pipeline, leveraging both human expertise and the multilingual generation capabilities of large language models (LLMs). The language set for XCOMPS aligns with the prior knowledge probing benchmark BMLAMA-17 (Qi et al., 2023), ensuring consistency in multilingual evaluation. The highly structured nature of conceptual minimal pair datasets, where positive and negative sentences primarily consist of two components-concepts and properties-enabled us to design a multi-step translation process that ensures high-quality multilingual data. \n\nThe construction process consists of four stages, as displayed in Figure 7. We use the GPT-4o model (GPT-4o-2024-08-06) via the OpenAI API as the translation assistant in the pipeline. In the first stage, we manually translated the original concepts and properties from English into German and Chinese using language experts. We used German and Chinese as additional seed languages to further reduce ambiguity, This multilingual seed data helped disambiguate concepts that might otherwise be unclear in translation. For example, the English word \"bat\" could refer to either the flying animal or the sports equipment. By including the German term \"Schl\u00e4ger\" and the Chinese term \"\u7403\u62cd\", which both unambiguously refer to the sports equipment, we ensured that the intended concept was accurately captured during translation. \n\nIn the second stage, we used LLMs to expand the seed data into the remaining 15 languages. LLMs were tasked with translating the concepts and properties, leveraging their multilingual machine translation capabilities. By providing seed data in three languages (English, German, and Chinese), we enhanced the LLMs' ability to generate accurate translations, as the additional context reduced the likelihood of semantic errors. \n\nIn the third stage, human experts for each target language manually reviewed and corrected the translated concepts and properties. This step ensured that the translations were accurate, culturally appropriate, and semantically aligned with the original dataset. Human intervention was particularly critical for low-resource languages, where LLMs often struggle with semantic precision in translation tasks.",
            "reference_string": "[276647924 | He et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 67,
            "citation_count": 587,
            "influential_citation_count": 82,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00474/2020699/tacl_a_00474.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.03193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2107063269",
                    "name": "Cynthia Gao"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "2158170998",
                    "name": "Peng-Jen Chen"
                },
                {
                    "authorId": "2293203",
                    "name": "Guillaume Wenzek"
                },
                {
                    "authorId": "3092435",
                    "name": "Da Ju"
                },
                {
                    "authorId": "1491706730",
                    "name": "Sanjan Krishnan"
                },
                {
                    "authorId": "1706809",
                    "name": "Marc'Aurelio Ranzato"
                },
                {
                    "authorId": "144204682",
                    "name": "Francisco Guzm\u00e1n"
                },
                {
                    "authorId": "144270981",
                    "name": "Angela Fan"
                }
            ],
            "abstract": "One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.",
            "corpus_id": 235358129,
            "sentences": [
                {
                    "corpus_id": "235358129",
                    "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
                    "text": "One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.",
                    "score": 0.6462726587884189,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7666015625
                }
            ],
            "relevance_judgement": 0.7666015625,
            "relevance_judgment_input_expanded": "# Title: The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjan Krishnan, Marc'Aurelio Ranzato, Francisco Guzm\u00e1n, Angela Fan\n## Abstract\nOne of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.\n",
            "reference_string": "[235358129 | Goyal et al. | 2021 | Citations: 587]"
        },
        {
            "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 61,
            "citation_count": 54,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.438.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.03080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3255454",
                    "name": "Momchil Hardalov"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "46190940",
                    "name": "Dimitrina Zlatkova"
                },
                {
                    "authorId": "1379925776",
                    "name": "Yoan Dinkov"
                },
                {
                    "authorId": "52553663",
                    "name": "Ivan Koychev"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ],
            "abstract": "We propose EXAMS -- a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. \nEXAMS offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. We perform various experiments with existing top-performing multilingual pre-trained models and we show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible before. The data, code, pre-trained models, and evaluation are available at this https URL.",
            "corpus_id": 226262367,
            "sentences": [
                {
                    "corpus_id": "226262367",
                    "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
                    "text": "Here, we present E\u03c7\u03b1\u00b5s, a new dataset and benchmark for multilingual and cross-lingual evaluation of models and methods for answering diverse school science questions (see Figure 1). \n\nOur contributions are as follows: \n\n\u2022 We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade. \u2022 We use fine-grained evaluation -per subject and per language -which yields more precise comparison between models. \u2022 We perform extensive experiments and analysis using top-performing multilingual models (mBERT, XLM-R), and we show that E\u03c7\u03b1\u00b5s offers several challenges that such models would need to overcome in the future, including multi-lingual and crosslingual knowledge retrieval, aggregation, and reasoning, among others. We release our code, pre-trained models and data for research purposes.1",
                    "score": 0.5787373864896417,
                    "section_title": "arXiv:2011.03080v1 [cs.CL] 5 Nov 2020",
                    "char_start_offset": 1995,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 185,
                            "end": 218
                        },
                        {
                            "start": 221,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1315
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7666015625
                },
                {
                    "corpus_id": "226262367",
                    "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
                    "text": "Jing et al. (2019) crowdsourced parallel paragraphs from novels in Chinese and English. A few datasets investigated multiple-choice school QA (Hardalov et al., 2019;Van Nguyena et al., 2020), albeit in a limited domain, and for lower school grades (1st-5th). Other efforts focused on building bi-lingual datasets that are similar in spirit to SQuAD (Rajpurkar et al., 2016) -extractive reading comprehension over open-domain articles. Such datasets are collected by crowdsourcing questions, following a procedure similar to (Rajpurkar et al., 2016), in Russian (Efimov et al., 2020), Korean (Lim et al., 2019), French (d'Hoffschmidt et al., 2020), or by translating existing English QA pairs to Spanish (Carrino et al., 2020). \n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks. Clark et al. (2020)  The task was to ask a question, and then the shortest span answering it from a list of paragraphs was selected. As these datasets are complementary, rather than making each other obsolete, hereby the recently released XTREME (Hu et al., 2020) benchmark combined them in a joint task. E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student. Finally, our fine-grained evaluation for different subjects, languages, and combinations thereof allows for indepth analysis and comparison.",
                    "score": 0.6415313824919707,
                    "section_title": "Related Work",
                    "char_start_offset": 5263,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 726
                        },
                        {
                            "start": 729,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1742
                        },
                        {
                            "start": 1743,
                            "end": 1883
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 142,
                            "end": 165,
                            "matchedPaperCorpusId": "199442384"
                        },
                        {
                            "start": 349,
                            "end": 373,
                            "matchedPaperCorpusId": "11816014"
                        },
                        {
                            "start": 524,
                            "end": 548,
                            "matchedPaperCorpusId": "11816014"
                        },
                        {
                            "start": 561,
                            "end": 582,
                            "matchedPaperCorpusId": "21711688"
                        },
                        {
                            "start": 703,
                            "end": 725,
                            "matchedPaperCorpusId": "209202447"
                        },
                        {
                            "start": 801,
                            "end": 821,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 833,
                            "end": 855,
                            "matchedPaperCorpusId": "204901567"
                        },
                        {
                            "start": 923,
                            "end": 947,
                            "matchedPaperCorpusId": "11816014"
                        },
                        {
                            "start": 1281,
                            "end": 1298,
                            "matchedPaperCorpusId": "214641214"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72900390625
                }
            ],
            "relevance_judgement": 0.7666015625,
            "relevance_judgment_input_expanded": "# Title: EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, Preslav Nakov\n## Abstract\nWe propose EXAMS -- a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. \nEXAMS offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. We perform various experiments with existing top-performing multilingual pre-trained models and we show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible before. The data, code, pre-trained models, and evaluation are available at this https URL.\n## arXiv:2011.03080v1 [cs.CL] 5 Nov 2020\nHere, we present E\u03c7\u03b1\u00b5s, a new dataset and benchmark for multilingual and cross-lingual evaluation of models and methods for answering diverse school science questions (see Figure 1). \n\nOur contributions are as follows: \n\n\u2022 We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade. \u2022 We use fine-grained evaluation -per subject and per language -which yields more precise comparison between models. \u2022 We perform extensive experiments and analysis using top-performing multilingual models (mBERT, XLM-R), and we show that E\u03c7\u03b1\u00b5s offers several challenges that such models would need to overcome in the future, including multi-lingual and crosslingual knowledge retrieval, aggregation, and reasoning, among others. We release our code, pre-trained models and data for research purposes.1\n\n## Related Work\nJing et al. (2019) crowdsourced parallel paragraphs from novels in Chinese and English. A few datasets investigated multiple-choice school QA (Hardalov et al., 2019;Van Nguyena et al., 2020), albeit in a limited domain, and for lower school grades (1st-5th). Other efforts focused on building bi-lingual datasets that are similar in spirit to SQuAD (Rajpurkar et al., 2016) -extractive reading comprehension over open-domain articles. Such datasets are collected by crowdsourcing questions, following a procedure similar to (Rajpurkar et al., 2016), in Russian (Efimov et al., 2020), Korean (Lim et al., 2019), French (d'Hoffschmidt et al., 2020), or by translating existing English QA pairs to Spanish (Carrino et al., 2020). \n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks. Clark et al. (2020)  The task was to ask a question, and then the shortest span answering it from a list of paragraphs was selected. As these datasets are complementary, rather than making each other obsolete, hereby the recently released XTREME (Hu et al., 2020) benchmark combined them in a joint task. E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student. Finally, our fine-grained evaluation for different subjects, languages, and combinations thereof allows for indepth analysis and comparison.",
            "reference_string": "[226262367 | Hardalov et al. | 2020 | Citations: 54]"
        },
        {
            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
            "venue": "NLPPOWER",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.06356",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.06356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "34725175",
                    "name": "Sandipan Dandapat"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                }
            ],
            "abstract": "Although recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.",
            "corpus_id": 248780386,
            "sentences": [
                {
                    "corpus_id": "248780386",
                    "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
                    "text": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition. \n\nRecently, there has been some interest in predicting performance of NLP models without actually evaluating them on a test set. Xia et al. (2020) showed that it is possible to build regression models that can accurately predict evaluation scores of NLP models under different experimental settings using various linguistic and dataset specific features. Srinivasan et al. (2021)   (c) Number of multilingual tasks containing test data for each of the 106 languages supported by the MMLMs (mBERT, XLMR). The bars are shaded according to the class taxonomy proposed by Joshi et al. (2020). \n\nFigure 1 results specifically for MMLMs towards predicting their performance on downstream tasks for different languages in zero-shot and few-shot settings, and Ye et al. (2021) propose methods for more reliable performance prediction by estimating confidence intervals as well as predicting fine-grained performance measures. \n\nIn this paper we argue that the performance prediction can be a possible avenue to address the current issues with Multilingual benchmarking by aiding in the estimation of performance of the MMLMs for the languages which lack any evaluation data for a given task. Not only this can help us give a better idea about the performance of a multilingual model on a task across a much larger set of languages and hence aiding in better model selection, but also enables applications in devising data collection strategies to maximize performance (Srinivasan et al., 2022) as well as in selecting the representative set of languages for a benchmark (Xia et al., 2020).",
                    "score": 0.5842719438886598,
                    "section_title": "Introduction",
                    "char_start_offset": 1631,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1330
                        },
                        {
                            "start": 1333,
                            "end": 1659
                        },
                        {
                            "start": 1662,
                            "end": 1925
                        },
                        {
                            "start": 1926,
                            "end": 2323
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 871,
                            "end": 888,
                            "matchedPaperCorpusId": "218487089"
                        },
                        {
                            "start": 1310,
                            "end": 1329,
                            "matchedPaperCorpusId": "215828350"
                        },
                        {
                            "start": 1494,
                            "end": 1510,
                            "matchedPaperCorpusId": "231861672"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.765625
                }
            ],
            "relevance_judgement": 0.765625,
            "relevance_judgment_input_expanded": "# Title: Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages\n# Venue: NLPPOWER\n# Authors: Kabir Ahuja, Sandipan Dandapat, Sunayana Sitaram, M. Choudhury\n## Abstract\nAlthough recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.\n## Introduction\nMachine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition. \n\nRecently, there has been some interest in predicting performance of NLP models without actually evaluating them on a test set. Xia et al. (2020) showed that it is possible to build regression models that can accurately predict evaluation scores of NLP models under different experimental settings using various linguistic and dataset specific features. Srinivasan et al. (2021)   (c) Number of multilingual tasks containing test data for each of the 106 languages supported by the MMLMs (mBERT, XLMR). The bars are shaded according to the class taxonomy proposed by Joshi et al. (2020). \n\nFigure 1 results specifically for MMLMs towards predicting their performance on downstream tasks for different languages in zero-shot and few-shot settings, and Ye et al. (2021) propose methods for more reliable performance prediction by estimating confidence intervals as well as predicting fine-grained performance measures. \n\nIn this paper we argue that the performance prediction can be a possible avenue to address the current issues with Multilingual benchmarking by aiding in the estimation of performance of the MMLMs for the languages which lack any evaluation data for a given task. Not only this can help us give a better idea about the performance of a multilingual model on a task across a much larger set of languages and hence aiding in better model selection, but also enables applications in devising data collection strategies to maximize performance (Srinivasan et al., 2022) as well as in selecting the representative set of languages for a benchmark (Xia et al., 2020).",
            "reference_string": "[248780386 | Ahuja et al. | 2022 | Citations: 16]"
        },
        {
            "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.04155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325478503",
                    "name": "Hengyu Luo"
                },
                {
                    "authorId": "2312330893",
                    "name": "Zihao Li"
                },
                {
                    "authorId": "2308478088",
                    "name": "Joseph Attieh"
                },
                {
                    "authorId": "2354172398",
                    "name": "Sawal Devkota"
                },
                {
                    "authorId": "51436367",
                    "name": "Ona de Gibert"
                },
                {
                    "authorId": "2312353271",
                    "name": "Shaoxiong Ji"
                },
                {
                    "authorId": "2266791225",
                    "name": "Peiqin Lin"
                },
                {
                    "authorId": "2354176011",
                    "name": "Bhavani Sai Praneeth Varma Mantina"
                },
                {
                    "authorId": "2329094725",
                    "name": "Ananda Sreenidhi"
                },
                {
                    "authorId": "2257040447",
                    "name": "Ra'ul V'azquez"
                },
                {
                    "authorId": "2355403639",
                    "name": "Mengjie Wang"
                },
                {
                    "authorId": "2354177102",
                    "name": "Samea Yusofi"
                },
                {
                    "authorId": "2237430242",
                    "name": "J\u00f6rg Tiedemann"
                }
            ],
            "abstract": "Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.",
            "corpus_id": 277621694,
            "sentences": [
                {
                    "corpus_id": "277621694",
                    "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models",
                    "text": "We integrated 20+ existing multilingual benchmarks into a unified pipeline, by standardizing all ISO 639-3 language codes in the different benchmarks 1 , which covers nearly all known languages worldwide. By aligning benchmark language identifiers with ISO 639-3 codes, we enable evaluations for specific languages or 1 https://iso639-3.sil.org/about arXiv:2504.04155v1 [cs.CL] 5 Apr 2025 language groups (e.g., Bantu, Dravidian, or Uralic languages), allowing the framework to automatically search among integrated benchmarks to find matching test sets. This mapping also makes it easier to incorporate new large-scale benchmarks that target mid-or low-resource languages, ensuring flexibility for future expansions. \n\n2. Language-Specific Prompt Templates. \n\nUsers can configure prompts for each language individually, thereby enabling more precise assessments of a model's instruction-following ability across diverse linguistic settings. All templates are maintained in a centralized prompt library that supports multilingual benchmarks, allowing easy customization as needed. In this way, each task within a benchmark can be run potentially using prompts in the task's original language, rather than defaulting to English prompts. To simplify cross-lingual adaptation, we also implemented Microsoft Translator2 integration that automatically propagates user-defined prompt templates from one single language to 130+ supported languages. \n\n3. Non-English-Centered Machine Translation Evaluation. GlotEval is designed to break away from the traditional Englishcentric paradigm. Thanks to translation benchmarks featuring fully or partially multialigned datasets, GlotEval enables non-English-centered translation evaluations by allowing any supported language to serve as the pivot: users simply update the pivot language in the configuration, to assess \"any-to-pivot\" / \"pivot-to-any\" translation directions. This flexibility ensures that GlotEval breaks from the traditional \"English \u2194 other language\" paradigm and adapts seamlessly to diverse, potentially low-resource, language pairs.",
                    "score": 0.6256079037396397,
                    "section_title": "Consistent Multilingual Benchmarking.",
                    "char_start_offset": 1968,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 717
                        },
                        {
                            "start": 720,
                            "end": 758
                        },
                        {
                            "start": 761,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1235
                        },
                        {
                            "start": 1236,
                            "end": 1441
                        },
                        {
                            "start": 1444,
                            "end": 1499
                        },
                        {
                            "start": 1500,
                            "end": 1580
                        },
                        {
                            "start": 1581,
                            "end": 1912
                        },
                        {
                            "start": 1913,
                            "end": 2091
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76171875
                }
            ],
            "relevance_judgement": 0.76171875,
            "relevance_judgment_input_expanded": "# Title: GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models\n# Venue: arXiv.org\n# Authors: Hengyu Luo, Zihao Li, Joseph Attieh, Sawal Devkota, Ona de Gibert, Shaoxiong Ji, Peiqin Lin, Bhavani Sai Praneeth Varma Mantina, Ananda Sreenidhi, Ra'ul V'azquez, Mengjie Wang, Samea Yusofi, J\u00f6rg Tiedemann\n## Abstract\nLarge language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.\n## Consistent Multilingual Benchmarking.\nWe integrated 20+ existing multilingual benchmarks into a unified pipeline, by standardizing all ISO 639-3 language codes in the different benchmarks 1 , which covers nearly all known languages worldwide. By aligning benchmark language identifiers with ISO 639-3 codes, we enable evaluations for specific languages or 1 https://iso639-3.sil.org/about arXiv:2504.04155v1 [cs.CL] 5 Apr 2025 language groups (e.g., Bantu, Dravidian, or Uralic languages), allowing the framework to automatically search among integrated benchmarks to find matching test sets. This mapping also makes it easier to incorporate new large-scale benchmarks that target mid-or low-resource languages, ensuring flexibility for future expansions. \n\n2. Language-Specific Prompt Templates. \n\nUsers can configure prompts for each language individually, thereby enabling more precise assessments of a model's instruction-following ability across diverse linguistic settings. All templates are maintained in a centralized prompt library that supports multilingual benchmarks, allowing easy customization as needed. In this way, each task within a benchmark can be run potentially using prompts in the task's original language, rather than defaulting to English prompts. To simplify cross-lingual adaptation, we also implemented Microsoft Translator2 integration that automatically propagates user-defined prompt templates from one single language to 130+ supported languages. \n\n3. Non-English-Centered Machine Translation Evaluation. GlotEval is designed to break away from the traditional Englishcentric paradigm. Thanks to translation benchmarks featuring fully or partially multialigned datasets, GlotEval enables non-English-centered translation evaluations by allowing any supported language to serve as the pivot: users simply update the pivot language in the configuration, to assess \"any-to-pivot\" / \"pivot-to-any\" translation directions. This flexibility ensures that GlotEval breaks from the traditional \"English \u2194 other language\" paradigm and adapts seamlessly to diverse, potentially low-resource, language pairs.",
            "reference_string": "[277621694 | Luo et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2072738714",
                    "name": "Sumanth Doddapaneni"
                },
                {
                    "authorId": "2281673850",
                    "name": "Mohammed Safi Ur Rahman Khan"
                },
                {
                    "authorId": "2326302270",
                    "name": "Dilip Venkatesh"
                },
                {
                    "authorId": "3209719",
                    "name": "Raj Dabre"
                },
                {
                    "authorId": "1711973",
                    "name": "Anoop Kunchukuttan"
                },
                {
                    "authorId": "2361078",
                    "name": "Mitesh M. Khapra"
                }
            ],
            "abstract": "Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.",
            "corpus_id": 273404117,
            "sentences": [
                {
                    "corpus_id": "273404117",
                    "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
                    "text": "Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.",
                    "score": 0.5801966760697993,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.755859375
                }
            ],
            "relevance_judgement": 0.755859375,
            "relevance_judgment_input_expanded": "# Title: Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs\n# Venue: arXiv.org\n# Authors: Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Dilip Venkatesh, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra\n## Abstract\nEvaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.\n",
            "reference_string": "[273404117 | Doddapaneni et al. | 2024 | Citations: 1]"
        },
        {
            "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 57,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.09151",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.09151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "40832517",
                    "name": "Noah Constant"
                },
                {
                    "authorId": "143936294",
                    "name": "Xavier Garc\u00eda"
                },
                {
                    "authorId": "145625142",
                    "name": "Adam Roberts"
                },
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "46617804",
                    "name": "Sharan Narang"
                },
                {
                    "authorId": "2345617",
                    "name": "Orhan Firat"
                }
            ],
            "abstract": "Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.",
            "corpus_id": 258187051,
            "sentences": [
                {
                    "corpus_id": "258187051",
                    "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining",
                    "text": "In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language. Finally, to the degree possible, benchmarks should be realistic, such that performing better on the benchmark gives us confidence that a model will do better on actual tasks facing language technology users. This is in contrast to \"intermediate structure\" tasks such as part-of-speech tagging. \n\nTyDi QA (Clark et al., 2020) is a multilingual question-answering benchmark covering a range of typologically diverse languages. Questions are written from scratch by native speakers in each language, ensuring culturally relevant content and the absence of \"translationese\". We use the \"GoldP\" task, which covers 9 languages. To evaluate candidate models, we use the \"in-language multitask\" setting (Hu et al., 2020)-fine-tuning on a mixture of all available languages, and evaluating in each language separately. To maximize per-language performance, we select per-language checkpoints based on the validation performance, and report validation metrics, as no test set is provided. \n\nThe WMT21 shared task on large-scale multilingual machine translation (Wenzek et al., 2021) tests the ability of single model to translate across many languages. We focus on the \"small track\" tasks, each testing translation between 6 languages, in all 30 combinations. As with TyDi QA, we finetune a single multilingual model on the mixture of all tasks and select per-language-pair checkpoints based on the test set performance. 3 To reduce heavy English bias, we restrict training data to 1,000 examples per language pair by randomly subsampling from the training data.",
                    "score": 0.5713651238106073,
                    "section_title": "EVALUATION TASKS",
                    "char_start_offset": 13982,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 72
                        },
                        {
                            "start": 73,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 901
                        },
                        {
                            "start": 904,
                            "end": 1032
                        },
                        {
                            "start": 1033,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1586
                        },
                        {
                            "start": 1589,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1857
                        },
                        {
                            "start": 1858,
                            "end": 2020
                        },
                        {
                            "start": 2021,
                            "end": 2160
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 912,
                            "end": 931,
                            "matchedPaperCorpusId": "212657414"
                        },
                        {
                            "start": 1303,
                            "end": 1319,
                            "matchedPaperCorpusId": "214641214"
                        },
                        {
                            "start": 1659,
                            "end": 1680,
                            "matchedPaperCorpusId": "245148999"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75341796875
                }
            ],
            "relevance_judgement": 0.75341796875,
            "relevance_judgment_input_expanded": "# Title: UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining\n# Venue: International Conference on Learning Representations\n# Authors: Hyung Won Chung, Noah Constant, Xavier Garc\u00eda, Adam Roberts, Yi Tay, Sharan Narang, Orhan Firat\n## Abstract\nPretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.\n## EVALUATION TASKS\nIn selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language. Finally, to the degree possible, benchmarks should be realistic, such that performing better on the benchmark gives us confidence that a model will do better on actual tasks facing language technology users. This is in contrast to \"intermediate structure\" tasks such as part-of-speech tagging. \n\nTyDi QA (Clark et al., 2020) is a multilingual question-answering benchmark covering a range of typologically diverse languages. Questions are written from scratch by native speakers in each language, ensuring culturally relevant content and the absence of \"translationese\". We use the \"GoldP\" task, which covers 9 languages. To evaluate candidate models, we use the \"in-language multitask\" setting (Hu et al., 2020)-fine-tuning on a mixture of all available languages, and evaluating in each language separately. To maximize per-language performance, we select per-language checkpoints based on the validation performance, and report validation metrics, as no test set is provided. \n\nThe WMT21 shared task on large-scale multilingual machine translation (Wenzek et al., 2021) tests the ability of single model to translate across many languages. We focus on the \"small track\" tasks, each testing translation between 6 languages, in all 30 combinations. As with TyDi QA, we finetune a single multilingual model on the mixture of all tasks and select per-language-pair checkpoints based on the test set performance. 3 To reduce heavy English bias, we restrict training data to 1,000 examples per language pair by randomly subsampling from the training data.",
            "reference_string": "[258187051 | Chung et al. | 2023 | Citations: 57]"
        },
        {
            "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 63,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2246317582",
                    "name": "Somnath Banerjee"
                },
                {
                    "authorId": "2273400615",
                    "name": "Sayan Layek"
                },
                {
                    "authorId": "2345820876",
                    "name": "Pratyush Chatterjee"
                },
                {
                    "authorId": "2286311932",
                    "name": "Animesh Mukherjee"
                },
                {
                    "authorId": "2273549702",
                    "name": "Rima Hazra"
                }
            ],
            "abstract": "Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the\"functional heads\"most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.",
            "corpus_id": 276408508,
            "sentences": [
                {
                    "corpus_id": "276408508",
                    "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
                    "text": "XSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset.",
                    "score": 0.6007322419251171,
                    "section_title": "A Additional experiment",
                    "char_start_offset": 26673,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 705
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74462890625
                }
            ],
            "relevance_judgement": 0.74462890625,
            "relevance_judgment_input_expanded": "# Title: Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment\n# Venue: arXiv.org\n# Authors: Somnath Banerjee, Sayan Layek, Pratyush Chatterjee, Animesh Mukherjee, Rima Hazra\n## Abstract\nEnsuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the\"functional heads\"most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.\n## A Additional experiment\nXSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset.",
            "reference_string": "[276408508 | Banerjee et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2404.16257",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2101318920",
                    "name": "Hyeonseok Moon"
                },
                {
                    "authorId": "2275193495",
                    "name": "Seungyoon Lee"
                },
                {
                    "authorId": "2298449941",
                    "name": "Seongtae Hong"
                },
                {
                    "authorId": "2144498836",
                    "name": "Seungjun Lee"
                },
                {
                    "authorId": "2115195904",
                    "name": "Chanjun Park"
                },
                {
                    "authorId": "83056580",
                    "name": "Heu-Jeoung Lim"
                }
            ],
            "abstract": "Translating major language resources to build minor language resources becomes a widely-used approach. Particularly in translating complex data points composed of multiple components, it is common to translate each component separately. However, we argue that this practice often overlooks the interrelation between components within the same data point. To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data. In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation. We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components. Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data. Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark.",
            "corpus_id": 269362697,
            "sentences": [
                {
                    "corpus_id": "269362697",
                    "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
                    "text": "For validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy. Notably, our proposed data translation pipeline demonstrates a more effective strategy to attain high-quality training data, compared to the individual translation of each data component.",
                    "score": 0.6179501219478962,
                    "section_title": "Introduction",
                    "char_start_offset": 3859,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 831
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 252,
                            "end": 272,
                            "matchedPaperCorpusId": "214794966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7421875
                }
            ],
            "relevance_judgement": 0.7421875,
            "relevance_judgment_input_expanded": "# Title: Translation of Multifaceted Data without Re-Training of Machine Translation Systems\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Hyeonseok Moon, Seungyoon Lee, Seongtae Hong, Seungjun Lee, Chanjun Park, Heu-Jeoung Lim\n## Abstract\nTranslating major language resources to build minor language resources becomes a widely-used approach. Particularly in translating complex data points composed of multiple components, it is common to translate each component separately. However, we argue that this practice often overlooks the interrelation between components within the same data point. To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data. In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation. We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components. Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data. Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark.\n## Introduction\nFor validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy. Notably, our proposed data translation pipeline demonstrates a more effective strategy to attain high-quality training data, compared to the individual translation of each data component.",
            "reference_string": "[269362697 | Moon et al. | 2024 | Citations: 0]"
        },
        {
            "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 23,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.06412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283821804",
                    "name": "Eunsu Kim"
                },
                {
                    "authorId": "2299329316",
                    "name": "Juyoung Suk"
                },
                {
                    "authorId": "2262214736",
                    "name": "Philhoon Oh"
                },
                {
                    "authorId": "47111345",
                    "name": "Haneul Yoo"
                },
                {
                    "authorId": "2262214700",
                    "name": "James Thorne"
                },
                {
                    "authorId": "2283768962",
                    "name": "Alice Oh"
                }
            ],
            "abstract": "Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in click, we provide fine-grained annotation of which cultural and linguistic knowledge is required to correctly answer the question. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs\u2019 proficiency in Korean language and culture.",
            "corpus_id": 268357672,
            "sentences": [
                {
                    "corpus_id": "268357672",
                    "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
                    "text": "To investigate how LMs can comprehend or generate text in other languages, there have been efforts to construct multilingual datasets.For instance, XGLUE (Liang et al., 2020) encompasses 100 languages that can be employed for both pre-training and evaluating cross-lingual tasks.XTREME (Hu et al., 2020) introduces an evaluation framework for cross-lingual benchmarks, while MEGA (Ahuja et al., 2023) focuses on assessing LLMs, providing 16 NLP datasets ranging from low-resource to high-resource languages.In addition, datasets that primarily focus on specific target languages, such as Chinese, Indian, and African languages, have been introduced (Huang et al., 2023;Doddapaneni et al., 2023;Adebara et al., 2023).\n\nThe popularity of commonsense datasets has increased because they reflect a wide array of sociocultural knowledge shared by humans (Liu and Singh, 2004).These datasets incorporate everyday concepts, such as CommonsenseQA (Talmor et al., 2019), scientific knowledge like ARC (Clark et al., 2018), and simple arithmetic reasoning like GSM8K (Cobbe et al., 2021).These datasets can be seen as a representation of general and practical knowledge that aligns with human intentions.Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, Ma et al. 2022 emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages.",
                    "score": 0.6198405442071793,
                    "section_title": "Multilingual and Commonsense",
                    "char_start_offset": 4062,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 134,
                            "end": 279
                        },
                        {
                            "start": 279,
                            "end": 507
                        },
                        {
                            "start": 507,
                            "end": 716
                        },
                        {
                            "start": 718,
                            "end": 871
                        },
                        {
                            "start": 871,
                            "end": 1078
                        },
                        {
                            "start": 1078,
                            "end": 1194
                        },
                        {
                            "start": 1194,
                            "end": 1438
                        },
                        {
                            "start": 1438,
                            "end": 1626
                        },
                        {
                            "start": 1626,
                            "end": 1781
                        },
                        {
                            "start": 1781,
                            "end": 1849
                        },
                        {
                            "start": 1849,
                            "end": 2073
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 174,
                            "matchedPaperCorpusId": "214794966"
                        },
                        {
                            "start": 849,
                            "end": 870,
                            "matchedPaperCorpusId": "266028051"
                        },
                        {
                            "start": 939,
                            "end": 960,
                            "matchedPaperCorpusId": "53296520"
                        },
                        {
                            "start": 1289,
                            "end": 1307,
                            "matchedPaperCorpusId": "250562882"
                        },
                        {
                            "start": 1416,
                            "end": 1437,
                            "matchedPaperCorpusId": "3523957"
                        },
                        {
                            "start": 1639,
                            "end": 1653,
                            "matchedPaperCorpusId": "247762223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7412109375
                }
            ],
            "relevance_judgement": 0.7412109375,
            "relevance_judgment_input_expanded": "# Title: CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean\n# Venue: International Conference on Language Resources and Evaluation\n# Authors: Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice Oh\n## Abstract\nDespite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in click, we provide fine-grained annotation of which cultural and linguistic knowledge is required to correctly answer the question. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs\u2019 proficiency in Korean language and culture.\n## Multilingual and Commonsense\nTo investigate how LMs can comprehend or generate text in other languages, there have been efforts to construct multilingual datasets.For instance, XGLUE (Liang et al., 2020) encompasses 100 languages that can be employed for both pre-training and evaluating cross-lingual tasks.XTREME (Hu et al., 2020) introduces an evaluation framework for cross-lingual benchmarks, while MEGA (Ahuja et al., 2023) focuses on assessing LLMs, providing 16 NLP datasets ranging from low-resource to high-resource languages.In addition, datasets that primarily focus on specific target languages, such as Chinese, Indian, and African languages, have been introduced (Huang et al., 2023;Doddapaneni et al., 2023;Adebara et al., 2023).\n\nThe popularity of commonsense datasets has increased because they reflect a wide array of sociocultural knowledge shared by humans (Liu and Singh, 2004).These datasets incorporate everyday concepts, such as CommonsenseQA (Talmor et al., 2019), scientific knowledge like ARC (Clark et al., 2018), and simple arithmetic reasoning like GSM8K (Cobbe et al., 2021).These datasets can be seen as a representation of general and practical knowledge that aligns with human intentions.Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, Ma et al. 2022 emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages.",
            "reference_string": "[268357672 | Kim et al. | 2024 | Citations: 23]"
        },
        {
            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 64,
            "citation_count": 120,
            "influential_citation_count": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.04387",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49192881",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2109472880",
                    "name": "Yuwei Yin"
                },
                {
                    "authorId": "2117947438",
                    "name": "Shicheng Li"
                },
                {
                    "authorId": "2146034504",
                    "name": "Liang Chen"
                },
                {
                    "authorId": "144202874",
                    "name": "Peiyi Wang"
                },
                {
                    "authorId": "1906099",
                    "name": "Shuhuai Ren"
                },
                {
                    "authorId": "2027599235",
                    "name": "Mukai Li"
                },
                {
                    "authorId": "14646700",
                    "name": "Yazheng Yang"
                },
                {
                    "authorId": "47883405",
                    "name": "Jingjing Xu"
                },
                {
                    "authorId": "2116530295",
                    "name": "Xu Sun"
                },
                {
                    "authorId": "47648549",
                    "name": "Lingpeng Kong"
                },
                {
                    "authorId": "2144831944",
                    "name": "Qi Liu"
                }
            ],
            "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.",
            "corpus_id": 259095896,
            "sentences": [
                {
                    "corpus_id": "259095896",
                    "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
                    "text": "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names).",
                    "score": 0.6189057073606792,
                    "section_title": "Stage IV: Key Datasets Translation",
                    "char_start_offset": 12869,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 803
                        },
                        {
                            "start": 804,
                            "end": 891
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.74072265625,
            "relevance_judgment_input_expanded": "# Title: M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning\n# Venue: arXiv.org\n# Authors: Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu\n## Abstract\nInstruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.\n## Stage IV: Key Datasets Translation\nTo boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names).",
            "reference_string": "[259095896 | Li et al. | 2023 | Citations: 120]"
        },
        {
            "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 55,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11824, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2335666192",
                    "name": "Chengyan Wu"
                },
                {
                    "authorId": "2335825370",
                    "name": "Bolei Ma"
                },
                {
                    "authorId": "2107995084",
                    "name": "Yihong Liu"
                },
                {
                    "authorId": "2144388883",
                    "name": "Zheyu Zhang"
                },
                {
                    "authorId": "2266470510",
                    "name": "Ningyuan Deng"
                },
                {
                    "authorId": "2345863610",
                    "name": "Yanshu Li"
                },
                {
                    "authorId": "2346275292",
                    "name": "Baolan Chen"
                },
                {
                    "authorId": "2345832186",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2284982105",
                    "name": "Barbara Plank"
                },
                {
                    "authorId": "2335829308",
                    "name": "Yun Xue"
                }
            ],
            "abstract": "Aspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research.",
            "corpus_id": 276408022,
            "sentences": [
                {
                    "corpus_id": "276408022",
                    "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis",
                    "text": "Zhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer. Moreover, this translated dataset only includes aspect terms and sentiment polarities, lacking the joint detection of aspect categories, which is crucial for ABSA, thereby limiting the scope of the evaluation. Therefore, a high-quality multilingual and parallel ABSA dataset is missing in the community. \n\nTo bridge this gap, this paper presents the M-ABSA dataset, which includes 21 languages and 7 distinct domains, making it the first comprehensive multilingual parallel ABSA dataset. Specifically, we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary. To further investigate the quality of M-ABSA and unveil its possible usage in multilingual ABSA research, we conduct evaluations on M-ABSA under various settings, including zero-shot cross-lingual transfer, cross-domain transfer, and zero-shot prompting with large language models (LLMs). Our contributions are as follows: \n\n\u2022 We present a high-quality multilingual parallel dataset for ABSA, covering 21 typologically diverse languages and multiple domains, ensuring its applicability in multilingual ABSA tasks for triplet extraction. \n\n\u2022 We provide a feasible method of extending monolingual datasets to multiple languages with high quality by automatic translation, quality evaluation, and manual inspection. \n\n\u2022 We investigate the robustness and applicability of M-ABSA by a comprehensive evaluation, including cross-lingual transfer, cross-domain transfer, and zero-shot prompting with LLMs. Our results highlight the potential of M-ABSA in future multilingual ABSA research. \n\n2 Background and Related Work",
                    "score": 0.6348408650137721,
                    "section_title": "Introduction",
                    "char_start_offset": 1738,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 729
                        },
                        {
                            "start": 732,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1571
                        },
                        {
                            "start": 1574,
                            "end": 1785
                        },
                        {
                            "start": 1788,
                            "end": 1961
                        },
                        {
                            "start": 1964,
                            "end": 2146
                        },
                        {
                            "start": 2147,
                            "end": 2230
                        },
                        {
                            "start": 2233,
                            "end": 2262
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 108,
                            "end": 130,
                            "matchedPaperCorpusId": "1021411"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7392578125
                }
            ],
            "relevance_judgement": 0.7392578125,
            "relevance_judgment_input_expanded": "# Title: M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis\n# Venue: arXiv.org\n# Authors: Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Barbara Plank, Yun Xue\n## Abstract\nAspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research.\n## Introduction\nZhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer. Moreover, this translated dataset only includes aspect terms and sentiment polarities, lacking the joint detection of aspect categories, which is crucial for ABSA, thereby limiting the scope of the evaluation. Therefore, a high-quality multilingual and parallel ABSA dataset is missing in the community. \n\nTo bridge this gap, this paper presents the M-ABSA dataset, which includes 21 languages and 7 distinct domains, making it the first comprehensive multilingual parallel ABSA dataset. Specifically, we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary. To further investigate the quality of M-ABSA and unveil its possible usage in multilingual ABSA research, we conduct evaluations on M-ABSA under various settings, including zero-shot cross-lingual transfer, cross-domain transfer, and zero-shot prompting with large language models (LLMs). Our contributions are as follows: \n\n\u2022 We present a high-quality multilingual parallel dataset for ABSA, covering 21 typologically diverse languages and multiple domains, ensuring its applicability in multilingual ABSA tasks for triplet extraction. \n\n\u2022 We provide a feasible method of extending monolingual datasets to multiple languages with high quality by automatic translation, quality evaluation, and manual inspection. \n\n\u2022 We investigate the robustness and applicability of M-ABSA by a comprehensive evaluation, including cross-lingual transfer, cross-domain transfer, and zero-shot prompting with LLMs. Our results highlight the potential of M-ABSA in future multilingual ABSA research. \n\n2 Background and Related Work",
            "reference_string": "[276408022 | Wu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 166,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2204051777",
                    "name": "Yusuke Sakai"
                },
                {
                    "authorId": "2300756",
                    "name": "Hidetaka Kamigaito"
                },
                {
                    "authorId": "2266807418",
                    "name": "Taro Watanabe"
                }
            ],
            "abstract": "It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense. This highlights the necessity of language-specific datasets for evaluation and training. Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation. The datasets are available at https://huggingface.co/datasets/yusuke1997/mCSQA.",
            "corpus_id": 270285718,
            "sentences": [
                {
                    "corpus_id": "270285718",
                    "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
                    "text": "The above datasets primarily focus on English, but there exist datasets in Japanese (Omura et al., 2020;Takahashi et al., 2019;Hayashibe, 2020), Chinese (Xu et al., 2021(Xu et al., , 2020;;Wang et al., 2022), Russian (Shavrina et al., 2020;Taktasheva et al., 2022), and Indonesian (Koto et al., 2022).For multilingual datasets, most are extended versions of existing ones through translation, such as X-COPA (Ponti et al., 2020) from COPA, X-CSQA (Lin et al., 2021) from CSQA, and X-CODAH (Lin et al., 2021) from CODAH (Chen et al., 2019).A few datasets, such as TyDiQA (Clark et al., 2020), are created for each language from scratch.\n\nMultilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023).",
                    "score": 0.7291832190540856,
                    "section_title": "Introduction",
                    "char_start_offset": 4705,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 301
                        },
                        {
                            "start": 301,
                            "end": 539
                        },
                        {
                            "start": 539,
                            "end": 635
                        },
                        {
                            "start": 637,
                            "end": 826
                        },
                        {
                            "start": 828,
                            "end": 1369
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 104,
                            "end": 127,
                            "matchedPaperCorpusId": "208030290"
                        },
                        {
                            "start": 153,
                            "end": 169,
                            "matchedPaperCorpusId": "233033383"
                        },
                        {
                            "start": 169,
                            "end": 189,
                            "matchedPaperCorpusId": "215745536"
                        },
                        {
                            "start": 189,
                            "end": 207,
                            "matchedPaperCorpusId": "256461363"
                        },
                        {
                            "start": 217,
                            "end": 240,
                            "matchedPaperCorpusId": "226222281"
                        },
                        {
                            "start": 240,
                            "end": 264,
                            "matchedPaperCorpusId": "253098482"
                        },
                        {
                            "start": 281,
                            "end": 300,
                            "matchedPaperCorpusId": "247581412"
                        },
                        {
                            "start": 408,
                            "end": 428,
                            "matchedPaperCorpusId": "218470125"
                        },
                        {
                            "start": 447,
                            "end": 465,
                            "matchedPaperCorpusId": "235421949"
                        },
                        {
                            "start": 489,
                            "end": 507,
                            "matchedPaperCorpusId": "235421949"
                        },
                        {
                            "start": 519,
                            "end": 538,
                            "matchedPaperCorpusId": "104292422"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7333984375
                }
            ],
            "relevance_judgement": 0.7333984375,
            "relevance_judgment_input_expanded": "# Title: mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe\n## Abstract\nIt is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense. This highlights the necessity of language-specific datasets for evaluation and training. Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation. The datasets are available at https://huggingface.co/datasets/yusuke1997/mCSQA.\n## Introduction\nThe above datasets primarily focus on English, but there exist datasets in Japanese (Omura et al., 2020;Takahashi et al., 2019;Hayashibe, 2020), Chinese (Xu et al., 2021(Xu et al., , 2020;;Wang et al., 2022), Russian (Shavrina et al., 2020;Taktasheva et al., 2022), and Indonesian (Koto et al., 2022).For multilingual datasets, most are extended versions of existing ones through translation, such as X-COPA (Ponti et al., 2020) from COPA, X-CSQA (Lin et al., 2021) from CSQA, and X-CODAH (Lin et al., 2021) from CODAH (Chen et al., 2019).A few datasets, such as TyDiQA (Clark et al., 2020), are created for each language from scratch.\n\nMultilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023).",
            "reference_string": "[270285718 | Sakai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.04582",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "36943315",
                    "name": "Leonhard Hennig"
                },
                {
                    "authorId": "143680610",
                    "name": "Philippe E. Thomas"
                },
                {
                    "authorId": "145733288",
                    "name": "Sebastian M\u00f6ller"
                }
            ],
            "abstract": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
            "corpus_id": 258557740,
            "sentences": [
                {
                    "corpus_id": "258557740",
                    "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
                    "text": "lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are:\n\n\u2022 We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese.\n\n\u2022 We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages.\n\n\u2022 We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors.",
                    "score": 0.6085050844476354,
                    "section_title": "Introduction",
                    "char_start_offset": 3611,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7333984375
                }
            ],
            "relevance_judgement": 0.7333984375,
            "relevance_judgment_input_expanded": "# Title: MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Leonhard Hennig, Philippe E. Thomas, Sebastian M\u00f6ller\n## Abstract\nRelation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.\n## Introduction\nlingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are:\n\n\u2022 We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese.\n\n\u2022 We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages.\n\n\u2022 We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors.",
            "reference_string": "[258557740 | Hennig et al. | 2023 | Citations: 8]"
        },
        {
            "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.10356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3449411",
                    "name": "Dieuwke Hupkes"
                },
                {
                    "authorId": "3444222",
                    "name": "Nikolay Bogoychev"
                }
            ],
            "abstract": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs covering 31 languages. MultiLoKo consists of three partitions: a main partition consisting of 500 questions per language, separately sourced to be locally relevant to the specific language, and two translated partitions, containing human-authored translations from 30 non-English languages to English and vice versa. For comparison, we also release corresponding machine-authored translations. The data is equally distributed over two splits: a dev split and a blind, out-of-distribution test split. MultiLoKo can be used to study a variety of questions regarding the multilinguality of LLMs as well as meta-questions about multilingual benchmark creation. We compute MultiLoKo scores for 11 base and chat models marketed to be multilingual and study their average performance, their performance parity across languages, how much their ability to answer questions depends on the question language, and which languages are most difficult. None of the models we studied performs well on MultiLoKo, as indicated by low average scores as well as large differences between the best and worst scoring languages. Furthermore, we find a substantial effect of the question language, indicating sub-optimal knowledge transfer between languages. Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models.",
            "corpus_id": 277780370,
            "sentences": [
                {
                    "corpus_id": "277780370",
                    "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
                    "text": "Two recent examples of studies incorporating consistency-based evaluations on factual knowledge questions are Qi et al. (2023) and Ohmer et al. (2023). Qi et al. (2023) focusses specifically on sample-level consistency of answers across different languages, requiring existing parallel benchmarks. Ohmer et al. (2023), instead, ask models to translate benchmark questions themselves before answering them again. This can, with some caveats, be applied to any existing monolingual benchmark, butrequiring multiple steps -it is more involved an a paradigm, and is somewhat bottlenecked by the translation ability of the model to be evaluated. \n\nTranslation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by Brown et al. (2020), but has not been common since. \n\nMonolingual non-English evaluation In our discussion, we have focussed on multilingual evaluation options that cover multiple other languages. After all, a benchmark to evaluate models on Bengali (e.g. Shafayat et al., 2024) or Arabic (e.g. Alwajih et al., 2024) can contribute to multilingual evaluation when combined with other benchmarks, but does not so on its own. Because such benchmarks are usually created by language experts for the respective languages, they usually target locally relevant skills and knowledge and are likely of higher quality than benchmarks created for many languages simultaneously (either through translation or from scratch). Yet, composing a suite including many languages that allows direct comparisons between languages remains challenging. We believe such benchmarks can be important for multilingual evaluation in LLMs, but will not further discuss benchmarks focussing on individual languages or very small sets of languages within one family here.",
                    "score": 0.6409364767028363,
                    "section_title": "E.2 Multilingual evaluation options for LLMs",
                    "char_start_offset": 61859,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 640
                        },
                        {
                            "start": 643,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 908
                        },
                        {
                            "start": 911,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 1898
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 110,
                            "end": 126,
                            "matchedPaperCorpusId": "264145744"
                        },
                        {
                            "start": 131,
                            "end": 150,
                            "matchedPaperCorpusId": "258840917"
                        },
                        {
                            "start": 152,
                            "end": 168,
                            "matchedPaperCorpusId": "264145744"
                        },
                        {
                            "start": 298,
                            "end": 317,
                            "matchedPaperCorpusId": "258840917"
                        },
                        {
                            "start": 857,
                            "end": 876,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1113,
                            "end": 1135,
                            "matchedPaperCorpusId": "271931333"
                        },
                        {
                            "start": 1152,
                            "end": 1173,
                            "matchedPaperCorpusId": "271432107"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72900390625
                }
            ],
            "relevance_judgement": 0.72900390625,
            "relevance_judgment_input_expanded": "# Title: MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages\n# Venue: arXiv.org\n# Authors: Dieuwke Hupkes, Nikolay Bogoychev\n## Abstract\nWe present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs covering 31 languages. MultiLoKo consists of three partitions: a main partition consisting of 500 questions per language, separately sourced to be locally relevant to the specific language, and two translated partitions, containing human-authored translations from 30 non-English languages to English and vice versa. For comparison, we also release corresponding machine-authored translations. The data is equally distributed over two splits: a dev split and a blind, out-of-distribution test split. MultiLoKo can be used to study a variety of questions regarding the multilinguality of LLMs as well as meta-questions about multilingual benchmark creation. We compute MultiLoKo scores for 11 base and chat models marketed to be multilingual and study their average performance, their performance parity across languages, how much their ability to answer questions depends on the question language, and which languages are most difficult. None of the models we studied performs well on MultiLoKo, as indicated by low average scores as well as large differences between the best and worst scoring languages. Furthermore, we find a substantial effect of the question language, indicating sub-optimal knowledge transfer between languages. Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models.\n## E.2 Multilingual evaluation options for LLMs\nTwo recent examples of studies incorporating consistency-based evaluations on factual knowledge questions are Qi et al. (2023) and Ohmer et al. (2023). Qi et al. (2023) focusses specifically on sample-level consistency of answers across different languages, requiring existing parallel benchmarks. Ohmer et al. (2023), instead, ask models to translate benchmark questions themselves before answering them again. This can, with some caveats, be applied to any existing monolingual benchmark, butrequiring multiple steps -it is more involved an a paradigm, and is somewhat bottlenecked by the translation ability of the model to be evaluated. \n\nTranslation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by Brown et al. (2020), but has not been common since. \n\nMonolingual non-English evaluation In our discussion, we have focussed on multilingual evaluation options that cover multiple other languages. After all, a benchmark to evaluate models on Bengali (e.g. Shafayat et al., 2024) or Arabic (e.g. Alwajih et al., 2024) can contribute to multilingual evaluation when combined with other benchmarks, but does not so on its own. Because such benchmarks are usually created by language experts for the respective languages, they usually target locally relevant skills and knowledge and are likely of higher quality than benchmarks created for many languages simultaneously (either through translation or from scratch). Yet, composing a suite including many languages that allows direct comparisons between languages remains challenging. We believe such benchmarks can be important for multilingual evaluation in LLMs, but will not further discuss benchmarks focussing on individual languages or very small sets of languages within one family here.",
            "reference_string": "[277780370 | Hupkes et al. | 2025 | Citations: 0]"
        },
        {
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321452295",
                    "name": "Guijin Son"
                },
                {
                    "authorId": "29830817",
                    "name": "Dongkeun Yoon"
                },
                {
                    "authorId": "2299329316",
                    "name": "Juyoung Suk"
                },
                {
                    "authorId": "2301578911",
                    "name": "Javier Aula-Blasco"
                },
                {
                    "authorId": "2327215494",
                    "name": "Mano Aslan"
                },
                {
                    "authorId": "2327216625",
                    "name": "Vu Trong Kim"
                },
                {
                    "authorId": "2232783785",
                    "name": "Shayekh Bin Islam"
                },
                {
                    "authorId": "2327215436",
                    "name": "Jaume Prats-Cristi\u00e0"
                },
                {
                    "authorId": "2327217057",
                    "name": "Luc\u00eda Tormo-Ba\u00f1uelos"
                },
                {
                    "authorId": "2184037220",
                    "name": "Seungone Kim"
                }
            ],
            "abstract": "As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\"meta-evaluation benchmarks\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.",
            "corpus_id": 273532388,
            "sentences": [
                {
                    "corpus_id": "273532388",
                    "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
                    "text": "In aligning LLMs, evaluator LLMs play a central role-from training (Pace et al., 2024) and evaluation (Zheng et al., 2023) to test-time inference (Gao et al., 2023;Wu et al., 2024a;Son et al., 2025). Naturally, the critical role of evaluator LLMs has led to the emergence of meta-evaluation benchmarks, which aim to directly assess the quality of evaluators. These benchmarks typically present pairs of candidate responses with a predefined preference-based on either correctness or human judgments-and measure the evaluator LLM's accuracy by how often it assigns a higher score to the preferred (chosen) response over the unpreferred (rejected) response. \n\nWhile multilingual LLMs also depend on automated evaluators, we still lack clear guidelines on evaluating them across diverse languages. A concurrent effort, M-RewardBench (Gureja et al., 2024), attempts to fill this gap by machine translating the original RewardBench into multiple languages. However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments.",
                    "score": 0.5806204852995731,
                    "section_title": "Preliminary",
                    "char_start_offset": 5592,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 655
                        },
                        {
                            "start": 658,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1529
                        },
                        {
                            "start": 1532,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1699
                        },
                        {
                            "start": 1702,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 1953
                        },
                        {
                            "start": 1956,
                            "end": 1997
                        },
                        {
                            "start": 1998,
                            "end": 2121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 102,
                            "end": 122,
                            "matchedPaperCorpusId": "259129398"
                        },
                        {
                            "start": 146,
                            "end": 164,
                            "matchedPaperCorpusId": "252992904"
                        },
                        {
                            "start": 830,
                            "end": 851,
                            "matchedPaperCorpusId": "261822638"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72900390625
                },
                {
                    "corpus_id": "273532388",
                    "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
                    "text": "This approach is referred to as a meta-evaluation benchmark because it aims to evaluate the performance of the evaluators. \n\nSimultaneously, the performance of multilingual LLMs that process questions or instructions in languages other than English is developing rapidly, thereby increasing the importance of thorough evaluation of multilingual outputs (Thellmann et al., 2024;Yue et al., 2024). However, in practice, evaluations of multilingual LLMs often rely on evaluator LLMs whose multilingual evaluation capabilities have not been thoroughly assessed (Lai et al., 2023;Liu et al., 2024;Aryabumi et al., 2024;Dang et al., 2024). Fundamentally, the absence of multilingual meta-evaluation benchmarks is a key reason why evaluator LLMs remain under-tested in multilingual settings. \n\nTo address this issue, we release MM-EVAL, a Multilingual Meta-Evaluation benchmark spanning a wide range of languages, including low-resource ones such as Swahili, Basque, and Galician. MM-EVAL consists of six subsets: five core subsets-Chat, Reasoning, Safety, Language Hallucination, and Linguistics-cover 18 languages, while the Language Consistency subset covers 122 languages for a broader analysis on the consistency of evaluator LLMs across low-to high-resource languages. \n\nA core attribute of MM-EVAL is its multilingual-first design philosophy. Rather than simply translating existing English meta-evaluation benchmarks into other languages, MM-EVAL is constructed to address challenges that are unique to multilingual settings. These challenges include: Linguistics, Language Hallucination, and Language Consistency. In addition, while prior meta-evaluation benchmarks primarily assess evaluators based on ranking accuracy over pairwise comparisons, MM-EVAL goes a step further by evaluating the consistency and fairness of absolute score values assigned by evaluator LLMs. This is achieved through the Language Consistency subset and a novel metric, the Language Consistency Index (LCI), which quantifies how uniformly evaluators score parallel inputs across languages. \n\nUsing MM-EVAL, we evaluate the performance of 12 evaluator LLMs, including open reward models, open judges models, and proprietary models.",
                    "score": 0.5895801042355314,
                    "section_title": "Introduction",
                    "char_start_offset": 1916,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 125,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 784
                        },
                        {
                            "start": 787,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1267
                        },
                        {
                            "start": 1270,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 2069
                        },
                        {
                            "start": 2072,
                            "end": 2210
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 377,
                            "end": 394,
                            "matchedPaperCorpusId": "273507992"
                        },
                        {
                            "start": 557,
                            "end": 575,
                            "matchedPaperCorpusId": "260334562"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.72900390625,
            "relevance_judgment_input_expanded": "# Title: MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models\n# Venue: arXiv.org\n# Authors: Guijin Son, Dongkeun Yoon, Juyoung Suk, Javier Aula-Blasco, Mano Aslan, Vu Trong Kim, Shayekh Bin Islam, Jaume Prats-Cristi\u00e0, Luc\u00eda Tormo-Ba\u00f1uelos, Seungone Kim\n## Abstract\nAs Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\"meta-evaluation benchmarks\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.\n## Introduction\nThis approach is referred to as a meta-evaluation benchmark because it aims to evaluate the performance of the evaluators. \n\nSimultaneously, the performance of multilingual LLMs that process questions or instructions in languages other than English is developing rapidly, thereby increasing the importance of thorough evaluation of multilingual outputs (Thellmann et al., 2024;Yue et al., 2024). However, in practice, evaluations of multilingual LLMs often rely on evaluator LLMs whose multilingual evaluation capabilities have not been thoroughly assessed (Lai et al., 2023;Liu et al., 2024;Aryabumi et al., 2024;Dang et al., 2024). Fundamentally, the absence of multilingual meta-evaluation benchmarks is a key reason why evaluator LLMs remain under-tested in multilingual settings. \n\nTo address this issue, we release MM-EVAL, a Multilingual Meta-Evaluation benchmark spanning a wide range of languages, including low-resource ones such as Swahili, Basque, and Galician. MM-EVAL consists of six subsets: five core subsets-Chat, Reasoning, Safety, Language Hallucination, and Linguistics-cover 18 languages, while the Language Consistency subset covers 122 languages for a broader analysis on the consistency of evaluator LLMs across low-to high-resource languages. \n\nA core attribute of MM-EVAL is its multilingual-first design philosophy. Rather than simply translating existing English meta-evaluation benchmarks into other languages, MM-EVAL is constructed to address challenges that are unique to multilingual settings. These challenges include: Linguistics, Language Hallucination, and Language Consistency. In addition, while prior meta-evaluation benchmarks primarily assess evaluators based on ranking accuracy over pairwise comparisons, MM-EVAL goes a step further by evaluating the consistency and fairness of absolute score values assigned by evaluator LLMs. This is achieved through the Language Consistency subset and a novel metric, the Language Consistency Index (LCI), which quantifies how uniformly evaluators score parallel inputs across languages. \n\nUsing MM-EVAL, we evaluate the performance of 12 evaluator LLMs, including open reward models, open judges models, and proprietary models.\n\n## Preliminary\nIn aligning LLMs, evaluator LLMs play a central role-from training (Pace et al., 2024) and evaluation (Zheng et al., 2023) to test-time inference (Gao et al., 2023;Wu et al., 2024a;Son et al., 2025). Naturally, the critical role of evaluator LLMs has led to the emergence of meta-evaluation benchmarks, which aim to directly assess the quality of evaluators. These benchmarks typically present pairs of candidate responses with a predefined preference-based on either correctness or human judgments-and measure the evaluator LLM's accuracy by how often it assigns a higher score to the preferred (chosen) response over the unpreferred (rejected) response. \n\nWhile multilingual LLMs also depend on automated evaluators, we still lack clear guidelines on evaluating them across diverse languages. A concurrent effort, M-RewardBench (Gureja et al., 2024), attempts to fill this gap by machine translating the original RewardBench into multiple languages. However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments.",
            "reference_string": "[273532388 | Son et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269539",
                    "name": "Klaudia Thellmann"
                },
                {
                    "authorId": "2325726089",
                    "name": "Bernhard Stadler"
                },
                {
                    "authorId": "2258551638",
                    "name": "Michael Fromm"
                },
                {
                    "authorId": "2258550838",
                    "name": "Jasper Schulze Buschhoff"
                },
                {
                    "authorId": "2325726070",
                    "name": "Alex Jude"
                },
                {
                    "authorId": "2325726142",
                    "name": "Fabio Barth"
                },
                {
                    "authorId": "2258552277",
                    "name": "Johannes Leveling"
                },
                {
                    "authorId": "2347259072",
                    "name": "Nicolas Flores-Herr"
                },
                {
                    "authorId": "2330411614",
                    "name": "Joachim K\u00f6hler"
                },
                {
                    "authorId": "2330412312",
                    "name": "Ren\u00e9 J\u00e4kel"
                },
                {
                    "authorId": "2258668067",
                    "name": "Mehdi Ali"
                }
            ],
            "abstract": "The rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of language-parallel multilingual benchmarks. We introduce a multilingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.",
            "corpus_id": 273323209,
            "sentences": [
                {
                    "corpus_id": "273323209",
                    "title": "Towards Multilingual LLM Evaluation for European Languages",
                    "text": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.",
                    "score": 0.9013597151469939,
                    "section_title": "Related Work",
                    "char_start_offset": 2745,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 1008
                        },
                        {
                            "start": 1009,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1460
                        },
                        {
                            "start": 1463,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 1951
                        },
                        {
                            "start": 1954,
                            "end": 2079
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1266,
                            "end": 1285,
                            "matchedPaperCorpusId": "246680398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7265625
                },
                {
                    "corpus_id": "273323209",
                    "title": "Towards Multilingual LLM Evaluation for European Languages",
                    "text": "We chose DeepL as translation service for its balance between translation accuracy and scalability. We translated five well-known datasets, ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), GSM8K (Cobbe et al., 2021), and MMLU (Hendrycks et al., 2020) from English into 20 European languages. These datasets encompass a mix of multiple-choice and open-ended generation tasks, each of which presented unique translation challenges. The translations maintained the original structure of each task to ensure the benchmarks were as consistent as possible across languages. Using DeepL's XML-tag handling, we avoided issues with prompt formatting and ensured that key contextual elements, such as 'Question' and 'Response' prompts, were properly localized (cf. Table 5). \n\nTo enhance the visibility and accessibility of our benchmarks, we launched a multilingual leaderboard 5 on Hugging Face, inspired by the popular Open LLM leaderboard6 .",
                    "score": 0.589544174509266,
                    "section_title": "Translation Process",
                    "char_start_offset": 6113,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 806
                        },
                        {
                            "start": 809,
                            "end": 977
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 211,
                            "end": 228,
                            "matchedPaperCorpusId": "237532606"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7138671875
                }
            ],
            "relevance_judgement": 0.7265625,
            "relevance_judgment_input_expanded": "# Title: Towards Multilingual LLM Evaluation for European Languages\n# Venue: arXiv.org\n# Authors: Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim K\u00f6hler, Ren\u00e9 J\u00e4kel, Mehdi Ali\n## Abstract\nThe rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of language-parallel multilingual benchmarks. We introduce a multilingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.\n## Related Work\nMultilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.\n\n## Translation Process\nWe chose DeepL as translation service for its balance between translation accuracy and scalability. We translated five well-known datasets, ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), GSM8K (Cobbe et al., 2021), and MMLU (Hendrycks et al., 2020) from English into 20 European languages. These datasets encompass a mix of multiple-choice and open-ended generation tasks, each of which presented unique translation challenges. The translations maintained the original structure of each task to ensure the benchmarks were as consistent as possible across languages. Using DeepL's XML-tag handling, we avoided issues with prompt formatting and ensured that key contextual elements, such as 'Question' and 'Response' prompts, were properly localized (cf. Table 5). \n\nTo enhance the visibility and accessibility of our benchmarks, we launched a multilingual leaderboard 5 on Hugging Face, inspired by the popular Open LLM leaderboard6 .",
            "reference_string": "[273323209 | Thellmann et al. | 2024 | Citations: 10]"
        },
        {
            "title": "SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21501, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "72437140",
                    "name": "Isidora Chara Tourni"
                },
                {
                    "authorId": "2328268737",
                    "name": "Sayontan Ghosh"
                },
                {
                    "authorId": "2328005500",
                    "name": "Brenda Miao"
                },
                {
                    "authorId": "2292259005",
                    "name": "Constantijn van der Poel"
                }
            ],
            "abstract": "This paper explores the problems of Question Answering (QA) and Named Entity Recognition (NER) in five diverse languages. We tested five Large Language Models with various prompting methods, including zero-shot, chain-of-thought reasoning, and translation techniques. Our results show that while some models consistently outperform others, their effectiveness varies significantly across tasks and languages. We saw that advanced prompting techniques generally improved QA performance but had mixed results for NER; and we observed that language difficulty patterns differed between tasks. Our findings highlight the need for task-specific approaches in multilingual NLP and suggest that current models may develop different linguistic competencies for different tasks.",
            "corpus_id": 273662078,
            "sentences": [
                {
                    "corpus_id": "273662078",
                    "title": "SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval",
                    "text": "Multilingual and Cross-lingual QA have emerged as critical areas of NLP research, driven by the advance of LLMs and the creation of diverse multilingual QA datasets. Benchmarks such as TYDI QA (Clark et al., 2020), MLQA (Lewis et al., 2019), XQuAD (Artetxe et al., 2019), andMKQA (Longpre et al., 2021) have been instrumental in enabling evaluation across a wide range of languages, representing different language families and typologies. Most of these multilingual datasets, however, are still limited to evaluation on benchmarking datasets and not yet tested in real-world settings. \n\nVarious works have focused on generating synthetic multilingual QA data for training downstream QA models (Riabi et al., 2020;Shakeri et al., 2020). Notable contributions include PAXQA (Li and Callison-Burch, 2023), generating crosslingual QA datasets by leveraging parallel corpora and without requiring annotated non-English QA data. More recently, Engl\u00e4nder et al. (2024) introduced M2QA, a multi-domain multilingual QA benchmark of Indian languages. \n\nFor QA, zero-shot transfer learning has been studied extensively; models are typically fine-tuned on English data and then applied directly to other languages, leveraging cross-lingual capabilities of models like mBERT (Devlin, 2018) and XLM-R (Conneau, 2019). CORA (Asai et al., 2020) introduces a unified model for cross-lingual retrieval and multilingual QA, using NMT for data augmentation, either by translating English QA pairs into target languages or by translating non-English queries to English at inference time. Duan et al. (2021) assemble triples from different languages and use a knowledge injection strategy, via link prediction, to enrich a model's multilinguality. \n\nLatest significant contributions include QAMELEON (Agrawal et al., 2023), which uses prompt-tuning to generate synthetic QA data with as few as 5 examples per language.",
                    "score": 0.7074796304548416,
                    "section_title": "Multilingual QA",
                    "char_start_offset": 6642,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 585
                        },
                        {
                            "start": 588,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1041
                        },
                        {
                            "start": 1044,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1726
                        },
                        {
                            "start": 1729,
                            "end": 1897
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 213,
                            "matchedPaperCorpusId": "212657414"
                        },
                        {
                            "start": 694,
                            "end": 714,
                            "matchedPaperCorpusId": "225067135"
                        },
                        {
                            "start": 1568,
                            "end": 1586,
                            "matchedPaperCorpusId": "245935213"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7158203125
                }
            ],
            "relevance_judgement": 0.7158203125,
            "relevance_judgment_input_expanded": "# Title: SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval\n# Venue: arXiv.org\n# Authors: Isidora Chara Tourni, Sayontan Ghosh, Brenda Miao, Constantijn van der Poel\n## Abstract\nThis paper explores the problems of Question Answering (QA) and Named Entity Recognition (NER) in five diverse languages. We tested five Large Language Models with various prompting methods, including zero-shot, chain-of-thought reasoning, and translation techniques. Our results show that while some models consistently outperform others, their effectiveness varies significantly across tasks and languages. We saw that advanced prompting techniques generally improved QA performance but had mixed results for NER; and we observed that language difficulty patterns differed between tasks. Our findings highlight the need for task-specific approaches in multilingual NLP and suggest that current models may develop different linguistic competencies for different tasks.\n## Multilingual QA\nMultilingual and Cross-lingual QA have emerged as critical areas of NLP research, driven by the advance of LLMs and the creation of diverse multilingual QA datasets. Benchmarks such as TYDI QA (Clark et al., 2020), MLQA (Lewis et al., 2019), XQuAD (Artetxe et al., 2019), andMKQA (Longpre et al., 2021) have been instrumental in enabling evaluation across a wide range of languages, representing different language families and typologies. Most of these multilingual datasets, however, are still limited to evaluation on benchmarking datasets and not yet tested in real-world settings. \n\nVarious works have focused on generating synthetic multilingual QA data for training downstream QA models (Riabi et al., 2020;Shakeri et al., 2020). Notable contributions include PAXQA (Li and Callison-Burch, 2023), generating crosslingual QA datasets by leveraging parallel corpora and without requiring annotated non-English QA data. More recently, Engl\u00e4nder et al. (2024) introduced M2QA, a multi-domain multilingual QA benchmark of Indian languages. \n\nFor QA, zero-shot transfer learning has been studied extensively; models are typically fine-tuned on English data and then applied directly to other languages, leveraging cross-lingual capabilities of models like mBERT (Devlin, 2018) and XLM-R (Conneau, 2019). CORA (Asai et al., 2020) introduces a unified model for cross-lingual retrieval and multilingual QA, using NMT for data augmentation, either by translating English QA pairs into target languages or by translating non-English queries to English at inference time. Duan et al. (2021) assemble triples from different languages and use a knowledge injection strategy, via link prediction, to enrich a model's multilinguality. \n\nLatest significant contributions include QAMELEON (Agrawal et al., 2023), which uses prompt-tuning to generate synthetic QA data with as few as 5 examples per language.",
            "reference_string": "[273662078 | Tourni et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 51,
            "citation_count": 53,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.562.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.12643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003628072",
                    "name": "Arij Riabi"
                },
                {
                    "authorId": "90745780",
                    "name": "Thomas Scialom"
                },
                {
                    "authorId": "1725411037",
                    "name": "Rachel Keraron"
                },
                {
                    "authorId": "68990982",
                    "name": "Beno\u00eet Sagot"
                },
                {
                    "authorId": "1679170",
                    "name": "Djam\u00e9 Seddah"
                },
                {
                    "authorId": "1767493",
                    "name": "Jacopo Staiano"
                }
            ],
            "abstract": "Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).",
            "corpus_id": 225067135,
            "sentences": [
                {
                    "corpus_id": "225067135",
                    "title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering",
                    "text": "XQuAD (Artetxe et al., 2020) is a human translation of the SQuAD en development set in 10 languages (Arabic, Chinese, German, Greek, Hindi, Russian, Spanish, Thai, Turkish, and Vietnamese), with 1k QA pairs for each language.\n\nMLQA (Lewis et al., 2020) is an evaluation dataset in 7 languages (English, Arabic, Chinese, German, Hindi, and Spanish). The dataset is built from aligned Wikipedia sentences across at least two languages (full alignment between all languages being impossible), with the goal of providing natural rather than translated paragraphs. The QA pairs are manually annotated on the English sentences and then human translated on the aligned sentences. The dataset contains about 46K aligned QA pairs in total.\n\nLanguage-specific benchmarks In addition to the two aforementioned multilingual evaluation data, we benchmark our models on three languagespecific datasets for French, Italian and Korean, as detailed below. We choose these datasets since none of these languages are present in XQuAD or MLQA. Hence, they allow us to evaluate our models in a scenario where the target language is not available during training, even for the synthetic questions.\n\nPIAF Keraron et al. (2020) provided an evaluation set in French following the SQuAD protocol, and containing 3835 examples.\n\nKorQuAD 1.0 the Korean Question Answering Dataset (Lim et al., 2019), a Korean dataset also built following the SQuAD protocol.\n\nSQuAD-it Derived from SQuAD en , it was obtained via semi-automatic translation to Italian (Croce et al., 2018).",
                    "score": 0.5858967911101147,
                    "section_title": "Multilingual Evaluation Sets",
                    "char_start_offset": 7876,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 6,
                            "end": 27,
                            "matchedPaperCorpusId": "204901567"
                        },
                        {
                            "start": 232,
                            "end": 252,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 1182,
                            "end": 1203,
                            "matchedPaperCorpusId": "218974030"
                        },
                        {
                            "start": 1522,
                            "end": 1542,
                            "matchedPaperCorpusId": "53238211"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7080078125
                }
            ],
            "relevance_judgement": 0.7080078125,
            "relevance_judgment_input_expanded": "# Title: Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Arij Riabi, Thomas Scialom, Rachel Keraron, Beno\u00eet Sagot, Djam\u00e9 Seddah, Jacopo Staiano\n## Abstract\nCoupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).\n## Multilingual Evaluation Sets\nXQuAD (Artetxe et al., 2020) is a human translation of the SQuAD en development set in 10 languages (Arabic, Chinese, German, Greek, Hindi, Russian, Spanish, Thai, Turkish, and Vietnamese), with 1k QA pairs for each language.\n\nMLQA (Lewis et al., 2020) is an evaluation dataset in 7 languages (English, Arabic, Chinese, German, Hindi, and Spanish). The dataset is built from aligned Wikipedia sentences across at least two languages (full alignment between all languages being impossible), with the goal of providing natural rather than translated paragraphs. The QA pairs are manually annotated on the English sentences and then human translated on the aligned sentences. The dataset contains about 46K aligned QA pairs in total.\n\nLanguage-specific benchmarks In addition to the two aforementioned multilingual evaluation data, we benchmark our models on three languagespecific datasets for French, Italian and Korean, as detailed below. We choose these datasets since none of these languages are present in XQuAD or MLQA. Hence, they allow us to evaluate our models in a scenario where the target language is not available during training, even for the synthetic questions.\n\nPIAF Keraron et al. (2020) provided an evaluation set in French following the SQuAD protocol, and containing 3835 examples.\n\nKorQuAD 1.0 the Korean Question Answering Dataset (Lim et al., 2019), a Korean dataset also built following the SQuAD protocol.\n\nSQuAD-it Derived from SQuAD en , it was obtained via semi-automatic translation to Italian (Croce et al., 2018).",
            "reference_string": "[225067135 | Riabi et al. | 2020 | Citations: 53]"
        },
        {
            "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 28,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2119151340",
                    "name": "Harman Singh"
                },
                {
                    "authorId": "2285178",
                    "name": "Nitish Gupta"
                },
                {
                    "authorId": "2136381352",
                    "name": "Shikhar Bharadwaj"
                },
                {
                    "authorId": "2298270541",
                    "name": "Dinesh Tewari"
                },
                {
                    "authorId": "2298271272",
                    "name": "Partha Talukdar"
                }
            ],
            "abstract": "As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench",
            "corpus_id": 269362363,
            "sentences": [
                {
                    "corpus_id": "269362363",
                    "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
                    "text": "\u2022 A qualitative analysis for assessing the gaps in current language technologies and define potential directions of future research. \n\nAs evident from the lists above, our benchmark provides a broad-coverage over languages with respect to their resourcedness, allowing users to evaluate language models on relatively highresource languages such as Hindi and extremely low-resource languages such as Manipuri in Meitei script on a single benchmark. \n\nTo curate the evaluation datasets for our benchmark, we use the following existing datasets as the source: CrossSum (Bhattacharjee et al., 2023) for cross-lingual summarization, FLO-RES (NLLB- Team et al., 2022) for machine translation, XQuAD (Artetxe et al., 2020) for multilingual QA, and XoRQA (Asai et al., 2021) for cross-lingual QA. From each of these datasets we select a subset of English examples to be a part of our benchmark, and then collect professional human translations for these examples in all target Indic languages. Some target languages are already covered by the source datasets in which case we re-purpose this existing data and only collect translations for the remaining languages. We also collect and release a small amount of training and validation data making possible evaluation of training techniques like fine-tuning, parameter-efficient training, in-context learning, and others. Why extend existing benchmarks? We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization  Sum (Bhattacharjee et al., 2023), a dataset for crosslingual summarization, which in turn is derived from XL-Sum (Hasan et al., 2021b). CrossSum contains multi-way parallel data in 45 languages where BBC news articles as source in a language are paired with corresponding summaries in other languages. Based on their matching criteria, different languages have different amount of sourcetarget pairs.",
                    "score": 0.6313979917233252,
                    "section_title": "Introduction",
                    "char_start_offset": 3587,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 135,
                            "end": 447
                        },
                        {
                            "start": 450,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1532
                        },
                        {
                            "start": 1535,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2062
                        },
                        {
                            "start": 2063,
                            "end": 2161
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7060546875
                }
            ],
            "relevance_judgement": 0.7060546875,
            "relevance_judgment_input_expanded": "# Title: IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Harman Singh, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, Partha Talukdar\n## Abstract\nAs large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench\n## Introduction\n\u2022 A qualitative analysis for assessing the gaps in current language technologies and define potential directions of future research. \n\nAs evident from the lists above, our benchmark provides a broad-coverage over languages with respect to their resourcedness, allowing users to evaluate language models on relatively highresource languages such as Hindi and extremely low-resource languages such as Manipuri in Meitei script on a single benchmark. \n\nTo curate the evaluation datasets for our benchmark, we use the following existing datasets as the source: CrossSum (Bhattacharjee et al., 2023) for cross-lingual summarization, FLO-RES (NLLB- Team et al., 2022) for machine translation, XQuAD (Artetxe et al., 2020) for multilingual QA, and XoRQA (Asai et al., 2021) for cross-lingual QA. From each of these datasets we select a subset of English examples to be a part of our benchmark, and then collect professional human translations for these examples in all target Indic languages. Some target languages are already covered by the source datasets in which case we re-purpose this existing data and only collect translations for the remaining languages. We also collect and release a small amount of training and validation data making possible evaluation of training techniques like fine-tuning, parameter-efficient training, in-context learning, and others. Why extend existing benchmarks? We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization  Sum (Bhattacharjee et al., 2023), a dataset for crosslingual summarization, which in turn is derived from XL-Sum (Hasan et al., 2021b). CrossSum contains multi-way parallel data in 45 languages where BBC news articles as source in a language are paired with corresponding summaries in other languages. Based on their matching criteria, different languages have different amount of sourcetarget pairs.",
            "reference_string": "[269362363 | Singh et al. | 2024 | Citations: 28]"
        },
        {
            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 79,
            "citation_count": 62,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14857",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "35871436",
                    "name": "Sneha Kudugunta"
                },
                {
                    "authorId": "2118211280",
                    "name": "Xinyan Velocity Yu"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "1557386977",
                    "name": "Machel Reid"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ],
            "abstract": "Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.",
            "corpus_id": 258865558,
            "sentences": [
                {
                    "corpus_id": "258865558",
                    "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
                    "text": "We create the BUFFET benchmark to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. We adhere to the following design principles with our benchmark. \n\nStandardized few-shot samples. BUFFET provides three different training and validation sets of k-shots (e.g., k = 32) per task for a nonclassification task, or per class for a classification task, for each language. \n\nTask diversity. Existing cross-lingual benchmarks often focus on classification or retrieval (Hu et al., 2020b;Ruder et al., 2021;Liang et al., 2020). BUFFET encompasses a broad range of task types, such as classification, generation, extraction, and structured prediction tasks. By converting all tasks into the same text-to-text format, we eliminate the need for task-specific model modifications or template conversions. 5. \n\nBeyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;Ponti et al. 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;Artetxe et al., 2020b). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
                    "score": 0.6368518842787947,
                    "section_title": "Design Principles",
                    "char_start_offset": 9586,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 235
                        },
                        {
                            "start": 238,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 453
                        },
                        {
                            "start": 456,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 882
                        },
                        {
                            "start": 885,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1368
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 549,
                            "end": 567,
                            "matchedPaperCorpusId": "214641214"
                        },
                        {
                            "start": 567,
                            "end": 586,
                            "matchedPaperCorpusId": "233241004"
                        },
                        {
                            "start": 586,
                            "end": 605,
                            "matchedPaperCorpusId": "214794966"
                        },
                        {
                            "start": 1068,
                            "end": 1086,
                            "matchedPaperCorpusId": "218470125"
                        },
                        {
                            "start": 1218,
                            "end": 1240,
                            "matchedPaperCorpusId": "216914383"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70166015625
                }
            ],
            "relevance_judgement": 0.70166015625,
            "relevance_judgment_input_expanded": "# Title: BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, Hannaneh Hajishirzi\n## Abstract\nDespite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.\n## Design Principles\nWe create the BUFFET benchmark to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. We adhere to the following design principles with our benchmark. \n\nStandardized few-shot samples. BUFFET provides three different training and validation sets of k-shots (e.g., k = 32) per task for a nonclassification task, or per class for a classification task, for each language. \n\nTask diversity. Existing cross-lingual benchmarks often focus on classification or retrieval (Hu et al., 2020b;Ruder et al., 2021;Liang et al., 2020). BUFFET encompasses a broad range of task types, such as classification, generation, extraction, and structured prediction tasks. By converting all tasks into the same text-to-text format, we eliminate the need for task-specific model modifications or template conversions. 5. \n\nBeyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;Ponti et al. 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;Artetxe et al., 2020b). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
            "reference_string": "[258865558 | Asai et al. | 2023 | Citations: 62]"
        },
        {
            "title": "Practical Transformer-based Multilingual Text Classification",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 39,
            "citation_count": 28,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-industry.16.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.naacl-industry.16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108725400",
                    "name": "Cindy Wang"
                },
                {
                    "authorId": "2339397",
                    "name": "Michele Banko"
                }
            ],
            "abstract": "Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.",
            "corpus_id": 235097287,
            "sentences": [
                {
                    "corpus_id": "235097287",
                    "title": "Practical Transformer-based Multilingual Text Classification",
                    "text": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation3 as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples. In the multi-all setting, we translate data from every language into every other language, yielding N 3 Data \n\nWe choose sentiment analysis and hate speech detection as evaluation tasks due to their relevance to industry applications and the availability of multilingual datasets. An overview of the datasets is shown in Table 2.",
                    "score": 0.6064286051589776,
                    "section_title": "Data Augmentation",
                    "char_start_offset": 6411,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 581
                        },
                        {
                            "start": 584,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 802
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70166015625
                }
            ],
            "relevance_judgement": 0.70166015625,
            "relevance_judgment_input_expanded": "# Title: Practical Transformer-based Multilingual Text Classification\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Cindy Wang, Michele Banko\n## Abstract\nTransformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.\n## Data Augmentation\nIn real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation3 as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples. In the multi-all setting, we translate data from every language into every other language, yielding N 3 Data \n\nWe choose sentiment analysis and hate speech detection as evaluation tasks due to their relevance to industry applications and the availability of multilingual datasets. An overview of the datasets is shown in Table 2.",
            "reference_string": "[235097287 | Wang et al. | 2021 | Citations: 28]"
        },
        {
            "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4",
            "venue": "DSTC",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2306.12794",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.12794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1672552269",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": null,
                    "name": "Ke Shi"
                },
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ],
            "abstract": "The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.",
            "corpus_id": 259224700,
            "sentences": [
                {
                    "corpus_id": "259224700",
                    "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4",
                    "text": "In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). \n\nDue to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and costintensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. \n\nFurthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it arXiv:2306.12794v3 [cs.CL] 14 Sep 2023 helps the transformation of state-of-the-art Englishonly dialogue systems into highly capable multilingual systems. Although multilingual pre-trained language models may exist and can be potentially used for training multilingual dialogue systems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some lowresource languages. To address this problem, we take advantage of recent advances in neural machine translation and paraphrasing systems. Using existing high-quality services and models, it is possible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows:",
                    "score": 0.585975871828136,
                    "section_title": "Introduction",
                    "char_start_offset": 1746,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 126,
                            "end": 399
                        },
                        {
                            "start": 402,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1532
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 104,
                            "end": 122,
                            "matchedPaperCorpusId": "235358130"
                        },
                        {
                            "start": 181,
                            "end": 208,
                            "matchedPaperCorpusId": "219980605"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69921875
                }
            ],
            "relevance_judgement": 0.69921875,
            "relevance_judgment_input_expanded": "# Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\n# Venue: DSTC\n# Authors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, Jo\u00e3o Sedoc, L. F. D\u2019Haro, Alexander I. Rudnicky\n## Abstract\nThe advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.\n## Introduction\nIn addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). \n\nDue to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and costintensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. \n\nFurthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it arXiv:2306.12794v3 [cs.CL] 14 Sep 2023 helps the transformation of state-of-the-art Englishonly dialogue systems into highly capable multilingual systems. Although multilingual pre-trained language models may exist and can be potentially used for training multilingual dialogue systems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some lowresource languages. To address this problem, we take advantage of recent advances in neural machine translation and paraphrasing systems. Using existing high-quality services and models, it is possible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows:",
            "reference_string": "[259224700 | Rodr'iguez-Cantelar et al. | 2023 | Citations: 10]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273323209",
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "text": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.",
            "score": 0.9013597151469939,
            "section_title": "Related Work",
            "char_start_offset": 2745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 1266,
                    "end": 1285,
                    "matchedPaperCorpusId": "246680398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "264405758",
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "text": "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], \n\nand ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text? \n\nFrom Table A2, it can be noted that our evaluation revealed ChatGPT to outperform other approaches in translations from English to both Chinese and Spanish, while Google Translate exhibits superior performance in English-to-Hindi translation. Thus, for optimal results in each non-English language, we harnessed the best-performing model to achieve the highest translation quality.",
            "score": 0.8085019644137421,
            "section_title": "A DETAILS OF DATASET CONSTRUCTION",
            "char_start_offset": 36719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "4623739"
                },
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "248887528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "215548041",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "text": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
            "score": 0.7901480277387508,
            "section_title": "Discussion",
            "char_start_offset": 24144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "254043519",
            "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources",
            "text": "Translating English data into the target languages are used in 33 datasets, but most datasets collected data in its original language. Yet, many recent and highly cited datasets for cross-lingual transfer evaluation (Artetxe et al., 2020b;Conneau et al., 2018) are created with translation-based approaches, which we discuss in detail below. Table 7 summarizes statistics on the motivation aspect, with a breakdown for the number of datasets for each motivation with and without translation. The most frequent driver for dataset creation was to cover multiple languages for a single task (62 datasets, covering 217 languages), often for downstream tasks with high economic demands, such as QA or summarization (Blasi et al., 2022). There were 16 languages (e.g., Chinese, Arabic) that had their own benchmark suites labeled as the monolingual general model category, which seemed to align with the availability of language-proficient NLP researchers. We discuss the relationship between dataset availability and the number of languageproficient researchers in Section 5.",
            "score": 0.7830390694243244,
            "section_title": "Input and Label Derivation via Translation.",
            "char_start_offset": 16100,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "276903455",
            "title": "X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation",
            "text": "Tab. 5 includes three multilingual evaluation datasets from Multilingual-General, as well as three additional translations in German, Portuguese, Spanish, and French. While the performance in English remains consistent with a one percentage point difference similar to before, the performance in other languages varies with some degree of decline. There are two main reasons for this difference: \n\nThe training corpus for QwenVL is primarily sourced from Chinese and English, with relatively fewer resources for other languages, making alignment with English datasets challenging. Errors introduced by the translation software, as all eight evaluation metrics are tested on English evaluation datasets, necessitating a translation back to English for non-English languages. In comparing with existing open-source models in the industry, such as PEA-Diffusion and Sana, the multilingual T2I models in this study demonstrate significant advantages. PEA-Diffusion is based on the SDXL model, while our comparison involves the official open-source \"Multilin-gualFLUX.1\". Sana is compared with the official opensource \"Sana 1600M 1024px MultiLing\", with most of the Chinese T2I generation results failing, hence the lack of presentation of objective results for Chinese.",
            "score": 0.7553474672052674,
            "section_title": "Multilingual T2I Generation",
            "char_start_offset": 35237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "257801559",
            "title": "Slovak Dataset for Multilingual Question Answering",
            "text": "The options for answering questions in non-English languages are limited. Such datasets usually include only one other language, very rarely a low-resource language. \n\nThere are several attempts to clone the original English SQuAD for other languages. Such datasets can be divided into two groups: created automatically using machine translation, or manually annotated based mostly on crowdsourcing. \n\nAutomatically translated datasets are easy to create but have the disadvantage of inaccuracy caused by machine translation. The second disadvantage is that they bring new information because they were created by translating the original articles. Manually created datasets do not have these shortcomings. However, human annotation is significantly more expensive and time-consuming. Therefore, such datasets are significantly smaller and only very rarely contain unanswerable questions. \n\nChandra et al. [19] provide a comprehensive survey on the recent progress of non-English MRC and open-domain QA datasets. The authors reviewed QA datasets in 14 main languages other than English, as well as several multilingual and cross-lingual QA datasets. \n\nA study by Rogers et al. [20] is the largest survey of current monolingual and multilingual QA/RC resources for languages other than English. This survey focuses exclusively on the typology of existing resources. It brings a systematic review of existing resources with respect to a set of criteria: questions versus statements or extractive, multiple choice, categorical, and freeform answers. The authors consider the conversational features of current resources, their domain coverage, and the available language. \n\nThe short overview of SQuAD-like monolingual, machinetranslated, and multilingual QA datasets for different languages other than English can also be found in the paper [12].",
            "score": 0.7442379830077226,
            "section_title": "C. NON-ENGLISH QUESTION ANSWERING DATASETS",
            "char_start_offset": 6508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 1839,
                    "end": 1843,
                    "matchedPaperCorpusId": "229478912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1661376953125
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "EuroLLM and Teuken-7B were evaluated using English (MMLU, Hellaswag, TruthfulQA) and a few multilingual benchmarks (Hendrycks et al., 2021;Zellers et al., 2019;Lin et al., 2022;Bandarkar et al., 2024). These evaluations were performed using machine-translated versions of English benchmarks regarding multilingual reasoning, QA, or common-sense inference (Thellmann et al., 2024). Salamandra, however, was evaluated on multilingual datasets that had been human-annotated or human-translated, but do not cover all European languages (Baucells et al., 2025). \n\nThe translated benchmarks are, like the original English versions, multiple-choice problems, where the model has to choose the correct answer or output from four possible answer options. The correct answer can be calculated by generating the correct output (Zellers et al., 2019) or measuring the log-likelihood of each possible answer (Wiland et al., 2024). The benefit of automatically translated benchmarks is that, besides cost-and timeefficiency in terms of dataset generation, the scores are comparable across languages. 1 his process has two pitfalls that complicate interpreting the benchmarks' scores. First, the benchmarks are not flawless translations of the English versions. Benchmarks that contain a significant amount of incorrect, incomplete or inadequate translations can distort model predictions -especially considering that a single incorrectly translated option in a multiple choice setting changes the whole problem space (Lin et al., 2022). \n\nTo overcome this issue human annotators or an improved and more comprehensive translation pipeline are needed. For instance, a translationverification process with a human in the loop could help enhance these benchmarks in terms of their translation quality. Another solution could be an iterative process of automatically ranking the translations and re-translating them until the total translation ranking has crossed a certain threshold. There are already models and metrics that can measure and score translation quality (Rei et al., 2020).",
            "score": 0.7408949664786004,
            "section_title": "Introduction",
            "char_start_offset": 1637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 177,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 177,
                    "end": 200,
                    "matchedPaperCorpusId": "261394577"
                },
                {
                    "start": 532,
                    "end": 555,
                    "matchedPaperCorpusId": "275821263"
                },
                {
                    "start": 1503,
                    "end": 1521,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493896484375
        },
        {
            "corpus_id": "273532366",
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "text": "The datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2. \n\n1) Datasets: Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets Fig. 3: Evaluation methods for MLLMs XTREME, XTREME-R, and XGLUE based on them. The goal of multilingual evaluation datasets is to cover more diverse tasks and languages so that we can better evaluate the generalization performance of MLLMs. At the same time, some researchers are also dedicated to building evaluation datasets for specific languages, such as IndicXTREME for Indian languages. \n\nIn general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation. \n\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks.",
            "score": 0.7367452113483335,
            "section_title": "B. Evaluation Tasks",
            "char_start_offset": 28760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 2242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "277993848",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "text": "The domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
            "score": 0.7362914146724128,
            "section_title": "Domain",
            "char_start_offset": 9088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 63,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 528,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 528,
                    "end": 546,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 690,
                    "end": 713,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 854,
                    "end": 872,
                    "matchedPaperCorpusId": "263829211"
                },
                {
                    "start": 889,
                    "end": 911,
                    "matchedPaperCorpusId": "271245129"
                },
                {
                    "start": 1177,
                    "end": 1198,
                    "matchedPaperCorpusId": "273850461"
                },
                {
                    "start": 1395,
                    "end": 1415,
                    "matchedPaperCorpusId": "270371898"
                },
                {
                    "start": 2088,
                    "end": 2104,
                    "matchedPaperCorpusId": "254043519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "273229002",
            "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
            "text": "Multilingual evaluation methods and the development of benchmarks not only facilitate the assessment of diverse language representations in LLMs but also help in monitoring cross-lingual generalization, to assess the effect of quantization across multiple languages (Marchisio et al., 2024), the development of language-specific models (Tejaswi et al., 2024), and the optimization of safety preferences (Li et al., 2024e), among others. In Table 4, we list benchmarks with the largest language coverage. This list includes benchmarks referenced by MEGA (Ahuja et al., 2023), MEGA-VERSE (Ahuja et al., 2024), xP3 (Muennighoff et al., 2023), the Aya collection (Singh et al., 2024), the lm-evaluation-harness framework (Gao et al., 2023;Biderman et al., 2024), and inter alia. These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation Lai et al. (2023b). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages. Despite the efforts reflected in Table 4, the community is still lacking highly multilingual benchmarks for tasks such as natural language understanding or text generation.",
            "score": 0.7331026145784374,
            "section_title": "A.2 MULTILINGUAL EVALUATION BENCHMARKS",
            "char_start_offset": 36079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1351
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 573,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 967,
                    "end": 985,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 1075,
                    "end": 1095,
                    "matchedPaperCorpusId": "235658519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "The above datasets primarily focus on English, but there exist datasets in Japanese (Omura et al., 2020;Takahashi et al., 2019;Hayashibe, 2020), Chinese (Xu et al., 2021(Xu et al., , 2020;;Wang et al., 2022), Russian (Shavrina et al., 2020;Taktasheva et al., 2022), and Indonesian (Koto et al., 2022).For multilingual datasets, most are extended versions of existing ones through translation, such as X-COPA (Ponti et al., 2020) from COPA, X-CSQA (Lin et al., 2021) from CSQA, and X-CODAH (Lin et al., 2021) from CODAH (Chen et al., 2019).A few datasets, such as TyDiQA (Clark et al., 2020), are created for each language from scratch.\n\nMultilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023).",
            "score": 0.7291832190540856,
            "section_title": "Introduction",
            "char_start_offset": 4705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 539
                },
                {
                    "start": 539,
                    "end": 635
                },
                {
                    "start": 637,
                    "end": 826
                },
                {
                    "start": 828,
                    "end": 1369
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 127,
                    "matchedPaperCorpusId": "208030290"
                },
                {
                    "start": 153,
                    "end": 169,
                    "matchedPaperCorpusId": "233033383"
                },
                {
                    "start": 169,
                    "end": 189,
                    "matchedPaperCorpusId": "215745536"
                },
                {
                    "start": 189,
                    "end": 207,
                    "matchedPaperCorpusId": "256461363"
                },
                {
                    "start": 217,
                    "end": 240,
                    "matchedPaperCorpusId": "226222281"
                },
                {
                    "start": 240,
                    "end": 264,
                    "matchedPaperCorpusId": "253098482"
                },
                {
                    "start": 281,
                    "end": 300,
                    "matchedPaperCorpusId": "247581412"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 447,
                    "end": 465,
                    "matchedPaperCorpusId": "235421949"
                },
                {
                    "start": 489,
                    "end": 507,
                    "matchedPaperCorpusId": "235421949"
                },
                {
                    "start": 519,
                    "end": 538,
                    "matchedPaperCorpusId": "104292422"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "273532388",
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "text": "As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\"meta-evaluation benchmarks\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.",
            "score": 0.7223566725449405,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "273950544",
            "title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding",
            "text": "Large language models (LLMs) evaluation is gaining increasing attention as these models are typically trained on general-domain datasets while demonstrating notable performance on tasks out of their training domains (Mosbach et al., 2023). The creation of evaluation datasets helps to identify the capabilities of LLMs, pinpoint shortcomings, and establish a measurable path for improvement. Based on Chang et al. (2024), LLM evaluation addresses questions such as what to evaluate (subjects and topics), where to evaluate (selecting appropriate datasets), and how to evaluate (the evaluation process). \n\nTo improve LLMs' capabilities and effectively assess their performance, researchers are creating benchmark datasets using a diverse range of domains and languages. This inclusive methodology allows for a more comprehensive evaluation of LLMs' performance across various domains and languages. Popular benchmark datasets like MMLU (Hendrycks et al., 2020) and MEGA-VERSE (Ahuja et al., 2023) cover a wide range of extensive world knowledge tasks and subjects. \n\nTo create evaluation benchmarks that are multilingual, researchers Koto et al. (2024); Li et al. (2023); Son et al. (2024) introduced benchmark datasets for different languages by translating a subset of the MMLU dataset. Beyond research efforts, translating existing benchmarks into different languages is an effective strategy to evaluate the multilingual capabilities of closed-source LLMs. These benchmarks evaluate multilingual understanding of models by presenting a range of extensive world knowledge tasks in the language of interest. While combining different subjects in a benchmark dataset may seem beneficial, it does not always provide a clear picture of the model's shortcomings. For example, using MMLU in different languages tests language and subject understanding simultaneously (Hendrycks et al., 2020). There should be evaluation benchmarks that disentangle language understanding and specific subject knowledge. \n\nLanguage understanding of LLM can be measured in numerous ways, and it is crucial to introduce benchmarks that evaluate complex text comprehension while considering each language's specific linguistic, cultural, and contextual nuances.",
            "score": 0.7219752042114613,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 935,
                    "end": 959,
                    "matchedPaperCorpusId": "267760288"
                },
                {
                    "start": 1133,
                    "end": 1151,
                    "matchedPaperCorpusId": "267760288"
                },
                {
                    "start": 1863,
                    "end": 1887,
                    "matchedPaperCorpusId": "267760288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "270258352",
            "title": "IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models",
            "text": "Furthermore, the few comprehensive evaluations that exist across languages often rely on machine translation of English benchmarks (Singh et al., 2024). While automatic translation from English benchmarks is a popular approach given the cost and time investment required for human translation, it often suffers from noise and biases (Vanmassenhove et al., 2021;Lee et al., 2022;Khiu et al., 2024;Hartung et al., 2023;Savoldi et al., 2021) or fail to reflect cultural context (Wang et al., 2022;Ji et al., 2023;Pudjiati et al., 2022). Automatic curation may also amplify any of the ubiquitous issues with the quality of broad pretraining sets (Luccioni and Viviano, 2021;Kreutzer et al., 2022;Ferrara, 2023). \n\nIn this paper, we seek to address both the diversity and breadth of evaluation coverage. We introduce IROKOBENCH, a human curated benchmark dataset for 17 typologically diverse African languages which encompasses three complex tasks: natural language inference (NLI), mathematical reasoning, and multi-choice knowledgebased QA. The datasets were created by human translating a subset of English cross-lingual NLI (XNLI) (Conneau et al., 2018), English Multilingual Grade School Math (MGSM) (Shi et al., 2023), and Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021a), evaluation datasets into each of the 16 languages using professional translators. \n\nWe conduct a large-scale evaluation of IROKOBENCH to assess zero-shot, few-shot, and translate-test settings (where test sets are translated into English) performance across 10 open and six proprietary LLMs. Our main contributions can be enumerated as follows: \n\n1. We introduce and release IROKOBENCH, a human-translated benchmark that includes 16 languages from various geographical regions in Africa, all with varying degrees of \"lowresourcedness\" (Joshi et al., 2020).",
            "score": 0.718957203532263,
            "section_title": "Introduction",
            "char_start_offset": 1562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 396,
                    "matchedPaperCorpusId": "267411831"
                },
                {
                    "start": 475,
                    "end": 494,
                    "matchedPaperCorpusId": "232307434"
                },
                {
                    "start": 670,
                    "end": 692,
                    "matchedPaperCorpusId": "232307434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "270380088",
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "text": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
            "score": 0.7185046265406501,
            "section_title": "Related Work",
            "char_start_offset": 20594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 1248,
                    "end": 1268,
                    "matchedPaperCorpusId": "258685666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "273662078",
            "title": "SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval",
            "text": "Multilingual and Cross-lingual QA have emerged as critical areas of NLP research, driven by the advance of LLMs and the creation of diverse multilingual QA datasets. Benchmarks such as TYDI QA (Clark et al., 2020), MLQA (Lewis et al., 2019), XQuAD (Artetxe et al., 2019), andMKQA (Longpre et al., 2021) have been instrumental in enabling evaluation across a wide range of languages, representing different language families and typologies. Most of these multilingual datasets, however, are still limited to evaluation on benchmarking datasets and not yet tested in real-world settings. \n\nVarious works have focused on generating synthetic multilingual QA data for training downstream QA models (Riabi et al., 2020;Shakeri et al., 2020). Notable contributions include PAXQA (Li and Callison-Burch, 2023), generating crosslingual QA datasets by leveraging parallel corpora and without requiring annotated non-English QA data. More recently, Engl\u00e4nder et al. (2024) introduced M2QA, a multi-domain multilingual QA benchmark of Indian languages. \n\nFor QA, zero-shot transfer learning has been studied extensively; models are typically fine-tuned on English data and then applied directly to other languages, leveraging cross-lingual capabilities of models like mBERT (Devlin, 2018) and XLM-R (Conneau, 2019). CORA (Asai et al., 2020) introduces a unified model for cross-lingual retrieval and multilingual QA, using NMT for data augmentation, either by translating English QA pairs into target languages or by translating non-English queries to English at inference time. Duan et al. (2021) assemble triples from different languages and use a knowledge injection strategy, via link prediction, to enrich a model's multilinguality. \n\nLatest significant contributions include QAMELEON (Agrawal et al., 2023), which uses prompt-tuning to generate synthetic QA data with as few as 5 examples per language.",
            "score": 0.7074796304548416,
            "section_title": "Multilingual QA",
            "char_start_offset": 6642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 213,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 694,
                    "end": 714,
                    "matchedPaperCorpusId": "225067135"
                },
                {
                    "start": 1568,
                    "end": 1586,
                    "matchedPaperCorpusId": "245935213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7158203125
        },
        {
            "corpus_id": "273323209",
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "text": "The development of Large Language Models (LLMs) has brought transformative advancements to natural language processing, enabling sophisticated tasks such as question-answering, summarization, and machine translation. However, despite these advances, the evaluation of LLMs across languages, especially those beyond English and Chinese, remains a significant challenge. Initiatives like CLEF1 , FIRE 2 , and WMT 3 have focused on multilingual evaluations, yet there is a lack of dedicated benchmarks for systematically comparing LLM performance across the languages of Europa. \n\nCreating bespoke benchmarks for each language is a costly and time-consuming endeavor. This not only limits the scalability of evaluations but also results in a fragmented understanding of model performance across different languages. In many cases, evaluations are conducted in isolation, without a basis for meaningful comparison between languages. Moreover, human evaluation is widely recognized as the gold standard for assessing language model performance, as it captures nuances that automated benchmarks may miss. However, human evaluations are resource-intensive and difficult to scale across multiple languages. \n\nTo address these challenges, we explore whether automatically translated benchmarks can provide a viable method to estimate human preferences in chat scenarios. While human evaluation would likely be the most accurate way to assess LLM performance, the resource demands of manual dataset creation and evaluation make it impractical for large-scale multilingual comparisons. Instead, this approach aims to reduce these resource demands while ensuring comprehensive evaluation coverage across all languages. \n\nIn this paper, we focus on evaluating multilingual LLMs by utilizing translated versions of existing benchmarks. Our goal is to determine the efficacy of translated benchmarks and assess whether they can substitute manually generated ones, allowing for a more scalable and uniform evaluation across multiple languages. \n\nWe make the following contributions: \n\n\u2022 We evaluate the performance of 40 state-ofthe-art LLMs for five multilingual datasets covering English and 20 additional languages in Section 4. \n\n\u2022 We assess whether translated benchmarks reflect human preferences to evaluate their viability in Section 5. \n\n\u2022 We examine how the choice of translation service (i.e., DeeplPro and ChatGPT) affects the arXiv:2410.08928v1 [cs.CL] 11 Oct 2024 performance of current LLMs in accurately responding to various benchmark tasks in Section 6.",
            "score": 0.7066863571799961,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2215
                },
                {
                    "start": 2218,
                    "end": 2327
                },
                {
                    "start": 2330,
                    "end": 2440
                },
                {
                    "start": 2441,
                    "end": 2554
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "266844358",
            "title": "Building Efficient and Effective OpenQA Systems for Low-Resource Languages",
            "text": "Various methods have been used to address the dataset bottleneck for QA in non-English languages [30]. One approach is to curate in-language datasets from scratch. A number of datasets for different languages have been created in this way. We provide a summary in Table 1. Datasets created in this manner are likely to be of high quality, but they are expensive, labor-intensive, and time-consuming to create. \n\nOne way to reduce the cost of creating in-language QA systems is to try to rely on the zero-shot transfer capabilities of cross-lingual models. In this approach, multilingual language models (e.g., mBERT [36]; XLM [37]; XLM-RoBERTa [38]) are finetuned on English QA datasets and then used to answer questions in target non-English languages. Multilingual models are efficient and cost effective, especially in large-scale applications requiring multiple language support. However, their performance on the target languages is relatively lower than models that use in-language embeddings [6,17,31,34,39]. \n\nAnother cost-effective approach is to rely on machine translation services. In this approach, in-language training datasets are automatically obtained by translating an existing English dataset using machine translation (MT). Previously, SQuAD1.0 [1] was translated into Arabic [16], French [17], and Spanish [40], and SQuAD2.0 [19] was translated into Persian [18]. Similar techniques have also been used in other areas of NLP [39,41]. For example, Senel et al. [42] recently introduced Karde\u015fNLU 3 using MT systems and Turkish resources in their process to obtain a cost-effective evaluation benchmark dataset for various Natural Language Understanding (NLU) tasks in other Turkic languages, which are often relatively less-resourced than Turkish in several NLP tasks. MT systems can also be effectively applied to extremely low-resource languages, including endangered Indigenous Languages [43][44][45][46][47]. \n\nUsing MT systems is undoubtedly productive, but relying on automatic translations for system assessment raises concerns about the validity of those assessments.",
            "score": 0.694025475598942,
            "section_title": "Multilingual Question Answering",
            "char_start_offset": 10658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "211126910"
                },
                {
                    "start": 1008,
                    "end": 1011,
                    "matchedPaperCorpusId": "233393696"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "224815907"
                },
                {
                    "start": 1265,
                    "end": 1268,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "189762189"
                },
                {
                    "start": 1309,
                    "end": 1313,
                    "matchedPaperCorpusId": "211126910"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "209202447"
                },
                {
                    "start": 1346,
                    "end": 1350,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "224815907"
                },
                {
                    "start": 1915,
                    "end": 1919,
                    "matchedPaperCorpusId": "259138596"
                },
                {
                    "start": 1919,
                    "end": 1923,
                    "matchedPaperCorpusId": "248811228"
                },
                {
                    "start": 1923,
                    "end": 1927,
                    "matchedPaperCorpusId": "258762157"
                },
                {
                    "start": 1927,
                    "end": 1931,
                    "matchedPaperCorpusId": "257985696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "264146033",
            "title": "Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance",
            "text": "We collected a new multilingual dataset (EC40) from OPUS, which holds potential implications for the field of multilingual machine translation. The EC40 dataset encompasses a diverse range of languages and language pairs, offering researchers and developers an expanded pool of data for training and evaluating translation models. It also serves as a benchmark for enabling fair comparisons and fostering advancements in multilingual translation research. Recognizing the inherent risks of mistranslation in machine translation data, we have made efforts to prioritize the incorporation of high-quality data, such as the MultiUN (Chen and Eisele, 2012) dataset (translated documents from the United Nations), to enhance the accuracy and reliability of the EC40 dataset. By sharing the EC40 dataset, we aim to contribute to the promotion of transparency and responsible use of machine translation data, facilitating collaboration and driving further progress in multilingual machine translation research.",
            "score": 0.690368910963713,
            "section_title": "Impact",
            "char_start_offset": 30417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1003
                }
            ],
            "ref_mentions": [
                {
                    "start": 629,
                    "end": 652,
                    "matchedPaperCorpusId": "10005952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.564453125
        },
        {
            "corpus_id": "215548041",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "text": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018;Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. \n\nClosely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (TRANSLATE-TEST), machine translating the training set into each target language and training the models on their respective languages (TRANSLATE-TRAIN), or using English data to finetune a multilingual model that is then transferred to the rest of languages (ZERO-SHOT). \n\nThe dataset creation and transfer procedures described above result in a mixture of original, 1 human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: TRANSLATE-TEST systems are trained on original data and evaluated on machine translated test sets, ZERO-SHOT systems are trained on original data and evaluated on human translated test sets, and TRANSLATE-TRAIN systems are trained on machine translated data and evaluated on human translated test sets. \n\nDespite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI).",
            "score": 0.6890037379174337,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 310,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 310,
                    "end": 331,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "256000095",
            "title": "JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its Applications",
            "text": "With the development of multilingual pre-trained language models, general language understanding frameworks for languages other than English have been created, and NLI datasets have been multilingualized. Conneau et al. [49] provided a cross-lingual NLI (XNLI) dataset by translating MNLI [12] into 15 languages, including languages with few language resources such as Urdu and Swahili. However, Japanese is not included in it. \n\nAs examples of non-English datasets, OCNLI [50] is a Chinese NLI dataset built from original Chinese multi-genre resources. Ham et al. [28] create KorNLI and KorSTS datasets by translating English NLI and STS-B [21] datasets into Korean. There are also attempts to translate the SICK dataset [24] into Dutch [51] and Portuguese [52]. \n\nRegarding Japanese, a Japanese SNLI (JSNLI) dataset [22] is constructed by employing machine translation to translate the English SNLI dataset into Japanese and automatically filtering out unnatural sentences. JSICK [23] dataset is manually translated from the English SICK dataset [24]. Yahoo Japan constructs a Japanese GLUE benchmark [53] from scratch without translation due to the cultural/social discrepancy between English and Japanese. Although employing machine translation incurs several possibilities to produce unnatural sentences, it is more cost-efficient than manual translation and reconstruction from scratch.",
            "score": 0.6857068988392183,
            "section_title": "Non-English data construction",
            "char_start_offset": 11328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1392
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "222291723"
                },
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "215238677"
                },
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "4421747"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "762228"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "233189530"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "52275377"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "762228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321533203125
        },
        {
            "corpus_id": "271050875",
            "title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
            "text": "In our work, we use machine translation to evaluate monolingual and multilingual models across multiple languages.Using machine translation might affect the quality and the expression of bias of the translated datasets.By evaluating the translation process with human evaluators as described in \u00a73.2, we aim to reduce these effects.Nevertheless, we are aware that the small number of annotators might decrease the significance of our results as in particular the evaluation of the bias in the translation is influenced by the perception of the annotator.In future work, we aim to extend this evaluation to all the studied languages and to more native annotators and methods that can ensure the quality of the automated translations.\n\nThe biases that exist in the benchmarks we used may be specific to English speaking regions.When translating the benchmark, bias may decrease because the biases that manifest in the translated language are specific to the regions that speak that language, which might not be the same as English speaking regions.Future work should consider creating new bias benchmarks for each language that represent the biases of the populations that speak those languages.Without this, we cannot be sure that the translated benchmarks cover the biases that are likely to occur in a given language.The significance of our results might be limited by CrowSpairs quality as shown in Blodgett et al. (2021).Blodgett et al. (2021) finds that 97% of the dataset are not admissible.Generating a french version of CrowS-pairs, also N\u00e9v\u00e9ol et al. (2022) scrutinizes and even improves the original CrowS-pairs dataset.They present the statistics of the different adaptation types (compare Table 2 in (N\u00e9v\u00e9ol et al., 2022).In addition to the sentences modified to suit the French culture, 150 samples in total (10% of the dataset) were adapted due to the identified limitations within the original CrowS-pairs dataset (non-minimal pairs (22), double switches (64) or bias-type mismatches ( 64)).Even if the findings of (Blodgett et al., 2021) show severe shortcomings, we decided on using CrowS-Pairs due to its broad usage in the literature and its coverage of many different bias categories and social groups.",
            "score": 0.6838799536119271,
            "section_title": "Limitations",
            "char_start_offset": 28902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 732
                },
                {
                    "start": 734,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1401,
                    "end": 1423,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1424,
                    "end": 1446,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1545,
                    "end": 1565,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 1711,
                    "end": 1732,
                    "matchedPaperCorpusId": "248780290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "270619905",
            "title": "Selected Languages are All You Need for Cross-lingual Truthfulness Transfer",
            "text": "To build a multilingual benchmark, we first select a few target languages from a diverse linguistic pool. Our evaluation system incorporates nine different languages: English (En), French (Fr), German (De), Spanish (Es), Chinese (Zh), Japanese (Ja), Russian (Ru), Thai (Th) and Arabic (Ar). For the questions and answers in original dataset, we translate them into the target languages through prompting GPT-4 2 . The prompt template we used is shown in Appendix A.1. \n\nConsidering the instability of LLM-based translation, we also recruited native speakers to validate the translations. Each translation is reviewed by three native speakers, who vote on its correctness. If at least two of the three agree, the translation is considered accurate. The average accuracy of the translation is 96.4% including both the questions and corresponding answers. We further manually checked and rectified these translation errors.",
            "score": 0.6835033178467427,
            "section_title": "Multilingualism Formulation",
            "char_start_offset": 6310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 920
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "268724238",
            "title": "Since the Scientific Literature Is Multilingual, Our Models Should Be Too",
            "text": "The use of English is implied and taken as the default language of research papers.This lack of evaluation datasets with non-English examples creates a blind spot in our understanding of how models perform on these papers, and causes a feedback loop: without an existing dataset, researchers will generally not be motivated to train models on other languages, for two reasons.First, there will be no straightforward way to know how well the model works for other languages, and second, for a fixed model size, introducing other languages dilutes the English representation of the model, which may lower its performance on existing benchmarks, even though the model performs better in a real-world setting.The introduction of OpenMSD represents a strong step in the right direction, although the creation and release of additional benchmarks is required for more comprehensive evaluation.\n\n4 Implicit Multilingual Evaluation",
            "score": 0.6828262248036414,
            "section_title": "How multilingual is the literature?",
            "char_start_offset": 5482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 887
                },
                {
                    "start": 889,
                    "end": 923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55908203125
        },
        {
            "corpus_id": "233481097",
            "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
            "text": "We consider three evaluation benchmarks. For cross-lingual understanding, we use cross-lingual natural language inference and question answering, and use the GLUE benchmark to evaluate the English performance.\n\nCross-lingual Natural Language Inference.\n\nThe XNLI dataset (Conneau et al., 2018) comes with ground-truth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency.\n\nCross-lingual Question Answering. We use the MLQA and XQuad benchmark from  and Artetxe et al. (2019), which extends the English SQuAD benchmark to more languages.\n\nWe report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English.\n\nThe English GLUE Benchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018).",
            "score": 0.6807465067679934,
            "section_title": "Evaluation",
            "char_start_offset": 4890,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1443,
                    "end": 1466,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1474,
                    "end": 1495,
                    "matchedPaperCorpusId": "990233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
            "score": 0.6769687993119665,
            "section_title": "Challenge 3: Cultural Bias",
            "char_start_offset": 13020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1304
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 149,
                    "matchedPaperCorpusId": "265445838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "263134532",
            "title": "Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation",
            "text": "We begin by examining models trained on English-centric multilingual translation datasets and proceed to evaluate these models directly on extensive zero-shot translations (e.g., non-English translations XX\u2194XX). We take the following three translation benchmarks into consideration. \n\n\u2022 IWSLT-4: Following Qu and Watanabe [54], we use the IWSLT-17 dataset to evaluate the performance of the models, and we remove four languages ( \"En, Ro, It, Nl\") from MMCR4NLP [55]. IWSLT-4 is a multialigned dataset with 145k training sentences for each language. \n\n\u2022 OPUS-100 (V1.0): We also conduct experiments on the OPUS-100 (v1.0) dataset from Yang et al. [53]. OPUS-100 [12] is an English-centric dataset that has 55 M samples with a maximum of 1 M sentence pairs for each language pair. It consists of parallel corpora between En and 100 other languages. Following Yang et al. [53], we construct OPUS-100 (v1.0) by removing 5 languages (\"An, Dz, Hy, Mn, Yo\") without training or testing data and removing all duplicate test sentence pairs from the training and testing sets. \n\n\u2022 WMT-5: To further evaluate the extremely unbalanced large-scale scenario, we adopt 4 popular WMT parallel training datasets, including WMT14 En\u2194De (4.5 M), WMT14 En\u2194Fr (35.7 M), WMT16 En\u2194Ro (608 k) and WMT17 En\u2194Zh (20.5 M). To prevent language cover bias during the evaluation, we use the multialigned Flores-200 devtest set 3 [56] for all translations. \n\nWe compare our model with two strong baselines, including vanilla and TLP&TGP:",
            "score": 0.6765043050596156,
            "section_title": "Multilingual Machine Translation Model",
            "char_start_offset": 28132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1506
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "252185372"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "237485480"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "272727920"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "237485480"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2303466796875
        },
        {
            "corpus_id": "267413041",
            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
            "text": "This work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
            "score": 0.668492984471956,
            "section_title": "Conclusions",
            "char_start_offset": 13897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "254043519",
            "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources",
            "text": "For Crowdsourcing. Our pilot study reveals both the difficulty of crowdsourcing for non-English languages and the high reliance on MT systems on English-centric platform. To conduct crowdsourcing on MTurk, one can either (1) adding language qualification newly introduced on MTurk for the 5 languages available, (2) introducing translation qualification and pruning workers based on their translation quality, and (3) translate original input into English and then crowdsource in English (Asai et al., 2021). We also recommend using language-specific crowdsourcing platforms, when available. * Alternative crowdsourcing platforms like Prolific or freelance platforms, such as Crowd-Flower * or Upwork, * can be explored, though they tend to be more expensive.\n\nFor Translating English Datasets. Another option to create a multilingual dataset is to translate datasets in high-resource languages into target languages (Conneau et al., 2018;Lewis et al., 2020). Fortunately, there are many crowdsourced translation services that offer semi-professional * https://statmt.org/wmt22/ large-scale-multilingual-translation-task.html * Toloka (https://toloka.yandex.com) is widely used by Russian language researchers. SelectStar (https:// selectstar.ai) and DeepNatural (https://deepnatural. ai) are South Korea-based crowdsourcing platforms. * https://visit.figure-eight.com/ People-Powered-Data-Enrichment_T * https://www.upwork.com translation at cheaper costs and better availabilities than translation services provided by trained professional translators. In our survey, Gengo * and One Hour Translation, * are the most highly used platforms for translation-based multilingual dataset creation. However, translation artifacts in these datasets remain unclear. Future studies can further quantify the quality of each translation method in existing translation-based datasets, the distribution of translation artifacts and mistakes, and the impact of such artifacts on final downstream task performances using our meta-annotations.\n\nOn Funding Sources. As discussed previously, multilingual dataset creation is often more expensive than English dataset",
            "score": 0.6638387060755899,
            "section_title": "Suggestions for Individual Researchers",
            "char_start_offset": 26772,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 939,
                    "end": 958,
                    "matchedPaperCorpusId": "204734128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "276558229",
            "title": "Chitrarth: Bridging Vision and Language for a Billion People",
            "text": "Figure 8 explores the impact of multilingual training data on the English academic benchmarks. We compare our model's performance when trained with only English, bilingual, and multilingual data across both stages. Consistent with the findings of [Scao et al., 2022], expanding the range of languages in the training data improves multilingual capabilities but leads to decreased performance on academic English datasets. This underscores a key challenge in balancing cross-lingual performance.",
            "score": 0.6630157265140592,
            "section_title": "Results",
            "char_start_offset": 19282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.434326171875
        },
        {
            "corpus_id": "273901276",
            "title": "Evaluating Open-Source LLMs in Low-Resource Languages: Insights from Latvian High School Exams",
            "text": "Recent advances on LLMs have led to impressive gains on NLU benchmarks, starting from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) with 10 tasks related to different NLU problems, followed by MMLU (Hendrycks et al., 2021) which covers nearly 60 subjects (including STEM: science, technology, engineering and mathematics), and Bigbench (Srivastava et al., 2023) with more than 200 tasks, as well as many other benchmarks. \n\nAgain, many well-known benchmarks are available only in English and other widely spoken languages. Google researchers addressed the need for a highly multilingual benchmark when the first transformer-based LLMs appeared by introducing the Cross-lingual Transfer Evaluation of Multilingual Encoders (XTREME) benchmark (Hu et al., 2020) which is used to evaluate cross-lingual generalization capabilities of multilingual representations. Although XTREME covers 40 typologically diverse languages, spanning 12 language families, Baltic languages are not included in this benchmark. Similarly, the dataset for the evaluation of multilingual LLMs developed by Okapi (Lai et al., 2023), in which the English part was translated with the help of ChatGPT, covers 26 languages except the Baltic languages (the \"smallest\" language is Danish with 6 million speakers, followed by Slovak with 7 million speakers). \n\nThe development of test sets for specific languages involves significant human resources. Therefore a widely used strategy is to apply machine translation, with or without manual postediting. Recently, this approach was chosen to translate the MMLU and COPA (Gordon et al., 2012) datasets into Latvian.1 Evaluation of OpenAI ChatGPT 3.5 Turbo and Google Gemini 1.0 Pro on the machine translated MMLU dataset shows that performance of these LLMs for Latvian is worse when compared to English (Bakanovs, 2024). It should be noted, that this dataset is not manually post-edited, and machine translation most likely has introduced some errors which can result in generating wrong answers.",
            "score": 0.6624511686245733,
            "section_title": "Related Work",
            "char_start_offset": 1763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 110,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 125,
                    "end": 144,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 211,
                    "end": 235,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 349,
                    "end": 374,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "260334562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433837890625
        },
        {
            "corpus_id": "207880568",
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "text": "We consider four evaluation benchmarks. For crosslingual understanding, we use cross-lingual natural language inference, named entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models. \n\nCross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other lan-guages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project. \n\nNamed Entity Recognition. For NER, we consider the CoNLL-2002(Sang, 2002) and CoNLL-2003 (Tjong Kim Sang andDe Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018). \n\nCross-lingual Question Answering. We use the MLQA benchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. GLUE Benchmark.",
            "score": 0.6622903727900309,
            "section_title": "Evaluation",
            "char_start_offset": 10139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 1655,
                    "end": 1675,
                    "matchedPaperCorpusId": "6042994"
                },
                {
                    "start": 1680,
                    "end": 1699,
                    "matchedPaperCorpusId": "52010710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Most language models are monolingual, using data in the English language only during pretraining. Such models, therefore, cannot be used to deal with tasks that are non-English-language-related. To overcome this issue, multilingual models were proposed to enable the processing of non-English languages. Such multilingual models can also be used for cross-lingual tasks like translation. However, models such as GPT-3 were potentially limited in dealing with cross-lingual tasks and generalization because most of these models had English-dominated training datasets. \n\nXGLM [53] focused on using a multilingual dataset (comprising a diverse set of languages) for finetuning. As a result, XGLM achieved cross-lingual solid transfer, demonstrating SOTA few-shot learning performance on the FLORES-101 machine translation benchmark between many language pairs. When BloomZ [81] was finetuned with xP3, a multilingual task dataset of 46 languages, the model achieved better zero-shot task generalization (than the P3-trained baseline) on English and non-English tasks. Furthermore, when xP3mt, a machinetranslated multilingual dataset of xP3, was used to finetune BloomZ on non-English prompts, the performance of held-out tasks with non-English human-written prompts significantly improved. In other words, as models generalize to tasks they had never intentionally seen, they learn the higher-level capabilities that are both task-and language-agnostic. \n\nTypically, a cross-lingual dataset is used to make the model language-agnostic, and, to make it task-agnostic, a multitask dataset is required. Also, for large multilingual models, zero-shot performance tends to be significantly lower than finetuned performance. So, to improve the multilingual model's zero-shot task generalization, BloomZ [81] focused on cross-lingual and multitask finetuning. This enabled the model to be usable for lowresource language tasks without further finetuning.",
            "score": 0.6602613821268356,
            "section_title": "Multilingual Finetuning",
            "char_start_offset": 61755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "245334784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468017578125
        },
        {
            "corpus_id": "270711451",
            "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
            "text": "Non-English QA Datasets Many multilingual QA and RC (reading comprehension) datasets [9][10][11]20] have been built by translating resources like Wikipedia, SQuAD [21,22] and Natural Question [23].TyDiQA [13] and XQA [24] are noteworthy exceptions, sourcing questions using human annotators and Wikipedia's \"Did you know?\" sections respectively.Monolingual QA datasets span a greater set of languages, including Arabic [25], Catalan [26], Czech [27], Korean [28], Persian [29], Slovak [30], Swahili [31], and Vietnamese [32].Recent survey papers [33,34] discuss multilingual datasets but do not include multilingual long-form QA datasets.Other types of multilingual datasets (e.g.instruction-tuning datasets [35]) can contain multilingual LFQA instances.\n\nCultural knowledge in LLMs Liu et al. [36], a recent survey of cultural NLP, taxonomizes cultural NLP works into 10 categories including values [37][38][39], norms and morals [40,41], and knowledge.Cultural knowledge has been explored through the creation of knowledge bases [42,43] as well as benchmarks and datasets for tasks like probing [44][45][46] and short-form QA.Cultural knowledge QA works include non-English information-seeking benchmarks in the style of MMLU [47][48][49][50][51][52] and common sense datasets [53][54][55] and evaluations [56].We are not aware of any existing LFQA datasets of cultural knowledge; surveys of cultural NLP [36,57] do not mention any such datasets.\n\nLong-Form QA Several English datasets have been created for LFQA [1,[5][6][7][8].The only non-English LFQA dataset we are aware of is the Chinese dataset WebCPM [4].",
            "score": 0.6512064609243755,
            "section_title": "Related Work",
            "char_start_offset": 4807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 754
                },
                {
                    "start": 756,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1448
                },
                {
                    "start": 1450,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1615
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 88,
                    "end": 92,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "196174566"
                },
                {
                    "start": 419,
                    "end": 423,
                    "matchedPaperCorpusId": "189762189"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "210077362"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "247358286"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "257801559"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "248524870"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "222066698"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "236447339"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "260334475"
                },
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "252907608"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "263829211"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "269626686"
                },
                {
                    "start": 1521,
                    "end": 1524,
                    "matchedPaperCorpusId": "259370859"
                },
                {
                    "start": 1524,
                    "end": 1527,
                    "matchedPaperCorpusId": "233189637"
                },
                {
                    "start": 1527,
                    "end": 1530,
                    "matchedPaperCorpusId": "21695200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164794921875
        },
        {
            "corpus_id": "249209473",
            "title": "Compositional Generalization in Multilingual Semantic Parsing over Wikidata",
            "text": "The field of KBQA has been saturated with work on English, due to both the inherent challenges of translating datasets and the reliance on English-only DBs. In this work, we presented a method for migrating the existing CFQ dataset to Wikidata and created a challenging multilingual dataset, MCWQ, targeting compositional generalization in multilingual and cross-lingual SP. In our experiments, we observe that pretrained multilingual language models struggle to transfer and generalize compositionally across languages. Our dataset will facilitate building robust multilingual semantic parsers by serving as a benchmark for evaluation of cross-lingual compositional generalization.",
            "score": 0.6482820371111835,
            "section_title": "Conclusion",
            "char_start_offset": 35692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 682
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "235358129",
            "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
            "text": "One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.",
            "score": 0.6462726587884189,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "276259236",
            "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
            "text": "To ensure high quality, we devise an annotation framework to optimize the dataset quality with human effort and LLM feedback. The process involves translating data from English to selected non-English languages using machine translation systems, post-editing each sample by three nativespeaking annotators with multiple iterations across most tasks, and picking the final translation version using a strong LLM that involves swapping sample positions for debiasing (Wang et al., 2024b;Li et al., 2024). \n\nLeading multilingual LLMs are evaluated on BenchMAX, revealing that language significantly influences language-agnostic capabilities of existing LLMs. Interestingly, simply increasing the parameters can boost average performance on these tasks but does not universally reduce the performance gap across languages. Moreover, compared to general translation, domain translation not only poses new challenges for LLMs but also requires new evaluation metrics. The main contributions can be summarized as follows: \n\n\u2022 We develop a comprehensive, multi-way multilingual benchmark across 17 languages for evaluating 6 crucial capabilities on 10 diverse tasks. \u2022 We propose a pipeline for curating high-quality mutlilingual datasets, involving both human annotation and LLM-as-a-judge.",
            "score": 0.6445967921292717,
            "section_title": "Introduction",
            "char_start_offset": 3804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1283
                }
            ],
            "ref_mentions": [
                {
                    "start": 465,
                    "end": 485,
                    "matchedPaperCorpusId": "258960339"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "254854247",
            "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
            "text": "In our experiment, we use multilingual parallel test sets created by machine translation from English to target low-resource languages. To explore the effect of machine translation-created test sets, we compare the cross-lingual transfer performance on translation test sets and original test sets of XNLI. The original XNLI datasets include two lowresource languages that we used in our experiments, i.e., Swahili (sw) and Urdu (ur). We also measure the translation quality by using the original sets as gold standard. The analysis results (",
            "score": 0.6444753675294838,
            "section_title": "A Effect of Translations",
            "char_start_offset": 29963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 542
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1566162109375
        },
        {
            "corpus_id": "231749968",
            "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics",
            "text": "Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench, 1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test data through a human-in-the-loop approach. \n\nIncreasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is the choice of the languages of the included datasets. It is often assumed that work on English transfers to other languages (Bender, 2011). However, this assumption does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b;Liu et al., 2020a;McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). \n\nAs a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where ML-Sum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020).",
            "score": 0.6440926044306032,
            "section_title": "Benchmarks in NLG",
            "char_start_offset": 7495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 826,
                    "end": 845,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1171,
                    "end": 1191,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1390,
                    "end": 1407,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1500,
                    "end": 1520,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1587,
                    "end": 1607,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1763,
                    "end": 1785,
                    "matchedPaperCorpusId": "216868259"
                },
                {
                    "start": 1801,
                    "end": 1822,
                    "matchedPaperCorpusId": "222177239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "226262367",
            "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
            "text": "Jing et al. (2019) crowdsourced parallel paragraphs from novels in Chinese and English. A few datasets investigated multiple-choice school QA (Hardalov et al., 2019;Van Nguyena et al., 2020), albeit in a limited domain, and for lower school grades (1st-5th). Other efforts focused on building bi-lingual datasets that are similar in spirit to SQuAD (Rajpurkar et al., 2016) -extractive reading comprehension over open-domain articles. Such datasets are collected by crowdsourcing questions, following a procedure similar to (Rajpurkar et al., 2016), in Russian (Efimov et al., 2020), Korean (Lim et al., 2019), French (d'Hoffschmidt et al., 2020), or by translating existing English QA pairs to Spanish (Carrino et al., 2020). \n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks. Clark et al. (2020)  The task was to ask a question, and then the shortest span answering it from a list of paragraphs was selected. As these datasets are complementary, rather than making each other obsolete, hereby the recently released XTREME (Hu et al., 2020) benchmark combined them in a joint task. E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student. Finally, our fine-grained evaluation for different subjects, languages, and combinations thereof allows for indepth analysis and comparison.",
            "score": 0.6415313824919707,
            "section_title": "Related Work",
            "char_start_offset": 5263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 165,
                    "matchedPaperCorpusId": "199442384"
                },
                {
                    "start": 349,
                    "end": 373,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 524,
                    "end": 548,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 561,
                    "end": 582,
                    "matchedPaperCorpusId": "21711688"
                },
                {
                    "start": 703,
                    "end": 725,
                    "matchedPaperCorpusId": "209202447"
                },
                {
                    "start": 801,
                    "end": 821,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 833,
                    "end": 855,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 923,
                    "end": 947,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1281,
                    "end": 1298,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "277780370",
            "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
            "text": "Two recent examples of studies incorporating consistency-based evaluations on factual knowledge questions are Qi et al. (2023) and Ohmer et al. (2023). Qi et al. (2023) focusses specifically on sample-level consistency of answers across different languages, requiring existing parallel benchmarks. Ohmer et al. (2023), instead, ask models to translate benchmark questions themselves before answering them again. This can, with some caveats, be applied to any existing monolingual benchmark, butrequiring multiple steps -it is more involved an a paradigm, and is somewhat bottlenecked by the translation ability of the model to be evaluated. \n\nTranslation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by Brown et al. (2020), but has not been common since. \n\nMonolingual non-English evaluation In our discussion, we have focussed on multilingual evaluation options that cover multiple other languages. After all, a benchmark to evaluate models on Bengali (e.g. Shafayat et al., 2024) or Arabic (e.g. Alwajih et al., 2024) can contribute to multilingual evaluation when combined with other benchmarks, but does not so on its own. Because such benchmarks are usually created by language experts for the respective languages, they usually target locally relevant skills and knowledge and are likely of higher quality than benchmarks created for many languages simultaneously (either through translation or from scratch). Yet, composing a suite including many languages that allows direct comparisons between languages remains challenging. We believe such benchmarks can be important for multilingual evaluation in LLMs, but will not further discuss benchmarks focussing on individual languages or very small sets of languages within one family here.",
            "score": 0.6409364767028363,
            "section_title": "E.2 Multilingual evaluation options for LLMs",
            "char_start_offset": 61859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 126,
                    "matchedPaperCorpusId": "264145744"
                },
                {
                    "start": 131,
                    "end": 150,
                    "matchedPaperCorpusId": "258840917"
                },
                {
                    "start": 152,
                    "end": 168,
                    "matchedPaperCorpusId": "264145744"
                },
                {
                    "start": 298,
                    "end": 317,
                    "matchedPaperCorpusId": "258840917"
                },
                {
                    "start": 857,
                    "end": 876,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1113,
                    "end": 1135,
                    "matchedPaperCorpusId": "271931333"
                },
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "271432107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages. \n\nMore recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO. This dataset is a suitable candidate as it shows good transfer learning capabilities, as well as being a popular choice for evaluating deep learning models (Craswell et al., 2020(Craswell et al., , 2021)). By translating the dataset we are able to leverages extensive manually annotated data. Lastly, as far as we know, no previous work has translated a large IR dataset such as the MS MARCO dataset to multiple languages. \n\nOur contributions are translating the MS MARCO (both training and evaluation sets) to 13 different languages such as Portuguese, Spanish, German, and Chinese. We finetuned mono and multilingual deep learning-based models both considering dense retrieval and reranking approaches using our translated dataset. Moreover, we evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Additionally, we made available the mMARCO dataset with 13 languages and all models finetuned on our translated dataset.",
            "score": 0.6407717231553907,
            "section_title": "Introduction",
            "char_start_offset": 1757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "253107357",
            "title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing",
            "text": "We create two benchmarks, XSPIDER and XKAGGLE-DBQA, by translating existing English Text-to-SQL datasets into other languages and evaluate our methods on these two benchmarks. and Vietnamese (vi) cross-domain Text-to-SQL datasets translated from SPIDER (Yu et al., 2018). More specifically, we use the English SPIDER training set as the candidate pool and training data for retriever-reranker models. We use the development sets of CSPIDER and VSPIDER for cross-lingual evaluation. We further translate the SPIDER development set into Farsi (fa) and Hindi (hi) for a more comprehensive evaluation. XKAGGLE-DBQA: This is a recently constructed dataset for more realistic and challenging Textto-SQL evaluation. The dataset is based on 8 databases from Kaggle. We translate the questions into Chinese (zh), Farsi (fa), and Hindi (hi) for cross-lingual evaluation. We use the English SPI-DER training set as the candidate pool.",
            "score": 0.6398323991670525,
            "section_title": "Datasets",
            "char_start_offset": 14407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 923
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 270,
                    "matchedPaperCorpusId": "52815560"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "259370885",
            "title": "Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya",
            "text": "More recently, Clark et al. (2020) presented TyDiQA, a dataset particularly designed to address information-seeking and natural questions covering 11 typologically diverse languages with a total of 204K samples. Longpre et al. (2021) presented an open domain dataset comprising 10K question-answer pairs aligned across 26 typologically diverse languages, yielding a total of 260K samples. Hu et al. (2020) presented XTREME, a multi-task benchmark for nine prominent NLP tasks including question-answering across 40 languages. Ruder et al. (2021) further extended the benchmark to XTREME-R, covering ten tasks across 50 typologically diverse languages. Xue et al. (2021) proposed a large multilingual pretrained model that handles 101 languages. Note that none of the aforementioned datasets and models include the Tigrinya language. \n\nTranslated QA datasets Another relatively inexpensive alternative to building a native annotated QA dataset is translating an existing English dataset to the target language. Carrino et al. (2020) explored this by proposing the Translate-Align-Retrieve (TAR) method to translate the English SQuAD1.1 dataset to Spanish. Then the resulting dataset was used to fine-tune a multilingual model achieving a performance of 68.1/48.3% F1/EM on MLQA (Lewis et al., 2020) and 77.6/61.8% F1/EM on XQuAD (Artetxe et al., 2020). Similar approaches were also adapted for the Japanese and French languages (Asai et al., 2018;Siblini et al., 2019), where a multilingual version of BERT  (Devlin et al., 2019) is trained on the English SQuAD1.1 and evaluated on the small translated corpus, reaching promising scores of 76.7% in F1 and 61.8% in EM.",
            "score": 0.6394358742375388,
            "section_title": "Question-Answering beyond English",
            "char_start_offset": 8041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1667
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 34,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 212,
                    "end": 233,
                    "matchedPaperCorpusId": "220871404"
                },
                {
                    "start": 389,
                    "end": 405,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 526,
                    "end": 545,
                    "matchedPaperCorpusId": "233241004"
                },
                {
                    "start": 1010,
                    "end": 1031,
                    "matchedPaperCorpusId": "209202447"
                },
                {
                    "start": 1277,
                    "end": 1297,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1328,
                    "end": 1350,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1507,
                    "end": 1528,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40869140625
        },
        {
            "corpus_id": "269004700",
            "title": "KazQAD: Kazakh Open-Domain Question Answering Dataset",
            "text": "TREC (Voorhees and Harman, 2005), the Cross-Language Evaluation Forum (CLEF) (Ferro and Peters, 2019), the Russian information retrieval evaluation initiative (also known as ROMIP) (Dobrov et al., 2004), and NII Testbeds and Community for Information access Research (NTCIR)2 evaluation campaigns have featured both cross-language retrieval (involving query and document collections in different languages) and monolingual retrieval in non-English languages.However, the datasets produced by these evaluation initiatives have been relatively small.With the proliferation of data-intensive neural methods in IR and QA, the demand for larger annotated collections has increased significantly.\n\nSince the release of the English SQuAD dataset (Rajpurkar et al., 2016), we have experienced a \"QA dataset explosion\" (Rogers et al., 2023) that has, inter alia, led to the emergence of many non-English datasets.These datasets were created through various approaches, including machine translations of SQuAD, such as the Spanish (Carrino et al., 2019) and Turkish ( \u00dcnl\u00fc Menev\u015fe et al., 2022) variants, as well as the application of the SQuAD annotation approach to Wikipedia in other languages (cf.Russian SberQuAD (Efimov et al., 2020)).\n\nSubsequently, multilingual QA datasets appeared, encompassing multiple languages simultaneously.The approaches to their creation also varied.For instance, XQuAD (Artetxe et al., 2020) involved manual translation of a small portion of English SQuAD questions and contexts into 10 languages, while MLQA (Lewis et al., 2020) focused on translating only the questions and annotating the answers in parallel contexts for each of the seven languages independently.TyDi QA (Clark et al., 2020) utilised annotators that independently generated questions based on short prompts from Wikipedia for 11 typologically different languages, annotated them at paragraph level, and extracted short answers.",
            "score": 0.6387741015577232,
            "section_title": "Related Work",
            "char_start_offset": 4281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 690
                },
                {
                    "start": 692,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1231
                },
                {
                    "start": 1233,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 202,
                    "matchedPaperCorpusId": "15531056"
                },
                {
                    "start": 739,
                    "end": 763,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 810,
                    "end": 831,
                    "matchedPaperCorpusId": "236447339"
                },
                {
                    "start": 1056,
                    "end": 1084,
                    "matchedPaperCorpusId": "256631094"
                },
                {
                    "start": 1208,
                    "end": 1229,
                    "matchedPaperCorpusId": "209439717"
                },
                {
                    "start": 1534,
                    "end": 1554,
                    "matchedPaperCorpusId": "204734128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "266999425",
            "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
            "text": "To comprehensively assess the cross-lingual proficiency of XCOT, we evaluate the method using the MGSM (Shi et al., 2023) benchmark, which extends the English GSM8K (Cobbe et al., 2021) dataset into ten typologically varied languages through the manual translation of problems. To conduct a thorough and wide-ranging evaluation of the multilingual mathematical problem-solving skills, we have also created an additional out-ofdomain test dataset called MSVAMP (Chen et al., 2023a), originating from the SVAMP (Patel et al., 2021) dataset. This dataset incorporates mathematical problems in 10 different languages, initially translated using machine translation and subsequently refined through careful human review and correction for accuracy and nuance. Finally, our method is evaluated on MGSM (Shi et al., 2023) and MSVAMP (Chen et al., 2023a) with the accuracy metric. In the experiments, we report the accuracy of all methods.",
            "score": 0.638286741771106,
            "section_title": "Evaluation",
            "char_start_offset": 9458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 931
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 121,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 509,
                    "end": 529,
                    "matchedPaperCorpusId": "232223322"
                },
                {
                    "start": 796,
                    "end": 814,
                    "matchedPaperCorpusId": "252735112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "257801559",
            "title": "Slovak Dataset for Multilingual Question Answering",
            "text": "From the above overview, it is clear that for some languages there are not enough resources for the preparation of a natural understanding system. Loginova et al. [61] discuss the current state of the art and the remaining challenges in multilingual QA. \n\nIn general, there exist three main approaches to solving multilingual question answering and transferring knowledge from English to other languages: \n\nZero-shot and few-shot approaches use multilingual language models trained on a corpus of data available in different languages. In the few-shot approach, the multilingual model is subsequently fine-tuned for the task in the target language. In addition, the validation data are from the target language. \n\nOn the contrary, the translate-train and translate-test approaches use machine translation. Machine translation can be used at several points in the machine learning chain. But machine translation creates additional inaccuracy and bias. In the case of the translate-train approach, the training set is translated from English to the target language, and then language models are created for the target languages. Test examples are from the target language. The translate-test approach involves the translation of test examples (questions and answers) from the source language into English, and the evaluation takes place with the English model. \n\nSeveral multilingual datasets have been published in the last five years. Table 4 compares the most significant publicly available multilingual question answering datasets. The table highlights the number of languages and total examples provided for each dataset.",
            "score": 0.6379724995657085,
            "section_title": "3) CROSSLINGUAL AND MULTILINGUAL QUESTION ANSWERING DATASETS",
            "char_start_offset": 13388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1624
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "212848801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406494140625
        },
        {
            "corpus_id": "276741970",
            "title": "Enhancing Non-English Capabilities of English-Centric Large Language Models through Deep Supervision Fine-Tuning",
            "text": "\u2022 SFT (Ouyang et al. 2022), which is instruction-tuned with English instruction datasets. \u2022 TFT (Zhu et al. 2023b), which is instruction-tuned using the original English instruction datasets translated into the target languages. \u2022 SDRRL (Zhang et al. 2024), which is a method based on Self-Distillation. Besides using English instructiontuning data and its multilingual code-switching extensions, it also incorporates partially translated data and completion data for fine-tuning. \u2022 DFT-logits, our method that applies logits-based supervision to guide the model's intermediate layers. \n\n\u2022 DFT-feature, our method that uses feature alignment to maintain consistent internal representations between English and tgt language. \n\nEvaluation Dataset understanding dataset to evaluate models' performance in cross-lingual inference tasks. \n\n\u2022 XCOPA (Cross-lingual Choice of Plausible Alternatives): XCOPA (Ponti et al. 2020) is a benchmark designed to evaluate the ability of models to apply commonsense reasoning, requiring both world knowledge and the ability to generalize it to new languages. \n\n\u2022 XStoryCloze (Cross-lingual Story Cloze Test) XSto-ryCloze (Lin et al. 2022) is a cross-lingual dataset for evaluating models' ability to understand stories and generate plausible endings. \n\n\u2022 MMLU (Massive Multitask Language Understanding) MMLU (Hendrycks et al. 2020) is a large-scale multitask language understanding dataset covering multiple domains (such as history, geography, science, law, etc.) and various languages. \n\nFor all evaluation datasets, we conducted tests using a zero-shot setting. We used the F1 score for XQuAD, MLQA, and MKQA, and the MC1 metric for TruthfulQA. For other NLU datasets, accuracy was used as the evaluation metric.",
            "score": 0.6378367387402888,
            "section_title": "Comparison of Methods",
            "char_start_offset": 13387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 26,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1153,
                    "end": 1169,
                    "matchedPaperCorpusId": "245334784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "258865558",
            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
            "text": "We create the BUFFET benchmark to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. We adhere to the following design principles with our benchmark. \n\nStandardized few-shot samples. BUFFET provides three different training and validation sets of k-shots (e.g., k = 32) per task for a nonclassification task, or per class for a classification task, for each language. \n\nTask diversity. Existing cross-lingual benchmarks often focus on classification or retrieval (Hu et al., 2020b;Ruder et al., 2021;Liang et al., 2020). BUFFET encompasses a broad range of task types, such as classification, generation, extraction, and structured prediction tasks. By converting all tasks into the same text-to-text format, we eliminate the need for task-specific model modifications or template conversions. 5. \n\nBeyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;Ponti et al. 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;Artetxe et al., 2020b). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
            "score": 0.6368518842787947,
            "section_title": "Design Principles",
            "char_start_offset": 9586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 567,
                    "end": 586,
                    "matchedPaperCorpusId": "233241004"
                },
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 1068,
                    "end": 1086,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1218,
                    "end": 1240,
                    "matchedPaperCorpusId": "216914383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70166015625
        },
        {
            "corpus_id": "204734128",
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "text": "First, we state our desired properties for a crosslingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section 3). We then describe our annotation protocol, which seeks to fulfil these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \n\nDiverse Languages A primary goal of crosslingual research is to develop systems that work well in many languages. The dataset should enable quantitative performance comparison across languages with different linguistic resources, language families and scripts. \n\nExtractive QA Cross-lingual understanding benchmarks are typically based on classification (Conneau et al., 2018). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extractive QA datasets in a number of languages (see Section 3), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \n\nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desirable to select a textual domain that matches existing extractive QA training resources, in order to isolate the change in performance due to language transfer. \n\nTo satisfy these desiderata, we identified the method described below and illustrated in Figure 1. Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset.",
            "score": 0.6366745478205195,
            "section_title": "The MLQA corpus",
            "char_start_offset": 4847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2026
                },
                {
                    "start": 2029,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2437
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "235358129",
            "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
            "text": "Table 3: Comparison of Various Evaluation Benchmarks. We compare FLORES-101 to a variety of popular, existing translation benchmarks, indicating language coverage, topic diversity, whether many-to-many translation is supported, if the translations are created by humans, and if the tasks of document-level translation or multimodal translation are supported. \n\nFigure 4: Translation Quality Score across Languages. We require the final translation quality score to be above 90% before the translation is of sufficient quality to include in FLORES-101. We depict the score distribution for all languages in FLORES-101. \n\nIn Table 4, we depict all FLORES-101 languages by their resource level. The amount of data available for a language is difficult to accurately denote, in part because quality is very important and thus, the amount of data does not necessary reflect its usefulness. Further, some data may be proprietary, and new datasets for new languages are actively being created by the research community. Thus, we report the amount of data to/from English and the amount of monolingual data available in OPUS, a public repository for multilingual data. OPUS is a heavily used resource, and itself is a collection of a large number of research datasets produced by the community over decades. The majority of languages have both bilingual data through English and monolingual data, though a number of languages have less than 100K sentences through English. Many of those also have no monolingual data available, making these truly low-resource. Examples include Shona and Nyanja.",
            "score": 0.6360221322926591,
            "section_title": "FLORES-101 101",
            "char_start_offset": 27334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1587
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1455078125
        },
        {
            "corpus_id": "270226553",
            "title": "The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding",
            "text": "SEB seeks to provide an estimate of the quality of embedding for Scandinavian languages and multilingual use cases.To do so, we focus on a) Coverage: The benchmark should cover a wide variety of tasks spanning distinctly different domains, usages, and embedding tasks; SEB compromises 24 datasets spanning at least 12 domains across nine different tasks with broad coverage for each language.\n\nb) Cultural integrity and model equity: Recent studies (Berdicevskis et al., 2023;Nielsen, 2023;Muennighoff et al., 2023) have increasingly adopted the strategy of leveraging translated English datasets as a means to evaluate the performance of models in low-resource language contexts.However, we avoid adding such translations, aiming to represent Scandinavian contexts accurately and mitigate the risk of artificially inflating multilingual model capabilities.This decision stems from the recognition that multilingual models, often trained on parallel or translated data (Reimers and Gurevych, 2020), may exhibit inflated performance when evaluated on similarly translated tasks -a hypothesis that, while plausible, remains to be conclusively shown.We choose to keep the existing translated datasets from MTEB within SEB to maintain compatibility.\n\nc) Cross-lingual generalization: Given the limited availability of datasets for the Scandinavian languages, we rely on the high degree of cross-lingual transfer (Nielsen, 2023) to estimate model performance more accurately.This approach capitalizes on intrinsic linguistic similarities and shared cultural contexts to bridge data gaps.\n\nd) Reproducibility and Accessibility: SEB expands upon the reproducibility of MTEB by including a model registry for all evaluated models to ensure the exact method (e.g., model prompts) for obtaining the results is known.Furthermore, to ensure that the benchmark is as widely accessible as possible, we have limited the size of most datasets to a maximum of 2048 examples.For most models, this allows running the benchmark on a consumer-grade laptop while ensuring proper performance estimation.The benchmark also implements a public cache, allowing users to experiment without needing to rerun models run by others.",
            "score": 0.6349193340171435,
            "section_title": "Design and Curation Rationale",
            "char_start_offset": 7183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1245
                },
                {
                    "start": 1247,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1582
                },
                {
                    "start": 1584,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2080
                },
                {
                    "start": 2080,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 476,
                    "matchedPaperCorpusId": "266164012"
                },
                {
                    "start": 476,
                    "end": 490,
                    "matchedPaperCorpusId": "257913673"
                },
                {
                    "start": 490,
                    "end": 515,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 969,
                    "end": 997,
                    "matchedPaperCorpusId": "216036089"
                },
                {
                    "start": 1408,
                    "end": 1423,
                    "matchedPaperCorpusId": "257913673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "276408022",
            "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis",
            "text": "Zhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer. Moreover, this translated dataset only includes aspect terms and sentiment polarities, lacking the joint detection of aspect categories, which is crucial for ABSA, thereby limiting the scope of the evaluation. Therefore, a high-quality multilingual and parallel ABSA dataset is missing in the community. \n\nTo bridge this gap, this paper presents the M-ABSA dataset, which includes 21 languages and 7 distinct domains, making it the first comprehensive multilingual parallel ABSA dataset. Specifically, we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary. To further investigate the quality of M-ABSA and unveil its possible usage in multilingual ABSA research, we conduct evaluations on M-ABSA under various settings, including zero-shot cross-lingual transfer, cross-domain transfer, and zero-shot prompting with large language models (LLMs). Our contributions are as follows: \n\n\u2022 We present a high-quality multilingual parallel dataset for ABSA, covering 21 typologically diverse languages and multiple domains, ensuring its applicability in multilingual ABSA tasks for triplet extraction. \n\n\u2022 We provide a feasible method of extending monolingual datasets to multiple languages with high quality by automatic translation, quality evaluation, and manual inspection. \n\n\u2022 We investigate the robustness and applicability of M-ABSA by a comprehensive evaluation, including cross-lingual transfer, cross-domain transfer, and zero-shot prompting with LLMs. Our results highlight the potential of M-ABSA in future multilingual ABSA research. \n\n2 Background and Related Work",
            "score": 0.6348408650137721,
            "section_title": "Introduction",
            "char_start_offset": 1738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2230
                },
                {
                    "start": 2233,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 130,
                    "matchedPaperCorpusId": "1021411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "248240015",
            "title": "IndicXNLI: Evaluating Multilingual Inference for Indian Languages",
            "text": "One of our work's key limitations is that the dataset IndicXNLI was created by machine translation of the original English XNLI dataset. Although In-dicXNLI is not human translated, it has been carefully evaluated for translation accuracy by a number of natural bilingual Indic speakers (2 for each language). Furthermore, as shown in our research (Table 2), employing automatic assessment measures such as round trip English-English evaluation via back translation and direct Indic-English sentence comparison is effective. In the past, such a metric has been shown to be highly beneficial for comparing without-reference machine translation (Bapna et al., 2022;Huang, 1990;Moon et al., 2020a,b). Furthermore, as did with the Hindi dataset in Appendix E, we might use correlation in the prediction score between human and machine translated sets for evaluating translation quality. \n\nSecond, adapting an existing dataset risks transferring biases and shortcomings from the original XNLI dataset into ours. However, it has been established that XNLI is a typical benchmark for evaluating multilingual and cross-lingual sentence representation, and it has been used to evaluate several multilingual models (Conneau et al., 2020b;Hu et al., 2020b). Morphological analysis of related languages, as well as insights into their performance behavior, may be useful. The authors, however, are not experts in that area, and such an assessment would have been outside the scope of the current work. This study might be expanded to include language groups other than Indian languages such as Indo-European. Third, because of limited resources, the current study did not include large versions of well-known models such as XLM-RoBERTa-Large and MuRIL-Large. However, for IndicBert, mBERT, XLM-RoBERTa, and MuRIL, we assessed model performance in relation to model size (#parameters) in Table 5.",
            "score": 0.634829053277129,
            "section_title": "Limitations",
            "char_start_offset": 13977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 663,
                    "end": 675,
                    "matchedPaperCorpusId": "2175391"
                },
                {
                    "start": 1205,
                    "end": 1228,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1228,
                    "end": 1245,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "269362363",
            "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
            "text": "\u2022 A qualitative analysis for assessing the gaps in current language technologies and define potential directions of future research. \n\nAs evident from the lists above, our benchmark provides a broad-coverage over languages with respect to their resourcedness, allowing users to evaluate language models on relatively highresource languages such as Hindi and extremely low-resource languages such as Manipuri in Meitei script on a single benchmark. \n\nTo curate the evaluation datasets for our benchmark, we use the following existing datasets as the source: CrossSum (Bhattacharjee et al., 2023) for cross-lingual summarization, FLO-RES (NLLB- Team et al., 2022) for machine translation, XQuAD (Artetxe et al., 2020) for multilingual QA, and XoRQA (Asai et al., 2021) for cross-lingual QA. From each of these datasets we select a subset of English examples to be a part of our benchmark, and then collect professional human translations for these examples in all target Indic languages. Some target languages are already covered by the source datasets in which case we re-purpose this existing data and only collect translations for the remaining languages. We also collect and release a small amount of training and validation data making possible evaluation of training techniques like fine-tuning, parameter-efficient training, in-context learning, and others. Why extend existing benchmarks? We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization  Sum (Bhattacharjee et al., 2023), a dataset for crosslingual summarization, which in turn is derived from XL-Sum (Hasan et al., 2021b). CrossSum contains multi-way parallel data in 45 languages where BBC news articles as source in a language are paired with corresponding summaries in other languages. Based on their matching criteria, different languages have different amount of sourcetarget pairs.",
            "score": 0.6313979917233252,
            "section_title": "Introduction",
            "char_start_offset": 3587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "246852846",
            "title": "AraConv: Developing an Arabic Task-Oriented Dialogue System Using Multi-Lingual Transformer Model mT5",
            "text": "Because Arabic is a low-resource language, no human-annotated Arabic dataset for task-oriented DS has been produced (to the best of our knowledge). To obtain a goodquality dataset, we decided to use an existing dataset, translating a benchmark dataset for task-oriented DS (BiToD [1]) to develop a suitable training dataset for Arabic task-oriented DS. \n\nTranslating existing datasets is a practice frequently observed in the literature for low-resource languages, with examples including [46][47][48]. Recent translation techniques for crowd-sourced annotated datasets have produced reasonable results on training data for different languages, enabling many studies to address the lack of datasets by translating existing datasets for many downstream tasks in NLP. For example, for question answering (QA), the SQuAD dataset has been translated into Arabic [46] and Bengali [47], and for conversation generation, the EmpatheticDialogues dataset has been translated into Arabic [48]. \n\nStill, it is imperative for the research community to develop multi-lingual benchmarks to evaluate the cross-lingual transferability of end-to-end systems in general and task-oriented DS in particular [49]. For task-oriented DS, many multi-lingual datasets can be obtained by translating the English datasets. Error! Reference source not found. presents some of these alongside their corresponding tasks and domains. Translation represents a good choice for low-resource languages to support the reuse of resources and save time spent creating and annotating long dialogues. Additionally, this enables the development of multi-lingual benchmarks for the research community to use.",
            "score": 0.6298350185051008,
            "section_title": "Arabic Task-Oriented DS Dataset",
            "char_start_offset": 11248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "229478912"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "227231115"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "229478912"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "227231115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458740234375
        },
        {
            "corpus_id": "265607892",
            "title": "Does the English Matter? Elicit Cross-lingual Abilities of Large Language Models",
            "text": "The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking. open but require the models' ability to solve logical mathematical tasks less related to the language. \n\nHowever, we decided to introduce them to observe whether our approach degrades performance in these tasks. The first two datasets selected are appropriately constructed for multi-language testing, while the second two are available only in English. Hence, we do a preliminary translation step as outlined below. Thus, descriptions of the benchmarks follow in the next paragraphs: \n\nMultiLingual Question Answering (MLQA) (Lewis et al., 2020) evaluates cross-lingual question answering performance using 5K extractive QA instances in the SQuAD (Rajpurkar et al., 2016) format in several languages. MLQA is highly parallel, with QA instances aligned across four languages on average. Although comprising different languages, some languages, such as Italian, are not represented. To conduct the experiments uniformly, we have translated the examples as also done in the forthcoming MMLU and BBH. \n\nCross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages. \n\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages3 . BIG-Bench Hard (BBH) (Suzgun et al., 2022) is a subset of challenging tasks related to navigation, logical deduction, and fallacy detection. Again, the benchmark is native English, and we have translated it into five languages ?? .",
            "score": 0.629624024681291,
            "section_title": "Output",
            "char_start_offset": 9909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 640,
                    "end": 660,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1163,
                    "end": 1185,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "257219401",
            "title": "Cross-Lingual Question Answering over Knowledge Base as Reading Comprehension",
            "text": "We evaluate our method on two datasets, QALD-M, a small evaluation dataset in 11 languages, and WebQSP-zh, a new dataset with a larger size and more realistic expressions.\n\nQALD-M QALD-M is a series of evaluation campaigns on question answering over linked data. We use the version provided by Zhou et al. (2021) and filter the out-of-scope ones. It consists of testing questions for 11 non-English languages (fa, de, ro, it, ru, fr, nl, es, hi, pt, pt_BR) over DBPedia. The numbers of used questions for each language range from 66 to 363. We use QALD-M mainly for zero-shot evaluation. See Appendix A.1 for more details.\n\nWebQSP-zh Considering that the size of QALD-M is small and its multilingual questions are mostly literal translations without language-dependent paraphrasing, we collect a new xKBQA dataset WebQSP-zh, with 3,098 questions for training and 1,639 questions for test.\n\nTo collect WebQSP-zh, we employ two Chinese native speakers proficient in English to manually translate all the questions in WebQSP (Yih et al., 2016), a widely-used English KBQA dataset, together with another annotator responsible for checking translation quality. To provide a more realistic benchmark for cross-lingual evaluation, the annotators are instructed to pay much attention to commonsense knowledge and natural vocabulary choices during translation. For example, in the upper example of Table 1, the phrase be in in the WebQSP question has multiple translations in Chinese. Based on the commonsense knowledge that Anna Kendrick is an actress, it is translated as \u51fa\u6f14/star in instead of its literal meaning \u5728\u505a/be doing. In the lower example of Table 1, the annotator chooses the Chinese word \u524d\u82cf\u8054/former Soviet Union for translation instead of \u82cf\u8054/Soviet Union because the former is more often used by native Chinese speakers. See Appendix A.2 for more statistics, annotation details, and examples.",
            "score": 0.6264589661724885,
            "section_title": "Datasets",
            "char_start_offset": 14513,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "7681779",
            "title": "A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets",
            "text": "Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets. The evaluated language pairs were Spanish-English, Spanish-French, Spanish-German, and English-Farsi. In each case a proficient speaker of both languages was selected to carry out the evaluation. The Pearson correlations of the human judges with the automatically generated scores were 0.89 for Spanish-English, 0.94 for Spanish-French, 0.91 for Spanish-German, and 0.92 for English-Farsi, showing the reliability of our cross-lingual dataset creation process and reinforcing the quality of the newly created monolingual datasets.",
            "score": 0.6259673061625941,
            "section_title": "Cross-lingual Datasets",
            "char_start_offset": 12157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 662
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "277066532",
            "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
            "text": "However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023). \n\nRecently, NLP researchers have addressed this limitation; several diverse linguistic and cultural context MMLU evaluation benchmarks have been developed to address the gap, such as Ara-bicMMLU (Koto et al., 2024) evaluates tasks from North Africa, the Levant, and the Gulf, In-doMMLU (Koto et al., 2023) incorporates Indonesia's local languages and cultures. Others include, CMMLU (Li et al., 2023), KMMLU (Son et al., 2024),TurkishMMLU (Y\u00fcksel et al., 2024), PersianMMLU (Ghahroodi et al., 2024), Global MMLU (Singh et al., 2024) evaluates cultural and linguistic bias in 42 languages. \n\nDespite these advancements, significant gaps remain for African LRLs and some Eastern European languages. Our work narrows this gap, by including Giriama, a LRL language spoken by approximately 2.6 million people in Kenya, which is currently absent from existing NLP benchmarks. Similarly, even though the Latvian language has been benchmarked before EU20-MMLU (Thellmann et al., 2024), with relatively high-quality datasets, the datasets lack human-curated translations. As a result, they encounter challenges, including inaccurate auto-translations that compromise reliable evaluations. Giriama, in particular, has a limited digital presence and has historically been underrepresented in linguistic resources, making this the first multilingual benchmark developed explicitly for the language. Our contributions can be summarized as follows: \n\n\u2022 Novel Datasets: we present LAG-MMLU, a curated dataset that includes Giriama's first gold-standard (human-translated) collection, alongside two silver-standard datasets for Latvian: one machine-translated and another machine-translated with subsequent human curation.",
            "score": 0.6257791749097127,
            "section_title": "Introduction",
            "char_start_offset": 1863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1717
                },
                {
                    "start": 1720,
                    "end": 1989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "277621694",
            "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models",
            "text": "We integrated 20+ existing multilingual benchmarks into a unified pipeline, by standardizing all ISO 639-3 language codes in the different benchmarks 1 , which covers nearly all known languages worldwide. By aligning benchmark language identifiers with ISO 639-3 codes, we enable evaluations for specific languages or 1 https://iso639-3.sil.org/about arXiv:2504.04155v1 [cs.CL] 5 Apr 2025 language groups (e.g., Bantu, Dravidian, or Uralic languages), allowing the framework to automatically search among integrated benchmarks to find matching test sets. This mapping also makes it easier to incorporate new large-scale benchmarks that target mid-or low-resource languages, ensuring flexibility for future expansions. \n\n2. Language-Specific Prompt Templates. \n\nUsers can configure prompts for each language individually, thereby enabling more precise assessments of a model's instruction-following ability across diverse linguistic settings. All templates are maintained in a centralized prompt library that supports multilingual benchmarks, allowing easy customization as needed. In this way, each task within a benchmark can be run potentially using prompts in the task's original language, rather than defaulting to English prompts. To simplify cross-lingual adaptation, we also implemented Microsoft Translator2 integration that automatically propagates user-defined prompt templates from one single language to 130+ supported languages. \n\n3. Non-English-Centered Machine Translation Evaluation. GlotEval is designed to break away from the traditional Englishcentric paradigm. Thanks to translation benchmarks featuring fully or partially multialigned datasets, GlotEval enables non-English-centered translation evaluations by allowing any supported language to serve as the pivot: users simply update the pivot language in the configuration, to assess \"any-to-pivot\" / \"pivot-to-any\" translation directions. This flexibility ensures that GlotEval breaks from the traditional \"English \u2194 other language\" paradigm and adapts seamlessly to diverse, potentially low-resource, language pairs.",
            "score": 0.6256079037396397,
            "section_title": "Consistent Multilingual Benchmarking.",
            "char_start_offset": 1968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2091
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "265157921",
            "title": "A Material Lens on Coloniality in NLP",
            "text": "By this process, even multilingual models are demonstrably skewed towards Western, and especially American, values and phrasing (Johnson et al., 2022;Arora et al., 2023;Naous et al., 2023;Papadimitriou et al., 2023;Durmus et al., 2023). \n\nIdeally, benchmark evaluations would capture cultural discrepancies. However, data curators often translate existing annotated data (Artetxe et al., 2020a), likely aiming to extend an English-centric resource with accepted value to new languages. These datasets may cover many languages, but the names, places, events, and other cultural concepts are transferred into the evaluation for new languages. As an example, out of the 47 Wikipedia articles in the XQuaD (Artetxe et al., 2020a) test set: 13 are primarily about North America (e.g. Super Bowl 50), 11 on Europe (e.g. Scottish Parliament), 2 on Asia (Yuan Dynasty & Genghis Khan), 1 on Australia (the state of Victoria), 1 on Africa (Kenya), 1 on Latin America (Amazon Rainforest), 1 on the Middle East (Islamism), and the remaining 17 are location agnostic (e.g. Oxygen). This is common practice: 8 out of 12 tasks across in the widely used multi-task XTREME (Hu et al., 2020) and XTREME-R (Ruder et al., 2021) benchmarks use data translated from English. However, such datasets may struggle to capture cultural misalignment and may in fact reward systems which replicate, especially in the range of tasks where cultural understanding has been shown to influence accuracy (Mohammad et al., 2016;Smith et al., 2016;Asai et al., 2021;Bauer et al., 2023;Lee et al., 2023;Akinade et al., 2023). \n\nIn this case, the data requirements of existing algorithms influence researchers to sample increasing amounts of Western data and the cultural biases of datasets underestimate the negative effects of this process.",
            "score": 0.6254914361064451,
            "section_title": "English Centric Multilingualism",
            "char_start_offset": 37466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 169,
                    "matchedPaperCorpusId": "247748753"
                },
                {
                    "start": 188,
                    "end": 215,
                    "matchedPaperCorpusId": "252815840"
                },
                {
                    "start": 371,
                    "end": 394,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 702,
                    "end": 725,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 1156,
                    "end": 1173,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1187,
                    "end": 1207,
                    "matchedPaperCorpusId": "233241004"
                },
                {
                    "start": 1469,
                    "end": 1492,
                    "matchedPaperCorpusId": "14935137"
                },
                {
                    "start": 1492,
                    "end": 1511,
                    "matchedPaperCorpusId": "17206686"
                },
                {
                    "start": 1511,
                    "end": 1529,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 1529,
                    "end": 1548,
                    "matchedPaperCorpusId": "258378215"
                },
                {
                    "start": 1548,
                    "end": 1565,
                    "matchedPaperCorpusId": "258486940"
                },
                {
                    "start": 1565,
                    "end": 1586,
                    "matchedPaperCorpusId": "258486877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "245502964",
            "title": "A Survey on non-English Question Answering Dataset",
            "text": "Using a search engine to construct a dataset (Ismail and Homsi, 2018;He et al., 2018). And others from book and news websites (Cui et al., 2016;Cui et al., 2019). \n\nMachine translation, because acquiring data source, annotating, and building a high-quality dataset is costly, some researchers translate existing datasets like SQuAD into their own language. (Croce et al., 2018;Atef et al., 2020;Abadani et al., 2021;Lee et al., 2019). But this will cause another problem, the dataset that is from native English is English centric such as American cultures, politics, events, the history which does not reflect information in other languages. Another is lost in translation that is not able to translate properly. \n\nHuman translation, to avoid those problems made by machine translation, this type of dataset construction is mainly used for multilingual and cross-lingual datasets which tackle a more challenging task. This is required in order to train the model to be able to answer the question in multiple languages. Moreover, some studies construct a dataset from native speakers which actually represents their culture and interest as in (Siblini et al., 2021;Longpre et al., 2020;Asai et al., 2021).",
            "score": 0.624728651891265,
            "section_title": "Data Collection",
            "char_start_offset": 2600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 162
                },
                {
                    "start": 165,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1206
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 69,
                    "matchedPaperCorpusId": "58952100"
                },
                {
                    "start": 69,
                    "end": 85,
                    "matchedPaperCorpusId": "3662564"
                },
                {
                    "start": 126,
                    "end": 144,
                    "matchedPaperCorpusId": "3933075"
                },
                {
                    "start": 144,
                    "end": 161,
                    "matchedPaperCorpusId": "3935253"
                },
                {
                    "start": 377,
                    "end": 395,
                    "matchedPaperCorpusId": "231618331"
                },
                {
                    "start": 395,
                    "end": 416,
                    "matchedPaperCorpusId": "235308420"
                },
                {
                    "start": 1187,
                    "end": 1205,
                    "matchedPaperCorpusId": "225040672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14306640625
        },
        {
            "corpus_id": "253762064",
            "title": "Why is sentence similarity benchmark not predictive of application-oriented task performance?",
            "text": "We would like to investigate other languages in this paper, but we are only concerned with the original English STS. Other languages than English also have benchmark datasets of the semantic similarity but are generally based on the STS framework. Since the GLUE (Wang et al., 2019), including STS, is facilitating model development for each task, a language-specific GLUE-like benchmark set (Le et al., 2020;Park et al., 2021) or cross-lingual benchmark set (Liang et al., 2020;Hu et al., 2020) are constructed. The benchmarks of the semantic similarity for each language are created in two methods: re-constriction by automatic translation or new construction by each language's expert following the original method. Crucially, the former method is likely to fundamentally face the same biases such as vocabulary distribution as those in the English benchmarks, albeit including the issue of translation quality. Regarding the latter, dataset creators may improve the original dataset creation method. For example, in the Korean GLUE (KLUE; Park et al., 2021), they added more detailed instructions on label definition when annotating the similarity by non-expert. Thus, it is necessary to re-consider the requirements for an appropriate benchmarks before straightforwardly following the original method when creating datasets.",
            "score": 0.6238199787334715,
            "section_title": "A.1 Limitation: Experiments on only English STS",
            "char_start_offset": 28403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1329
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "209202658"
                },
                {
                    "start": 459,
                    "end": 479,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 479,
                    "end": 495,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277099609375
        },
        {
            "corpus_id": "259859119",
            "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
            "text": "For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets. Similar to previous approach, we perform both zero-shot and sequential fine-tuning for translated datasets and report the results in table 5.  4: Comparison of monolingual fine-tuning vs multilingual fine-tuning for all datasets. Four source language models, XLM_ENG, XLM_HI, XLM_IT and XLM_KOR are the fine-tuned XLM-R models on English, Hindi, Italian and Korean datasets respectively. Last four columns correspond to sequential fine-tuning of all datasets using source language models. Best F1-scores are shown in bold.",
            "score": 0.6231250241702588,
            "section_title": "MTL based on Translation",
            "char_start_offset": 18270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "277451551",
            "title": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment",
            "text": "Training Dataset is based on AlpaGasus (Chen et al., 2024a), which contains 9K highquality English instruction following data filtered from the 52K Alpaca dataset (Taori et al., 2023). We randomly sample 1K prompts from AlpaGasus and use Google Translate to translate them into 9 languages: Italian (it), Chinese (zh), Portugese (pt), Korean (ko), Spanish (es), German (de), Arabic (ar), Japanese (jp) and French (fr). For our multilingual experiments, we split the 1K prompts equally into 10 non-overlapping partitions, i.e. 100 prompts per language. For monolingual experiments, we use the full 1K prompts. \n\nEvaluation Datasets include: (1) aya evaluation suite for instruction-following (Singh et al., 2024b). aya evaluation suite contains multilingual open-ended conversation-style prompts \n\nto evaluate multilingual open-ended generation quality. It is a high-quality prompt-based benchmark that contains translations and edits by human experts. ( 2) Global MMLU and multilingual version of ARC challenge for alignment tax (Singh et al., 2024a). Global MMLU improves upon previous translated MMLU variants by incorporating human-verified translations and annotating subsets for cultural sensitivity. This enables robust evaluation of LLMs across both culturally agnostic and culturally sensitive tasks. ARC challenge is a benchmark of grade-school science questions that are specifically selected to be unsolvable by simple retrieval or co-occurrence methods, requiring advanced reasoning and knowledge understanding from models (Clark et al., 2018;Lai et al., 2023).",
            "score": 0.6230206887232306,
            "section_title": "Datasets and Metrics",
            "char_start_offset": 14739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1573
                }
            ],
            "ref_mentions": [
                {
                    "start": 691,
                    "end": 712,
                    "matchedPaperCorpusId": "267617144"
                },
                {
                    "start": 1555,
                    "end": 1572,
                    "matchedPaperCorpusId": "260334562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "261242630",
            "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
            "text": "To evaluate the performance of the It-LLMs and the impact of the translation-based semantic alignment approach, we used two cross-lingual (XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2020)) and two multi-task (MMLU (Hendrycks et al., 2021) and BBH (Suzgun et al., 2022)) benchmarks. XQUAD and MLQA focus on understanding ques-tions and answers through translation into different languages. MMLU and BBH being multi-task benchmarks include subtasks related to Boolean expressions and QA on basic-level subjects (e.g., chemistry, physics). However, we decided to introduce them to observe whether our approach degrades performance in these tasks. The first two datasets selected are appropriately constructed for multi-language testing, while the second two are available only in English. So we do a preliminary translation step as outlined below. \n\nMultiLingual Question Answering (MLQA) (Lewis et al., 2020) evaluatates cross-lingual question answering performance. The benchmark comprises over 5K extractive QA instances in the SQuAD (Rajpurkar et al., 2016) format in several languages. MLQA is highly parallel, with QA instances aligned across four languages on average. Although comprising different languages, some languages are not represented, such as Italian. To conduct the experiments uniformly, we have translated the examples as also done in the forthcoming MMLU and BBH. \n\nCross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages. \n\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4 .",
            "score": 0.6217735371657279,
            "section_title": "Benchmarks",
            "char_start_offset": 14084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 167,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 174,
                    "end": 194,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 893,
                    "end": 913,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1441,
                    "end": 1463,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "261822560",
            "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
            "text": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200\u2014a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.",
            "score": 0.6210946866274704,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "268727026",
            "title": "PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for the Neural Processing of Portuguese",
            "text": "Producing benchmarks to evaluate language models in downstream tasks is a daunting endeavour.The more complex the task, the more difficult it is to produce quality data that can be used to train models in a fine-tuning approach and test their capabilities.While highly resourced languages, such as English, include quite elaborate benchmarks (Wang et al., 2018(Wang et al., , 2019)), few evaluation datasets are available for other, less-resourced languages. 1The particular case of Portuguese is a paradigmatic example, with only a few tasks being available for this purpose (Fonseca et al., 2016;Real et al., 2020;Santos et al., 2006;Freitas et al., 2010).\n\nA few examples of manually produced multilingual parallel corpora exist (Yang et al., 2019;Artetxe et al., 2020b;Ponti et al., 2020;Sen et al., 2022), as well as collections of tasks in multiple languages (Srivastava et al., 2023).At the same time, machine translation has come to a point in which it can be useful to create corpora that, while lacking human curation, can, up to a certain extent, be used to evaluate language models in the target languages (Conneau et al., 2018;Eger et al., 2018;Yang et al., 2019;Carrino et al., 2020;d'Hoffschmidt et al., 2020).Some have been created to allow cross-lingual evaluation of pre-trained encoders (Hu et al., 2020;Liang et al., 2020).\n\nState-of-the-art MT systems still struggle to produce accurate translations in several situations.Short texts, for instance, often lack enough context to obtain proper translations (Wan et al., 2022).Because of this, translation at the sentence level often falls short of translating longer texts, which provide more context (Jin et al., 2023).Translating from mostly gender-poor to gender-rich languages is also often a source of translation errors (Savoldi et al., 2021).",
            "score": 0.6208363470463967,
            "section_title": "Related Work",
            "char_start_offset": 2521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1225
                },
                {
                    "start": 1225,
                    "end": 1343
                },
                {
                    "start": 1345,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1818
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 360,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 360,
                    "end": 382,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 598,
                    "end": 616,
                    "matchedPaperCorpusId": "211264953"
                },
                {
                    "start": 616,
                    "end": 636,
                    "matchedPaperCorpusId": "562554"
                },
                {
                    "start": 636,
                    "end": 657,
                    "matchedPaperCorpusId": "14829156"
                },
                {
                    "start": 732,
                    "end": 751,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 751,
                    "end": 773,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 773,
                    "end": 792,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 792,
                    "end": 809,
                    "matchedPaperCorpusId": "252693442"
                },
                {
                    "start": 865,
                    "end": 890,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1118,
                    "end": 1140,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1140,
                    "end": 1158,
                    "matchedPaperCorpusId": "51715039"
                },
                {
                    "start": 1158,
                    "end": 1176,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 1176,
                    "end": 1197,
                    "matchedPaperCorpusId": "209202447"
                },
                {
                    "start": 1197,
                    "end": 1224,
                    "matchedPaperCorpusId": "211126910"
                },
                {
                    "start": 1306,
                    "end": 1323,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1323,
                    "end": 1342,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 1526,
                    "end": 1544,
                    "matchedPaperCorpusId": "247331992"
                },
                {
                    "start": 1670,
                    "end": 1688,
                    "matchedPaperCorpusId": "258840875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "268357672",
            "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
            "text": "To investigate how LMs can comprehend or generate text in other languages, there have been efforts to construct multilingual datasets.For instance, XGLUE (Liang et al., 2020) encompasses 100 languages that can be employed for both pre-training and evaluating cross-lingual tasks.XTREME (Hu et al., 2020) introduces an evaluation framework for cross-lingual benchmarks, while MEGA (Ahuja et al., 2023) focuses on assessing LLMs, providing 16 NLP datasets ranging from low-resource to high-resource languages.In addition, datasets that primarily focus on specific target languages, such as Chinese, Indian, and African languages, have been introduced (Huang et al., 2023;Doddapaneni et al., 2023;Adebara et al., 2023).\n\nThe popularity of commonsense datasets has increased because they reflect a wide array of sociocultural knowledge shared by humans (Liu and Singh, 2004).These datasets incorporate everyday concepts, such as CommonsenseQA (Talmor et al., 2019), scientific knowledge like ARC (Clark et al., 2018), and simple arithmetic reasoning like GSM8K (Cobbe et al., 2021).These datasets can be seen as a representation of general and practical knowledge that aligns with human intentions.Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, Ma et al. 2022 emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages.",
            "score": 0.6198405442071793,
            "section_title": "Multilingual and Commonsense",
            "char_start_offset": 4062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 716
                },
                {
                    "start": 718,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1626
                },
                {
                    "start": 1626,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 174,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 849,
                    "end": 870,
                    "matchedPaperCorpusId": "266028051"
                },
                {
                    "start": 939,
                    "end": 960,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 1289,
                    "end": 1307,
                    "matchedPaperCorpusId": "250562882"
                },
                {
                    "start": 1416,
                    "end": 1437,
                    "matchedPaperCorpusId": "3523957"
                },
                {
                    "start": 1639,
                    "end": 1653,
                    "matchedPaperCorpusId": "247762223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7412109375
        },
        {
            "corpus_id": "26127787",
            "title": "A Survey of Cross-lingual Word Embedding Models",
            "text": "The first two widely used tasks are intrinsic evaluation tasks: They evaluate cross-lingual embeddings in a controlled in vitro setting that is geared towards revealing certain characteristics of the representations. The major downside with these tasks is that good performance on them does not generalize necessarily to good performance on downstream tasks (Tsvetkov, Sitaram, Faruqui, Lample, Littell, Mortensen, Black, Levin, & Dyer, 2016;Schnabel, Labutov, Mimno, & Joachims, 2015).\n\nWord similarity This task evaluates how well the notion of word similarity according to humans is emulated in the vector space. Multi-lingual word similarity datasets are multilingual extensions of datasets that have been used for evaluating English word representations. Many of these originate from psychology research and consist of word pairs -ranging from synonyms (e.g., car -automobile) to unrelated terms (e.g., noon -string) -that have been annotated with a relatedness score by human subjects. The most commonly used ones of these human judgement datasets are: a) the RG dataset (Rubenstein & Goodenough, 1965); b) the MC dataset (Miller & Charles, 1991); c) the WordSim-353 dataset (Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002), a superset of MC; and d) the SimLex-999 dataset (Hill, Reichart, & Korhonen, 2015). Extending them to the multilingual setting then mainly involves translating the word pairs into different languages: WordSim-353 has been translated to Spanish, Romanian, and Arabic (Hassan & Mihalcea, 2009) and to German, Italian, and Russian (Leviant & Reichart, 2015); RG was translated to German (Gurevych, 2005), French, (Joubarne &Inkpen, 2011), Spanish andFarsi (Camacho-Collados, Pilehvar, &Navigli, 2015); and SimLex-999 was translated to German, Italian and Russian (Leviant & Reichart, 2015)",
            "score": 0.6198174387205757,
            "section_title": "Tasks",
            "char_start_offset": 70954,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 442,
                    "matchedPaperCorpusId": "6157443"
                },
                {
                    "start": 442,
                    "end": 485,
                    "matchedPaperCorpusId": "6197592"
                },
                {
                    "start": 1077,
                    "end": 1108,
                    "matchedPaperCorpusId": "18309234"
                },
                {
                    "start": 1128,
                    "end": 1152,
                    "matchedPaperCorpusId": "145580646"
                },
                {
                    "start": 1181,
                    "end": 1255,
                    "matchedPaperCorpusId": "12956853"
                },
                {
                    "start": 1305,
                    "end": 1339,
                    "matchedPaperCorpusId": "3226120"
                },
                {
                    "start": 1523,
                    "end": 1548,
                    "matchedPaperCorpusId": "1856431"
                },
                {
                    "start": 1641,
                    "end": 1657,
                    "matchedPaperCorpusId": "16584933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33984375
        },
        {
            "corpus_id": "220363855",
            "title": "El Departamento de Nosotros: How Machine Translated Corpora Affects Language Models in MRC Tasks",
            "text": "Historically, most of NLP tasks, datasets, and benchmarks were created in English, e.g., the Penn Treebank [10], SQuAD [12], GLUE [14]. Therefore, most of the large-scale pre-trained models were trained in the English-only mode, e.g., BERT [6] employed Wikipedia and the BookCorpus [17] as training datasets. Later on, the NLP community sought after increasing language diversity and multilingual models started to appear, such as mBERT or XLM [4]. \n\nHowever, large and diverse enough pre-training corpora of high quality often do not exist. Several methods have been developed to bridge this gap, e.g., applying machine translation frameworks to English corpora, or performing cross-lingual transfer learning [16]. Multilingual language models and pre-trained non-English language models are definitely in the focus of the NLP community. Still, the language understanding capabilities (hence, the performance) of language models largely depend on data collection and cleaning steps. In the MRC dimension, for instance, Italian SQuAD [5] is obtained via direct translation from the English version whereas French FQuAD [7] and Russian SberQuAD [8] have been created based on their language-specific part of Wikipedia often being much smaller than original SQuAD. \n\nWith the surge of language-specific pre-trained LMs several benchmarks have been developed that aim at evaluating multi-and cross-lingual characteristics of such LMs. Specifically, for the machine reading comprehension and question answering (QA) task there exist XQuAD [1] and MLQA [9]. In this work, we study how LMs perform in QA tasks in Spanish when fine-tuning on datasets of possibly different quality, i.e., directly machine translated and curated with the human-in-the-loop strategy.",
            "score": 0.6191265080283375,
            "section_title": "RELATED WORK",
            "char_start_offset": 1950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "252796"
                },
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 130,
                    "end": 134,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "53238211"
                },
                {
                    "start": 1144,
                    "end": 1147,
                    "matchedPaperCorpusId": "209439717"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310302734375
        },
        {
            "corpus_id": "259095896",
            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
            "text": "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names).",
            "score": 0.6189057073606792,
            "section_title": "Stage IV: Key Datasets Translation",
            "char_start_offset": 12869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 891
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "In this section, we verify how monolingual and multilingual models perform when varying pretraining, finetuning, and evaluation languages. As the number of pairwise language combinations is large, we only use English and Portuguese in this ablation study. The monolingual models we use are an English (Raffel et al., 2019) and a Portuguese (Carmo et al., 2020)  Table 5 shows the results. Rows (1) to (3) use models mostly pretrained on a single language (T5 and miniLM), and rows (4) to (7) report results for multilingual pretrained models (MT5 and MMINILM). When finetuned on monolingual datasets, we observe insignificant differences between monolingual and multilingual models in both English and Portuguese evaluation sets. We observe that for the monolingual configuration, MINILM outperforms T5 in English (1). We argue that the model was able to leverage training data for both languages and thus reduced most inaccuracies introduced by noisy translations. \n\nWhen considering the results from English dataset, it is important to mention that both multilingual rerankers were finetuned on a smaller amount of English examples than the models in row (1). This shows that the translation did not harm the resulting datasets significantly. Albeit the lexical mismatch problem discussed in Section 4, both rerankers were able to learn the task from the translated data. \n\nRows (4) and (5) in Table 5 exhibit the results for MT5 finetuned on monolingual datasets. Whilst the results were lower for the English dataset when compared to monolingual T5 (1), the results on the Portuguese version were marginally higher than the ones observed in row (2). This cross-lingual evaluation shows how a multilingual model finetuned on one language and evaluated on another still can reach good results. Although this observation is not true for the Portuguese-English cross-lingual evaluation (row 5), as the result was below the monolingual one (row 1), the difference is small.",
            "score": 0.6184235880509402,
            "section_title": "Ablation Study",
            "char_start_offset": 21080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4150390625
        },
        {
            "corpus_id": "269362697",
            "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
            "text": "For validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy. Notably, our proposed data translation pipeline demonstrates a more effective strategy to attain high-quality training data, compared to the individual translation of each data component.",
            "score": 0.6179501219478962,
            "section_title": "Introduction",
            "char_start_offset": 3859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 831
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 272,
                    "matchedPaperCorpusId": "214794966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "270226445",
            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
            "text": "We evaluate models trained on the xGQA dataset with MaXM (Changpinyo et al., 2023), a recently proposed evaluation-only benchmark for multilingual VQA.The MaXM dataset covers seven different languages: English (en), French (fr), Hindi (hi), Hebrew (iw), Romanian (ro), Thai (th), and Chinese (zh  7. The accuracy of the original English evaluation set is increased in both MERGE and TAG compared to solely using translated samples.The overall scores for the translate-test are also improved with data augmentation.These results indicate that augmenting training data with both human and MT texts is helpful for cross-lingual transfer while maintaining its performance on the original English texts.",
            "score": 0.6176658872532852,
            "section_title": "Experiments with MaXM dataset",
            "char_start_offset": 15544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 82,
                    "matchedPaperCorpusId": "232417173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "246852846",
            "title": "AraConv: Developing an Arabic Task-Oriented Dialogue System Using Multi-Lingual Transformer Model mT5",
            "text": "Because Arabic is a low-resource language, no human-annotated Arabic dataset for task-oriented DS has been produced (to the best of our knowledge). To obtain a goodquality dataset, we decided to use an existing dataset, translating a benchmark dataset for task-oriented DS (BiToD [1]) to develop a suitable training dataset for Arabic task-oriented DS. \n\nTranslating existing datasets is a practice frequently observed in the literature for low-resource languages, with examples including [46][47][48]. Recent translation techniques for crowd-sourced annotated datasets have produced reasonable results on training data for different languages, enabling many studies to address the lack of datasets by translating existing datasets for many downstream tasks in NLP. For example, for question answering (QA), the SQuAD dataset has been translated into Arabic [46] and Bengali [47], and for conversation generation, the EmpatheticDialogues dataset has been translated into Arabic [48]. \n\nStill, it is imperative for the research community to develop multi-lingual benchmarks to evaluate the cross-lingual transferability of end-to-end systems in general and taskoriented DS in particular [49]. For task-oriented DS, many multi-lingual datasets can be obtained by translating the English datasets. Table 2 presents some of these alongside their corresponding tasks and domains. Translation represents a good choice for low-resource languages to support the reuse of resources and save time spent creating and annotating long dialogues. Additionally, this enables the development of multi-lingual benchmarks for the research community to use.",
            "score": 0.6160876444544103,
            "section_title": "Arabic Task-Oriented DS Dataset",
            "char_start_offset": 12950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "229478912"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "227231115"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "229478912"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "227231115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.491943359375
        },
        {
            "corpus_id": "222310410",
            "title": "Multilingual Argument Mining: Datasets and Analysis",
            "text": "translation step increases prediction run-time, and on our data also tends to perform worse.\n\nWe examine two text sources for performance evaluation on non-English texts. The first one is a \"pseudo test\" set -an automatic translation of an English evaluation set for a task. While such texts can be easily generated, it is not clear how well they represent \"real\" texts, authored by humans. Hence, we also examine human-authored texts, in several non-English languages, collected via crowdsourcing specifically for this work. Both datasets are released as part of this work. 1 When translating the evaluation set, either automatically or by a human translator, one would like to assume that the initial label of the English text is maintained after translation. While this is often the case, we show that this assumption becomes more dubious as the argument mining task becomes more complex and subjective, as well as when the original labels are not clearly agreed upon.\n\nIn summary, the main contributions of this paper are: (1) a comparative analysis of the translatetrain approach on three central argument mining tasks using different subsets of languages, showing that training on more data helps, but that, in some cases, training on related languages is sufficient; (2) multilingual benchmark datasets for the three tasks;\n\n(3) an analysis of the three tasks, showing how well labels are preserved across translation, and the impact that has on the success of the translate-train approach.",
            "score": 0.6157456394627471,
            "section_title": "Introduction",
            "char_start_offset": 2085,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "277993848",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "text": "The remarkable capabilities of large language models (LLMs) have transformed natural language processing (NLP), with applications spanning diverse domains and languages worldwide [Ouyang et al., 2022, Sanh et al., 2022, OpenAI, 2023, Touvron et al., 2023, Anil et al., 2023, Mesnard et al., 2024, Yang et al., 2024, DeepSeek-AI et al., 2025]. As these technologies increasingly serve users across linguistic boundaries, robust multilingual evaluation becomes not merely academic but essential [Zhu et al., 2024, Qin et al., 2025]. Despite advances in multilingual pre-training, significant disparities persist in how models perform across different languages, writing systems, and cultural contexts. Current evaluation frameworks often fail to capture this diversity, with benchmarks predominantly focused on high-resource languages and evaluation methodologies that inadequately represent realworld use cases in global contexts [Hada et al., 2024]. This paper presents a comprehensive analysis of multilingual benchmarking practices to understand past approaches, assess present correlations with human judgments, and chart future directions for more equitable, representative, and effective multilingual evaluation of language technologies. \n\nTo provide a comprehensive analysis, we first establish a robust dataset of multilingual benchmarks by collecting and annotating papers from the arXiv cs.CL category (2021-2024) (Section 3). After filtering 370,000 papers through both automated LLM-based screening and expert review, we identify 2,024 relevant studies containing multilingual benchmarks from 148 countries. Our analysis follows a temporal framework organized around three key questions: \n\n1. PAST: What benchmarks do we currently have? (Section 4) We document historical trends, revealing significant disparities in language representation, the evolution of task types, translation methods, and more. Our analysis reveals several key patterns: despite deliberate exclusion of English-only benchmarks, English remains the most represented language, with high-resource languages dominating while low-resource languages are underrepresented. Most benchmarks (61.4%) use original language content rather than translations, with human translations accounting for just 13.2%.",
            "score": 0.6156067359796065,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 199,
                    "end": 218,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 295,
                    "end": 314,
                    "matchedPaperCorpusId": "259164635"
                },
                {
                    "start": 929,
                    "end": 948,
                    "matchedPaperCorpusId": "261822638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "216914383",
            "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
            "text": "More generally, most of these datasets are limited to relatively close languages and comparable corpora. \n\nLack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are non-English counterparts for more challenging tasks such as question answering (Cui et al., 2019;Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD   (Artetxe et al., 2020b), MLQA (Lewis et al., 2019) and TyDi QA (Clark et al., 2020) cover a wider set of languages, but a comprehensive benchmark that evaluates multilingual representations on a diverse set of tasks-in the style of GLUE (Wang et al., 2018)-and languages has been missing until very recently. The contemporaneous XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) benchmarks try to close this gap, but they are still restricted to languages where existing labelled data is available. Finally, an additional issue is that a large part of these benchmarks were created through translation, which was recently shown to introduce artifacts (Artetxe et al., 2020a). \n\nWe present a summary of the methodological issues discussed in Table 2.",
            "score": 0.6154802964751442,
            "section_title": "Evaluation practices",
            "char_start_offset": 28963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 385,
                    "matchedPaperCorpusId": "67856005"
                },
                {
                    "start": 412,
                    "end": 433,
                    "matchedPaperCorpusId": "153313061"
                },
                {
                    "start": 454,
                    "end": 475,
                    "matchedPaperCorpusId": "43943649"
                },
                {
                    "start": 504,
                    "end": 527,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 557,
                    "end": 579,
                    "matchedPaperCorpusId": "21732778"
                },
                {
                    "start": 586,
                    "end": 609,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 620,
                    "end": 642,
                    "matchedPaperCorpusId": "21732778"
                },
                {
                    "start": 813,
                    "end": 838,
                    "matchedPaperCorpusId": "44077554"
                },
                {
                    "start": 886,
                    "end": 909,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1000,
                    "end": 1018,
                    "matchedPaperCorpusId": "202121521"
                },
                {
                    "start": 1018,
                    "end": 1035,
                    "matchedPaperCorpusId": "202712637"
                },
                {
                    "start": 1123,
                    "end": 1146,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1186,
                    "end": 1206,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 1360,
                    "end": 1379,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31298828125
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "No 43.46 Yes 42.24  (2021). First row is the reported XGLM results, and the second is our reproduction of their results to validate our multilingual evaluation setup. Last two rows show that our multilingual model matches the XGLM results. \n\nEnglish-only evaluation. We first evaluate our multilingual model on the same set of English benchmarks we have used previously, in Table 6. \n\nMultilinguality significantly lowers accuracy on the English benchmark, which is in line with the results from Lin et al. ( 2021). \n\nMultilingual evaluation. Zero-shot multilingual evaluation is more challenging to setup because it requires writing new prompts for each new language. Therefore, instead of manually writing prompts for each language, we follow the strategy proposed by Lin et al. ( 2021), using English prompts for non-English examples-this can be viewed as cross-lingual zero-shot generalization. They validated this strategy by demonstrating its ability to achieve zero-shot performance on par with (and sometimes even better than) humanwritten language-specific prompts. This strategy also demonstrates cross-lingual abilities. \n\nWe evaluate on XNLI (Conneau et al., 2018), a multilingual NLI dataset that covers 8 of the languages we use for training. Our evaluation is different from the zero-shot evaluation of the XTREME benchmark (Hu et al., 2020). XTREME first finetunes the model on the English training data of each downstream task, then evaluates it on the non-English dataset, attempting cross-lingual generalization. Our evaluation avoids any finetuning, and instead relies entirely on zero-shot generalization. Results. Language-specific scaling laws. To explore how scale influences multilinguality, we train a wider range of models (i.e. 0.3-6B parameters) on a larger corpus of more than 300B tokens of text drawn from a variety of languages (Lauren\u00e7on et al., 2022).",
            "score": 0.6153206304081191,
            "section_title": "Embedding Norm Average EAI Results",
            "char_start_offset": 15670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1886
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "204734128",
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "text": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
            "score": 0.6149261334429743,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "276647924",
            "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs",
            "text": "Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, Zhang et al. (2024) introduced MELA to assess multilingual linguistic acceptability, while Mueller et al. (2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021b), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020b) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018b), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2020). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020;Ladhak et al., 2020). \n\nWith the rise of multitask instruction finetuning, multilingual instruction datasets like Supernatural Instructions (Wang et al., 2022) andxP3 (Muennighoff et al., 2023) have further expanded the scope of multilingual evaluation. Despite these advancements, a critical gap remains in evaluating multilingual conceptual understanding, particularly in the format of conceptual minimal pairs. Our work addresses this gap by introducing XCOMPS, a multilingual benchmark specifically designed to evaluate conceptual understanding across languages.",
            "score": 0.6141693729926606,
            "section_title": "Multilingual Evaluation Benchmark",
            "char_start_offset": 10381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "265213331"
                },
                {
                    "start": 369,
                    "end": 390,
                    "matchedPaperCorpusId": "218470403"
                },
                {
                    "start": 532,
                    "end": 555,
                    "matchedPaperCorpusId": "231740666"
                },
                {
                    "start": 564,
                    "end": 581,
                    "matchedPaperCorpusId": "264145744"
                },
                {
                    "start": 831,
                    "end": 849,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 860,
                    "end": 880,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 1059,
                    "end": 1082,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1091,
                    "end": 1110,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 1121,
                    "end": 1141,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1276,
                    "end": 1298,
                    "matchedPaperCorpusId": "216868259"
                },
                {
                    "start": 1298,
                    "end": 1318,
                    "matchedPaperCorpusId": "222177239"
                },
                {
                    "start": 1438,
                    "end": 1461,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "247358286",
            "title": "PersianQuAD: The Native Question Answering Dataset for the Persian Language",
            "text": "The SberQuAD [24] is a Russian native QA dataset and contains above 50,000 samples. The DRCD [25] is a native Chinese QA dataset, consists of 30,000+ questions posed by the annotators on 10,014 paragraphs extracted from 2,108 Wikipedia articles. KorQuAD [26] is a Korean QA dataset and PIAF [27] is a French QA dataset, consisting of 70,000+ and 3835 question-answer pairs, respectively. Since large-scale QA datasets in languages other than English rarely exist and building native QA datasets is time-and cost-consuming, developing QA systems for these languages is challenging. Cross-lingual QA datasets have been developed to address this challenge. These datasets are typically deployed in training the QA model on one language and transfer the model to another language. It has been shown that the resulted models perform well in the zero-shot setting [28]. MLQA [29] is a cross-lingual QA dataset developed for seven languages: English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. It consists of over 12,000 samples in English and 5,000 samples in other languages. MMQA [30] is a parallel QA dataset in Hindi and English, containing 5,000+ parallel instances. BiPar [31] is another parallel QA dataset in English and Chinese. XQA [32] consists of a training set in English and the development and test sets in eight languages: English, French, German, Portuguese, Polish, Chinese, Russian, Ukrainian, and Tamil. XQuAD contains 1190 instances from SQuAD, along with their translations in 10 languages. \n\nThere are only a few works on building open-domain QA datasets for the Persian language. Abadani et al. [33] automatically translated SQuAD into the Persian language and built a translated Persian QA dataset, called ParSQuAD. They created two versions of ParSQuAD: ParSQuAD(manual) and ParSQuAD(automatic), with 25000 and 70000 instances, respectively.",
            "score": 0.6140923394782766,
            "section_title": "B. OTHER LANGUAGES",
            "char_start_offset": 10758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "209439717"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "218974030"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "21711688"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "202767252"
                },
                {
                    "start": 1249,
                    "end": 1253,
                    "matchedPaperCorpusId": "196174566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265625
        },
        {
            "corpus_id": "212657414",
            "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
            "text": "One approach to creating multilingual data is to translate an English corpus into other languages, as in XNLI (Conneau et al., 2018). However, the process of translation-including human translation-tends to introduce problematic artifacts to the output language such as preserving source-language word order as when translating from English to Czech (which allows flexible word order) or the use of more constrained language by translators (e.g. more formal). The result is that a corpus of so-called Translationese may be markedly different from purely native text (Lembersky et al., 2012;Volansky et al., 2013;Avner et al., 2014;Eetemadi and Toutanova, 2014;Rabinovich and Wintner, 2015;Wintner, 2016). Questions that originate in a different language may also differ in what is left underspecified or in what topics will be discussed. For example, in TYDI QA, one Bengali question asks What does sapodilla taste like?, referring to a fruit that is unlikely to be mentioned in an English corpus, presenting unique challenges for transfer learning. Each of these issues makes a translated corpus more English-like, potentially inflating the apparent gains of transfer-learning approaches. Two recent multilingual QA datasets have used this approach. MLQA (Lewis et al., 2019) includes 12k SQuAD-like English QA instances; a subset of articles are matched to six target language articles via a multilingual model and the associated questions are translated. XQuAD (Artetxe et al., 2019) includes 1,190 QA instances from SQuAD 1.1, with both questions and articles translated into 10 languages. 9 Compared to TYDI QA, these datasets are vulnerable to Translationese while MLQA's use of a model-in-themiddle to match English answers to target language answers comes with some risks: (1) of selecting answers containing machine-translated Wikipedia content; and (2) of the dataset favoring models that are trained on the same parallel data or that use a similar multilingual model architecture.",
            "score": 0.6101880190437519,
            "section_title": "Why not translate?",
            "char_start_offset": 9370,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 590,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 612,
                    "end": 631,
                    "matchedPaperCorpusId": "1389695"
                },
                {
                    "start": 631,
                    "end": 660,
                    "matchedPaperCorpusId": "15699208"
                },
                {
                    "start": 660,
                    "end": 689,
                    "matchedPaperCorpusId": "940724"
                },
                {
                    "start": 689,
                    "end": 703,
                    "matchedPaperCorpusId": "45820866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31396484375
        },
        {
            "corpus_id": "269004468",
            "title": "How Many Languages Make Good Multilingual Instruction Tuning? A Case Study on BLOOM",
            "text": "We evaluate the instruction-tuned models on three multilingual benchmarks.XCOPA (Ponti et al., 2020) is a multilingual dataset for causal commonsense reasoning in 11 languages XStoryCloze (Lin et al., 2021) is a multilingual dataset derived from the English StoryCloze dataset (Mostafazadeh et al., 2017) for commonsense reasoning in the story, translated into 10 non-English languages.The test involves a system selecting the appropriate ending for a four-sentence story.XWinograd (Tikhonov & Ryabinin, 2021) is a multilingual compilation of Winograd Schemas (Levesque et al., 2012) available in six languages, designed for assessing cross-lingual commonsense reasoning abilities.We utilize zero-shot evaluation techniques facilitated by the Language Model Evaluation Harness (lm-evaluation-harness) (Gao et al., 2023).Different models, trained with progressively added languages, are evaluated on these benchmarks using accuracy (%) as the evaluation metric.",
            "score": 0.6095293429385952,
            "section_title": "Benchmarks and Evaluation",
            "char_start_offset": 11221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 820
                },
                {
                    "start": 820,
                    "end": 960
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 99,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 277,
                    "end": 304,
                    "matchedPaperCorpusId": "13746570"
                },
                {
                    "start": 482,
                    "end": 508,
                    "matchedPaperCorpusId": "235606305"
                },
                {
                    "start": 560,
                    "end": 583,
                    "matchedPaperCorpusId": "15710851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "265294831",
            "title": "TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes",
            "text": "Dataset: We initially translated two datasets, Alpaca-52K-GPT4 and Dolly-15K, into 132 languages using the Google Cloud Translation service. Due to computational resource constraints, we focused our experiment on three low-resource languages-Sanskrit, Nepali, and Maithili-and one highresource language, Persian. Aware of the risk of translationese, a common issue in machine translations where the output deviates from native linguistic norms through literal translations, foreign syntax, and odd phrases (Borah, 2021;Koppel & Ordan, 2011), we conducted a manual evaluation for translation quality. We selected a sample of 1000 sentences from each language and assessed them using the BLEU score, calculated with ScarceBLEU (Post, 2018), ChrF (Popovi\u0107, 2017), and the Translation Error Rate (TER) (Snover et al., 2006), by translating from English to the target language and back (en-xx-xx-en). \n\nIn our multilingual fine-tuning process, we employed a chain-of-thought approach to transform the translated instruction dataset. This method involves breaking down the complex problem of crosslinguality into manageable reasoning steps, akin to a chain of thought. By combining the English dataset with another language, we crafted question-answer pairs called as TaCo datasets. Here, instructions are given in a non-English language, while outputs are provided in both English and the original language. The process unfolds by first translating the instruction into English, formulating the response in English, and then translating this back into the non-English language. We avoid translating the input from the target language (xx) into English in our output. This method fosters the model's comprehension and utilization of the original language nuances. By combining the Alpaca-52K and Dolly-15K datasets, we have curated a comprehensive collection of 67K questionanswer prompts. The example of a TaCo data point is given in Fig. 1",
            "score": 0.6095136442657678,
            "section_title": "EXPERIMENT",
            "char_start_offset": 7270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 744,
                    "end": 759,
                    "matchedPaperCorpusId": "12942757"
                },
                {
                    "start": 798,
                    "end": 819,
                    "matchedPaperCorpusId": "263887736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "196174566",
            "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
            "text": "In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish,  Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. \n\nThe experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is. Based on the results, we further discuss potential improvement for cross-lingual OpenQA systems. \n\nWe will release the dataset and baseline systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding.",
            "score": 0.6093905206193642,
            "section_title": "Introduction",
            "char_start_offset": 2104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 515,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46484375
        },
        {
            "corpus_id": "258557740",
            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
            "text": "lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are:\n\n\u2022 We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese.\n\n\u2022 We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages.\n\n\u2022 We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors.",
            "score": 0.6085050844476354,
            "section_title": "Introduction",
            "char_start_offset": 3611,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "220936592",
            "title": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning",
            "text": "Evaluation Data To ensure high quality evaluation of languages covered in ML50, we include publicly available, widely used evaluation sets. We source these evaluation datasets from translation workshops such as WMT, IWSLT, WAT, and other published research works. We follow the evaluation protocol, including tokenization, used for each of these evaluation sets, to ensure our results are comparable with existing work. We release these scripts to make it easier for others. scratch We compare to bilingual finetuning (BL-FT) and multilingual translation from scratch (ML-SC). On average, multilingual finetuning (ML-FT) improves 2.61 BLEU in Many-to-one (N\u21921), \u22120.47 BLEU in one-to-Many (1\u2192N), and \u22120.15 and \u22120.35 BLEU for to-English and from-English respectively in Many-to-Many (N\u2194N) settings compared to the strongest baselines ML-SC many-to-one, BL-FT, and ML-SC many-to-one and BL-FT finetuning (combined baselines for ML-FT many-to-many) respectively. \n\nother datasets such as OPUS100, we choose to use high quality existing evaluation datasets rather than use part of the training data as evaluation. This is because training data, particularly for low resource languages, is often very noisy and unreliable.",
            "score": 0.6081317700240547,
            "section_title": "ML50 Benchmark",
            "char_start_offset": 21127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "246294502",
            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
            "text": "2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research. 4) Our results and additional analyses take stock of the current gaps and challenges in cross-lingual V&L research. 5) To further facilitate replicable research in this area, we re-implement the existing multilingual V&L pretrained encoders in a unified framework (VOLTA; Bugliarello et al. 2021), which also provides access to five English V&L BERTs and 12 V&L tasks. We provide data and code for the evaluation of multilingual V&L models at https://iglue-benchmark.github.io/.",
            "score": 0.6074718635293049,
            "section_title": "Introduction",
            "char_start_offset": 4870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "220871404",
            "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
            "text": "Cross-Lingual Modeling Recent work trains cross-lingual representations with unsupervised language modeling over many languages, including Multilingual BERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and Multilingual T5 (Xue et al., 2021). Transfer learning techniques are often applied to these cross-lingual representations to overcome the dearth of non-English data (Cui et al., 2019a;Hsu et al., 2019;Lee and Lee, 2019;Kumar et al., 2019). Recent investigations into cross-lingual modeling have revealed \"translation artifacts\" in datasets where machine translation systems are used, or human translation tasks are not carefully curated (Artetxe et al., 2020a;Wintner, 2016;Rabinovich and Wintner, 2015). \"Translationese\" results in hidden linguistic cues in translated text that render the task easier than a natural translation. \n\nEnglish QA Resources A majority of question answering research focuses on English, which of-fers ample selection of evaluation datasets, including SQuAD (Rajpurkar et al., 2016), Trivi-aQA (Joshi et al., 2017), and Natural Questions (Kwiatkowski et al., 2019). Open Domain QA, pioneered by Green et al. (1986), is the task of answering open questions using external knowledge sources. A common approach is to combine retrieval and extractive techniques (Chen et al., 2016(Chen et al., , 2017;;Dhingra et al., 2017;Cui et al., 2017). \n\nMonolingual QA Resources Non-English question answering resource options remain comparatively rare, with most options spanning only one other language, and rarely low-resource languages. DuReader (He et al., 2018), CMRC (Cui et al., 2019b), andDRCD (Shao et al., 2018) all offer high-quality Chinese QA datsets. Similarly, XCMRC (Liu et al., 2019b) and BiPar (Jing et al., 2019) present parallel, cross-lingual QA dataset between English and Chinese.",
            "score": 0.6067097780810498,
            "section_title": "Related Work",
            "char_start_offset": 6362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1832
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 178,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 186,
                    "end": 208,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "202712637"
                },
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "174802358"
                },
                {
                    "start": 651,
                    "end": 674,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 674,
                    "end": 688,
                    "matchedPaperCorpusId": "45820866"
                },
                {
                    "start": 1000,
                    "end": 1024,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1036,
                    "end": 1056,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1080,
                    "end": 1106,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1300,
                    "end": 1318,
                    "matchedPaperCorpusId": "6360322"
                },
                {
                    "start": 1318,
                    "end": 1340,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1340,
                    "end": 1361,
                    "matchedPaperCorpusId": "6529193"
                },
                {
                    "start": 1361,
                    "end": 1378,
                    "matchedPaperCorpusId": "9205021"
                },
                {
                    "start": 1578,
                    "end": 1595,
                    "matchedPaperCorpusId": "3662564"
                },
                {
                    "start": 1602,
                    "end": 1626,
                    "matchedPaperCorpusId": "52984852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.301025390625
        },
        {
            "corpus_id": "271953940",
            "title": "How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data",
            "text": "NTREX-128. NTREX2 (Federmann et al., 2022) is initially proposed as an evaluation dataset, expanding multilingual testing for translation from English into 128 target languages, which consists of 1997 samples per language and mainly focus on the News domain. Given the multi-parallel organization of NTREX data, we can easily build arbitrary pairwise data across 128 languages. In this paper, we leverage NTREX to create our finetuning datasets and conduct experiments to highlight the big impact of such a tiny amount of data. \n\nEuroparl-8. Europarl3 (Koehn, 2005) consists of 20 English-centric language pairs from the proceedings of the European Parliament, with sizes ranging from 399K to 2M. A characteristic of Europarl is that part of the samples are multi-way aligned. In this paper, we select the most resource-rich 8 languages, i.e., EN, DA, DE, ES, FI, FR, IT, and NL, to mine a fully multi-parallel dataset named Europarl-8 via aligning multi-way sentences with the same English part. This results in about 1.2M fully multi-parallel data instances, where each sentence has 7 counterparts in other languages. EC30. To ensure a more diverse and inclusive large-scale evaluation, we follow Tan and Monz (2023); Wu and Monz (2023) and use the EC30 dataset, which is built from WMT (Bojar et al., 2017) and OPUS (Tiedemann, 2012) corpora. EC30 comprises 61 million English-centric bilingual sentences for training, encompassing 30 non-English languages with diverse resource levels (High: 5M, Medium: 1M, Low: 100K). Each resource group includes languages from 5 families with multiple writing systems. \n\nEvaluation Benchmark. For all of the experiments in this paper, we evaluate translations via the Flores-101 benchmark (Goyal et al., 2022). Flores comprises 3001 sentences sourced from English Wikipedia, which covers a variety of topics and domains and is translated into 101 languages by professional translators. We use dev and devtest as the validation and test dataset, consisting of 997 and 1012 samples, respectively.",
            "score": 0.60650537246116,
            "section_title": "Datasets",
            "char_start_offset": 6296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "270199507",
            "title": "Multilingual Text Style Transfer: Datasets & Models for Indian Languages",
            "text": "We explore two cross-lingual alternatives that bypass the requirement for manually created multilingual datasets. Firstly, we employ English sentences from the parallel dataset, machine-translate them into all the respective languages, and use these translated texts for training (En-IP-TR-Train). Secondly, we take the English output generated by the model trained on a parallel English dataset and machine-translate it into the target languages (En-OP-TR). These cross-lingual approaches offer insights into multilingual text style transfer for the case when no data is available in the target languages.",
            "score": 0.6064824292345102,
            "section_title": "Cross-Lingual Style Transfer",
            "char_start_offset": 13986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265869140625
        },
        {
            "corpus_id": "235097287",
            "title": "Practical Transformer-based Multilingual Text Classification",
            "text": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation3 as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples. In the multi-all setting, we translate data from every language into every other language, yielding N 3 Data \n\nWe choose sentiment analysis and hate speech detection as evaluation tasks due to their relevance to industry applications and the availability of multilingual datasets. An overview of the datasets is shown in Table 2.",
            "score": 0.6064286051589776,
            "section_title": "Data Augmentation",
            "char_start_offset": 6411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70166015625
        },
        {
            "corpus_id": "271270611",
            "title": "INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages",
            "text": "In this section, we provide a catalog of the datasets constituting this benchmark, complete with a thorough exposition of their original accessibility and the modifications we have implemented. These datasets are either pre-existing or have been released as part of this work. Following is a detailed description of each dataset. \n\n1. Hindi SQuAD: This dataset is a translated version of the original SQuAD (Rajpurkar et al., 2016) into Hindi. It consists of nearly 5,000 instances, translated using the Google Translate API. We translated that from Hindi to other Indic languages. \n\n2. XQuAD: XQuAD (Cross-lingual Question Answering Dataset) (Artetxe et al., 2019) is a benchmark for evaluating cross-lingual question answering performance. It consists of 240 paragraphs and 1,190 question-answer pairs sourced from the SQuAD v1.1 development set (Rajpurkar et al., 2016), with professional translations into ten languages. However, we use the version from (Singh et al., 2024), which includes manual translations for all Indic languages. \n\n3. ChaII Dataset (Thirumala and Ferracane, 2022): This question-answering dataset features context-question-answer triples in Hindi and Tamil, gathered directly without translation. Created by expert data annotators who are native speakers, the dataset presents a realistic information-seeking task focused on predicting answers to genuine questions about Wikipedia articles. It was used in a Kaggle challenge and includes 1104 questions in Hindi and Tamil, we used the Hindi part of the data and translated it to 10 other Indian languages. \n\n4. Indic QA (Doddapaneni et al., 2022): This dataset is a manually curated cloze-style reading comprehension dataset designed for evaluating question-answering models in 10 Indic languages Since this dataset doesn't have Gujarati translation we translated it from Hindi to Gujarati and validated the translation as described in the [2.2] section. \n\n5. MLQA (Lewis et al., 2019): MLQA (Multi-Lingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.",
            "score": 0.6062748214051965,
            "section_title": "Datasets",
            "char_start_offset": 7393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39794921875
        },
        {
            "corpus_id": "276618526",
            "title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
            "text": "We provide the prompts used during the data extraction in Table 13 and Table 14. This dataset is a multiple-choice question answering dataset focused on commonsense knowledge. It is designed to evaluate a model's ability to understand and apply commonsense reasoning to answer questions correctly. The dataset is widely used in the natural language processing domain to benchmark the performance of models on tasks requiring commonsense understanding. \n\nTask Explanation: The task involves providing a question along with multiple answer choices, where the input is the question and the list of possible answers. The output is the correct answer choice. The task is evaluated based on the accuracy of the model's predictions compared to the correct answers provided in the dataset. \n\nGPT-4 12 Batch CoT Acc 86 \n\n2301.08745 8 Flores-101 De \u2192 En Dataset Summary: This dataset is a multilingual translation benchmark designed to evaluate machine translation systems across a wide range of languages. It covers 101 languages, providing a comprehensive resource for assessing translation quality and performance. The dataset is intended for tasks involving language translation, with a focus on both high-resource and low-resource languages. Key characteristics include its diverse language pairings and standardized evaluation metrics, which aim to facilitate consistent and fair comparisons of translation models. \n\nTask Explanation: The primary task associated with this dataset is machine translation. The input consists of text in one language, and the output is the translated text in another language. For this specific subset, the task involves translating text from German (De) to English (En). Evaluation of the task is typically conducted using metrics such as BLEU, which measures the accuracy and fluency of the translated output compared to reference translations. Subset Description: The De\u2192En subset specifically focuses on translations from German to English. This subset is used to evaluate the performance of translation models in converting German text into English, providing insights into the model's ability to handle this particular language pair. \n\nGPT-4 0 Direct BLEU 46 \n\n2303.07992 3 KQApro xx Dataset Summary: This dataset is a large-scale knowledge-based question answering dataset designed to evaluate the ability of models to understand and reason over complex questions.",
            "score": 0.605168004710452,
            "section_title": "F Prompts Used during Data Extraction",
            "char_start_offset": 32858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2166
                },
                {
                    "start": 2169,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1922607421875
        },
        {
            "corpus_id": "267617144",
            "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
            "text": "Improved performance was shown when smaller cross-lingual models were fine-tuned with data generated by LLMs. Some recently released datasets focus on specialized language domains such as law (Niklaus et al., 2023), education (Zhang et al., 2023c), or healthcare (Wang et al., 2023). \n\nThese corpora often suffer from inadequate data quality and require extensive cleaning (Abadji et al., 2022;Kreutzer et al., 2022). Task-specific datasets (Ponti et al., 2020;Conneau et al., 2018) are often smaller in scale but offer higher quality data targeted at a specific model capability such as cross-lingual understanding and transfer learning. This type of data is crucial for evaluating and enhancing the performance of models in diverse linguistic contexts. Such datasets are aggregated in multilingual benchmarks (Hu et al., 2020;Ruder et al., 2021;Cahyawijaya et al., 2021). Recently, (Ruder et al., 2023) released XTREME-UP, which covers data in 88 under-represented languages across 9 user-centric technologies. \n\nNo Language Left Behind (NLLB-Team et al., 2022) open-sourced bitext, mined bitext, and data generated using back-translation in 200+ languages specifically for text-to-text translation. While Seamless4MT (Barrault et al., 2023) released the metadata of SeamlessAlign, an open multimodal translation dataset, there are relatively fewer works for data creation/curation in lowresource languages. (Cahyawijaya et al., 2023) introduced NusaCrowd, a standardized collection of 137 datasets covering 19 Indonesian local languages in text, speech, and image modalities. Our work differs from previous datasets as we create a large-scale instruction-tuning dataset spanning hundreds of different tasks, yet retain high quality by involving human annotation and rigorous quality control across the entire data creation process.",
            "score": 0.6051268995904566,
            "section_title": "B.1 Multilingual datasets",
            "char_start_offset": 38947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 394,
                    "matchedPaperCorpusId": "246015576"
                },
                {
                    "start": 811,
                    "end": 828,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 847,
                    "end": 872,
                    "matchedPaperCorpusId": "233289621"
                },
                {
                    "start": 1410,
                    "end": 1436,
                    "matchedPaperCorpusId": "254853901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "265150273",
            "title": "Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study",
            "text": "Pre-trained multilingual models: We consider the base-cased versions of two multilingual language models pre-trained on masked language model (MLM) objectives: mBERT, i.e., a multilingual variant of BERT (Devlin et al., 2019), and XLM-R, a RoBERTa-based multilingual model (Conneau and Lample, 2019). Additionally, we use the pre-trained multilingual sentence encoder LaBSE (Feng et al., 2022) that maps sentences to 768-dimensional single vector representations. \n\nTransfer strategies: To transfer from the English ID training data to non-English test samples, we use 3 widely adopted strategies (Fig. 1, top row): \n\n(1) zero-shot: finetunes the multilingual model on the English ID training and validation set, followed by directly evaluating the OOD test samples in the non-English languages. \n\n(2) translate-test: finetunes the multilingual model on the English ID training and validation datasets. However, prior to making predictions for OOD test samples, the samples are translated into English. \n\n(3) translate-train: first translates the English ID training and validation datasets to the target OOD test language. Subsequently, the multilingual model is trained on this translated data to then make predictions for the original, untranslated, OOD test samples in that non-English language. \n\nNote that in case where both translate-train and CAD are used, the English CAD training and validation data are translated to the target OOD test language. For both translate-test and translatetrain, we use OpenAI's ChatGPT-turbo (v0301) (Ouyang et al., 2022) as the LLM to translate from English to non-English languages and vice versa. We adopt OpenAI's default parameter values. See Appendix A for translation prompts.",
            "score": 0.6047428214525371,
            "section_title": "Zero-shot cross-lingual transfer",
            "char_start_offset": 13405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1723
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 273,
                    "end": 299,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 374,
                    "end": 393,
                    "matchedPaperCorpusId": "220347683"
                },
                {
                    "start": 1540,
                    "end": 1561,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37158203125
        },
        {
            "corpus_id": "258967462",
            "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
            "text": "Evaluated ChatGPT and similar LLMs for multilingual natural language processing tasks. Exhibited inferior performance compared to previous models, indicating the necessity for additional research. (Berant et al., 2013) dataset. In these datasets, the task is to answer a question asked in English by leveraging the contents of Wikipedia or the Web. Moreover, we also conduct a comprehensive human evaluation on the EfficientQA dataset (Min et al., 2021), which is also derived from the NQ-Open dataset. Based on our extensive analysis, we observe several key findings in the EfficientQA dataset, such as many questions are time-sensitive, while many answers contain outdated gold answers. \n\nReading Comprehension: We use the RACE dataset (both Middle and Hard versions) (Lai et al., 2017) to evaluate ChatGPT for the reading comprehension task. The Race dataset is constructed from English reading comprehension exams designed for middle and high school students in China. In addition, we use the SQuAD 2.0 dataset (Rajpurkar et al., 2018) for this task. \n\nCommonsense Reasoning: To evaluate the reasoning capability of ChatGPT, we use the following datasets: PIQA (Bisk et al., 2020) Neural Machine Translation: We select various languages (English (en), French (fr), German (de), Romanian (rn), Kazakh (kk)) based on different scenarios to evaluate the performance of Chat-GPT in language translation. Similar to (Chowdhery et al., 2022), for English-centric language pairs, we use the WMT'14 (Bojar et al., 2014) for English-French translation in high-resource scenarios, WMT'16 (Bojar et al., 2016) English-German in medium-resource while English-Romanian for low-resource scenarios; WMT'19 (Barrault et al., 2019) for direct translation between non-English languages: German-French and for extremely lowresource language pairs: English-Kazakh. \n\nCode Generation: We evaluate the coding ability of ChatGPT on the MBPP (Austin et al., 2021) and the HumanEval (Chen et al., 2021a) datasets.",
            "score": 0.6035844369599951,
            "section_title": "Lai et al. (2023)",
            "char_start_offset": 43353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1848
                },
                {
                    "start": 1851,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 218,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 1165,
                    "end": 1184,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1495,
                    "end": 1515,
                    "matchedPaperCorpusId": "15535376"
                },
                {
                    "start": 1582,
                    "end": 1602,
                    "matchedPaperCorpusId": "14421595"
                },
                {
                    "start": 1695,
                    "end": 1718,
                    "matchedPaperCorpusId": "201741133"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10614013671875
        },
        {
            "corpus_id": "276249513",
            "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
            "text": "This study highlights the importance of expanding image captioning evaluation to include multilingual and multicultural research, encouraging more inclusive frameworks in this field. Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models. \n\nWe also propose a finetuning strategy to better leverage and learn from both multi-cultural data and human preferences, and test our models on a set of different datasets and tasks. Our findings show that multilingual models trained with the same amount but with less English-specific data perform equally well on English tasks, while excelling in multilingual and multicultural ones. This reveals the potential of multilingual models to generalize across languages, making them more versatile assets. Additionally, our finetuning approach significantly boosted the model's ability to handle complex linguistic challenges, such as quantifiers, plurality, and numeric inconsistencies, highlighting its adaptability to more intricate language patterns. \n\nFurther to machine translation of English data, we also propose a strategy to adapt multilingual datasets from other tasks to support captioning evaluation. The integration of natively multilingual and multicultural datasets into both training and evaluation processes mitigates cultural information loss, reinforcing the reliability of our proposed pipeline for training and evaluating multilingual CLIP models, and making them effective tools for real-world multilingual and multicultural evaluation. \n\nOverall, our work contributes to multilingual captioning evaluation, both in terms of modelling and benchmarking. We hope it will inspire and support further work in this under-researched field.",
            "score": 0.6035584650444683,
            "section_title": "Conclusions",
            "char_start_offset": 30484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1752
                },
                {
                    "start": 1755,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1949
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "233241129",
            "title": "Adaptive Sparse Transformer for Multilingual Translation",
            "text": "Datasets. Models are evaluated on two widely used multilingual translation datasets at different scales. More details of these datasets are included in Appendix. \n\n\u2022 Public-24. This medium-scale dataset contains parallel corpora between English and 24 languages, collected from public sources such as WMT shared tasks (Liu et al., 2020). It provides O2M and M2O translations. \n\n\u2022 OPUS-100. This is a large-scale multilingual translation dataset covering 100 languages (Zhang et al., 2020). It serves for M2M translations. \n\nBaselines. We include the following strong baselines which are commonly used in multilingual translation. \n\n\u2022 Multilingual Transformer (Vaswani et al., 2017). A single Transformer model for multilingual translation shares all parameters among languages. \n\n\u2022 Multi-decoder Transformer (Sen et al., 2019;Kong et al., 2021). Similar to the multilingual Transformer, it has an encoder-decoder architecture, but replaces the decoder with multiple decoders. It is used for one-to-many translation, and target languages are clustered into families based on their proximity (Lewis, 2009). One decoder is shared by target languages from the same family, and each family has an exclusive decoder. \n\n\u2022 Multi-encoder Transformer. Similar to multidecoder Transformer, it replaces a single encoder with multiple encoders. It is used for many-to-one translation, and each encoder corresponds to one family of source languages. \n\n\u2022 Adapter based Transformer (Bapna and Firat, 2019) Transformer. Each language pair is routed to its corresponding adapter layers which are finetuned on the same data with other Transformer parameters frozen. \n\nEvaluation metrics. As we consider both translation quality and model efficiency, two metrics are used for model evaluation. BLEU measures the translation quality by comparing the predicted and reference translations. As for the efficiency, we report the decoding speed, i.e., the number of decoded tokens per second (tok/s) when one GPU is used with a batch size of 4096 tokens during inference.",
            "score": 0.6035490349235393,
            "section_title": "Experimental Setup",
            "char_start_offset": 18710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 488,
                    "matchedPaperCorpusId": "272727920"
                },
                {
                    "start": 659,
                    "end": 681,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 808,
                    "end": 826,
                    "matchedPaperCorpusId": "196170603"
                },
                {
                    "start": 1466,
                    "end": 1489,
                    "matchedPaperCorpusId": "202660912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "258865762",
            "title": "Cross-lingual QA: A Key to Unlocking In-context Cross-lingual Performance",
            "text": "Overall. For non-QA tasks, we measure the exact match (EM) of the generated answers, and for QA tasks, we measure the F1 score. As shown in Table 3, the Crosslingual QA prompt consistently outperforms Source Language prompting, indicating that using question-answer pairs in the target language within the in-context examples enhances the model's cross-lingual transferability. Surprisingly, most Cross-lingual QA prompting results even surpass those of Target Language prompting. These findings validate that our method is crucial in building alignment between the source and target languages through in-context learning. \n\nEffect of Translation Method. Both tested QA datasets are translated from the same English source: SQuAD v.1 (Rajpurkar et al., 2016). The validation set used for constructing in-context examples is human-annotated for MLQA, whereas we construct a quality-filtered machinetranslated set for XQuAD ( \u00a74.1). Despite the differences in translation method, the overall trend is similar; Crosslingual QA prompting consistently outperforms both Source and Target Language prompting. This suggests that using a few machine-translated QA pairs can significantly enhance a model's cross-lingual transferability without the need for expensive human annotations or translating entire examples. \n\nModel Performance at Scale. We compare the average F1 performance for the XQuAD task across model size variants. As shown in Figure 2, cross-lingual transferability increases with model size. We observe that Cross-lingual QA prompting becomes more effective at the 7B scale, with consistent increases on average from 1B to 7B, following the scaling law (Kaplan et al., 2020). Cross-lingual QA prompts show the following performance gains: +7.25 and +4.27 points for BLOOM and XGLM, respectively. \n\nAdditionally, we demonstrate that larger models are more effective at narrowing the performance gap between Target Language and Cross-lingual QA prompting. We attribute 1B 3B 7B to Target Language prompting being easier to achieve with larger models, as it only requires understanding the target language, whereas Cross-lingual QA prompting assesses the model's understanding of both the source and target languages.",
            "score": 0.603532897052836,
            "section_title": "Results",
            "char_start_offset": 9508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 734,
                    "end": 758,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "257038384",
            "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
            "text": "In this section, we investigate the multilingual capabilities of GPT models beyond translation. Specifically, we aim to assess how well the models perform on emerging reasoning tasks 15 for various languages compared to English. We are interested in understanding the degree of multilingual support that GPT models can offer given their translation performance. That is, can we use the translation performance as a proxy for the multilingual performance on other tasks? \n\nWe use MGSM Benchmark (Shi et al., 2022) which is a Multilingual Grade School Math (MGSM) arithmetic reasoning benchmark. The multilingual problems are human translated from the English dataset GSM8K which is Englishlanguage human-annotated grade-school math problem dataset. The dataset supports a set of ten languages other than English (EN): Bengali (BN), Chinese (ZH), French (FR), German (DE), Japanese (JA), Russian (RU), Spanish (ES), Swahili (SW), Telugu (TE), and Thai (TH). \n\nTable 11 presents the results on the MSGM benchmark. We first use Native-CoT, which uses prompts and CoT in the native language of each dataset. We observe that text-davinci-003 surpasses text-davinci-002 for all languages, highlighting the effectiveness of text-davinci-003 on multilingual tasks. The performance is especially high on EN, DE, FR and ES, while RU, JA and ZH exhibit lower scores than the Latin languages. The low-resource languages, however, achieve limited performance, indicating the need for better approaches to attain truly multilingual support. \n\nWe then use Translate-EN, which translates all prompts and CoT into English. We find that this setup enhances the performance on the non-Latin group (RU, JA and ZH) as well as the low-resource group (TH, TE, BN and SW), although the enhancements are not uniform across languages. Surprisingly, this setup shows a deterioration on the Latin languages. \n\nOur third and final setup is Translate-EN+, which is similar to Translate-EN, but keeps the template in English for all sentences instead of translating it.",
            "score": 0.6029780822172196,
            "section_title": "Multilingual Capabilities beyond Translation",
            "char_start_offset": 52642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 2037
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.256103515625
        },
        {
            "corpus_id": "277993848",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "text": "Most benchmarks (61.4%) use original language content rather than translations, with human translations accounting for just 13.2%. Discriminative tasks (66.5%) significantly outnumber generative ones (23.5%), with text classification remaining dominant while question answering shows dramatic growth following the emergence of LLMs in 2023. Dataset sizes have consistently expanded across all categories over the years, with an estimated cost of $11 million USD. Most content is sourced from publicly accessible domains, such as news (17%) and social media (13.3%), while high-value domains like healthcare and law remain underrepresented. evelopment efforts are concentrated in China, India, Germany, the UK, and the USA, with Europe emphasizing academic research and China and the USA demonstrating stronger academia-industry collaborations. \n\n2. PRESENT: What is the current status of multilingual evaluation? (Section 5) We present two key insights about multilingual LLM evaluation. First, users across different languages (English, Chinese, French, German, Spanish, and Russian) share remarkably similar interests, with writing tasks dominating (30-45%), followed by commonsense reasoning and programming tasks, suggesting universal priorities despite linguistic differences. Second, when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources.",
            "score": 0.6029165550626578,
            "section_title": "Introduction",
            "char_start_offset": 2166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "259202876",
            "title": "On Evaluating Multilingual Compositional Generalization with Translated Datasets",
            "text": "ates us to extend this idea to cross-lingual transfer of benchmarks and models. We propose to utilize rule-based machine translation (RBMT) to create parallel versions of MCWQ and yield a robust multilingual benchmark measuring CG. We build an MT framework based on synchronous context-free grammars (SCFG) and create new Chinese and Japanese translations of MCWQ questions, which we call MCWQ-R (Multilingual Compositional Wikidata Questions with Rule-based translations). We conduct experiments on the datasets translated with GT and RBMT to investigate the effect of translation method and quality on CG in multilingual and cross-lingual scenarios.\n\nOur specific contributions are as follows:\n\n\u2022 We propose a rule-based method to faithfully and robustly translate CG benchmarks.\n\n\u2022 We introduce MCWQ-R, a CG benchmark for semantic parsing from Chinese and Japanese to SPARQL.\n\n\u2022 We evaluate the translated dataset through both automatic and human evaluation and show that its quality greatly surpasses that of MCWQ (Cui et al., 2022).\n\n\u2022 We experiment with two different semantic parsing architectures and provide an analysis of their CG abilities within language and across languages.",
            "score": 0.6028960317391923,
            "section_title": "Introduction",
            "char_start_offset": 1797,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "220871404",
            "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
            "text": "Training and evaluation data for question answering (QA) is severely lacking outside of highresource languages like English. As unsupervised, transfer learning and zero/few-shot methods narrow the multilingual performance gap with English (Conneau et al., 2020;Lee and Lee, 2019;Cui et al., 2019a;Lewis et al., 2020), their real progress is hard to measure without challenging, realistic, and linguistically diverse evaluation sets. Existing multilingual QA datasets are realistic and challenging, but they lack linguistic diversity, comparable evaluation between languages, and are often limited to passages provided with the dataset (see Table 2). \n\nWe introduce Multilingual Knowledge Questions and Answers (MKQA) for evaluation of open-1 MKQA data and evaluation scripts are available at https://github.com/apple/ml-mkqa. domain question answering. MKQA selects 10k realistic English queries from the Natural Questions dataset (NQ, Kwiatkowski et al., 2019) and human translates them into 25 additional languages and dialects. Accompanying these query translations we replace NQ's passage embedded answer spans with high-quality, language-and retrieval-independent answer annotations, linked directly against Wikidata entities and a limited set of well-defined value types (numbers, dates, strings, etc.). 2  See one full example in Table 1. More flexible than existing multilingual datasets, MKQA's grading procedure ensures these labels are sufficient to evaluate any QA method, including knowledge graph and generative approaches. The objective of this evaluation set is to facilitate fair comparison between languages, without imposing assumptions on the underlying QA approach. We see MKQA as a useful tool enabling practitioners to benchmark a variety of multilingual open domain question answering methods against the widest range of available languages yet. Below, we discuss its central properties as an evaluation benchmark. \n\nRealistic & Reliable Annotations Of crucial importance to any evaluation set is (a) how well it reflects realistic, real-world settings, and (b) the reliability of its annotations.",
            "score": 0.6023461616388932,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 261,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 936,
                    "end": 961,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "257206042",
            "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
            "text": "Due to all these challenges and the ubiquity of language technologies that are used by large populations of non-English speaking users, addressing fairness and bias, taking into account diverse linguistic, socio-linguistic, and cultural factors, is of utmost importance. Interdisciplinary and multicultural teams are crucial to identifying, measuring, and mitigating harms caused by bias in multilingual models. Better evaluation benchmarks covering diverse linguistic phenomena and cultures will lead to better fairness evaluation. \n\nRegarding data collection, as discussed in Section 3.1, it would be prudent to avoid directly translating datasets for training or evaluation in applications where fairness is critical. As we have shown in this survey, it is not enough to collect datasets in multiple languages for measuring and mitigating bias, although even these are lacking for most languages worldwide. Zero-shot techniques that ignore the cultural nuances of a language should be used with care in fairness-critical applications, as linguistically similar languages may have different cultural values and vice versa. Finally, multilingual models and systems need to incorporate shared value systems that take into account diverse cultures, although some cultural differences may still go unacknowledged.",
            "score": 0.6022108323995312,
            "section_title": "Moving Towards Inclusive Systems in All Languages",
            "char_start_offset": 34171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "235683093",
            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
            "text": "In this section, we list the standard multilingual benchmarks we selected for evaluation. The main desideratum for our datasets is that training data is available in multiple languages. Ideally, all such Model XNLI PAWS-X XQuAD mBERT mT5-Base Table 1: English was out-performed () by other source languages in 5/6 experimental settings. training sets would be produced by humans (or verifiably high-quality). In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer. We will be mindful of this when drawing conclusions from our experiments. \n\nTo ensure that all language-specific subsets have the same size and informational content, we occasionally depart from the established way of using some of these datasets, as elaborated below. \n\nXNLI The Cross-lingual Natural Language Inference corpus (Conneau et al., 2018) consists of premise/hypothesis pairs that are either entailments, contradictions, or neutral. XNLI extends the English MultiNLI dataset (Williams et al., 2018) to 15 languages, including low-resource ones such as Swahili and Urdu; training sets are machine-translated, while the development and test sets are human-translated. \n\nPAWS-X The Cross-lingual Paraphrase Adversaries from Word Scrambling corpus (Yang et al., 2019) is a binary classification task for paraphrase identification. Its 6 training sets were machinetranslated from the English PAWS dataset (Zhang et al., 2019). The development and test sets were human-translated. \n\nXQuAD The Cross-lingual Question Answering Dataset (Artetxe et al., 2020) requires answering questions by identifying answer spans in accompanying paragraphs. XQuAD consists of human translations of the development and test sets of the English SQuAD 1.1 corpus (Rajpurkar et al., 2016) into 10 languages.",
            "score": 0.6021168470447716,
            "section_title": "Datasets",
            "char_start_offset": 10278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 1110,
                    "end": 1132,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1269,
                    "end": 1292,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1538,
                    "end": 1557,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 1694,
                    "end": 1714,
                    "matchedPaperCorpusId": "91184042"
                },
                {
                    "start": 1822,
                    "end": 1844,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4638671875
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "This section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
            "score": 0.6020732988246955,
            "section_title": "B. Bias Benchmark",
            "char_start_offset": 48672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 404,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 406,
                    "end": 411,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 448,
                    "end": 453,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 455,
                    "end": 460,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 540,
                    "end": 545,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 546,
                    "end": 551,
                    "matchedPaperCorpusId": "252547725"
                },
                {
                    "start": 970,
                    "end": 975,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 977,
                    "end": 982,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1076,
                    "end": 1081,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1364,
                    "end": 1369,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1383,
                    "end": 1388,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1455,
                    "end": 1460,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1649,
                    "end": 1654,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1666,
                    "end": 1671,
                    "matchedPaperCorpusId": "252547725"
                },
                {
                    "start": 1988,
                    "end": 1993,
                    "matchedPaperCorpusId": "135465247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "258715321",
            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
            "text": "Based on the collected dataset, we report baseline results on two subtasks: (a) candidate ranking, which selects the best English candidate that contains the information to answer the non-English question; (b) answer generation, which generates a natural-sounding non-English answer to present to the user based on the selected English candidate. We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability. The translation-based approach is the most effective for candidate ranking while the multilingual-finetuning works the best for answer generation. Nonetheless, on both tasks, there is a substantial gap between the English-based and cross-lingual performances. In the following, we first elaborate on the problem formulation for the cross-lingual PQA task ( \u00a72), then explain the xPQA data collection process ( \u00a73), and present experiment results ( \u00a75.2) and conclusions ( \u00a76).",
            "score": 0.6018778046654503,
            "section_title": "Answer:",
            "char_start_offset": 2499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.413330078125
        },
        {
            "corpus_id": "227231710",
            "title": "SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment",
            "text": "We then constructed the cross-lingual datasets automatically, leveraging word pair translations and scores in five target languages. For this, we followed the established methodology, used for creating cross-lingual semantic similarity datasets (Camacho-Collados et al., 2015;Camacho-Collados et al., 2017). In short, we first intersect aligned concept pairs (obtained through translation) in two languages: e.g., father-ancestor in English and otac-predak in Croatian are used to create cross-lingual pairs father-predak and otac-ancestor. \n\nWe then computed the graded LE scores of cross-lingual pairs as averages of corresponding monolingual scores. Finally, we retained only cross-lingual pairs for which the corresponding monolingual scores differ by \u2264 1.0: this heuristic (Camacho-Collados et al., 2017) mitigates the undesirable inter-language semantic shift. Table 1 shows examples of word pairs with score annotations from monolingual and cross-lingual datasets. \n\nFinal Shared Task Datasets. The obtained monolingual datasets slightly vary in size due to the elimination of unavoidable same-word pairs (i.e., the pairs in which both English words got translated into the same target language word). Cross-lingual datasets additionally vary in size because of the elimination of cross-lingual pairs for which the scores of the corresponding monolingual pairs mutually differed by more than 1.0. For each monolingual and cross-lingual dataset we separated 500 word pairs for the development portions and retained all remaining word pairs for the test portions used in the final evaluation. 2 For the   2. The distribution of graded LE scores in monolingual test sets and (a sample of) cross-lingual test sets is given by Figure 1 and Figure 2, respectively. The majority of pairs are in the outer intervals (i.e., [0, 1) and [5, 6]), with this being more pronounced for cross-lingual datasets.",
            "score": 0.6018751634435631,
            "section_title": "Data",
            "char_start_offset": 7498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 276,
                    "matchedPaperCorpusId": "7681779"
                },
                {
                    "start": 276,
                    "end": 306,
                    "matchedPaperCorpusId": "7665329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "234679313",
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "text": "In many languages, a common problem when using machine learning models for natural language processing (NLP) tasks is the low availability of high-quality datasets for finetuning (Huang et al., 2019), as data annotation is costly both in terms of money and time spent (Dandapat et al., 2009;Sabou et al., 2012). In contrast, multilingual pretrained models show surprisingly good cross-lingual zero-shot performance, i.e., models fine-tuned only on a dataset of a high-resource language, such as English, perform well on another language on the same task (Wu and Dredze, 2019;Conneau et al., 2020;Xue et al., 2021). Due to improvements in machine translation in the last few years (Wu et al., 2016;Lepikhin et al., 2020), automatically translating datasets from a high-resource to a low-resource language has also become an effective cross-lingual transfer strategy. \n\nCurrent literature, however, mostly focuses on the development and understanding of transfer learning methods that potentially lead to a better model with respect to some target task metrics, ignoring development costs, such as training data translation, and recurring costs, such as inference cost per example. This work analyzes the feasibility and cost-effectiveness of cross-lingual methods to answer the following question: given the availability of large supervised datasets in English and models pretrained on various languages, what is the most cost-effective way to use these resources for tasks in other languages? The answer to this question allows us to effectively develop and deploy natural language processing systems for tasks where there is not sufficient labeled data to fine-tune the models. To answer it, we analyze the following transfer learning techniques: 1) fine-tuning a model on a source language and evaluating it on the target language without translation, i.e., in a zeroshot manner; 2) automatic translation of the training dataset to the target language; 3) automatic translation of the test set to the source language at inference time and evaluation of a model fine-tuned on English. \n\nOur main contribution is to evaluate crosslingual transfer methods while also considering their financial and computational costs.",
            "score": 0.6009706930122274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "202539539"
                },
                {
                    "start": 268,
                    "end": 291,
                    "matchedPaperCorpusId": "7380903"
                },
                {
                    "start": 291,
                    "end": 310,
                    "matchedPaperCorpusId": "9295759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3291015625
        },
        {
            "corpus_id": "276408508",
            "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
            "text": "XSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset.",
            "score": 0.6007322419251171,
            "section_title": "A Additional experiment",
            "char_start_offset": 26673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 705
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "270562190",
            "title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning",
            "text": "To test the developed method, we need to consider datasets with related languages that either belong to the same language family or are geographically related, which we expect to be \"helpful\" during the training procedure.Therefore, we select a subset from the Large-Scale Multilingual Machine Translation Shared Task (Small Track #2) (Wenzek et al., 2021) and the subset of Sami languages from the multilingual benchmark for Finno-Ugric languages (Yankovskaya et al., 2023).We describe each dataset in detail in the following paragraphs.\n\nSouth East Asian languages Dataset.For the first round of experiments, we select one of the small tracks, Large-Scale Multilingual Machine Translation Shared Task, comprising translation pairs between fairly related languages and English and not requiring substantial computational resources at training time.We stick to Javanese, Indonesian, Malay, Tagalog, and Tamil as input languages and English as output.As target languages, we utilize Javanese and Tagalog as the smallest language pairs in the dataset.We perform our experiments on multiple dataset scales: 80K (small), 150K (medium), and 500K (large).Our primary goal is to test the method; therefore, we do not perform experiments on the whole dataset, leaving this to future work.For additional experiments, we utilize the Hungarian dataset from Small Track #1.\n\nAll the dataset statistics are provided in Table 4 for the initial dataset and for the datasets created for our experiments.\n\nFinno-Samic Languages Dataset.Regarding the dataset compiled from the Finno-Ugric benchmark (Yankovskaya et al., 2023), we stick to the Sami languages as the only option matching our criteria: parallel training datasets of differ-ent sizes with the same output language (Finnish) for those pairs, parallel development and test datasets of good quality.Unfortunately, such data is available only for Finno-Samic languages3 , such as tartuNLP/finno-ugric-benchmark North Sami, South Sami, Inari Sami, Skolt Sami.The dataset statistics are presented in Table 5.In future experiments, we plan to extend the datasets to other languages and directions from the benchmark.",
            "score": 0.5998755163249481,
            "section_title": "Datasets",
            "char_start_offset": 15414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 538
                },
                {
                    "start": 540,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 849
                },
                {
                    "start": 849,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1149
                },
                {
                    "start": 1149,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1361
                },
                {
                    "start": 1363,
                    "end": 1487
                },
                {
                    "start": 1489,
                    "end": 1519
                },
                {
                    "start": 1519,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 1999
                },
                {
                    "start": 1999,
                    "end": 2047
                },
                {
                    "start": 2047,
                    "end": 2154
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 474,
                    "matchedPaperCorpusId": "258765268"
                },
                {
                    "start": 1581,
                    "end": 1607,
                    "matchedPaperCorpusId": "258765268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1666259765625
        },
        {
            "corpus_id": "273323302",
            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
            "text": "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs. \n\nE GENERALIZING TO EXTREME SCENARIOS E.1 PERFORMANCE ON WEAKER MODEL: LLAMA2 \n\nTable 13 demonstrates that even when starting with a model with weaker multilingual capabilities, such as Llama2-7B-Chat, which exhibits extremely low performance in languages like Russian (ru), German (de), and French (fr) on the X-AlpacaEval, significant improvements can be achieved. By leveraging language imbalance-driven rewarding for self-multilingual optimization across two iterations, the model shows substantial enhancement across all training languages, particularly in those where the original model's performance was initially weaker. To assess the impact of our approach on these languages, we conducted experiments using Llama3-8b-Instruct as the base model. Table 14 shows that even though the model performs weakly in these languages, our approach remains effective in low-resource settings and can iteratively improve the model's performance across all languages.",
            "score": 0.5995995503953536,
            "section_title": "D.3 HOW TO AVOID TRANSLATIONESE BIAS IN MULTILINGUAL BENCHMARKS EVALUATION",
            "char_start_offset": 34821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1127,
                    "matchedPaperCorpusId": "271244246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "258291367",
            "title": "How to Design Translation Prompts for ChatGPT: An Empirical Study",
            "text": "We conduct experiments on a range of benchmarks, including multilingual translations, multireference translations and multi-domain translations. For the multilingual scenario, we choose English \u2194 Spanish and English \u2194 French, which ChatGPT can perform high-quality translation as shown in Figure 1, and we further conduct Spanish \u2194 French translations to evaluate ChatGPT in the non-English-centric scenario. We use the Flores-101 dataset (Goyal et al., 2022) for all of the above translation directions, which contains 1012 sentences from Wikipedia articles for each language. Since the text generated by ChatGPT is more flexible and diverse than that of conventional translation systems, it can be challenging to evaluate ChatGPT's actual performance with a single reference. Consequently, we employ multireference test sets which collect 10 different human translations for each source sentence to evaluate the translation quality of ChatGPT, and the data is released by (Ott et al., 2018). Due to the response delay caused by the available computing resources, using ChatGPT for translation can be a time-consuming and labour-intensive process. Therefore, we follow the strategy outlined in (Jiao et al., 2023) to randomly sample 50 sentence pairs from each test set for our final test sets.",
            "score": 0.5995263464811557,
            "section_title": "Datasets",
            "char_start_offset": 8540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 974,
                    "end": 992,
                    "matchedPaperCorpusId": "4375156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1505126953125
        },
        {
            "corpus_id": "276259236",
            "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
            "text": "Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.",
            "score": 0.5986215589582607,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "225075677",
            "title": "Cross-lingual Machine Reading Comprehension with Language Branch Knowledge Distillation",
            "text": "To verify the effectiveness of our method. We use the following datasets to conduct our experiments. \n\nMLQA A cross-lingual machine reading comprehension benchmark (Lewis et al., 2019). The instances in MLQA cover 7 languages. We evaluate our method on three languages (English, German, Spanish) with translation training method, and also test our method under the setting of zero-shot transfer on the other three languages (Arabic, Hindi, Vietnamese). \n\nXQuAD Another cross-lingual question answering dataset (Artetxe et al., 2019). XQuAD contains instances in 11 languages, and we cover 9 languages in our experiments. Similar to the setting above, we evaluate our method on English, German, Spanish. In addition, we test our method on Arabic, Hindi, Vietnamese, Greek, Russian, and Turkish under the setting of zero-shot transfer.",
            "score": 0.5981863774235902,
            "section_title": "Datasets and Evaluation Metrics",
            "char_start_offset": 18117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 100
                },
                {
                    "start": 103,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341552734375
        },
        {
            "corpus_id": "259108559",
            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
            "text": "Transfer learning paradigms such as fine-tuning, few-shot learning, and zero-shot learning rely on pretrained language models (PLMs), that require having large compilations of raw data (Devlin et al. 2019;Brown et al. 2020;Chowdhery et al. 2022;Scao et al. 2022). These PLMs showed some ability to model different linguistic phenomena (Goldberg 2019;Jawahar et al. 2019) in addition to memorizing facts related to real-world knowledge. While there is a drive to have multilingual models, English is still the language that is better supported due to the abundance of large English raw corpora, diverse datasets, and benchmarks. Moreover, monolingual non-English PLMs are still being pretrained for other high-resource languages. As a way to probe the non-English and multilingual PLMs, researchers tend to translate English benchmarks into other languages, which might degrade the quality of the samples especially if the translation is performed automatically. While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks. \n\nLAMA (Petroni et al., 2019) and ParaRel (Elazar et al., 2021) are two benchmarks developed to quantify the factual knowledge of the English PLMs. They used a setup in which a language model is said to know a specific fact if it can predict the right object for a prompt in a fill-the-gap setup (e.g., For the prompt \"The capital of England is [MASK]\", the model needs to fill the masked gap with \"London\"). Multilingual versions of these benchmarks namely: mLAMA (Kassner et al., 2021), and mParaRel (Fierro and S\u00f8gaard, 2022) were released to evaluate the performance of multilingual PLMs by translating LAMA and ParaRel into 53 and 46 languages respectively. The subjects and objects of the triples within these benchmarks were translated using their multilingual labels on Wikidata, while the templates were automatically translated from the English ones used in the original benchmarks.",
            "score": 0.5976233893366185,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 1152,
                    "end": 1174,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1610,
                    "end": 1632,
                    "matchedPaperCorpusId": "231740666"
                },
                {
                    "start": 1647,
                    "end": 1673,
                    "matchedPaperCorpusId": "247596661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "239009796",
            "title": "GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems",
            "text": "multilingual ToD dataset in three new targetlanguages via machine translation and crawled ontologies in the target-language countries.\n\nOur method only requires minor human efforts to post-edit a few hundred machine-translated dialogue templates in the target languages for evaluation. Besides, as cross-lingual transfer via pre-2 See comparisons of these use cases in Appendix A trained multilingual models (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2021) has proven effective in many cross-lingual tasks, we further investigate another question: How do these multilingual models trained on the English ToD dataset transfer knowledge to our globalized dataset? To answer this question, we prepare a few baselines by evaluating popular ToD systems on our created test datasets in a zero-shot cross-lingual transfer setting as well as a few-shot setting.\n\nOur contributions include the following:\n\n\u2022 To the best of our knowledge, we provide the first step towards analyzing three unexplored use cases for multilingual ToD systems.\n\n\u2022 We propose a cost-effective method that creates a new multilingual ToD dataset from an existing English dataset. Our dataset consists of highquality test sets which are first translated by machines and then post-edited by professional translators in three target languages (Chinese, Spanish and Indonesian). We also leverage machine translation to extend the language coverage of test data to another 17 target languages.\n\n\u2022 Our experiments show that current multilingual systems and translate-train methods fail in zeroshot cross-lingual transfer on the dialogue state tracking task. To tackle this problem, we propose several data augmentation methods to train strong baseline models in both zero-shot and fewshot cross-lingual transfer settings.\n\nIn order to globalize an existing English ToD dataset for the three aforementioned use cases, we propose an approach consisting of four steps as shown in Figure 2: (1) we first extract dialogue templates from the English ToD dataset by replacing English-specific entities with a set of generalpurpose placeholders ( \u00a72.1); (2) we then translate the templates to a target language for both training and test data, with one key distinction that we only post-edit the test data by professional",
            "score": 0.596763642138088,
            "section_title": "Introduction",
            "char_start_offset": 3900,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "259108873",
            "title": "LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification",
            "text": "For exploring the multilingual MTL, we used multilingual (e.g. EXIST) and entirely non-English datasets (e.g. HaSpeeDe). We conducted two experiments: MTL on Task A with multilingual version of hs dataset, which contains texts in English, Italian, and Spanish, and MTL on Task B with evalita and exist datasets, also comprised of these three languages. The multilingual dataset statistics are shown in Table 8. We used roberta-large model for English-only setup and xlm-roberta-large for the multilingual one.\n\nA, hs B, evalita, exist Multilingual 0.8175 0.5963 English 0.8382 0.6259 Table 9: F1-macro scores of models trained on multilingual and English-only data in the MTL setup.\n\nThe results are presented in Table 9. Since the multilingual setup performs noticeably worse, we decided not to pursue this direction of research.",
            "score": 0.5964983499247234,
            "section_title": "D Multilingual MTL",
            "char_start_offset": 16656,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18408203125
        },
        {
            "corpus_id": "277857325",
            "title": "Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks",
            "text": "Figure 3: The three-step process used to translate the passage, question, and answer from bAbI to its Hindi equivalent form. The same process is followed for other selected languages from Table 2. \n\nidentified. The passage, question, and answer are then translated into six languages, as described in Section 2.2, resulting in a parallel multilingual dataset. As illustrated in Table 2, this structure supports direct cross-lingual comparison and aligns with the format of existing multilingual benchmarks such as MLQA (Lewis et al. 2020) and ML-Needle (Hengle et al. 2024). To make the dataset more representative across languages, we replace character names in bAbI with culturally appropriate alternatives for each target language, as shown in Table A.1. Following this pre-processing step, we use the Google Translate API6 to translate the passage, question, and answer from English to all selected languages7 . Figure 3 provides an example of the three-step translation process. \n\nThe resulting dataset preserves structural consistency and ensures that the semantics of each passage, question, and answer remain intact across all seven languages. We defend the absence of human translation in Appendix A.1.",
            "score": 0.5955178798384929,
            "section_title": "3.",
            "char_start_offset": 13122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1211
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 537,
                    "matchedPaperCorpusId": "204734128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "277781442",
            "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish",
            "text": "The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings. Our results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages.",
            "score": 0.5953176093750242,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "247594499",
            "title": "Challenges and Strategies in Cross-Cultural NLP",
            "text": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
            "score": 0.5952304361972165,
            "section_title": "Data Collection",
            "char_start_offset": 25564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 1947,
                    "end": 1965,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 1965,
                    "end": 1984,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1984,
                    "end": 2001,
                    "matchedPaperCorpusId": "237503047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388427734375
        },
        {
            "corpus_id": "277271948",
            "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model",
            "text": "Existing multilingual LLM benchmarks typically translate English datasets (Shi et al., 2022;Hasan et al., 2021) into other languages. Recent efforts like P-MMEval (Zhang et al., 2024) and Bench-MAX (Huang et al., 2025) use parallel corpora to fairly assess LLMs' cross-lingual capabilities, stripping away cultural biases to focus on fundamental language abilities. Safety in multilingual settings is equally critical, with studies showing non-English prompts are more likely to jailbreak LLMs (Jain et al., 2024;de Wynter et al., 2024;Deng et al., 2023;Xu et al., 2023). However, similar research in LVLMs remains limited.",
            "score": 0.5946639676445282,
            "section_title": "Multilingual Benchmark",
            "char_start_offset": 4359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 623
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310546875
        },
        {
            "corpus_id": "260736143",
            "title": "Extrapolating Large Language Models to Non-English by Aligning Languages",
            "text": "We use stanford alpaca6 as the code base. More training details are provided in Appendix A. Evaluation Dataset To evaluate LLM's performance on non-English languages, we use two benchmark cross-lingual datasets, XQUAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020), which requires the model to reason over the given context and answer the given question. In addition, we create a new multilingual evaluation set MI-EVAL (introduced in Appendix B) to assess the capability of LLM in following multilingual instructions. These multilingual multi-way test sets also allow us to compare language ability across languages. To evaluate LLM's translation ability, we follow Zhu et al. (2023) and use multilingual translation dataset FLORES-101 (Goyal et al., 2022). Details of the prompts used for all these tasks are provided in Appendix C. \n\nEvaluation Metrics On XQUAD, MLQA and MI-EVAL, we follow Liu et al. (2023) and Wang et al. (2023a) to use ChatGPT for generation quality evaluation. On XQUAD, MLQA, we also report exact-matching results in Appendix D. For translation tasks, we use COMET (Rei et al., 2020), BLEURT (Sellam et al., 2020) and sentence-piece BLEU (Papineni et al., 2002) as metrics7 . \n\nMore evaluation details can be referred to Appendix D.",
            "score": 0.592836226910483,
            "section_title": "EXPERIMENT SETTING",
            "char_start_offset": 12657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1264
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 240,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 250,
                    "end": 270,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1097,
                    "end": 1115,
                    "matchedPaperCorpusId": "221819581"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "215548699"
                },
                {
                    "start": 1170,
                    "end": 1193,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "222140777",
            "title": "DaNetQA: a yes/no Question Answering Dataset for the Russian Language",
            "text": "The creation of new datasets, aimed at new, challenging tasks, describing complex phenomena, related to various aspects of language understanding and usage, is core to the current view of modern language technologies. However, the majority of the datasets, created and published at the best venues, target the English-language tasks and cultural aspects, related to English-speaking society. \n\nIn response to the bias towards English, new datasets and benchmarks are developed, that comprise multiple languages. One of the well-known examples of such multilingual datasets is XNLI [4], which is a natural language inference dataset. Although this dataset is developed for 15 languages, including lowresource ones, the approach to its creation still excessively utilizes English data: \n\nThe first two authors have equal contribution. arXiv:2010.02605v2 [cs.CL] 15 Oct 2020 the dataset entries are manually translated from English to other languages, without any specific adjustments. Although the translation-based approach is quick and dirty and allows us to overcome the lack of dataset for any language, other concerns arise. From a general point of view, we understand, that translating is different from natural everyday language usage [8]. Thus translated datasets may have different statistics and word usage in comparison to text, composed from scratch. This may affect the quality of the models, trained further on the translated datasets, when applied to real-life data. \n\nAnother approach to dataset creation involves collecting datasets following the guidelines and annotation schemes, designed for original datasets in English. RuRED [7] is a recent example of such a dataset: it is created following TA-CRED [25] annotation scheme and collection pipeline. This approach however is criticized for the obvious lack of novelty. \n\nNevertheless, in this paper we stick to the second approach to the dataset creation and more or less follow the pipeline, developed for BoolQ [2], to create a new dataset for binary questions in Russian, which we refer to as DaNetQA3 . We only deviate from the BoolQ pipeline, if we do not have access to proprietary data sources and instead use crowdsourcing.",
            "score": 0.5925790158485449,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "60634852"
                },
                {
                    "start": 1646,
                    "end": 1649,
                    "matchedPaperCorpusId": "229204578"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "matchedPaperCorpusId": "3782112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29248046875
        },
        {
            "corpus_id": "263310506",
            "title": "Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation",
            "text": "For our investigation into cross-lingual transfer in the extractive QA task and the evaluation of our models across diverse languages, we utilized three well-suited datasets. \n\nXQUAD We incorporate the XQuAD dataset [20] consisting of 240 paragraphs and 1,190 question-answer from SQuAD v1.1 [34] pairs translated into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. These translations were performed by professional human translators. \n\nMLQA The MLQA benchmark [9] includes QA instances in seven languages: English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese. It comprises over 12,000 instances in English and approximately 5,000 instances in each of the other languages, with an average of four language pairs for each instance. The dataset is divided into development and test splits, consisting of 4,199 and 41,244 examples, respectively. The creation of MLQA involved a meticulous process encompassing professional translation and human annotation. It consists of three steps: parallel sentence mining, English QA annotation, and target language QA annotation. MLQA aims to drive research in cross-lingual QA and bridge the gap between training and testing language performance. \n\nTyDiQA-goldp TyDiQA [19] is a comprehensive question-answering dataset that spans 11 typologically diverse languages and includes three tasks: Passage Selection (SelectP), Minimal Answer Span (MinSpan), and Gold Passage (GoldP). For our study, we focus on the Gold Passage task (GoldP), which involves predicting the contiguous span of characters that answer the question when a passage containing the answer is provided. This task enables comparison with prior works and allows compatibility with existing datasets for extractive QA. The dataset aims to evaluate the models's ability to generalize across a wide range of languages by including linguistic phenomena that are not typically found in English-only corpora. Additionally, the dataset is collected directly in each language, without relying on translations. \n\nIn our work, we use the XQuAD for training, selecting the languages present in MLQA.",
            "score": 0.5925238625337539,
            "section_title": "Datasets",
            "char_start_offset": 19019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "234679313",
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "text": "An effective method for cross-lingual transfer is to fine-tune a bilingual or multilingual model on a supervised dataset in one language and evaluating it on another language in a zero-shot manner. Translating examples at training time or inference time are also viable alternatives. However, there are costs associated with these methods that are rarely addressed in the literature. In this work, we analyze cross-lingual methods in terms of their effectiveness (e.g., accuracy), development and deployment costs, as well as their latencies at inference time. Our experiments on three tasks indicate that the best cross-lingual method is highly task-dependent. Finally, by combining zero-shot and translation methods, we achieve the state-of-the-art in two of the three datasets used in this work. Based on these results, we question the need for manually labeled training data in a target language. Code and translated datasets are available at https://github.com/unicamp-dl/cross-lingual-analysis",
            "score": 0.5921602380232992,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482666015625
        },
        {
            "corpus_id": "253762071",
            "title": "NTREX-128 \u2013 News Test References for MT Evaluation of 128 Languages",
            "text": "We release NTREX-128, a data set for machine translation (MT) evaluation from English into a total of 128 target languages. The paper describes the data creation process and proposes a quality filtering method based on human evaluation. We show experimental results which confirm that the directionality of test sets translation indeed plays an important role wrt. the usefulness of the corresponding metrics\u2019 scores. Thus, we recommend that the NTREX-128 data set should be used for evaluation of English-sourced translation models but not in reverse direction. The test set release introduces another benchmark for the evaluation of massively multilingual machine translation research.",
            "score": 0.5907740881001071,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023).However, (1) translated datasets often do not account for language-specific culture, knowledge, common sense, or linguistic phenomena, leading to a bias towards the background of the source language (Hu et al., 2021;Lin et al., 2021;Acharya et al., 2020;Clark et al., 2020;Park et al., 2021;Kurihara et al., 2022).\n\n(2) Simply compiling datasets curated for each individual language could allow the evaluation of language-specific knowledge and common sense.However, it is difficult to align tasks across languages since most tasks differ in their creation methods data sources or philosophies.Thus, it just leads to evaluating the transfer capability among comparable tasks, and not evaluating the true transfer capabilities across languages.Therefore, (3) only the datasets created from multilingual resources can enable the evaluation of language transfer capability, considering the differences in language-specific knowledge and common sense.Nevertheless, the manual creation of such datasets is limited by the availability of annotators and financial costs.",
            "score": 0.590549531475148,
            "section_title": "Introduction",
            "char_start_offset": 5533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 855
                },
                {
                    "start": 857,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 290,
                    "end": 307,
                    "matchedPaperCorpusId": "215745460"
                },
                {
                    "start": 307,
                    "end": 330,
                    "matchedPaperCorpusId": "6325317"
                },
                {
                    "start": 417,
                    "end": 437,
                    "matchedPaperCorpusId": "222141483"
                },
                {
                    "start": 456,
                    "end": 481,
                    "matchedPaperCorpusId": "13075323"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "21732778"
                },
                {
                    "start": 757,
                    "end": 774,
                    "matchedPaperCorpusId": "235421949"
                },
                {
                    "start": 814,
                    "end": 832,
                    "matchedPaperCorpusId": "234790338"
                },
                {
                    "start": 832,
                    "end": 854,
                    "matchedPaperCorpusId": "249687727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "258615158",
            "title": "How Good are Commercial Large Language Models on African Languages?",
            "text": "Large language models have proven successful in multilingual settings. Lin et al. (2021) train several multilingual models, of which the largest one (7.5B parameters) sets a state-of-the-art in few-shot learning on more than 20 languages. Their model outperforms GPT3 on several multilingual tasks. Muennighoff et al. (2022) perform multitask prompted finetuning on multilingual pretrained language models and observe impressive zero-shot generalization to tasks in unseen languages. Following findings from Blevins & Zettlemoyer (2022) that non-English dataset present in the pretraining corpora of English language models explains their surprising cross-lingual ability, Chowdhery et al. (2022) deliberately introduce non-English corpora (\u2248 22%) into the pretraining corpora of their PaLM model and achieve impressive few-shot multilingual performance. Shi et al. (2022) evaluate GPT3 and PaLM on a newly introduced grade school mathematics multilingual benchmark. They find that using prompts with intermediate reasoning steps in English consistently led to competitive or better results than those written in the native language of the question. They also set a new state-of-the-art on a common-sense reasononing multilingual benchmark, XCOPA (Ponti et al., 2020), using few-shot examples. Zhao & Sch\u00fctze (2021) show that prompting yields better cross-lingual transfer in few-shot settings than finetuning and in-language training of multilingual natural language inference. Furthermore, Winata et al. (2021) evaluate the multilingual ability of GPT (Radford et al., 2019) and T5 (Raffel et al., 2020) models on multi-class text classification, and find that they work well on non-English languages given a few English examples. Concurrent work (Jiao et al., 2023) evaluate ChatGPT on machine translation and find that, while it is competitive with other commercial translation APIs such as Google translate 5 , it is less robust on other domains such as biomedical.",
            "score": 0.5904138149460781,
            "section_title": "MULTILINGUAL IN-CONTEXT LEARNING",
            "char_start_offset": 3909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 1247,
                    "end": 1267,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1294,
                    "end": 1315,
                    "matchedPaperCorpusId": "237439234"
                },
                {
                    "start": 1492,
                    "end": 1512,
                    "matchedPaperCorpusId": "237532173"
                },
                {
                    "start": 1584,
                    "end": 1605,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.150390625
        },
        {
            "corpus_id": "244920860",
            "title": "Dataset Geography: Mapping Language Data to Language Users",
            "text": "We apply the process described above on several datasets, chosen mostly for their language and typological diversity. Our process is not dataset-or language-dependent, 4 and could easily be applied on any NL dataset. We briefly describe the datasets we include in our study below, with detailed statistics in Appendix C. NER Datasets We study the WikiANN dataset (Pan et al., 2017) that is commonly used in the evaluation of multilingual models. We additionally study the MasakhaNER dataset (Adelani et al., 2021), which was created through participatory design (\u2200 et al., 2020) in order to focus on African languages. Since these datasets are already annotated with named entities, we only need to perform entity linking. Question Answering We study four question answering datasets (focusing on the questions rather than contexts), namely SQuAD (Rajpurkar et al., 2016), MLQA (Lewis et al., 2020), TyDi-QA (Clark et al., 2020), and Natural Questions (Kwiatkowski et al., 2019, NQ;), which have unique characteristics that lend themselves to interesting comparisons. SQuAD is a large English-only dataset (although it has been translated through efforts like XQuAD (Artetxe et al., 2020)). MLQA is a n-way parallel multilingual dataset covering 7 languages, created by translating an English dataset. TyDi-QA is another multilingual dataset covering 11 languages, but each language portion is derived separately for each language, without translating them. Last, NQ is an English QA dataset created based on real-world queries on the Google search engine for which annotators found relevant Wikipedia context, unlike the other datasets that were created by annotators forming questions given a context.",
            "score": 0.5904054891263376,
            "section_title": "Dataset-Country Maps",
            "char_start_offset": 6815,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 381,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 847,
                    "end": 871,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 878,
                    "end": 898,
                    "matchedPaperCorpusId": "204734128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366943359375
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "Can you choose the correct answer in Table 1?Each choice is semantically very close, making it difficult for non-native speakers to distinguish them.However, native speakers who have languagespecific commonsense and knowledge can choose the most plausible choice considering subtle nuances.Despite the need to consider different backgrounds for each language, the datasets to evaluate the natural language understanding (NLU) capabilities of language models (LMs) are mostly for Table 1: Examples require language-specific knowledge.\n\nThey cannot be solved without such knowledge, as the translations consolidate the nuances into a single word.\n\na few major languages such as English, and thus, many languages lack such datasets.When focusing on the cross-lingual capability of LMs, datasets created from scratch in multiple languages are lim-ited, and currently, evaluations mostly use datasets created through translation.However, as can be seen from the example in Table 1, datasets created through translation cannot accurately evaluate language-specific commonsense or knowledge.Therefore, it is necessary to create datasets for each language from scratch, but the manual creation of such datasets is limited by the availability of annotators and financial costs.\n\nTo tackle this problem, as shown in Figure 1, we propose a method to efficiently create multilingual NLU datasets from multilingual resources by replacing some of the manual annotation processes with generative multilingual LMs.In this study, we focus on CommonsenseQA (CSQA) (Talmor et al., 2019), a dataset for evaluating commonsense reasoning capabilities within NLU evaluations.CSQA is a major commonsense reasoning Question-Answering dataset manually created from the multilingual knowledge base Concept-Net (Speer et al., 2017).However, due to such limitations, CSQA has been created from scratch only in English and Japanese, JCommonsenseQA (JCSQA) (Kurihara et al., 2022).Therefore, we create a Multilingual CommonsenseQA (mCSQA) that extends CSQA to eight languages 1 using our proposed method.",
            "score": 0.5899463787536433,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 45,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 533
                },
                {
                    "start": 535,
                    "end": 644
                },
                {
                    "start": 646,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1268
                },
                {
                    "start": 1270,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1652
                },
                {
                    "start": 1652,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 1546,
                    "end": 1567,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 1783,
                    "end": 1803,
                    "matchedPaperCorpusId": "15206880"
                },
                {
                    "start": 1926,
                    "end": 1949,
                    "matchedPaperCorpusId": "249687727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "271769732",
            "title": "Benchmarking Low-Resource Machine Translation Systems",
            "text": "We considered four parallel machine-translation benchmark datasets OPUS100 (Zhang et al., 2020), Europarl (Koehn, 2005), IWSLT2017 (Cettolo et al., 2017), and FLORES-200 (NLLB Team et al., 2022). The statistics of the datasets are in Table 1 in the form of token and parallel pair counts. All the datasets have the same number of parallel pairs across languages, except for IWSLT2017. In this case, we averaged the number of pairs for the languages considered in this experiment. \n\nOPUS100 (Zhang et al., 2020) is a parallel translation dataset randomly sampled from the OPUS corpus (Tiedemann, 2012) that covers 100 languages, focused on English. The represented domains in the dataset were not balanced, but sampling filters were applied to ensure no cross-lingual data leakage. This also means that the dataset is not sentence-aligned across languages, i.e., the test sets have different content w.r.t. the language, despite having the same document size. Europarl (Koehn, 2005) is a parallel translation dataset from the Proceedings of the European Parliament that covers 11 languages. We used the common-test-set, a cross-lingual sentence-aligned split, as presented by Koehn (2005) (Devlin et al., 2019). \n\nalso introduced an unofficial bilingual task to follow previous editions of the venue that extended the English-centric dataset to 4 other languages. \n\nThe content and the document size of each test set differ for each language. FLORES-200 (NLLB Team et al., 2022) is a manually curated dataset that covers 204 languages, based on Wikinews, Wikijunior, and Wikivoyage. The translations were done by professional translators and followed a series of automatic and manual quality review processes. All documents have the same content. As the test set of the dataset is kept blind, in our experiments we evaluated the performance on the devtest split.",
            "score": 0.5898767868282654,
            "section_title": "Datasets",
            "char_start_offset": 14802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 95,
                    "matchedPaperCorpusId": "272727920"
                },
                {
                    "start": 106,
                    "end": 119,
                    "matchedPaperCorpusId": "38407095"
                },
                {
                    "start": 131,
                    "end": 153,
                    "matchedPaperCorpusId": "21675165"
                },
                {
                    "start": 490,
                    "end": 509,
                    "matchedPaperCorpusId": "272727920"
                },
                {
                    "start": 583,
                    "end": 600,
                    "matchedPaperCorpusId": "15453873"
                },
                {
                    "start": 968,
                    "end": 981,
                    "matchedPaperCorpusId": "38407095"
                },
                {
                    "start": 1175,
                    "end": 1187,
                    "matchedPaperCorpusId": "38407095"
                },
                {
                    "start": 1188,
                    "end": 1209,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15380859375
        },
        {
            "corpus_id": "273532388",
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "text": "This approach is referred to as a meta-evaluation benchmark because it aims to evaluate the performance of the evaluators. \n\nSimultaneously, the performance of multilingual LLMs that process questions or instructions in languages other than English is developing rapidly, thereby increasing the importance of thorough evaluation of multilingual outputs (Thellmann et al., 2024;Yue et al., 2024). However, in practice, evaluations of multilingual LLMs often rely on evaluator LLMs whose multilingual evaluation capabilities have not been thoroughly assessed (Lai et al., 2023;Liu et al., 2024;Aryabumi et al., 2024;Dang et al., 2024). Fundamentally, the absence of multilingual meta-evaluation benchmarks is a key reason why evaluator LLMs remain under-tested in multilingual settings. \n\nTo address this issue, we release MM-EVAL, a Multilingual Meta-Evaluation benchmark spanning a wide range of languages, including low-resource ones such as Swahili, Basque, and Galician. MM-EVAL consists of six subsets: five core subsets-Chat, Reasoning, Safety, Language Hallucination, and Linguistics-cover 18 languages, while the Language Consistency subset covers 122 languages for a broader analysis on the consistency of evaluator LLMs across low-to high-resource languages. \n\nA core attribute of MM-EVAL is its multilingual-first design philosophy. Rather than simply translating existing English meta-evaluation benchmarks into other languages, MM-EVAL is constructed to address challenges that are unique to multilingual settings. These challenges include: Linguistics, Language Hallucination, and Language Consistency. In addition, while prior meta-evaluation benchmarks primarily assess evaluators based on ranking accuracy over pairwise comparisons, MM-EVAL goes a step further by evaluating the consistency and fairness of absolute score values assigned by evaluator LLMs. This is achieved through the Language Consistency subset and a novel metric, the Language Consistency Index (LCI), which quantifies how uniformly evaluators score parallel inputs across languages. \n\nUsing MM-EVAL, we evaluate the performance of 12 evaluator LLMs, including open reward models, open judges models, and proprietary models.",
            "score": 0.5895801042355314,
            "section_title": "Introduction",
            "char_start_offset": 1916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 394,
                    "matchedPaperCorpusId": "273507992"
                },
                {
                    "start": 557,
                    "end": 575,
                    "matchedPaperCorpusId": "260334562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "273323209",
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "text": "We chose DeepL as translation service for its balance between translation accuracy and scalability. We translated five well-known datasets, ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), GSM8K (Cobbe et al., 2021), and MMLU (Hendrycks et al., 2020) from English into 20 European languages. These datasets encompass a mix of multiple-choice and open-ended generation tasks, each of which presented unique translation challenges. The translations maintained the original structure of each task to ensure the benchmarks were as consistent as possible across languages. Using DeepL's XML-tag handling, we avoided issues with prompt formatting and ensured that key contextual elements, such as 'Question' and 'Response' prompts, were properly localized (cf. Table 5). \n\nTo enhance the visibility and accessibility of our benchmarks, we launched a multilingual leaderboard 5 on Hugging Face, inspired by the popular Open LLM leaderboard6 .",
            "score": 0.589544174509266,
            "section_title": "Translation Process",
            "char_start_offset": 6113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 977
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "258715321",
            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
            "text": "In contrast to the above, here we apply all translation processes in training, or offline, so that no additional latency is added at runtime. In the zero-shot scenario, we machine-translate all questions and answers in the ePQA dataset into each of the 12 languages we consider. The resulting dataset, termed ePQA_MT, is used to train a multilingual model. In the fine-tuned scenario, we further finetune the model on the xPQA dataset. As the model is defined to be multilingual, it can directly take input questions in their original languages and output answers in the target languages without any translation process. \n\nMultilingual Finally, this approach is similar to the translate-train one in that both use multilingual models rather than an English-only model, but the difference is that the multilingual approach requires no translations at training time. In the zeroshot scenario, it trains a multilingual pretrained model directly on the English-only ePQA dataset and relies only on its own pretrained multilingual knowledge to adapt to other languages. In the finetuned scenario, we further fine-tune the model on the xPQA dataset. Note that this approach still requires runtime post-translation of the generated English answer into the target language. This is because we find that multilingual models can only generate English answers when trained only on English datasets. Although special decoding constraints could be use to restrict output vocabulary to that of the target language, zero-shot multilingual adaptation in generation tasks is still an open challenge (Chen et al., 2022;Zhang et al., 2022). It is worth mentioning that the three types of approaches can be combined. For example, we could follow the translate-train approach to train the candidate ranker and follow the multilingual approach to train the answer generator. Details of the model implementation are in Appendix C.",
            "score": 0.5891828890038355,
            "section_title": "Translate-train",
            "char_start_offset": 8865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 1582,
                    "end": 1601,
                    "matchedPaperCorpusId": "237091323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445556640625
        },
        {
            "corpus_id": "218470125",
            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
            "text": "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language,3 and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation). Detailed translation guidelines are available in Appendix A. \n\nLanguage Sampling. Multilingual evaluation benchmarks assess the expected performance of a model across languages. However, should such languages be randomly sampled from the distribution of digital texts? Or rather, should the sample represent the distribution over the languages spoken around the world? Resource-rich languages tend to belong to the same families and areas, which facilitates knowledge transfer and leads to an overestimation of the expected performance in the second sense (Gerz et al., 2018;Ponti et al., 2019a). Moreover, rather than samples that account for independent and identically distributed draws from the 'true' language distribution (known as probability sampling), we opt for a uniform distribution of linguistic phenomena, which encourages the inclusion of outliers (known as variety sampling; Rijkhoff et al., 1993;Dryer, 1989). Thus, the performance on XCOPA also reflects the robustness of a model, i.e. its resilience to phenomena that are unlikely to be observed in the training data.",
            "score": 0.5869212259470584,
            "section_title": "Annotation Design",
            "char_start_offset": 4684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 628,
                    "matchedPaperCorpusId": "9497990"
                },
                {
                    "start": 1588,
                    "end": 1607,
                    "matchedPaperCorpusId": "52229163"
                },
                {
                    "start": 1607,
                    "end": 1627,
                    "matchedPaperCorpusId": "49564714"
                },
                {
                    "start": 1923,
                    "end": 1945,
                    "matchedPaperCorpusId": "170125293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "277994236",
            "title": "Compass-V2 Technical Report",
            "text": "This advancement not only represents a technological breakthrough but also establishes new benchmarks for multilingual services in global e-commerce platforms. \n\nA multilingual dataset has been constructed, comprising three primary categories: 1) open-source instruction datasets (e.g., xP3 (Muennighoff et al., 2023), xP3mt (Muennighoff et al., 2023), Alpaca-GPT4-Portuguese (Chen et al., 2023b), COIG (Zhang et al., 2023), Vietnamese-395k-meta-math-MetaMathQA-gg-translated), 2) traditional NLP benchmarks (e.g., SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), RACE (Lai et al., 2017), NewsQA (Trischler et al., 2017), CoQA (Reddy et al., 2019)), and 3) multilingual data obtained by translating selected subsets of these NLP benchmarks into multiple target languages. Open-source instruction data enhances the model's ability to generalize instructionfollowing tasks across languages, while traditional NLP benchmarks strengthen fundamental language comprehension and reasoning skills. The translated data further promotes cross-lingual transfer and linguistic diversity, supporting robust multilingual instruction-following performance. \n\nTo align with Shopee's application scenarios in Southeast Asia (SEA), the dataset emphasizes SEA languages and maintains a balanced linguistic distribution across Thai, Portuguese, English, Indonesian, Malay, Vietnamese, and Chinese. Additionally, we standardize heterogeneous raw data into uniform generative and multiple-choice templates, facilitating efficient training and ensuring compatibility across diverse data sources. To further refine the dataset, we apply heuristic filtering to remove irrelevant tasks (e.g., event linking) and reduce the proportion of overly simplistic tasks (e.g., text simplification), improving data relevance.",
            "score": 0.5864375179824532,
            "section_title": "MULTILINGUAL INSTRUCTIONS",
            "char_start_offset": 26632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 608,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 619,
                    "end": 638,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 645,
                    "end": 663,
                    "matchedPaperCorpusId": "6826032"
                },
                {
                    "start": 672,
                    "end": 696,
                    "matchedPaperCorpusId": "1167588"
                },
                {
                    "start": 703,
                    "end": 723,
                    "matchedPaperCorpusId": "52055325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69580078125
        },
        {
            "corpus_id": "259224700",
            "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4",
            "text": "In addition, their performance may be highly dependent on the datasets used for training and evaluation (Yeh et al., 2021). \n\nDue to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and costintensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. \n\nFurthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it arXiv:2306.12794v3 [cs.CL] 14 Sep 2023 helps the transformation of state-of-the-art Englishonly dialogue systems into highly capable multilingual systems. Although multilingual pre-trained language models may exist and can be potentially used for training multilingual dialogue systems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some lowresource languages. To address this problem, we take advantage of recent advances in neural machine translation and paraphrasing systems. Using existing high-quality services and models, it is possible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics. To this end, we propose two subtasks in our track, and their details are listed as follows:",
            "score": 0.585975871828136,
            "section_title": "Introduction",
            "char_start_offset": 1746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 126,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 122,
                    "matchedPaperCorpusId": "235358130"
                },
                {
                    "start": 181,
                    "end": 208,
                    "matchedPaperCorpusId": "219980605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "236447339",
            "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
            "text": "One way in which non-English work seems to be easier to publish is multilingual resources. Some of them are data from cross-lingual shared tasks10 , and also independent academic resources (such as English-Chinese cloze-style XCMRC [179]). But in terms of number of languages, the spotlight is currently on the following larger-scale resources: \n\n\u2022 MLQA [162] targets extractive QA over Wikipedia with partially parallel texts in seven languages: English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. The questions are crowdsourced and translated. \n\n\u2022 XQuAD [11] is a subset of SQuAD professionally translated into 10 languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. \n\n\u2022 XQA [178] is an open-domain QA dataset targeting entities; it provides training data for English, and test and development data for English and eight other languages: French, German, Portuguese, Polish, Chinese, Russian, Ukrainian, and Tamil. \n\n\u2022 TydiQA [66] is the first resource of \"natural\" factoid questions in ten typologically diverse languages in addition to English: Arabic, Bengali, Finnish, Japanese, Indonesian, Kiswahili, Korean, Russian, Telugu, and Thai. \n\n\u2022 XOR QA [14] builds on Tidy QA data to pose the task of cross-lingual QA: answering questions, where the answer data is unavailable in the same language as the question. It is a subset of TidyQA with data in seven languages: Arabic, Bengali, Finnish, Japanese, Korean, Russian and Telugu, with English as the \"pivot\" language (professionally translated). \n\n\u2022 XQuAD-R and MLQA-R [250] are based on the above-mentioned XQuAD and MLQA extractive QA resources, recast as multilingual information retrieval tasks. \n\n\u2022 MKQA [182] is based on professional translations of a subset of Natural Questions [155], professionally translated into 26 languages, focusing on \"translation invariant\" questions.",
            "score": 0.5859427564540582,
            "section_title": "Multilingual resources",
            "char_start_offset": 58180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 237,
                    "matchedPaperCorpusId": "199668659"
                },
                {
                    "start": 354,
                    "end": 359,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 747,
                    "end": 752,
                    "matchedPaperCorpusId": "196174566"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199462890625
        },
        {
            "corpus_id": "225067135",
            "title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering",
            "text": "XQuAD (Artetxe et al., 2020) is a human translation of the SQuAD en development set in 10 languages (Arabic, Chinese, German, Greek, Hindi, Russian, Spanish, Thai, Turkish, and Vietnamese), with 1k QA pairs for each language.\n\nMLQA (Lewis et al., 2020) is an evaluation dataset in 7 languages (English, Arabic, Chinese, German, Hindi, and Spanish). The dataset is built from aligned Wikipedia sentences across at least two languages (full alignment between all languages being impossible), with the goal of providing natural rather than translated paragraphs. The QA pairs are manually annotated on the English sentences and then human translated on the aligned sentences. The dataset contains about 46K aligned QA pairs in total.\n\nLanguage-specific benchmarks In addition to the two aforementioned multilingual evaluation data, we benchmark our models on three languagespecific datasets for French, Italian and Korean, as detailed below. We choose these datasets since none of these languages are present in XQuAD or MLQA. Hence, they allow us to evaluate our models in a scenario where the target language is not available during training, even for the synthetic questions.\n\nPIAF Keraron et al. (2020) provided an evaluation set in French following the SQuAD protocol, and containing 3835 examples.\n\nKorQuAD 1.0 the Korean Question Answering Dataset (Lim et al., 2019), a Korean dataset also built following the SQuAD protocol.\n\nSQuAD-it Derived from SQuAD en , it was obtained via semi-automatic translation to Italian (Croce et al., 2018).",
            "score": 0.5858967911101147,
            "section_title": "Multilingual Evaluation Sets",
            "char_start_offset": 7876,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 27,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 232,
                    "end": 252,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1182,
                    "end": 1203,
                    "matchedPaperCorpusId": "218974030"
                },
                {
                    "start": 1522,
                    "end": 1542,
                    "matchedPaperCorpusId": "53238211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "248986585",
            "title": "Local Byte Fusion for Neural Machine Translation",
            "text": "Multilingual Many-to-One Translation: We use the OPUS public data (Tiedemann, 2012) to construct a multilingual parallel corpus that has a fair mix of high-resource and low-resource languages. We train a multilingual translation model from seven source languages to English. Table 1 shows the statistics of the training data. We use the Flores-101 (Goyal et al., 2022) benchmark to evaluate the performance of our models.\n\nZero-shot Cross-lingual Translation: Following Neubig and Hu (2018), we use the same Ted Talk dataset that include four language pairs where each pair has a high-resource language (HRL) and a low-resource languages (LRL) written in the same script. Table 2 shows the statistics of the dataset.",
            "score": 0.5854347048598088,
            "section_title": "Datasets",
            "char_start_offset": 13983,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 83,
                    "matchedPaperCorpusId": "15453873"
                },
                {
                    "start": 470,
                    "end": 490,
                    "matchedPaperCorpusId": "51976920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2252197265625
        },
        {
            "corpus_id": "252568105",
            "title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
            "text": "A key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL. \n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04. \n\nIn Table 1 we compare mRobust04 with other multilingual IR datasets. Despite having a modest number of queries and documents, mRobust04 has much more annotations per query, which, we conjecture, makes it a reliable benchmark for evaluating future multilingual retrieval models. We also evaluate on this dataset two multilingual models that are close to the state of the art. dently to the API. The resulting translations of these chunks were then concatenated to form the translated document. The annotated query-document pairs are the same for all languages.",
            "score": 0.5852733588282164,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "233296851"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "248817938"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "42162974"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "38347697"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "262629903"
                },
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "1720123"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "246275873"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "237213465"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "59910946"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "25834866",
            "title": "Detecting Cross-Lingual Plagiarism Using Simulated Word Embeddings",
            "text": "This section describes the evaluation of the proposed CLPD model using datasets that contain cross lingual plagiarism, they include the Pan 2011 and 2012 evaluation corpora on plagiarism detection; both of them are described below. \n\nThe Pan 2011 Evaluation corpus: this corpus contains 5142 manually and automatic generated cases of CLP distributed across 550 suspect documents of which 4709 were generated automatically using internet translation services, and 433 were generated using both automatic and manual correction processes. The automatic generated cases were created using Google translate to translate text passages from one language to another; the process typically involves removing a passage from a non-English source document, translating the passage into English and inserting it in a document written in English. The manually created cases were artificially generated and corrected by humans to appear like real plagiarism cases. The translations are from {Spanish and Danish) \uf0e0English. \n\nThe Pan 2012 Evaluation corpus: this corpus contains artificially generated cases of cross lingual plagiarism distributed across 500 suspect documents. The cross lingual plagiarism cases were created using the multi-lingual Europarl-corpus; from a non-English source document, a text passage is removed and used to retrieve its corresponding English version from the multi-lingual Euro-Parl corpus. The retrieved English version is then inserted into a Gutenberg book (suspicious documents). \n\nThe translated passages are from {Spanish and Danish} \uf0e0English.",
            "score": 0.5851126492611125,
            "section_title": "Experiments:",
            "char_start_offset": 25173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1566
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06451416015625
        },
        {
            "corpus_id": "271769361",
            "title": "Do LLMs Speak Kazakh? A Pilot Evaluation of Seven Models",
            "text": "Popular benchmarks include MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2021) that measures LLM's knowledge across 57 subjects and GSM8K (Grade School Math) (Cobbe et al., 2021), aimed at evaluating multi-step math reasoning. MMLU contains multiple-choice ques-tions, while GSM8K accepts numerical answers. There are multilingual adaptions of these datasets: Lai et al. (2023b) employed ChatGPT to translate the original MMLU dataset in multiple languages; MGSM dataset contains 250 problems from the GSM8K manually translated into 10 typologically diverse languages (Shi et al., 2023). \n\nStudies that evaluate LLMs on non-English tasks are fewer that those targeting English and vary in their scope (Chang et al., 2024;Laskar et al., 2023). Some focus on multilingual datasets (Lai et al., 2023a;Ahuja et al., 2023), while others concentrate on a specific language, e.g. Arabic (Abdelali et al., 2024) or Russian (Fenogenova et al., 2024). Our study belongs to the latter type. LLMs, as expected, are better in solving problems formulated in English than in other languages. Moreover, fine-tuned models such as XLM-R (Conneau et al., 2020) in general outperform LLMs on specific tasks. The quality on non-English tasks can be improved by preceding actual task formulation with English prompts, or by explicitly stating in the prompt that the task must first be translated into English (Huang et al., 2023;Zhang et al., 2023). The multilingual abilities of LLMs also depend on the task type. It can be concluded that LLMs are better at \"understanding\" a language other than English than at generating a non-English answer (Bang et al., 2023). Thus, models do better in multilingual classification, reasoning, and multiple-choice question answering and struggle with generative tasks.",
            "score": 0.5850291340935512,
            "section_title": "Related Work",
            "char_start_offset": 7168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 99,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 589,
                    "end": 607,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 742,
                    "end": 762,
                    "matchedPaperCorpusId": "258967462"
                },
                {
                    "start": 800,
                    "end": 819,
                    "matchedPaperCorpusId": "258079179"
                },
                {
                    "start": 819,
                    "end": 838,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 901,
                    "end": 924,
                    "matchedPaperCorpusId": "258865657"
                },
                {
                    "start": 1140,
                    "end": 1162,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "258615377"
                },
                {
                    "start": 1428,
                    "end": 1447,
                    "matchedPaperCorpusId": "258947405"
                },
                {
                    "start": 1644,
                    "end": 1663,
                    "matchedPaperCorpusId": "256662612"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227783203125
        },
        {
            "corpus_id": "246442089",
            "title": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers",
            "text": "One of them is the lack of available benchmarks. To the best of our knowledge, there are only 3 mKGQA datasets in the research community (QALD-9 [3], RuBQ 2.0 [4], and CWQ [5]) and all of them are not fulfilling completely the practical needs of researchers and developers (see Section II). Hence, even if one develops an mKGQA system, there are not so many opportunities for full-fledged evaluation. In this regard, it was decided to extend the well-known QALD-9 dataset in order to enlarge its language coverage and improve the quality of non-English questions. The English questions from the dataset are considered as a reference. To create the extension, a number of crowd workers were asked to translate the questions into their mother tongue manually (and validate translations by others). As a result, 17 volunteer participants and 290 crowd workers from Amazon Mechanical Turk 4and Yandex Toloka5 translated questions into 8 languages, such as: German, Russian, French, Armenian, Belarusian, Lithuanian, Bashkir, and Ukrainian, the latter five of which, to our knowledge, have never been considered in the KGQA context. In addition, the SPARQL queries in the dataset were transformed from DBpedia to Wikidata in order to increase the usability of the dataset. We name the extended dataset as \"QALD-9-plus\" and plan to continuously extend it over time. Thus, the main contribution of this work is that we introduce the mKGQA benchmark with high-quality translations that covers 9 languages in total (including English) some of that were never covered in the KGQA research community before. Moreover, Belarusian and Bashkir languages are considered as endangered by UNSECO 6 . We value our contribution as a small but very important step towards improving multilingual accessibility of KGQA systems. \n\nThis paper is structured as follows: in Section II we describe currently available multilingual KGQA datasets, Section III describes in details the process of creation and analysis of QALD-9-plus. We demonstrate evaluation results of multilingual KGQA systems on QALD-9-plus in Section IV and conclude the paper in Section V.",
            "score": 0.5847499054926901,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "235271356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "235313293",
            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
            "text": "mechanism for cross-lingual transfer of NLU models (Upadhyay et al., 2018;Chen et al., 2018;Schuster et al., 2019a). In fact, even a small number of word alignments in the actual ToD domain of interest is sufficient to refine the CLWE space  or simulate code-switching data .\n\nAvailable Datasets. While multilingual NLU is better resourced than other tasks in modular ToD, the landscape of existing datasets is still sparsely populated. Table 1 Table 2: Multilingual NLU datasets. \u2660 xSID is an evaluation-only dataset, whereas all the other datasets listed here also contain target-language training sets.\n\navailable monolingual NLU datasets in languages other than English and Table 2  Ideally, NLU models should be able to generalise both over languages and over domains. Most existing datasets, however, either cover multiple domains in a single language (Hakkani-T\u00fcr et al., 2016; or the same domain across different languages . Fortunately, the most recent generation of NLU datasets (Li et al., 2021;van der Goot et al., 2021;Majewska et al., 2022) is both multi-lingual and multi-domain, thus opening up the possibility to assess the true generality of current cross-lingual transfer approaches.  Table 3: Multilingual DST datasets. Shaded rows correspond to datasets created from scratch, white rows to datasets translated from English. Abbreviations: human-to-machine (H2M) and human-to-human (H2H). A non-exhaustive list of English DST datasets is given in Table 10 in the Appendix.",
            "score": 0.5846137409616463,
            "section_title": "Natural Language Understanding (NLU)",
            "char_start_offset": 21509,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 74,
                    "matchedPaperCorpusId": "52291331"
                },
                {
                    "start": 74,
                    "end": 92,
                    "matchedPaperCorpusId": "52050424"
                },
                {
                    "start": 92,
                    "end": 115,
                    "matchedPaperCorpusId": "53110354"
                },
                {
                    "start": 858,
                    "end": 884,
                    "matchedPaperCorpusId": "5775306"
                },
                {
                    "start": 989,
                    "end": 1006,
                    "matchedPaperCorpusId": "221246333"
                },
                {
                    "start": 1006,
                    "end": 1032,
                    "matchedPaperCorpusId": "234742634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354248046875
        },
        {
            "corpus_id": "252873067",
            "title": "DATScore: Evaluating Translation with Data Augmented Translations",
            "text": "We compare the performance of our metric against BLEU and three other reference-based unsupervised metrics: BERTScore5 , MoverScore6 and BARTScore7 (detailed in Subsection 2.1 and Section 3), using their official implementations. Experimental results are reported in Table 1 and 2. Following their original settings, we use different underlying language models for each baseline metric. For BERTScore and MoverScore, RoBERTa-Large (RL; Liu et al., 2019) and Bert-Base (BB) are used respectively when we evaluate to-English translations, and mBERT (Devlin et al., 2019) for from-English translations. In the case of BARTScore, we use a BART-Large (BL) checkpoint (finetuned on CNNDM (See et al., 2017) and (Hu et al., 2019) datasets) for evaluating to-English translations, and an mBART-50 model (Escolano et al., 2021) for from-English translations. \n\nOverall, results show that, on average, across all language pairs, DATScore significantly outperforms all 4 baseline metrics under their original model settings (rows 1a-1d and 2a-2d). Specifically, with respect to the best performing baseline BERTScore (row 1b and 2b), our metric provides a performance boost of 0.7 for to-English case and of 9.8 for from-English case on WMT17 dataset in Table 1, and achieves a gain of 0.7 and of 7.3 respectively on WMT18 dataset in Table 2. These averaging results demonstrate the superiority and applicability of DATScore in evaluating general machine translations of many languages. Moreover, it is interesting to note that our improvement is much more significant in from-English case, which makes DATScore particularly well-suited to evaluate hypothesis translations in non-English languages, often with low resource. We hypothesize that this is due to the inconsistency of underlying language models. The baselines adopt a monolingual model for evaluating English, but a multilingual one for non-English languages. However, DATScore uses a single multilingual M2M-100 model for both cases.",
            "score": 0.5845888218903699,
            "section_title": "Main results",
            "char_start_offset": 15800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 568,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 682,
                    "end": 700,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "207984304"
                },
                {
                    "start": 795,
                    "end": 818,
                    "matchedPaperCorpusId": "215754272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1373291015625
        },
        {
            "corpus_id": "248780386",
            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
            "text": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition. \n\nRecently, there has been some interest in predicting performance of NLP models without actually evaluating them on a test set. Xia et al. (2020) showed that it is possible to build regression models that can accurately predict evaluation scores of NLP models under different experimental settings using various linguistic and dataset specific features. Srinivasan et al. (2021)   (c) Number of multilingual tasks containing test data for each of the 106 languages supported by the MMLMs (mBERT, XLMR). The bars are shaded according to the class taxonomy proposed by Joshi et al. (2020). \n\nFigure 1 results specifically for MMLMs towards predicting their performance on downstream tasks for different languages in zero-shot and few-shot settings, and Ye et al. (2021) propose methods for more reliable performance prediction by estimating confidence intervals as well as predicting fine-grained performance measures. \n\nIn this paper we argue that the performance prediction can be a possible avenue to address the current issues with Multilingual benchmarking by aiding in the estimation of performance of the MMLMs for the languages which lack any evaluation data for a given task. Not only this can help us give a better idea about the performance of a multilingual model on a task across a much larger set of languages and hence aiding in better model selection, but also enables applications in devising data collection strategies to maximize performance (Srinivasan et al., 2022) as well as in selecting the representative set of languages for a benchmark (Xia et al., 2020).",
            "score": 0.5842719438886598,
            "section_title": "Introduction",
            "char_start_offset": 1631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 888,
                    "matchedPaperCorpusId": "218487089"
                },
                {
                    "start": 1310,
                    "end": 1329,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1494,
                    "end": 1510,
                    "matchedPaperCorpusId": "231861672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "261394577",
            "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
            "text": "The absence of high-quality, parallel evaluation benchmarks is a major obstacle in assessing the text comprehension capabilities of multilingual models. NLP datasets with high language coverage do exist, such as FLORES-200 (NLLB et al., 2022), but they primarily focus on machine translation. Popular multilingual evaluation benchmarks, such as multilingual question answering (Lewis et al., 2020;Clark et al., 2020), natural language inference (NLI) (Conneau et al., 2018), and summarization (Ladhak et al., 2020;Hasan et al., 2021), altogether only cover around 30 languages. And while understanding and generative text services are used across the globe in 100+ languages, the lack of labeled data provides a major obstacle to building functional systems in most languages. \n\nSimultaneously, large language models (LLMs) have become increasingly popular. Certain LLMs, like BLOOM (Scao et al., 2022), are trained on multilingual data and tout their innate multilingual capabilities. Others like GPT-3 (Brown et al., 2020) and LLAMA (Touvron et al., 2023a) have demonstrated multilingual competence despite their training data being predominantly in English. Even so, LLMs benefit from pretraining data that is linguistically diverse, intentionally or not, as well as from cross-lingual transfer (Zoph et al., 2016;Artetxe et al., 2020;Muller et al., 2021b). But how multilingual are these models really? Beyond LLMs, significant scientific progress needs to be made before NLP systems can be built effectively and efficiently in low-resource languages. Many modeling techniques are being presented as languageagnostic but have only truly been evaluated in a small number of languages (Bender, 2011), risking not being applicable to diverse typologically phenomena (Bender, 2009). We believe that largescale, parallel, and discriminative datasets are crucial for studying the multilingual capabilities of such models and understanding how the technological disparity between high-and low-resource languages is evolving.",
            "score": 0.5842424522530415,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 397,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 397,
                    "end": 416,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "222177239"
                },
                {
                    "start": 514,
                    "end": 533,
                    "matchedPaperCorpusId": "235658519"
                },
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1298,
                    "end": 1317,
                    "matchedPaperCorpusId": "16631020"
                },
                {
                    "start": 1317,
                    "end": 1338,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1338,
                    "end": 1359,
                    "matchedPaperCorpusId": "231718746"
                },
                {
                    "start": 1687,
                    "end": 1701,
                    "matchedPaperCorpusId": "317797"
                },
                {
                    "start": 1767,
                    "end": 1781,
                    "matchedPaperCorpusId": "5449971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "266999601",
            "title": "Question Translation Training for Better Multilingual Reasoning",
            "text": "Below we discuss potential limitations of our work: \n\n\u2022 Chain-of-Thought in English: When receiving non-English questions, our language-aligned LLM typically produces an English CoT before giving the final numerical answer. While the language used for the CoT is not explicitly specified as a requirement for the multilingual mathematical reasoning task, providing a CoT consistent with the query's language could enhance the model's utility. \n\n\u2022 Scale of the pre-trained LLM: Our experiment is constrained by available computational resources, leading us to utilize the LLaMA2-7B and LLaMA2-13B models. Should resources allow in the future, we aim to broaden our research to include larger-scale models, such as LLaMA2-70B. \n\nA Analyzing the Quality of the Translated Dataset \n\nIn the work of (Chen et al., 2023), the authors employ ChatGPT to translate GSM8K into several non-English languages, resulting in the creation of the multilingual dataset GSM8KINSTRUCT. \n\nBelow we analyze the translation quality of this dataset and highlight the challenges associated with translating complex CoT responses. We evaluate the translation quality of both questions and responses in a reference-free condition with COMETKiwi10 (Rei et al., 2022). The evaluation results in Table 8 show that the quality of the translated responses is significantly inferior to that of the translated questions. This gap demonstrates the difficulties inherent in translating CoT content. \n\nTable 9 provides some examples of typical translation errors. Based on this analysis, we suggest that constructing a multilingual CoT dataset through a translation engine is fraught with errors and cannot ensure the quality of the dataset. In constrast, our devised framework provides a more effective and efficient solution, which does not require translated multilingual CoT.",
            "score": 0.5842045849873193,
            "section_title": "Limitation",
            "char_start_offset": 20290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 54,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 1220,
                    "end": 1238,
                    "matchedPaperCorpusId": "252222165"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30615234375
        },
        {
            "corpus_id": "264306217",
            "title": "A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems",
            "text": "In scenario (2), we perform zeroshot cross-lingual transfer evaluation for each tar- mt for ID, SL, DST, and NLG tasks on MULTI3WOZ. Performance evaluations were conducted for two distinct model categories: large models, namely XLM-R large and mT5 large , and small models, specifically XLM-R base and mT5 small . The reported results follow the format of \"large model small model \". For example, the entry 93.2 92.0 denotes XLM-R large achieves 93.2 accuracy in English ID whereas XLM-R base achieves 92.0. get language. To achieve this, we train an English system P ENG (\u2022) using the D ENG dataset, and subsequently assess its performance on the target language datasets, namely D ARA , D FRA , and D TUR . \n\nLastly, in scenario (3), we utilise the 'F&E' proportion of the GlobalWOZ dataset (Ding et al., 2022) as D TGT mt . This dataset leverages a Google Translate to convert English utterances into the target language while maintaining the slot values associated with entities in English. 8  Table 1 presents the experimental results for the three scenarios. Despite employing state-of-theart mPLMs, we observe a substantial cross-lingual transfer gap (Hu et al., 2020) across all tasks, underscoring the significance of in-domain in-language data for the development of TOD systems. Moreover, in zero-shot setups, larger models exhibit better cross-lingual transferability for SL, ID, and DST. However, this advantage diminishes when full-sized training data becomes available. \n\nFor translate-train, we observe that the system trained using the MT-ed dataset D TGT mt underperforms the fully supervised systems P TGT (\u2022). Interestingly, the translate-train system performs worse than the English system P ENG (\u2022), even in the zeroshot cross-lingual transfer setup, for ID and SL. 9 8 Instead of employing an MT system to translate D ENG into D TGT mt , we utilise the GlobalWOZ dataset as our translate-train dataset.",
            "score": 0.5839934985665893,
            "section_title": "Results and Discussion",
            "char_start_offset": 21564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 1158,
                    "end": 1175,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134033203125
        },
        {
            "corpus_id": "212644529",
            "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity",
            "text": "Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language). Moreover, the previous SimLex-based multilingual datasets inherit the main deficiencies of the English original version, such as the focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken and do not account for the variety of the world's languages. Our long-term goal is devising a standardized methodology to extend the coverage also to languages that are resource-lean and/or typologically diverse (e.g., Welsh, Kiswahili as in this work). \n\nMultilingual Datasets for Natural Language Understanding. The Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual BERT (Devlin et al. 2019) or XLM (Conneau and Lample 2019) are typically probed on XNLI test data (Conneau et al. 2018b) for cross-lingual natural language inference. XNLI was created by translating examples from the English MultiNLI dataset, and projecting its sentence labels (Williams, Nangia, and Bowman 2018). Other recent multilingual datasets target the task of question answering based on reading comprehension: i) MLQA (Lewis et al. 2019) includes 7 languages ii) XQuAD (Artetxe, Ruder, and Yogatama 2019) 10 languages; iii) TyDiQA (Clark et al. 2020) 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X (Yang et al. 2019), focused on the paraphrase identification task and was created translating the original English PAWS (Zhang, Baldridge, and He 2019) into 6 languages.",
            "score": 0.5836602297030185,
            "section_title": "Previous Work and Evaluation Data",
            "char_start_offset": 15090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 1065,
                    "end": 1085,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1093,
                    "end": 1118,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1158,
                    "end": 1179,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1338,
                    "end": 1373,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1601,
                    "end": 1620,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "258841859",
            "title": "Revisiting Machine Translation for Cross-lingual Classification",
            "text": "We next dissect the specific sources of cross-lingual transfer gap that each approach is sensitive to: Translate-test. All the degradation comes from MT, as no multilingual model is used. We distinguish between (i) the information lost in the translation process (either caused by translation errors or superficial patterns removed by MT), and (ii) the distribution shift between the original data seen during training and the machine translated data seen during evaluation (e.g., stylistic differences like translationese that existing models might struggle generalizing to, even if no information is lost). \n\nZero-shot. All the degradation comes from the multilingual model, as no MT is used. We distinguish between (i) source-language representation quality 6 relative to a monolingual model (English-only models being typically stronger than their multilingual counterparts, but only usable with translate-test), (ii) target-language representation quality relative to the source language (the representations of the target languagetypically less-resourced-being worse than those of the source language), and (iii) representation misalignment between the source and the target language (even when a model has a certain capability in both languages, there can be a performance gap when generalizing from the source to the target language if the languages are not well-aligned). \n\nTranslate-train. The degradation comes from both MT and the multilingual model. However, while both source and target language representation quality have an impact, 7 this approach is not sensitive to representation misalignment, as the model is trained and tested in the same language. Regarding MT, there is no translation and therefore no information lost at test time, so we can consider potential translation errors at training time to be further inducing a distribution shift. \n\nFinally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those 6 We consider that a pretrained model A has learned higher quality representations than model B if fine-tuning it in our target task results in better downstream performance.",
            "score": 0.5835820518131563,
            "section_title": "Sources of cross-lingual transfer gap",
            "char_start_offset": 18036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "235683093",
            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
            "text": "While popular multilingual zero-shot benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) provide development and test sets in tens of diverse low-resource languages, their transfer sets are limited to English. Even for tasks such as TyDi QA (Clark et al., 2020) that originally had training data in multiple languages, the authors of the benchmark removed any non-English data from the transfer set. The effect is that most studies mentioned above defaulted their analysis to English as the only transfer language. More recently however, the XTREME-R benchmark (Ruder et al., 2021) introduced two truly cross-lingual retrieval tasks, where both training and evaluation data use a mixture of languages (Roy et al., 2020;Botha et al., 2020). \n\nPrevious studies on the effectiveness of source languages focus on a single target language, or a small set of related ones. Lauscher et al. (2020) observe strong correlations between transfer performance and multiple measures of linguistic proximity between the transfer and target language, including syntax and morphology. Lin et al. (2019) automatically identify the most effective transfer languages via a ranker that leverages various distances (geographic, genetic, syntactic, phonological, etc) between a single target language and multiple transfer candidates. In contrast, we seek to find languages that transfer to many targets, potentially not even known a priori. \n\nThe pursuit of a language that can help others is also reminiscent of pivot-based machine translation (source \u2192 pivot + pivot \u2192 target), where a high-resource pivot bridges the gap between pairs of languages with insufficient parallel training data (Cheng et al., 2017;Kim et al., 2019;Dabre et al., 2021). English was shown to not always be the best pivot for machine translation (Paul et al., 2013;Dabre et al., 2015), which prompted us to investigate whether that is also the case for zeroshot cross-lingual transfer.",
            "score": 0.5834184964964233,
            "section_title": "Transfer Language Selection",
            "char_start_offset": 6446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 111,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 724,
                    "end": 742,
                    "matchedPaperCorpusId": "215745460"
                },
                {
                    "start": 742,
                    "end": 761,
                    "matchedPaperCorpusId": "226254361"
                },
                {
                    "start": 890,
                    "end": 912,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1091,
                    "end": 1108,
                    "matchedPaperCorpusId": "170078772"
                },
                {
                    "start": 1693,
                    "end": 1713,
                    "matchedPaperCorpusId": "12887697"
                },
                {
                    "start": 1713,
                    "end": 1730,
                    "matchedPaperCorpusId": "202712706"
                },
                {
                    "start": 1825,
                    "end": 1844,
                    "matchedPaperCorpusId": "17222548"
                },
                {
                    "start": 1844,
                    "end": 1863,
                    "matchedPaperCorpusId": "6167315"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15087890625
        },
        {
            "corpus_id": "52098405",
            "title": "Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach",
            "text": "In this section, we describe the evaluation tasks, the datasets used, and the experimental details of the proposed approach. Evaluation tasks. We evaluate our approach on several tasks: \u2022 To evaluate the quality of the bilingual mappings generated, we evaluate our algorithms primarily for the bilingual lexicon induction (BLI) task, i.e., word translation task and compare Precision@1 with previously reported state-of-the-art results on benchmark datasets (Dinu and Baroni, 2015;Artetxe et al., 2016;Conneau et al., 2018). \n\n\u2022 We also evaluate on the cross-lingual word similarity task using the SemEval 2017 dataset. \n\n\u2022 To ensure that quality of embeddings on monolingual tasks does not degrade, we evaluate the quality of our embeddings on the monolingual word analogy task (Artetxe et al., 2016). \n\n\u2022 To illustrate the utility of representing embeddings of multiple language in a single latent space, we evaluate our multilingual embeddings on the one-hop translation task, i.e., a direct dictionary between the source and target languages is not available, but the source and target languages share a bilingual dictionary with a pivot language. Datasets. For bilingual and multilingual experiments, we report results on the following widely used, publicly available datasets: \n\n\u2022 VecMap: This dataset was originally made available by Dinu and Baroni (2015) with subsequent extensions by other researchers (Artetxe et al., 2017(Artetxe et al., , 2018a)). It contains bilingual dictionaries from English (en) to four languages: Italian (it), German (de), Finnish (fi) and Spanish (es). The detailed experimental settings for this BLI task can be found in Artetxe et al. (2018b). \n\n\u2022 MUSE: This dataset was originally made available by Conneau et al. (2018). It contains bilingual dictionaries from English to many languages such as Spanish (es), French (fr), German (de), Russian (ru), Chinese (zh), and vice versa.",
            "score": 0.5823023657963369,
            "section_title": "Experimental Settings",
            "char_start_offset": 18372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 481,
                    "matchedPaperCorpusId": "17910711"
                },
                {
                    "start": 481,
                    "end": 502,
                    "matchedPaperCorpusId": "1040556"
                },
                {
                    "start": 502,
                    "end": 523,
                    "matchedPaperCorpusId": "3470398"
                },
                {
                    "start": 779,
                    "end": 801,
                    "matchedPaperCorpusId": "1040556"
                },
                {
                    "start": 1341,
                    "end": 1363,
                    "matchedPaperCorpusId": "17910711"
                },
                {
                    "start": 1412,
                    "end": 1433,
                    "matchedPaperCorpusId": "13335042"
                },
                {
                    "start": 1433,
                    "end": 1459,
                    "matchedPaperCorpusId": "4334731"
                },
                {
                    "start": 1660,
                    "end": 1682,
                    "matchedPaperCorpusId": "21728524"
                },
                {
                    "start": 1740,
                    "end": 1761,
                    "matchedPaperCorpusId": "3470398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "246294502",
            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
            "text": "We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer. \n\nBy virtue of the newly created IGLUE benchmark, we also run the first systematic comparative evaluation of cuttingedge multilingually pretrained V&L architectures (Ni et al., 2021;Zhou et al., 2021;Liu et al., 2021), as well as a series of representative monolingual V&L models combined with machine translation (Lu et al., 2019;Chen et al., 2020, inter alia) across a range of diverse V&L tasks and languages. \n\nThis evaluation offers new insights on the models' core strengths and current limitations. Foreshadowing, we showcase large gaps between performance in English and transfer performance, where (to a limited extent) the gaps are more prominent in lower-resource languages and languages more distant from English. In addition, leveraging target-language in-task few shots is remarkably arduous: whereas in previous text-only experiments (Lauscher et al., 2020), ceteris paribus there are huge gains of few-shot scenarios over their zero-shot counterparts, we demonstrate that current V&L models often require thousands of examples before showing signs of improvement. Finally, performance also seems correlated with task difficulty: for instance, NLI shows the smallest gaps between English and other languages and QA benefits the most from few-shot adaptation. On the other hand, visually grounded reasoning and cross-modal retrieval appear to be harder in both respects. \n\nContributions. 1) In order to guide and inspire more work in the area of multilingual V&L research, we present a first evaluation benchmark for cross-lingual transfer learning for V&L tasks, spanning 20 languages, 5 datasets, and 4 different tasks. 2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research.",
            "score": 0.5821292463007914,
            "section_title": "Introduction",
            "char_start_offset": 3104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 330,
                    "matchedPaperCorpusId": "232478575"
                },
                {
                    "start": 330,
                    "end": 347,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 444,
                    "end": 461,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 461,
                    "end": 478,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 979,
                    "end": 1002,
                    "matchedPaperCorpusId": "226262344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "226282329",
            "title": "Low-Resource Adaptation of Neural NLP Models",
            "text": "Cross-lingual learning has a fairly short history in NLP, and has mainly been restricted to traditional NLP tasks, such as PoS tagging and parsing. In contrast to these tasks, which have seen much cross-lingual attention Bjerva, 2017;de Lhoneux et al., 2018), there has been relatively little work on cross-lingual NLU, partly due to a lack of benchmark datasets. Existing work has mainly been focused on NLI (Agi\u0107 and Schluter, 2018;Conneau et al., 2018), and to a lesser degree on RE (Faruqui and Kumar, 2015;Verga et al., 2016) and QA (Lewis et al., 2020;Abdou et al., 2019). Previous research generally reports that cross-lingual learning is challenging and that it is hard to beat a machine translation baseline (e.g., Conneau et al. (2018)). Such a baseline is (for instance) suggested by Faruqui and Kumar (2015), where the text in the target language is automatically translated to English. For many language pairs, a machine translation model may be available, which can be used to obtain data in the target language. To evaluate the impact of using such data, in much of previous research work, the English training data is translated into the target language using a machine translation system. Then, the model is fine-tuned on the translated data and evaluated on the test set of target languages and reported as a TRANSLATE-TRAIN baseline. Alternatively, after fine-tuning the model on the English training data, a TRANSLATE-TEST baseline is introduced by evaluating the model on the test data that is translated from the target language to English using the machine translation system.\n\nIn this chapter, we show that our meta-learning based framework can achieve competitive performance compared to a machine translation baseline (for XNLI), and propose a method that requires no training instances for the target task in the target language.",
            "score": 0.58204517500553,
            "section_title": "Cross-Lingual NLU",
            "char_start_offset": 230001,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 258,
                    "matchedPaperCorpusId": "52111211"
                },
                {
                    "start": 409,
                    "end": 434,
                    "matchedPaperCorpusId": "3659314"
                },
                {
                    "start": 434,
                    "end": 455,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 486,
                    "end": 511,
                    "matchedPaperCorpusId": "1387162"
                },
                {
                    "start": 511,
                    "end": 530,
                    "matchedPaperCorpusId": "9206785"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "199577725"
                },
                {
                    "start": 724,
                    "end": 745,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 795,
                    "end": 819,
                    "matchedPaperCorpusId": "1387162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12286376953125
        },
        {
            "corpus_id": "248834196",
            "title": "Consistent Human Evaluation of Machine Translation across Language Pairs",
            "text": "In a second study, we examined the introduction of a calibration set to create meaningful scores that can be compared across language pairs. This enables for instance the decision if a machine translation system for a language pair is good enough to be put into production. \n\nEvaluators judge 1012 sentences pairs for a single language pair in both language directions. In this study, we only use the XSTS score. Translations are judged against the source sentence. Machine translations a generated with a state-of-the-art multilingual machine translation system. Evaluators also judge the human reference translation. \n\nThe crucial addition to the sentence pairs to be judged is a calibration set of sentence pairs that is common across all languages. It consists of 1000 pairs of a machine translation into English and a corresponding English reference translation. These sentence pairs are carefully selected to span a wide quality range, based on human-scored translations from previous evaluations where multiple evaluators agreed on the score. These scores were used as the consensus quality score. \n\nA fair objection to using such a calibration set is that we are asking evaluators to perform two different tasks -comparing machine translation against a source sentence (English and non-English), and comparing machine translation against a reference (English and English)but posit that they will use the same standards when making quality assessments. \n\nBecause the calibration set is fixed, its quality is fixed, and the average score each evaluator assigns to the sentence pairs in the set should be the same. Hence, we can use the actual score assigned by each evaluator and the official fixed score as the basis to make adjustments to each evaluator's score. For instance, if an evaluator gives the calibration too high score, then we detect that they are too lenient and their scores need to be corrected downward. \n\nNote that there is also a second fixed point that could be used for score adjustment: the average score each evaluator gives to the reference translation. These professionally translated and vetted translations should receive high scores, and we could adjust each evaluator's scores so that they average adjusted score for reference translations is a fixed value. The underlying assumption here is that reference translations are of identical quality across all language pairs.",
            "score": 0.5820089956894771,
            "section_title": "Study on Calibration",
            "char_start_offset": 15183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "258557740",
            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
            "text": "learning (Hu et al., 2020;Nag et al., 2021). Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;Ozaki et al., 2021;Yarmohammadi et al., 2021).\n\nThe MultiTACRED dataset we present in this work covers 12 languages from 9 language families. 1 We select typologically diverse languages which span a large set of linguistic phenomena such as compounding, inflection and pronoun-drop, and for which a monolingual pretrained language model is available. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances. In addition, we experimentally evaluate fine-tuned pretrained monoand multilingual language models (PLM) in common training scenarios, using source language (English), target language, or a mixture of both as training data. We also evaluate an English data fine-tuned model on back-translated test instances to estimate the effect of noise introduced by the MT system on model performance. Our results show that in-language training works well, given a suitable PLM. Cross-lingual zero-shot transfer is acceptable for languages well-represented in the multilingual PLM, and combining English and target language data for training considerably improves performance across the board.\n\nTo summarize, our work aims to answer the following research questions: Can we reaffirm the usefulness of MT and cross-lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can",
            "score": 0.5819451439131464,
            "section_title": "Introduction",
            "char_start_offset": 1679,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 43,
                    "matchedPaperCorpusId": "239024671"
                },
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "233189585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "259370570",
            "title": "Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation",
            "text": "We find that gains coming from high quality data are further enhanced when training multilingual models of closely related high-and low-resource languages, and even more so when augmenting the dataset via backtranslation. \n\nOverall, our results show that employing relatively small but high-quality, professionally translated datasets constitutes a promising and viable way towards achieving performant machine translation for low-resource languages, especially for those with high-resource relatives. This holds true even for languages for which some pre-existing data might already be publicly available, further highlighting the importance of high-quality training datasets. \n\nNotably, parallel datasets of the scale discussed here are compact enough that coverage for a new language could plausibly be collected by a relatively small group of volunteers in a week, making these results relevant for the usage of machine translation technologies in crisis situations (Lewis et al., 2011). \n\nOur main contributions are: \n\n1. The creation and public release of a professionally translated seed dataset for 39 lowresource languages. \n\n1 \n\n2. An analysis of the impact of this high-quality data, both in isolation and also when combined with pre-existing datasets, based on hundreds of trained models. \n\n3. A study of how gains from high-quality parallel data compound when using multilingual training and backtranslation, showing that benefits from high-quality data do not get washed away when using stronger models or data augmentation.",
            "score": 0.58191029193558,
            "section_title": "Introduction",
            "char_start_offset": 1935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1538
                }
            ],
            "ref_mentions": [
                {
                    "start": 970,
                    "end": 990,
                    "matchedPaperCorpusId": "2636150"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41162109375
        },
        {
            "corpus_id": "273638478",
            "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework",
            "text": "Evaluation Tasks We conduct evaluations on a variety of multilingual benchmarks, covering both generation and classification tasks. 1) For generation tasks, we consider FLORES (Team\", 2022), a benchmark for machine translation, and MGSM (Shi et al.), a multilingual math reasoning task. 2) For classification tasks, we utilize XNLI (Conneau et al., 2018), XCOPA (Ponti et al., 2020), andXStoryCloze (Lin et al., 2022), which are widely used generic reasoning datasets. \n\nFor the evaluation of MGSM, we utilize MGSM8KInstruct (Chen et al., 2023a) as the training set, which translates the GSM8K into nine non-English languages. For the evaluation of the other tasks, we follow Li et al. (2024a) and utilize Bactrian-X (Li et al., 2023b), which has been translated into 52 languages from Alpaca (Taori et al., 2023) and Dolly (Conover et al., 2023), as the training set. See Appendix A.3 for more details about the datasets we used in the experiment. \n\nMetrics For MGSM, we implement a rule-based extraction strategy (Chen et al., 2023a) to derive accuracy results in a zero-shot manner. We utilize the evaluation framework introduced by Zhang et al. (2024d) for assessing the other benchmarks in a 4-shot manner. Specifically, we assess the performance on the FLORES dataset using ChrF++ (Popovi\u0107, 2017) score, while the performance on the other datasets is evaluated based on rank classification accuracy. 6 raining Setup We incorporate LLMs from different families, such as Llama (Touvron et al., 2023), BLOOM (Scao et al., 2022), and XGLM (Lin et al., 2022), in our experiments. We utilize English as the dominant language in these three model families, as its data predominates in their corresponding pre-training corpus.",
            "score": 0.5815056804778913,
            "section_title": "Experiment Settings",
            "char_start_offset": 13202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 1136,
                    "end": 1156,
                    "matchedPaperCorpusId": "267760108"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "258715321",
            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
            "text": "Figure 1: Cross-lingual PQA: The user asks questions about a product in their language (such as Hindi), then the system searches for product information in English and generates an answer in the same language as the question. \n\nthemselves or seek help from humans (Li et al., 2017;Carmel et al., 2018). In our globalized world, it is essential to enable this technology for customers from different backgrounds. However, existing research focuses predominantly on English and leaves aside other language users. One of the biggest obstacles is the lack of datasets, which prevents us from training, evaluating and developing non-English PQA systems. Despite the growing number of multilingual QA datasets, their main focus is on general domains such as Wikipedia, which generalize poorly when applied to the PQA task, as we show in our experiments. \n\nTo address this, we present xPQA, the first largescale dataset for cross-lingual PQA enabling non-English questions to be answered from English content. Most comprehensive product information is usually available in a majority language such as English. Therefore, searching for relevant information in English often has a better chance of finding an answer. 2 This paper explores how to effectively train systems that retrieve information from English and generate answers in the question language to allow users to ask questions in any language. Fig 1 shows an example. \n\nMost existing multilingual QA datasets are cre-ated by translating English questions, introducing translation artifacts and discrepencies from native speakers' real information-seeking behaviors (Clark et al., 2020a). Instead, we collect questions from the original market places as written by native speakers, hire bilingual annotators to check the relevant product information and write the final answers in their target languages. This eliminates the need for translations and ensures that the information-seeking behaviors of native speakers are accurately represented. \n\nBased on the collected dataset, we report baseline results on two subtasks: (a) candidate ranking, which selects the best English candidate that contains the information to answer the non-English question; (b) answer generation, which generates a natural-sounding non-English answer to present to the user based on the selected English candidate.",
            "score": 0.5811003207877604,
            "section_title": "Answer:",
            "char_start_offset": 500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 228,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 281,
                    "matchedPaperCorpusId": "26030198"
                },
                {
                    "start": 281,
                    "end": 301,
                    "matchedPaperCorpusId": "195350173"
                },
                {
                    "start": 1618,
                    "end": 1639,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.564453125
        },
        {
            "corpus_id": "261557946",
            "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
            "text": "There are also existing datasets for multilingual QA. For example, XOR-TYDI (Asai et al., 2021) is a cross-lingual QA dataset built on TYDI by annotating answers in English Wikipedia for questions that TYDI considers unanswerable in the original source (non-English) language. This setup, unfortunately, does not allow researchers to examine monolingual retrieval in non-English languages. \n\nAnother point of comparison is MKQA (Longpre et al., 2021), which comprises 10k question-answer pairs aligned across 26 typologically diverse languages. Questions are paired with exact answers in the different languages, and evaluation is conducted in the open-retrieval setting by matching those answers in retrieved text-thus, MKQA is not a ''true'' retrieval dataset. Furthermore, because the authors translated questions to achieve cross-lingual alignment, the translations may not be ''natural'', as pointed out by Clark et al. (2020). 'natural'', i.e., generated by native speakers (vs. humanor machine-translated), and for queries, in natural language (vs. keywords or entities); Human Labels indicates human-generated labels (vs. synthetically generated labels); # Lang is the number of languages supported; Avg # Q is the average number of queries for each language; Avg # Labels/Q is the average number of labels provided per query; Total # Labels is the total number of human labels (both positive and negative) across all languages (including synthetic labels in CLIRMatrix). Training? indicates whether the dataset provides sufficient data for training neural models.",
            "score": 0.5810562318541044,
            "section_title": "Comparison to Multilingual QA Datasets",
            "char_start_offset": 5171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1572
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 95,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "220871404"
                },
                {
                    "start": 912,
                    "end": 931,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386474609375
        },
        {
            "corpus_id": "273532388",
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "text": "In aligning LLMs, evaluator LLMs play a central role-from training (Pace et al., 2024) and evaluation (Zheng et al., 2023) to test-time inference (Gao et al., 2023;Wu et al., 2024a;Son et al., 2025). Naturally, the critical role of evaluator LLMs has led to the emergence of meta-evaluation benchmarks, which aim to directly assess the quality of evaluators. These benchmarks typically present pairs of candidate responses with a predefined preference-based on either correctness or human judgments-and measure the evaluator LLM's accuracy by how often it assigns a higher score to the preferred (chosen) response over the unpreferred (rejected) response. \n\nWhile multilingual LLMs also depend on automated evaluators, we still lack clear guidelines on evaluating them across diverse languages. A concurrent effort, M-RewardBench (Gureja et al., 2024), attempts to fill this gap by machine translating the original RewardBench into multiple languages. However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments.",
            "score": 0.5806204852995731,
            "section_title": "Preliminary",
            "char_start_offset": 5592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 122,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 146,
                    "end": 164,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 830,
                    "end": 851,
                    "matchedPaperCorpusId": "261822638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "273404117",
            "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
            "text": "Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.",
            "score": 0.5801966760697993,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "270924100",
            "title": "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs",
            "text": "There have been limited efforts on multilingual preference optimization to-date.(Lai et al., 2023) present a multilingual instruction tuning framework, where they preference train multilingual LLMs such as BLOOMZ (Muennighoff et al., 2023b) for 26 non-English languages with RLHF.They synthetically generated a preference dataset by translating an extended version of the Alpaca dataset (Taori et al., 2023), generating responses from their target LLM and ranking back-translated (into English) responses with ChatGPT.9In contrast to our work, they perform preference optimization for each language separately.However, due to their potentially low-quality dataset which heavily relies on translations, their resulting language-specific models are outperformed by other massively multilingual LLMs without preference optimization (\u00dcst\u00fcn et al., 2024;Aryabumi et al., 2024).Wu et al. ( 2024) study cross-lingual transfer in reward model (RM) training where they propose using preference data in one source language to train an RM for target language alignment.They show that RMs based on amultilingual base model exhibit zero-shot cross-lingual transfer consistently across different languages.However, they do not experiment with using multiple source languages in training, which we show is crucial in the preference optimization both for offline optimization such as DPO and online RL methods such as RLOO.A number of existing works also explore methods for preference optimization in a highly related setting where LLMs are aligned to preferences of diverse groups of people around the world (Zhao et al., 2024;Jiang et al., 2023b;Hwang et al., 2023;Deshpande et al., 2023).",
            "score": 0.5798852282743056,
            "section_title": "Preference Optimization for Multilingual LLMs",
            "char_start_offset": 25383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 80,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 519
                },
                {
                    "start": 519,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1192
                },
                {
                    "start": 1192,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1676
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 98,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 213,
                    "end": 240,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 1594,
                    "end": 1613,
                    "matchedPaperCorpusId": "264289064"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1588134765625
        },
        {
            "corpus_id": "235651933",
            "title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders",
            "text": "Besides multilingual language generation, we also test the performance on cross-lingual language generation tasks, where the target language is different from the source language. We perform experiments on machine translation, cross-lingual text summarization, and data-to-text generation. \n\nMachine Translation As for machine translation, we evaluate the models on the large-scale WMT-10 benchmark dataset (Wang et al., 2020;Ma et al., 2020). This dataset is a collection of parallel data in different languages from the WMT shared tasks. The parallel data is between English (En) and other 10 languages, including French (Fr), Czech (Cs), German (De), Finnish (Fi), Latvian (Lv), Estonian (Et), Romanian (Ro), Hindi (Hi), Turkish (Tr) and Gujarati (Gu). It contains 32.5 million sentence pairs in the training set. We combine all the parallel data in different languages as the training set and evaluate the models on the test sets in each language. We report the case-sensitive detokenized BLEU using sacreBLEU 1 (Post, 2018). We adopt WikiLingua (Ladhak et al., 2020) as the benchmark dataset. It is a large-scale multilingual dataset with about 770k article-summary pairs. The dataset is constructed from WikiHow. Following the previous work (Gehrmann et al., 2021), we perform experiments in the language directions from Spanish (Es), Russian (Ru), Vietnamese (Vi) and Turkish (Tr) to English (En). The evaluation metric is ROUGE (Lin, 2004), including ROUGE-1, ROUGE-2, and ROUGE-L. For a fair comparison with the previous work (Gehrmann et al., 2021), we report the results on the validation set.",
            "score": 0.5796020661908095,
            "section_title": "Cross-lingual Language Generation",
            "char_start_offset": 10745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 426,
                    "matchedPaperCorpusId": "222141016"
                },
                {
                    "start": 1016,
                    "end": 1028,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 1436,
                    "end": 1447,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384033203125
        },
        {
            "corpus_id": "267406903",
            "title": "Automatic Translation of Span-Prediction Datasets",
            "text": "The most popular QA dataset is SQuAD v1.1 (Rajpurkar et al., 2016), containing 100K questionanswer pairs in English. It has been extended in SQuAD v2.0 1 (Rajpurkar et al., 2018), with 50K questions that have no answer in the given content. \n\nA popular non-English version of this benchmark is the XQuAD 2 benchmark dataset for evaluating cross-lingual QA performance. This dataset consists of only 1,190 question-answer pairs from the development set of SQuAD v1.1 translated into ten languages by professional translators (Artetxe et al., 2020). It also includes the XQuAD-Translate-train dataset, a machine-translated version of the full SQuAD v1.1 train set. \n\nThe current state-of-the-art results on SQuAD v1.1 were achieved by using Google's T5-11B model (Raffel et al., 2019), with F1 and EM scores of 96.22 and 91.26, respectively. In comparison, the equivalent multilingual model, mT5, trained on XQuAD-Translate-train dataset achieved only 85.2/71.3 F1 and EM scores (Xue et al., 2021(Xue et al., , 2022) ) averaged on ten languages (Arabic, German, Greek, Spanish, Hindu, Russian, Thai, Turkish, Vietnamese, Chinese), highlighting a significant gap of 20% in EM performance. \n\nMoreover, even when using large amounts of unlabeled data for pre-training large language models (LLMs), better results can be achieved in the QA task by fine-tuning the model using a dataset labeled explicitly for the task. GPT-3, as an example, achieves F1 of 69.8 on SQuAD v2.0 (Rajpurkar et al., 2018) when attempting few-shot predictions. In contrast, the fine-tuned current state-ofthe-art models achieve F1 of 93 on the same dataset (Brown et al., 2020).",
            "score": 0.5790049418429297,
            "section_title": "Related Work",
            "char_start_offset": 6812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1649
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 66,
                    "matchedPaperCorpusId": "253279499"
                },
                {
                    "start": 154,
                    "end": 178,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 524,
                    "end": 546,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1469,
                    "end": 1493,
                    "matchedPaperCorpusId": "47018994"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "226262367",
            "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
            "text": "Here, we present E\u03c7\u03b1\u00b5s, a new dataset and benchmark for multilingual and cross-lingual evaluation of models and methods for answering diverse school science questions (see Figure 1). \n\nOur contributions are as follows: \n\n\u2022 We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade. \u2022 We use fine-grained evaluation -per subject and per language -which yields more precise comparison between models. \u2022 We perform extensive experiments and analysis using top-performing multilingual models (mBERT, XLM-R), and we show that E\u03c7\u03b1\u00b5s offers several challenges that such models would need to overcome in the future, including multi-lingual and crosslingual knowledge retrieval, aggregation, and reasoning, among others. We release our code, pre-trained models and data for research purposes.1",
            "score": 0.5787373864896417,
            "section_title": "arXiv:2011.03080v1 [cs.CL] 5 Nov 2020",
            "char_start_offset": 1995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "274149965",
            "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
            "text": "In this section we list some limitations of our work. \n\nLanguages included. Our primary focus is on English-German language training, as these two Germanic family languages share linguistic similarities (Lewis et al., 2015). German is one of the most well-represented languages in the mC4 dataset, facilitating model comparisons with varying amounts of German and English data. Furthermore, the availability of extensive public resources for German, including translation systems and translated evaluation data, further supports our emphasis on this language pair. We experimented with seven additional datasets including French, Spanish, Italian, Portuguese, Korean, Japanese, and Chinese. However, we note that there are many other languages within mC4 and more broadly which can benefit from having auxiliary English data for pretraining. Due to limited evaluation benchmarks and availability of target language data for comparison, we leave investigation for truly low-resource languages to future work. \n\nEvaluation data. Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models. Model size. Finally, this work studies three model sizes up to 3B models. We note that there are many standard benchmarks that can be evaluated at 1B-3B scale, however many more benchmarks and patterns can appear at larger model sizes. It is important future work to evaluate whether the results extend to larger scales including evaluating potential \"emergent behaviors\" as well as risks at larger scales (Wei et al., 2022).",
            "score": 0.5786196967823409,
            "section_title": "Limitations",
            "char_start_offset": 32229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "271270611",
            "title": "INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages",
            "text": "In light of the approaches discussed previously in Section 1, context-grounded question-answering datasets can generally be categorized into two types: abstractive and extractive. Although many extractive datasets exist for high-resource languages, the few available for Indian languages lack diversity in domains and question types, limiting their usefulness for benchmarking. Hence, we extended the benchmark suite available in English to these Indian languages by translating. We utilized IndicTrans24 (Gala et al., 2023) for translation, an open-source transformer-based multilingual NMT model that supports high-quality translations across all the 22 scheduled Indian languages. We segmented the context paragraph into sentences using the Spacy library, translated each sentence, and then recombined them. This approach yielded better translation results, and importantly, the model did not lose context when translating, thus preserving the coherence of the text. In the list of datasets for benchmarking, some are available only in English (e.g., NQ-open, ORQA, llama index,MS-Marco), while others are available in both English and Hindi (e.g., Hindi SQuAD, CHAII, MLQA, Synthetic data). Additionally, a few datasets (e.g., IndicQA, XSQuAD) are also available in all 10 or 11 languages with verified translations. For all the datasets not found in the respective language, we translated them and applied the filtering methods discussed below. \n\nTo assess the quality of our translations, we initially translated each dataset from the source language to the target language, followed by backtranslation from the target language back to the source language. We then calculated the CHRF scores (Popovi\u0107, 2015) between the original and back-translated sentences, using a threshold of 50 to filter the instances. Additionally, we manually verified a subset of the filtered data to ensure accuracy. For the translation process, we initially translated the English data directly into Hindi. After filtering the data, we then translated it from Hindi to other Indian languages, rather than directly from English. This approach was based on our observation that the translation quality from Hindi to other Indian languages was superior.",
            "score": 0.5785710828530047,
            "section_title": "Data Curation Methodology",
            "char_start_offset": 13702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 1698,
                    "end": 1713,
                    "matchedPaperCorpusId": "15349458"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46630859375
        },
        {
            "corpus_id": "247626152",
            "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
            "text": "Despite the need for evaluating LLMs for a wide range of languages, bias benchmarks that cover non-English languages are rare (Zhou et al., 2019;Joshi et al., 2020). As a solution, simply translating existing English benchmarks is not ideal: manual translation is a labor-intensive and highly skilled task, while automated translations are prone to errors and could potentially introduce new algorithmic sources of bias. Moreover, translated benchmarks may only test for Anglo-centric biases, which do not necessarily hold in many non-Western cultural contexts. For instance, many gender bias evaluations focus on Western professions, which are grammatically gendered in some languages (Chen et al., 2021;Zhou et al., 2019) or may not cover other prevalent occupations outside the U.S. (Escud\u00e9 Font and Costa-juss\u00e0, 2019). WinoMT (Stanovsky et al., 2019) is one of the few benchmarks that covers multiple languages, but it comes with its own downsides. The sentences are generated from templates that capture a limited range of actual language use; the samples are translated from English examples, which may not reflect how stereotypes would occur in other languages; and the scope is limited to machine translation systems, and therefore WinoMT may not be suitable for multilingual models that are not trained on this specific task. The tightly coupled nature of bias and cultural context should be emphasized when designing a multilingual bias benchmark.",
            "score": 0.5785255850654599,
            "section_title": "Bias Measures are Anglo-centric",
            "char_start_offset": 21025,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 145,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 145,
                    "end": 164,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 705,
                    "end": 723,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 830,
                    "end": 854,
                    "matchedPaperCorpusId": "173991101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404052734375
        },
        {
            "corpus_id": "260736143",
            "title": "Extrapolating Large Language Models to Non-English by Aligning Languages",
            "text": "Existing large language models show disparate capability across different languages, due to the imbalance in the training data. Their performances on English tasks are often stronger than on tasks of other languages. In this paper, we empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We start from targeting individual languages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e. tuning it with translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws to investigate the advantages of using scalable translation data. Then we perform multilingual instruction-tuning (MuIT) with mixed resources to build multilingual m-LLaMA. We also illustrate how we leverage the scaling laws to optimize data allocation in a resource-constrained setting. Experiment results on cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the English instruction-tuned counterpart (Alpaca) by an average of 27.83% across six non-English languages. Evaluation results on translation dataset Flores-101 show that x-LLaMAs outperform previous LLaMA-based models by an average of 18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on individual languages and demonstrates the ability to follow multilingual instructions. Further analysis on response content and representation space reveals the alignment of the multilingual semantic space within the middle layers of m-LLaMA.",
            "score": 0.578491120998688,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45947265625
        },
        {
            "corpus_id": "277510403",
            "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology",
            "text": "To mitigate the potential impact of language proficiency on reasoning evaluation, we adopt a crosslingual benchmarking approach. We construct two bilingual benchmark sets by translating ANES-BENCH and AMCQA in complementary directions. Specifically, ANESBENCH, originally developed in English, is translated into Chinese (EN2CN), while AMCQA, originally in Chinese, is translated into English (CN2EN). This process ensures that each dataset exists in both languages, enabling direct cross-lingual comparisons of model performance. \n\nWe employ GPT-4o [45] to achieve high-quality translations, which preserve the original meaning, logical structure, and domain-specific terminology. Furthermore, to enhance translation accuracy, we perform post-translation validation, including expert review and consistency checks against standardized anesthesiology references. The full prompt for translation is provided in Appendix C. \n\nThis cross-lingual benchmarking allows us to investigate critical aspect of LLM performance: how language transferability affects complex decision-making processes. By analyzing performance discrepancies between EN2CN and CN2EN tasks, we gain insights into how different linguistic structures influence the model's reasoning ability.",
            "score": 0.5783372478329789,
            "section_title": "Cross-Language Benchmarking",
            "char_start_offset": 10201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1257
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "264935209",
            "title": "COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances",
            "text": "Our data evaluation involves three different setups: monolingual, cross-lingual, translate-test, and colloquial test. Each setup is tailored to a specific scenario. For monolingual setup, we use the same language (Indonesian and Colloquial Indonesian) for training and testing. Zero-shot cross-lingual utilizes the English COPA dataset as the few-shot examples or training data for zero-shot classification and uses the COPAL-ID dataset for testing. \n\nIn contrast, the translate-test employs an Englishtranslated version of the COPAL-ID for testing instead. We use Seamless-M4T (Communication et al., 2023) for translation. \n\nThe evaluation under the monolingual setup is performed twice, one using COPAL-ID with standard Indonesian and the other with the colloquial dataset instead. We forego performing the colloquial testing on the two other setups, noting that the high lexical distinction (Section 4.4) would hamper the translation quality on the translate-test setup and that prior work has noted big degradation in cross-lingual performances of Indonesian local languages even for those with high lexical similarity with standard Indonesian (Winata et al., 2023). \n\nDetailed data and model setup can be seen in Table 3 and will be elaborated further in this section.",
            "score": 0.578065468260617,
            "section_title": "Experiment Setup",
            "char_start_offset": 14398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1273
                }
            ],
            "ref_mentions": [
                {
                    "start": 1148,
                    "end": 1169,
                    "matchedPaperCorpusId": "249209909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2073974609375
        },
        {
            "corpus_id": "7665329",
            "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity",
            "text": "Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013;de Melo, 2015;Vuli\u0107 and Moens, 2016;Ammar et al., 2016;Upadhyay et al., 2016), while at the same time cross-lingual applications have also been increasingly studied (Xiao and Guo, 2014;Franco-Salvador et al., 2016). However, there have been very few reliable datasets for evaluating cross-lingual systems. Similarly to the case of multilingual datasets, these cross-lingual datasets have been constructed on the basis of conventional English word similarity datasets: MC-30 and WordSim-353 (Hassan and Mihalcea, 2009), and RG-65 (Camacho-Collados et al., 2015). As a result, they inherit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets, constructed according to the procedure of Camacho-Collados et al. (2015), in a semi-automatic manner exploiting the monolingual datasets of subtask 1. These datasets constitute a reliable evaluation framework across five languages.",
            "score": 0.577891834269249,
            "section_title": "Subtask 2: Cross-lingual Semantic Similarity",
            "char_start_offset": 6498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1541
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "931054"
                },
                {
                    "start": 193,
                    "end": 207,
                    "matchedPaperCorpusId": "29661510"
                },
                {
                    "start": 207,
                    "end": 229,
                    "matchedPaperCorpusId": "16452496"
                },
                {
                    "start": 248,
                    "end": 270,
                    "matchedPaperCorpusId": "5357629"
                },
                {
                    "start": 358,
                    "end": 378,
                    "matchedPaperCorpusId": "501172"
                },
                {
                    "start": 378,
                    "end": 407,
                    "matchedPaperCorpusId": "30389163"
                },
                {
                    "start": 683,
                    "end": 710,
                    "matchedPaperCorpusId": "1856431"
                },
                {
                    "start": 722,
                    "end": 753,
                    "matchedPaperCorpusId": "7681779"
                },
                {
                    "start": 1021,
                    "end": 1047,
                    "matchedPaperCorpusId": "1856431"
                },
                {
                    "start": 1352,
                    "end": 1382,
                    "matchedPaperCorpusId": "7681779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "258298856",
            "title": "PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale",
            "text": "We presented PAXQA, a synthetic data generation method for cross-lingual QA which leverages indirect supervision from parallel datasets. We decompose the task into two stages: English QA generation, then QA translation informed by annotation projection. Unlike prior methods, PAXQA requires no training of new models, nor any non-English QA data to use for supervision. This means our method can even be applied to low-resource languages. We proposed the novel use of lexicallyconstrained MT to better translate questions, which assists in proper translation of uncommon entities. Finally, we showed that training on PAXQA data allows downstream models to significantly outperform zero-shot baselines, and achieve a new stateof-the-art on the MLQA benchmark. In order to facilitate future research in the field, we released our code and datasets. \n\nThe main limitation of our method is that it requires datasets which are parallel to English. However, because of the great efforts placed into collecting resources for machine translation, such datasets are relatively available. In the MT field, \"lowresource\" generally means less than 1M parallel sentences (Haddow et al., 2022). This is ample enough data to train automatic word aligners between English and some language, needed by our method. \n\nBecause of resource constraints on our end, we only ran our method end-to-end for three languages. However, weave claimed that by decomposing cross-lingual QG into English QG and MT steps, our method allows for QA generation in low-resource languages. As an initial step, we are running the PAXQA pipeline on the FLoRes v1 (Guzm\u00e1n et al., 2019) dataset, which covers Nepali and Sinhala. After the dataset is generated, we will investigate how we can evaluate the quality of generations for these languages not been studied by the QA community yet. While back-translation using NMT could be a first start, more likely this requires finding native human annotators. \n\nBeyond the parallel dataset limitation, we acknowledge that the English-centric nature of our approach is not ideal. We inherit this problem from the general body of cross-lingual QA research.",
            "score": 0.5776312164679361,
            "section_title": "Conclusion",
            "char_start_offset": 27353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 1158,
                    "end": 1179,
                    "matchedPaperCorpusId": "237371890"
                },
                {
                    "start": 1622,
                    "end": 1643,
                    "matchedPaperCorpusId": "263883936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "254043519",
            "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources",
            "text": "interpreting the landscape of multilingual NLP datasets: we observe the prevalence of automatically induced labels particularly in low resource languages; onethird of the datasets we surveyed use automatically induced labels, and 68% of the languages have no manually annotated data. While Wikipedia and news texts are available for a wide range of languages, texts specifically written for the NLP task are not available for most languages. Furthermore, about 20% of surveyed datasets involved translation in their creation, and these were used at a much higher rate in highly cited resources to study crosslingual transfer. These types of quantitative and qualitative disparities persist in multilingual supervised datasets, motivating us to identify underlying bottlenecks to create multilingual datasets.\n\nWe investigate whether the resources required for NLP data collection affect the prevalence of multilingual datasets. These include the availability of crowdworkers and NLP researchers with sufficient language proficiency as well as raw input text (Section 5). These data collection resources all correlate with the number of datasets in each language, suggesting paths for a more equitable data landscape. To assess the paths for crowdsourcing in non-English languages on a popular crowdsourcing platform, we design controlled data collection experiments in six languages, quantifying the challenges even in relatively high-resource languages.\n\nWe conclude our survey by relating researchderived suggestions to the NLP community and to individual researchers for multilingual data construction.\n\nWe also provide concrete suggestions for crowdsourcing platforms and crowdsourcing quality control tips, recommended translation services, and publication venues. We host our survey at https: //multilingual-dataset-survey.github.io, permitting readers to review multilingual resources and submit their datasets following our schema. Kreutzer et al. (2022) study the quality of raw text data available for researchers, such as ParaCrawl (Espl\u00e0 et al., 2019), Wikimatrix (Schwenk et al., 2021), and mC4 (Xue et al., 2021), by manually evaluating 100 sample texts for each language. The data quality for low-resource languages is poor, and parallel sentences are often misaligned (Ko",
            "score": 0.5775272626538592,
            "section_title": "Introduction",
            "char_start_offset": 1878,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.395263671875
        },
        {
            "corpus_id": "234778227",
            "title": "Investigating Math Word Problems using Pretrained Multilingual Language Models",
            "text": "In this paper, we revisit math word problems (MWPs) from the cross-lingual and multilingual perspective. We construct our MWP solvers over pretrained multilingual language models using the sequence-to-sequence model with copy mechanism. We compare how the MWP solvers perform in cross-lingual and multilingual scenarios. To facilitate the comparison of cross-lingual performance, we first adapt the large-scale English dataset MathQA as a counterpart of the Chinese dataset Math23K. Then we extend several English datasets to bilingual datasets through machine translation plus human annotation. Our experiments show that the MWP solvers may not be transferred to a different language even if the target expressions share the same numerical constants and operator set. However, it can be better generalized if problem types exist on both source language and target language.",
            "score": 0.5769514040184123,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327392578125
        },
        {
            "corpus_id": "269033414",
            "title": "XNLIeu: a dataset for cross-lingual NLI in Basque",
            "text": "Cross-lingual NLI.The best results on NLI benchmarks to date are based on supervised learning, which requires large amounts of training data that are only available for resource-rich languages such as English.Examples of English NLI datasets are the Stanford NLI corpus (Bowman et al., 2015), the Multi-genre NLI corpus (Williams et al., 2018) and the Adversarial NLI corpus (Nie et al., 2020).The NLI task is also included among the tasks of the popular NLU benchmarks GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019).Crosslingual NLI is an alternative approach that leverages pre-trained multilingual models which are fine-tuned in resource-rich languages, then tested in the desired target language.This transfer approach, called zero-shot, is often compared to strategies that involve machine translation: translate-train, where the training set is translated to each target language and used to train the models on their respective language and translate-test, where the test set is translated to the high-resource language, usually English.Alternatively, large multilingual autoregressive models are also known to perform well in cross-lingual settings, by providing them with a set of correct input/label pairs as prompts for new inputs (Brown et al., 2020).\n\nXNLI.The Cross-lingual NLI corpus (XNLI) (Conneau et al., 2018) comprises development and test sets in 15 high-and low-resource languages, meant as a cross-lingual benchmark for this task.Later, this corpus was expanded to include additional languages such as Korean (Ham et al., 2020).",
            "score": 0.5767056196874865,
            "section_title": "Related work",
            "char_start_offset": 4475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 18,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1275
                },
                {
                    "start": 1277,
                    "end": 1282
                },
                {
                    "start": 1282,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1563
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 291,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 320,
                    "end": 343,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "207756753"
                },
                {
                    "start": 475,
                    "end": 494,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1544,
                    "end": 1562,
                    "matchedPaperCorpusId": "215238677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361572265625
        },
        {
            "corpus_id": "253098917",
            "title": "Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings",
            "text": "In this sense, our experiments show that, contrary to what previous research sug-gested (Fei et al., 2020;Li et al., 2021), the zeroshot model-based approach obtains the best results when high-capacity multilingual models including the target language and domain are available. Second, when the performance of the multilingual language model is not optimal for the specific target language or domain (for example when working on a text genre and domain for which available language models have not been trained), or when the required hardware to work with high-capacity language models is not easily accessible, then datatransfer based on translate and project constitutes a competitive option. Third, we observe that machine translation data often generates training and test data which is, due to important differences in language use, markedly different to the signal received when using gold standard data in the target language. These discrepancies seem to explain the larger error rate of the translate and project method with respect to the zero-shot technique. Finally, we create manually projected datasets for four languages and automatically projected datasets for seven languages. We use them to train and evaluate cross-lingual sequence labelling models. Additionally, they are also used to extrinsically evaluate machine translation and word alignment systems. These new datasets, together with the code to generate them are publicly available to facilitate the reproducibility of results and its use in future research.1 2 Related work 2.1 Data-based cross-lingual transfer Data-based cross-lingual transfer methods aim to automatically generate labelled data for a target language. Some of these methods exploit parallel data. Ehrmann et al. (2011) automatically annotate the English version of a multi-parallel corpus and projects the annotations into all the other languages using statistical alignments of phrases. Wang and Manning (2014) project model expectations rather than labels, which facilities transfer of model uncertainty across languages. Ni et al. (2017) use a heuristic scheme that effectively selects goodquality projection-labeled data from noisy data. They also project word embeddings from a target language into a source language, so that the source-language sequence labelling system can be applied to the target language without re-training.",
            "score": 0.5765810909903222,
            "section_title": "Introduction",
            "char_start_offset": 4011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "215754572"
                },
                {
                    "start": 1743,
                    "end": 1764,
                    "matchedPaperCorpusId": "18220955"
                },
                {
                    "start": 1934,
                    "end": 1957,
                    "matchedPaperCorpusId": "6723413"
                },
                {
                    "start": 2070,
                    "end": 2086,
                    "matchedPaperCorpusId": "13410520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306640625
        },
        {
            "corpus_id": "235421949",
            "title": "Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning",
            "text": "To evaluate ML-LMs for commonsense reasoning in a cross-lingual zero-shot transfer setting, we create two benchmark datasets, namely X-CSQA and X-CODAH. Table 3 shows the statistics of the two datasets. Specifically, we use online commercial services such as DeepL Pro Translate to collect high-quality translations of the examples in CSQA and CODAH for 15 languages other than English. The size of CODAH is small (only 2.7k), so we use 7k SWAG validation examples as additional training data which share the same formulation. We discuss the reduction of cultural differences and quality control of automatic translations as well as other details in Ethical Considerations (the paragraph for cultural bias reduction) and Appendix (A). As our goal is to evaluate different ML-LMs (instead of different languages) in a unified evaluation protocol for cross-lingual commonsense reasoning, we argue that such automatically translated examples, although might contain noise, can serve as a starting benchmark for us to obtain meaningful analysis before more humantranslated datasets will be available in the future.",
            "score": 0.5761733403203284,
            "section_title": "X-CSQA & X-CODAH: Two New Benchmarks for Evaluating XCSR",
            "char_start_offset": 23633,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "270521667",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "text": "They evaluate several baseline multilingual models, finding that while translation quality for high-resource languages is relatively strong, performance on low-resource languages still lags significantly behind.Overall, FLORES-101 provides a more reliable, high-quality benchmark for assessing performance on low-resource languages and the authors hope it will help spur progress in this area.\n\nThe INDICXNLI dataset (Aggarwal et al., 2022) is a Natural Language Inference benchmark for 11 major Indian languages, created by translating the English XNLI dataset using the IndicTrans model.The dataset's quality is validated through human evaluation and automatic metrics.Fine-tuning experiments with XLM-R, IndicBERT, mBERT, and MuRIL investigate cross-lingual transfer techniques, the impact of multilingual models, and the effectiveness of using English data for fine-tuning.Results show that MuRIL generally performs best, and training on both English and Indic data yields the highest scores.The dataset aims to address the lack of inference resources for Indian languages and enable analysis of linguistic features and crosslingual transfer.\n\nThe IndicNLG Benchmark (Kumar et al., 2022) is a collection of datasets for 5 diverse NLG tasks spanning 11 Indic languages, with approximately 8M examples in total.It covers biography generation, headline generation, sentence summarization, paraphrasing, and question generation.The authors use the datasets to benchmark monolingual and multilingual models leveraging pre-trained sequence-to-sequence models.Results show the advantage of language-specific pre-trained models like IndicBART (Dabre et al., 2022b) compared to language-agnostic models like mT5.The datasets were created using simple methods like scraping news articles and Wikipedia infoboxes, making the approach applicable to other low-resource languages.To the authors' knowledge, IndicNLG is the first NLG benchmark for Indic languages and the most linguistically diverse multilingual NLG dataset.The datasets and models are publicly available.",
            "score": 0.575619987534912,
            "section_title": "Multi-lingual performance evaluation",
            "char_start_offset": 53430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 393
                },
                {
                    "start": 395,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 996
                },
                {
                    "start": 996,
                    "end": 1146
                },
                {
                    "start": 1148,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2014
                },
                {
                    "start": 2014,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 1639,
                    "end": 1660,
                    "matchedPaperCorpusId": "248780491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "271270611",
            "title": "INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages",
            "text": "India, with a population of almost 1.4 billion people, is home to numerous major languages that are considered low-resource by the natural language processing (NLP) community. Despite the growing capabilities of Large Language Models (LLMs) in tasks like context-grounded question answering (CQA) in English, their performance in non-English languages remains underexplored due to a lack of high-quality datasets. To address this gap, we introduce Indic-QA, the largest publicly available context-grounded question-answering dataset for 11 major Indian languages from two language families. This dataset encompasses both extractive and abstractive QA tasks, incorporating existing datasets as well as English QA datasets translated 1 Source code and Data are available at https://github. com/ayushayush591/IndicQA-Benchmark. into Indian languages. Additionally, we generate a synthetic dataset using the Gemini model, with manual verification for quality assurance.Our evaluation of various multilingual LLMs and their instruction-fine-tuned variants on the Indic-QA benchmark reveals subpar performance, particularly for low-resource languages. This outcome highlights the English language bias inherent in these models due to predominantly English pretraining data. \n\nWe tested the Translate-Test paradigm as an alternative, which translates input from the source language to English, utilizes the LLM's problemsolving ability in English, and then translates the response back to the source language. Our investigation shows that while multilingual LLMs perform better in mid-resource languages, the Translate-Test paradigm significantly outperforms them in low-resource languages. What distinguishes our benchmark from other existing Multilingual Indic context-grounded question-answering Benchmarks? There are numerous context-grounded question-answering benchmarks available for high-resource languages like English. However, there are very few benchmarks available for Indic languages [1], and those that do exist often lack domain diversity and are limited in size. To address these gaps, we developed the Indic-QA benchmark. We sampled a variety of Wikipedia and Common Crawl pages, focusing on paragraphs rich in cultural nuances, to create a comprehensive and culturally diverse benchmark for Indic languages.",
            "score": 0.575082083272381,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
            "score": 0.575056623651157,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "275336125",
            "title": "Understand, Solve and Translate: Bridging the Multilingual Mathematical Reasoning Gap",
            "text": "Mathematics Benchmarks Mathematical reasoning has emerged as a crucial capability for language models (Hurst et al., 2024;Alibaba, 2024;Zhao et al., 2024c), leading to the development of numerous benchmarks and datasets (Ling et al., 2017;Amini et al., 2019;Patel et al., 2021;Saxton et al., 2019). Traditional datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) primarily target grade-school to undergraduate-level problems, while more recent efforts introduce Olympiad-level challenges (Zheng et al., 2021;He et al., 2024;Huang et al., 2024;Fang et al., 2024;Gao et al., 2024). Although these benchmarks prove valuable for evaluating English-language mathematical reasoning, fewer resources exist for non-English or bilingual math problems (Shi et al., 2022;Chen et al., 2023;Wu et al., 2024). In the Korean context, most benchmarks emphasize language understanding (Park, 2021;Son et al., 2023a), general knowledge (Son et al., 2023b;Kim et al., 2024), or commonsense reasoning (Son et al., 2024b,a), with mathematics being largely underrepresented. While the Open Ko-LLM Leaderboard (Park et al., 2024a) has begun translating some popular English benchmarks into Korean, those translated sets are not publicly accessible. Meanwhile, KMMLU (Son et al., 2024c) includes only about 100 math problems, insufficient for broader evaluation. To address this gap, we propose HRM8K, a large-scale bilingual Korean-English math benchmark comprising 8,011 problems, covering both competition-level Korean questions and parallel translations of existing English benchmarks.",
            "score": 0.5750151206320195,
            "section_title": "Related Work",
            "char_start_offset": 2640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1591
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2430419921875
        },
        {
            "corpus_id": "257913598",
            "title": "LAHM : Large Annotated Dataset for Multilingual & Multi-Domain Hate Speech Identification",
            "text": "We adopted the methodology to utilize open source machine translation models for translation of English data to multilingual data and vice-versa. To select the best machine translation model for each of our languages, we evaluated a number of models on a small manually annotated dataset. The translations were carried out for each of the these languages: Hindi, Arabic, French, German and Spanish. Two types of translations were carried out using the translation models: \n\n1. Translation of English dataset collected from various open source hate speech datasets. \n\nAll the English language samples in this dataset were translated to other 5 languages. 2. Translation of multilingual data collected from Twitter. This data was collected for the above 5 languages, and each of this language data was translated into corresponding English language data. \n\nConsideration for choosing and evaluating the translation models was based on whether they were open source, free/easy to use, and the translation quality. Translation models used were: \n\n1. Google sheets translation 2. m2m-100-1.2B 10 . IndicTrans 11The evaluation metrics used were bleu, rougeL and semantic similarity. For semantic similarity, all-mpnet-base-v2 model from sentence-transformers library was used to calculate the cosine score between input and translated sentence embeddings. Figure 4.3.2 shows comparisons of bleu, rougeL and semantic similarity scores for the 3 models on different languages. After comparing the performances and taking other considerations into account, indicTrans model was selected for all translations of Indic languages (Hindi), and m2m-100-1.2B was selected for the other languages.",
            "score": 0.5748038058483504,
            "section_title": "Machine Translation Experiments",
            "char_start_offset": 13221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1681
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1488037109375
        },
        {
            "corpus_id": "235313293",
            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
            "text": "language spills its lexical and structural properties over the target language (Lembersky et al., 2012;Volansky et al., 2015). These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level. We refer the reader to the paper for some concrete examples.\n\nSecondly, the information and the topics touched upon in a conversation (e.g., in the domain of airline travels, names of destinations or flight companies) may vary across cultures and locales. However, translation-based approaches reflect the perspective of the Englishspeaking culture, its 'presupposed' factual knowledge, and the worldview of its community of speakers (Clark et al., 2020). For these reasons, ToD benchmarks should be ideally based on original utterances grounded in the appropriate locale and culture.\n\nRecognising this need, there have been some very recent developments in the direction of creating localised and culturally adapted ToD datasets. Namely, Ding et al. (2022) undertake an automatic approach to localisation in which the English slots are substituted by their local counterparts, obtained via Web-crawling suitable values. In contrast, Majewska et al. (2022) ask the native speakers to",
            "score": 0.5742645506165274,
            "section_title": "Outlook for Multilingual ToD Datasets",
            "char_start_offset": 35207,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 103,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 1666,
                    "end": 1686,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.167724609375
        },
        {
            "corpus_id": "263829517",
            "title": "CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation",
            "text": "Existing code translation datasets also neglect important demands from real-world applications, including modernizing early software systems developed in niche programming languages and migrating code across different deep learning frameworks. \n\nTo address these limitations and advance neural code translation models, we construct a largescale comprehensive multilingual code translation benchmark CodeTransOcean, summarized in Table 1. CodeTransOcean is an innovative benchmark that aims to provide a unified platform for evaluating various models on a comprehensive set of code translation tasks that reflect real-world demands. Based on this goal, each dataset in Code-TransOcean is specifically designed to tackle a key challenge in the field of code translation. Code-TransOcean includes three multilingual datasets, namely, the MultilingualTrans dataset (including eight popular programming languages), the NicheTrans dataset (translating between thirtyseven niche programming languages and the eight popular ones 1 ), and a specialized dataset LLM-Trans (including 350 data samples and their executed results) to evaluate executability of code translated by large language models (LLMs), and a cross-framework dataset DLTrans facilitating our proposed task for translating code between deep learning frameworks to enhance code reusability. 1 We define popular and niche programming languages based on the TIOBE Programming Community Index, which is a metric of the popularity of programming languages. \n\nDLTrans includes 408 samples covering four mainstream deep learning frameworks. \n\nMultilingual modeling shows great potential in neural machine translation (Aharoni et al., 2019;Wang et al., 2020;Zhu et al., 2023), but it has not been systematically explored for code translation. We investigate multilingual modeling for code translation using our MultilingualTrans, NicheTrans, and DLTrans datasets. Experimental results demonstrate that multilingual modeling significantly improves translation quality for both high-resource and low-resource language pairs and improves the model training efficiency. \n\nRecent research indicates that the proficiency of the LLM ChatGPT in natural language translation is on par with commercial-grade translation systems (Jiao et al., 2023). To the best of our knowledge, our work is the first to systematically investigate the potential of ChatGPT in code translation.",
            "score": 0.5741573384134051,
            "section_title": "Introduction",
            "char_start_offset": 2157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 1668,
                    "end": 1690,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 1690,
                    "end": 1708,
                    "matchedPaperCorpusId": "222141016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "264832929",
            "title": "From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities",
            "text": "VLP datasets are usually large-scale datasets of image-text pairs automatically extracted from online sources. As quality Subjective human opinion Subjective answers, controversial topics assurance is difficult for such large-scale datasets, it often leads to image-text misalignment issues and data redundancy [275]. VLP datasets are also primarily English-based and translations of native English answers are usually erroneous [124]. Current training strategies relying on multilingual captioning [276] use human-annotated captions to overcome annotation artifacts. However, cross-lingual performance is still low compared to its native English counterparts [124]. Potential research directions may include introducing non-English VLP datasets or improving cross-lingual capabilities by enhancing cross-lingual model architecture.",
            "score": 0.574069176334514,
            "section_title": "VLP Dataset",
            "char_start_offset": 89571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128662109375
        },
        {
            "corpus_id": "272968955",
            "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
            "text": "\u2022 bigcode-evaluation-harness: https://github.com/bigcode-project/bigcode-evaluation-harness. \n\nTo further demonstrate the effectiveness and superiority of our method compared to other model merging methods, we utilized the mergekit package (Goddard et al. 2024) to merge models by using several merging methods, including Task Arithmetic, TIES, and DARE-TIES. The mergekit package can be found at the following link: \n\n\u2022 mergekit: https://github.com/arcee-ai/mergekit Then, we introduce the specific datasets for text translation, math reasoning, and code generation tasks, as well as their metrics. \n\nTranslation Tasks To evaluate the multilingual translation capabilities of LLMs, we leveraged a set of translation tasks in the lm-evaluation-harness package, including WMT141 , WMT162 (Sennrich, Haddow, and Birch 2016), and IWSLT2017 (Cettolo et al. 2017). These tasks evaluate the model's translation accuracy and fluency across diverse language pairs. For all translation tasks, we use the \"chrf\" metric, which measures translation quality based on character n-gram precision and recall. \n\nGSM8K GSM8K (Cobbe et al. 2021) is a dataset meticulously designed for mathematical problem-solving tasks, comprising over 8,000 high-quality problems that span from basic arithmetic to complex algebra. The primary objective of this dataset is to evaluate the model's reasoning and computational abilities when tackling structured mathematical problems. For evaluating the GSM8K dataset, we employ the \"flexible match\" metric, which allows for minor variations in the final answer. \n\nHumanEval HumanEval (Chen et al. 2021) is a benchmark dataset proposed by OpenAI, specifically designed to evaluate code generation capabilities. The dataset comprises 164 programming problems, where each problem requires the model to generate a Python function based on a natural language description. The evaluation metric of HumanEval is pass@100. The model is allowed to generate up to 100 code solutions for each problem. This metric assesses whether at least one of these generated solutions passes all test cases.",
            "score": 0.5740130322648492,
            "section_title": "Detail of Experimental Setup",
            "char_start_offset": 43699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 858,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41455078125
        },
        {
            "corpus_id": "231709509",
            "title": "Meta-Learning for Effective Multi-task and Multilingual Modelling",
            "text": "The recently released XTREME dataset (Hu et al., 2020) is a multilingual multi-task benchmark consisting of classification, structured prediction, QA and retrieval tasks. Each constituent task has associated datasets in multiple languages. The sources of POS and NER datasets are Universal Dependency v2.5 treebank (Nivre et al., 2020) and WikiAnn (Pan et al., 2017) respectively, with ground-truth labels available for each language. Large-scale datasets for QA, NLI and PA were originally available only for English. The PAWS-X (Yang et al., 2019) dataset contains machine-translated training pairs and human-translated evaluation pairs for PA. The authors of XTREME train a custom-built translation system to obtain translated datasets for QA and NLI. For the NLI task, we train using MultiNLI  and evaluate on XNLI (Conneau et al., 2018). For the QA task, SQuAD 1.1 (Rajpurkar et al., 2016) was used for training and MLQA (Lewis et al., 2019) for evaluation.\n\nRegarding evaluation metrics, for QA we report F1 scores and for the other four tasks (PA, NLI, POS, NER) we report accuracy scores.",
            "score": 0.5737992698869435,
            "section_title": "Evaluation Benchmark",
            "char_start_offset": 10862,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 53,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 315,
                    "end": 335,
                    "matchedPaperCorpusId": "216056404"
                },
                {
                    "start": 348,
                    "end": 366,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 530,
                    "end": 549,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 870,
                    "end": 894,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "274423394",
            "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
            "text": "The rapid advancement of AI technologies underscores the importance of developing LLMs that are proficient across diverse linguistic and cultural contexts, ensuring fair and equitable performance for stakeholders from various language groups. However, the lack of high-quality evaluation benchmarks in many languages discourages practitioners from training multilingual LLMs to meet this challenge. This evaluation gap limits the effective deployment of LLMs for many regions, exacerbates digital divides, and inhibits the economic and societal value of AI tools in many underserved communities. \n\nThe source of this gap is the multitude of challenges in evaluating LLMs for multilingual contexts. First, at a meta-level, the majority of benchmarks for LLMs are only in English (Hendrycks et al., 2020, inter alia). While non-English benchmarks exist for some tasks (Singh et al., 2024;Aakanksha et al., 2024;Pozzobon et al., 2024), they usually focus on single languages (Li et al., 2023;Koto et al., 2024), specific regions (Adelani et al., 2024;Ca\u00f1ete et al., 2020;Guevara-Rukoz et al., 2020;Cahyawijaya et al., 2022;Etxaniz et al., 2024b), or a particular domain (Wang et al., 2024a), ignoring the importance of joint evaluation to trace and unlock the benefits that multilingual capabilities could bring to low-resource languages (Pfeiffer et al., 2022;\u00dcst\u00fcn et al., 2024;Aryabumi et al., 2024). \n\nTechnical challenges also abound due to the manner in which multilingual datasets are often collected. Certain datasets are constructed using manually applied templates, resulting in low prompt and completion diversity (Muennighoff et al., 2022). Many more are composed of translations from high-resource languages (e.g., English; Holtermann et al., 2024;Myung et al., 2024;Lai et al., 2023;Foroutan et al., 2023).",
            "score": 0.5737887363498103,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 866,
                    "end": 886,
                    "matchedPaperCorpusId": "267617144"
                },
                {
                    "start": 1048,
                    "end": 1068,
                    "matchedPaperCorpusId": "260681542"
                },
                {
                    "start": 1068,
                    "end": 1095,
                    "matchedPaperCorpusId": "218973838"
                },
                {
                    "start": 1335,
                    "end": 1358,
                    "matchedPaperCorpusId": "248721770"
                },
                {
                    "start": 1358,
                    "end": 1377,
                    "matchedPaperCorpusId": "267627803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "253098629",
            "title": "Multilingual Multimodal Learning with Machine Translated Text",
            "text": "Our paper investigates the benefits and limitations of machine translated data towards multilingual multimodal learning. In doing so, we solely rely on the M2M-100 model (Fan et al., 2021). This is a large, multi-to-multi translation system, which proved to be easy to use. Our analyses and results are based on the performance of this model. It would be instructive to investigate how the expected performance of translation systems6 affects (i) the proportion of sentences with high 'badness' scores, and (ii) the resulting performance of the multilingual multimodal systems. Moreover, while machine translating a large corpus is a cheaper effort than manually translating the data or scraping it from the web, there is still a one-time effort required to translate the data before using it for training new models. Therefore, we release our multilingual pretraining and fine-tuning datasets. \n\nFrom an experimental angle, although the proposed framework can be applied to any existing architecture, we only evaluate a single model due to computational constraints. \n\nWe would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones. \n\nFinally, a core limitation of the overall translate data framework is that it centers English as the source language. For example, this means only concepts mentioned in English captions can be grounded across languages (Liu et al., 2021), and hence some non-English concepts might never be modelled. However, we show that machine translating data provides a strong starting point that can effortlessly be integrated in a pipeline, upon which language-specific annotations can be added.",
            "score": 0.5736565133753787,
            "section_title": "Limitations",
            "char_start_offset": 27304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 188,
                    "matchedPaperCorpusId": "224814118"
                },
                {
                    "start": 1648,
                    "end": 1666,
                    "matchedPaperCorpusId": "238198104"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4580078125
        },
        {
            "corpus_id": "215548041",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "text": "By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI). While improvements brought by machine translation have previously been attributed to data augmentation (Singh et al., 2019), we reject this hypothesis and show that the phenomenon is only present in translated test sets, but not in original ones. Instead, our analysis reveals that Multilingual benchmarks. Most benchmarks covering a wide set of languages have been created through translation, as it is the case of XNLI (Conneau et al., 2018) for NLI, PAWS-X (Yang et al., 2019) for adversarial paraphrase identification, and XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) for Question Answering (QA). A notable exception is TyDi QA (Clark et al., 2020), a contemporaneous QA dataset that was separately annotated in 11 languages. Other cross-lingual datasets leverage existing multilingual resources, as it is the case of MLDoc (Schwenk and Li, 2018) for document classification and Wikiann (Pan et al., 2017) for named entity recognition. Concurrent to our work, Hu et al. (2020) combine some of these datasets into a single multilingual benchmark, and evaluate some well-known methods on it. \n\nAnnotation artifacts. Several studies have shown that NLI datasets like SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) contain spurious patterns that can be exploited to obtain strong results without making real inferential decisions. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length.",
            "score": 0.5734345367816133,
            "section_title": "Introduction",
            "char_start_offset": 2117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 49,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 733,
                    "end": 755,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 772,
                    "end": 791,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 845,
                    "end": 867,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 877,
                    "end": 897,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1154,
                    "end": 1176,
                    "matchedPaperCorpusId": "21732778"
                },
                {
                    "start": 1217,
                    "end": 1235,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 1499,
                    "end": 1520,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1534,
                    "end": 1557,
                    "matchedPaperCorpusId": "3432876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "271902824",
            "title": "Hindi-BEIR : A Large Scale Retrieval Benchmark in Hindi",
            "text": "We translate a subset of the existing English datasets from the BEIR benchmark into Hindi. This approach was necessitated by the need to ensure comprehensive coverage across multiple domains and to maintain a high level of data quality and complexity. We utilized the Indic-Trans2 model4 (Gala et al., 2023), a multilingual NMT model supporting translations across all 22 scheduled Indic languages (including English). We employ back-translation technique to retain good translations. Specifically, given an English query/document we translate it to Hindi. This Hindi-translated query/document is translated back to English. We calculate the Chrf(++) score (Popovi\u0107, 2017) between the original English query/document and the backtranslated English query/document. We retained only those translations with a Chrf++ score exceeding a threshold. We empirically set the threshold to 50 after manually verifying the translation quality of texts obtained from different thresholds. \n\nThis strategy enables us to leverage the wealth of existing high-quality datasets in English while making them accessible and useful for Hindi language information retrieval tasks. 9 out of the 15 datasets which includes Arguana, FiQA-2018, TREC-COVID, SCIDOCS, SciFact, Touch\u00e9-2020, NQ, FEVER, and, Climate-FEVER were created in by this method.",
            "score": 0.573346662991663,
            "section_title": "Translating English Datasets to Hindi:",
            "char_start_offset": 7968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1323
                }
            ],
            "ref_mentions": [
                {
                    "start": 657,
                    "end": 672,
                    "matchedPaperCorpusId": "12942757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "7665329",
            "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity",
            "text": "Subtask 1, i.e., multilingual semantic similarity, has five datasets for the five languages of the task, i.e., English, Farsi, German, Italian, and Spanish. These datasets were manually created with the help of trained annotators (as opposed to Mechanical Turk) that were native or fluent speakers of the target language. Based on these five datasets, 10 cross-lingual datasets were automatically generated (described in Section 2.2) for subtask 2, i.e., cross-lingual semantic similarity. \n\nIn this section we focus on the creation of the evaluation test sets. We additionally created a set of small trial datasets by following a similar process. These datasets were used by some participants during system development.",
            "score": 0.5732485431188588,
            "section_title": "Task Data",
            "char_start_offset": 8053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 720
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "248780442",
            "title": "Make the Best of Cross-lingual Transfer: Evidence from POS Tagging with over 100 Languages",
            "text": "At present, for a large majority of natural language processing tasks, the most successful approach is fine-tuning pre-trained models with task-specific labelled data. Unfortunately, for many languages, and especially low-resource languages, such taskspecific labelled data is often not available. A potential solution is cross-lingual fine-tuning of multilingual pre-trained language models (Conneau et al., 2020;Devlin et al., 2018), using available data from some source language to model the phenomenon in a different target language for which labelled data does not exist. \n\nCross-lingual generalisability of large pretrained language models is often evaluated by finetuning multilingual models on English data and testing them on unseen languages (Conneau et al., 2018;Artetxe et al., 2020;Lewis et al., 2020;Hu et al., 2020). Of course, this approach is influenced by the availability of English training data for given tasks, but also then comes with the implicit assumption that English is a representative source language. This, however, may not be true in practice. Specifically, depending on the task, aspects of similarity between source and target language may be relevant for cross-lingual transfer performance (de Vries et al., 2021). If similarity between source and target language impacts performance, crosslingual transfer should not be assessed using only a single predetermined source language, especially if training sets in multiple languages are available. \n\nFurthermore, target test languages are generally selected based on data availability for the evaluated tasks, but availability may not result in a representative subset of the world's languages. The XTreme benchmark collection (Hu et al., 2020), for example, attempts to alleviate this problem by including a varied selection of languages from different language families. This collection contains token classification, text classification, question answering and retrieval tasks in 40 languages. The language selection does, however, obfuscate the fact that for most non-Indo-European languages no data is available for semantically rich tasks such as question answering. This imbalance regarding tasks in this type of collections may consequently inflate the perceived performance for these languages.",
            "score": 0.5729230059798547,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2156
                },
                {
                    "start": 2157,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 796,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 796,
                    "end": 815,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1226,
                    "end": 1249,
                    "matchedPaperCorpusId": "233864536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410400390625
        },
        {
            "corpus_id": "238353880",
            "title": "Sicilian Translator: A Recipe for Low-Resource NMT",
            "text": "So to push our BLEU scores into the thirties, we trained a larger model. \n\nMore broadly, Fan et al. (2020) developed the strategies to collect data for and to train a model that can directly translate between 100 languages. Previous efforts had resulted in poor translation quality in non-English directions because the data consisted entirely of translations to and from English. \n\nTo overcome the limitations of English-centric data, Fan et al. strategically selected pairs to mine data for, based on geography and linguistic similarity. Training a model on such a more multilingual dataset yielded very large improvements in translation quality in non-English directions, while matching translation quality in English directions. \n\nGiven such potential to expand the directions in which languages can be translated and to improve the quality with which they can be translated, an important question is what the model learns. Does it learn to represent similar sentences in similar ways regardless of language? Or does it represent similar languages in similar ways? \n\nJohnson et al. examined two trained trilingual models. In one, they observed similiar representations of translated sentences, while in the second they noticed that the representations of zero-shot translations were very different. Kudugunta et al. (2019) examined the question in a model trained on 103 languages and found that the representations depend on both the source and target languages and they found that the encoder learns a representation in which linguistically similar languages cluster together. \n\nIn other words, because similar languages learn similar representations, our model would learn Sicilian-English better from Italian-English data than from Polish-English data. And other Romance languages, like Spanish, would also be good languages to consider. \n\nWe can collect some of that parallel text from the resources at OPUS (Tiedemann, 2012), an open repository of parallel corpora. Because it contains so many language resources, Zhang et al. (2020) recently used it to develop the OPUS-100 corpus, an open-source collection of English-centric parallel text for 100 languages. \n\nBecause it's a \"rough and ready\" massively multilingual dataset, it highlights some of the challenges facing massively multilingual translation. In particular, Zhang et al. show that a model trained with a vanilla setup exhibits off-target translation issues in zero-shot directions.",
            "score": 0.5726950501861128,
            "section_title": "Multilingual Translation",
            "char_start_offset": 14136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 75,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2170
                },
                {
                    "start": 2173,
                    "end": 2317
                },
                {
                    "start": 2318,
                    "end": 2456
                }
            ],
            "ref_mentions": [
                {
                    "start": 1917,
                    "end": 1934,
                    "matchedPaperCorpusId": "15453873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433837890625
        },
        {
            "corpus_id": "237532506",
            "title": "Towards Zero-shot Cross-lingual Image Retrieval and Tagging",
            "text": "Image retrieval is a well studied problem in both academia and industry [8,21,32,37,40]. Most research looks at image retrieval in a monolingual setup for a couple of reasons: \n\n\u2022 Lack of multi-lingual Vision-Language datasets supporting a wide range of languages \u2022 Extensibility towards new and low-resource language support \n\nMulti-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28,31] rely on direct translations of English captions while others [12,20] have used independent image and language text pairs. Based on previous research learning we try to explore following ideas in this paper: \n\n\u2022 One-model-fits-all: Can we use pre-trained cross-lingual embeddings with monolingual image-text training data to learn representations in a common embedding space for image retrieval and tagging? \n\nFigure 1: Overview of our approach. \n\n\u2022 Multi-lingual Eval Dataset: Build an evaluation set for multilingual image retrieval to test in a zero-shot retrieval setup. \n\nIn our approach we try to take advantage of the recent developments in cross-lingual sentence embeddings [3,38] which are effective in aligning multiple languages in a common embedding space. Due to scarcity of multi-lingual sentence test datasets, for evaluation we combine 10 non-English language annotations to create a Cross-lingual Test Dataset called XTD10. \n\nAs an extension of [1], in this paper we also try to take advantage of the One-model-fits-all approach on multi-lingual image tagging and tackle it as a zero-shot problem. Monolingual image tagging models, in languages other than English face extensibility issues and training data constraints similar to the image retrieval problem. Using an English image tagger and direct translation at tag level is a straightforward solution but its error prone because of word ambiguity which word-level translation systems lack the context to capture. Spring in French has two translations: Printemps(Season Spring) and Ressort(Bouncy Spring).",
            "score": 0.5720163014247617,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 75,
                    "matchedPaperCorpusId": "7060187"
                },
                {
                    "start": 75,
                    "end": 78,
                    "matchedPaperCorpusId": "1153609"
                },
                {
                    "start": 81,
                    "end": 84,
                    "matchedPaperCorpusId": "22367645"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "50776405"
                },
                {
                    "start": 552,
                    "end": 555,
                    "matchedPaperCorpusId": "21688999"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "19396711"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "202772083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "253116659",
            "title": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning",
            "text": "The XNLI dataset (Conneau et al., 2018) comes with ground-truth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages in two modes: (i)zero-shot: the model is fine-tuned only using the English training data and (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency.\n\nPAWS-X The PAWS (Paraphrase Adversaries from Word Scrambling) dataset (Zhang et al., 2019) requires to determine whether two sentences are paraphrases. We use the subset of the PAWS dev and test sets translated to six other languages by professional translators, dubbed as PAWS-X (Yang et al., 2019) for evaluation, while using the PAWS set for training.\n\nXQuAD The English SQuAD v1.1 (Rajpurkar et al., 2016) requires identifying the answer to a question as a span in the corresponding paragraph. In XQuAD(Artetxe et al., 2019), a subset of the English dev set was translated into ten other languages by professional translators which is then used for evaluation.\n\nMLQA The Multilingual Question Answering(Lewis et al., 2019) dataset is another crosslingual question answering dataset. In this dataset, the evaluation data for English and six other languages was obtained by automatically mining target language sentences that are parallel to sentences in English from Wikipedia, crowd-sourcing annotations in English, and translating the question and aligning the answer spans in the target languages. We use the SQuAD v1.1 (Rajpurkar et al., 2016) training data for training and evaluate on the test data of the corresponding task.\n\nTyDiQA-GP We use the gold passage version of the Typologically Diverse Question Answering (Clark et al., 2020a) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simplified",
            "score": 0.5717514849256746,
            "section_title": "XNLI",
            "char_start_offset": 27707,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "270440784",
            "title": "Deep Exploration of Cross-Lingual Zero-Shot Generalization in Instruction Tuning",
            "text": "In the setting, we instruction tune for two languages metadataset separately and evaluate the other language's unseen tasks.Specifically, to examine how effective cross-lingual zero-shot generalization is for a different language, we conduct a comparative analysis between cross-lingual instruction tuning and monolingual instruction tuning, where the latter indicates models trained and tested on the same arXiv:2406.08796v1[cs.CL] 13 Jun 2024 language's meta-dataset.\n\nThe collection of a non-English meta-dataset (Triantafillou et al., 2020) is imperative for the comprehensive examination of cross-lingual generalization of instruction tuning.However, collecting diverse supervised task datasets for non-English poses a substantial challenge due to the limited availability of open-source data in non-English languages compared to English.To address this issue, we propose a novel non-English language meta-dataset named KORANI, short for KOReAn Natural Instruction.This meta-dataset comprises 51 diverse Korean benchmarks, including 34 NLU benchmarks and 17 NLG benchmarks.Notably, KORANI surpasses the quantity of non-English benchmarks explored in previous multilingual research (Wang et al., 2022;Muennighoff et al., 2022) and approaches the size of P3 datasets (Sanh et al., 2022), which we employ as English benchmarks in our study.\n\nFurthermore, in the cross-lingual setting, the language and instructional format of templates are different during the training and inference phase.These discrepancies in the template may contribute to suboptimal model performance (Muennighoff et al., 2022;Liang et al., 2023;Sun et al., 2024).To address this issue, we construct cross-lingual templates to align the template between the training and inference phases.\n\nOur experiments show that cross-lingual instruction tuning consistently improves the zero-shot performance of unseen tasks in both English and Korean.Surprisingly, these cross-lingual performances are comparable to those of monolingual instruction-tuned models across various tasks, and they even surpass some tasks when cross-lingual templates are applied.",
            "score": 0.5715244233746212,
            "section_title": "Introduction",
            "char_start_offset": 1876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 469
                },
                {
                    "start": 471,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1342
                },
                {
                    "start": 1344,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1638
                },
                {
                    "start": 1638,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 544,
                    "matchedPaperCorpusId": "71145737"
                },
                {
                    "start": 1186,
                    "end": 1205,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 1270,
                    "end": 1289,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1620,
                    "end": 1637,
                    "matchedPaperCorpusId": "259203613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461181640625
        },
        {
            "corpus_id": "261696828",
            "title": "BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models",
            "text": "they only evaluated GPT-4 on NLI and question answering. A key finding from the study was that machine-translated prompts often contain translation errors which influence task performance negatively, and therefore it was recommended that machine-translated prompts be verified by native speakers. We took this into consideration and asked native speakers to manually translate all our prompts. BUFFET [10] was also established to evaluate language models across a broad range of tasks and languages, but was more targeted at evaluating few-shot cross-lingual transfer. Unlike MEGA and BUFFET, BHASA focuses on zero-shot instead of few-shot evaluations.\n\nFinally, M3Exam [13] was released as a multilingual, multimodal and multilevel benchmark for LLMs. It comprises 12,317 multiple-choice examination paper questions across 9 languages, including Vietnamese, Thai and Javanese and was tested on GPT-3.5-Turbo, GPT-4, BLOOM and Vicuna. However, it is more similar to MMLU and does not specifically target any aspect of NLU or NLG.\n\nOverall, although there have been a few endeavors to measure multilingual performance of LLMs, the evaluations done for SEA languages often only cover tasks for which multilingual datasets are more conveniently BHASA available, do not include linguistic diagnostics or cultural diagnostics, often use machine-translated prompts which are known to have translation errors that negatively impact models' performance, and also usually have not been tested on GPT-4.",
            "score": 0.571423425658242,
            "section_title": "Evaluation of LLMs in SEA Languages",
            "char_start_offset": 9670,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239013671875
        },
        {
            "corpus_id": "258187051",
            "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining",
            "text": "In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language. Finally, to the degree possible, benchmarks should be realistic, such that performing better on the benchmark gives us confidence that a model will do better on actual tasks facing language technology users. This is in contrast to \"intermediate structure\" tasks such as part-of-speech tagging. \n\nTyDi QA (Clark et al., 2020) is a multilingual question-answering benchmark covering a range of typologically diverse languages. Questions are written from scratch by native speakers in each language, ensuring culturally relevant content and the absence of \"translationese\". We use the \"GoldP\" task, which covers 9 languages. To evaluate candidate models, we use the \"in-language multitask\" setting (Hu et al., 2020)-fine-tuning on a mixture of all available languages, and evaluating in each language separately. To maximize per-language performance, we select per-language checkpoints based on the validation performance, and report validation metrics, as no test set is provided. \n\nThe WMT21 shared task on large-scale multilingual machine translation (Wenzek et al., 2021) tests the ability of single model to translate across many languages. We focus on the \"small track\" tasks, each testing translation between 6 languages, in all 30 combinations. As with TyDi QA, we finetune a single multilingual model on the mixture of all tasks and select per-language-pair checkpoints based on the test set performance. 3 To reduce heavy English bias, we restrict training data to 1,000 examples per language pair by randomly subsampling from the training data.",
            "score": 0.5713651238106073,
            "section_title": "EVALUATION TASKS",
            "char_start_offset": 13982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 912,
                    "end": 931,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 1303,
                    "end": 1319,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1659,
                    "end": 1680,
                    "matchedPaperCorpusId": "245148999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "In a related approach, Ara\u00fajo et al. (2020) demonstrated that using machine translation to translate examples to English in order to use an English monolingual model culminates in better results when compared to monolingual models in languages other than English. In addition, there are cross-lingual datasets such as XQuAD (Artetxe et al., 2019), which consists of paragraphs and question-answer pairs from the development set of SQuAD v1.1 translated into ten languages by professional translators and XNLI (Conneau et al., 2018), an extension of NLI corpus into 15 languages. Finally, Xue et al. (2020) and Xue et al. (2021) showed that finetuning multilingual models on translated NLI and question answering datasets improved results. \n\nIn the IR community, there have been several efforts to create multilingual resources. Initiatives like TREC1 , CLEF2 , FIRE3 , and NTCIR4 have proposed annotated collections to evaluate IR systems. However, given the small size of most of these resources, they are not appropriate to finetune large models such as transformer-based rerankers. Moreover, the text data used to build such collections often comes from specific domains. This lack of diversity ends up harming the model's generalization ability. On the other hand, as the MS MARCO dataset assembles documents from different sources and styles, the translation process maintains this diversity in the target languages. \n\nClosest to our work is MR. TYDI (Zhang et al., 2021a), a multilingual IR dataset derived from the TyDi QA dataset (Clark et al., 2020). A key difference is that Mr. TyDi uses Wikipedia as a corpus, whereas mMARCO's corpus consists of diverse passages extracted from web pages. Another difference is related to query-document relevant pairs for training per language. Mr. TyDi has an irregular distribution of examples over all languages. The Korean language has the minimum number of examples, 1,317, while the Arabic language has the maximum, 12,377. The dataset mean of training examples is 4,466. mMARCO contains 532,761 query-passage relevant pairs for each language.",
            "score": 0.5712890578881618,
            "section_title": "Related Work",
            "char_start_offset": 5965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 43,
                    "matchedPaperCorpusId": "199386551"
                },
                {
                    "start": 1456,
                    "end": 1477,
                    "matchedPaperCorpusId": "237213465"
                },
                {
                    "start": 1538,
                    "end": 1558,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "221246333",
            "title": "MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
            "text": "model on this parallel data.\n\nCross-Lingual Task-Oriented Semantic Parsing Due to the ubiquity of digital assistants, the task of cross-lingual and multilingual task-oriented dialog has garnered a lot of attention recenty, and few multilingual benchmark datasets have been released for the same. To the best of our knowledge, all of them only contain simple non-compositional utterances, suitable for the intent and slots detection tasks. Upadhyay et al. (2018) release a benchmark dataset in Turkish and Hindi (600 training examples), obtained by translating utterances from the ATIS corpus (Price, 1990) and using Amazon Mechanical Turk to generate phrase level slot annotation on translations. Schuster et al. (2019a) release a bigger multilingual dataset for task-oriented dialog in English, Spanish and Thai across 3 domains. They also propose various modeling techniques such as using XLU embeddings (see Ruder et al. (2017) for literature review) for cross-lingual transfer, translate-train and ELMo (Peters et al., 2018) for target language training. BERT-style multilingual pre-trained models have also been applied to task-oriented semantic parsing. Castellucci et al. (2019) use multilingual BERT for joint intent classification and slot filling, but they don't evaluate on existing multilingual benchmarks. Instead, they introduce a new Italian dataset obtained via automatic machine translation of SNIPS (Coucke et al., 2018), which is of lower quality. For zero shot transfer,  study the idea of selecting some parallel word pairs to generate codeswitching sentences for learning the inter-lingual semantics across languages and compare the performance using various cross-lingual pre-trained models including mBERT and XLM.",
            "score": 0.5712183425937002,
            "section_title": "Pre-trained Cross-lingual Representation",
            "char_start_offset": 5221,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 461,
                    "matchedPaperCorpusId": "52291331"
                },
                {
                    "start": 697,
                    "end": 720,
                    "matchedPaperCorpusId": "53110354"
                },
                {
                    "start": 1007,
                    "end": 1028,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461669921875
        },
        {
            "corpus_id": "235358129",
            "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
            "text": "A major challenge in machine translation, particularly as the field shifts its focus to low-resource languages, is the lack of availability of evaluation benchmarks. Much recent work has focused on the creation of training corpora (Auguste Tapo et al., 2021;Ali et al., 2021;Adelani et al., 2021;Gezmu et al., 2021;Nyoni and Bassett, 2021;Chauhan et al., 2021) and development of models (Koneru et al., 2021;Nagoudi et al., 2021;Aulamo et al., 2021), but evaluation is critical to being able to assess and improve translation quality. \n\nTraditionally, the yearly Workshop on Machine Translation (WMT) and its associated shared tasks have provided standardized benchmarks and metrics to the community, fostering progress by providing means of fair comparison among various approaches. Over recent years, the main translation task at WMT has challenged participants with lowresource languages, but the evaluation has been limited to a handful of languages -for example, Latvian in 2017 (Ondrej et al., 2017), Kazakh in 2018(rej Bojar et al., 2018), Gujarati and Lithuanian in 2019 (Barrault et al., 2019), and Inuktitut, Khmer, Pashto, and Tamil in 2020 (Barrault et al., 2020a). Moreover, these tasks have considered translation to and from English only, while the field has been recently focusing on large-scale multilingual models (Johnson et al., 2016;Aharoni et al., 2019;Freitag and Firat, 2020;Fan et al., 2020). \n\nTo date, the largest resource of parallel data which can also be used for evaluation purposes is OPUS (Tiedemann, 2012), which is itself a collection of publicly available parallel datasets. While OPUS has by far the largest coverage of languages, particularly to and from English, it consists of a mixture of manually translated and mined data, which results in a large variety of datasets and domains with varying level of quality.",
            "score": 0.5705161550810594,
            "section_title": "Related Work",
            "char_start_offset": 5404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 449,
                    "matchedPaperCorpusId": "235258250"
                },
                {
                    "start": 984,
                    "end": 1005,
                    "matchedPaperCorpusId": "28232901"
                },
                {
                    "start": 1021,
                    "end": 1045,
                    "matchedPaperCorpusId": "53247198"
                },
                {
                    "start": 1079,
                    "end": 1102,
                    "matchedPaperCorpusId": "201741133"
                },
                {
                    "start": 1354,
                    "end": 1375,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 1375,
                    "end": 1399,
                    "matchedPaperCorpusId": "224802999"
                },
                {
                    "start": 1522,
                    "end": 1539,
                    "matchedPaperCorpusId": "15453873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "273532297",
            "title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment",
            "text": "Dataset We curate a synthetic preference dataset of 86k instances 3 from five representative English preference datasets: SafeRLHF (Dai et al., 2024), WildGuard (Han et al., 2024), HelpSteer2 (Wang et al., 2024e), Offsetbias (Park et al., 2024), and Magpie (Xu et al., 2024b). Using English data, we create four parallel machine-translated versions 4 , utilizing X-ALMA (Xu et al., 2024a). \n\n3 Refer to Appendix A for detailed process. 4 Spanish (Sp), Italian (It), Korean (Ko), and Chinese (Ch) Models Two state-of-the-art 3B multilingual pretrained language models are fine-tuned5 as reward models: Llama-3.2-3B-Instruct (Dubey et al., 2024) and Qwen2.5-3B-Instruct (Yang et al., 2024). \n\nEvaluation We prepare four non-English Multilingual RewardBench by translating RewardBench (Lambert et al., 2024) to assess the cross-lingual transfer in RMs.",
            "score": 0.5701596433044522,
            "section_title": "Experimental Details",
            "char_start_offset": 3871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 849
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 149,
                    "matchedPaperCorpusId": "264306078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52197265625
        },
        {
            "corpus_id": "257038496",
            "title": "Exploring the Potential of Machine Translation for Generating Named Entity Datasets: A Case Study between Persian and English",
            "text": "There exist several multilingual datasets that include Persian as a part of their data, and are distinguished by their coarse-grained [1] and fine-grained [2] entities. However, established benchmarks such as CoNLL 2003 [3] and WNUT 2017 [4] do not include the Persian language in their respective corpora. In the following paragraphs, we elucidate the main characteristics of two Persian and four English datasets, along with some existing cross-lingual approaches.",
            "score": 0.5701174699141991,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 2073,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "26988506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1402587890625
        },
        {
            "corpus_id": "7665329",
            "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity",
            "text": "While the English community has been using standard word similarity datasets as a common evaluation benchmark, semantic representation for other languages has generally proved difficult to evaluate. A reliable multilingual word similarity benchmark can be hugely beneficial in evaluating the robustness and reliability of semantic representation techniques across languages. Despite this, very few word similarity datasets exist for languages other than English: The original English RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002) datasets have been translated into other languages, either by experts (Gurevych, 2005;Joubarne and Inkpen, 2011;Granada et al., 2014;Camacho-Collados et al., 2015), or by means of crowdsourcing (Leviant and Reichart, 2015), thereby creating equivalent datasets in languages other than English. However, the existing English word similarity datasets suffer from various issues: \n\n1. The similarity scale used for the annotation of WordSim-353 and MEN (Bruni et al., 2014) does not distinguish between similarity and relatedness, and hence conflates these two. As a result, the datasets contain pairs that are judged to be highly similar even if they are not of similar type or nature. For instance, the WordSim-353 dataset contains the pairs weather-forecast or clothes-closet with assigned similarity scores of 8.34 and 8.00 (on the [0,10] scale), respectively. Clearly, the words in the two pairs are (highly) related, but they are not similar. \n\n2. The performance of state-of-the-art systems have already surpassed the levels of human inter-annotator agreement (IAA) for many of the old datasets, e.g., for RG-65 and WordSim-353. This makes these datasets unreliable benchmarks for the evaluation of newly-developed systems.",
            "score": 0.5701104386430025,
            "section_title": "Subtask 1: Multilingual Semantic Similarity",
            "char_start_offset": 2144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1794
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 523,
                    "matchedPaperCorpusId": "18309234"
                },
                {
                    "start": 540,
                    "end": 566,
                    "matchedPaperCorpusId": "12956853"
                },
                {
                    "start": 637,
                    "end": 653,
                    "matchedPaperCorpusId": "16584933"
                },
                {
                    "start": 653,
                    "end": 679,
                    "matchedPaperCorpusId": "6425853"
                },
                {
                    "start": 679,
                    "end": 700,
                    "matchedPaperCorpusId": "11800486"
                },
                {
                    "start": 700,
                    "end": 730,
                    "matchedPaperCorpusId": "7681779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44873046875
        },
        {
            "corpus_id": "276647924",
            "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs",
            "text": "To construct XCOMPS, a multilingual benchmark covering 17 languages (Table 3), we adopted a human-LLM interactive translation pipeline, leveraging both human expertise and the multilingual generation capabilities of large language models (LLMs). The language set for XCOMPS aligns with the prior knowledge probing benchmark BMLAMA-17 (Qi et al., 2023), ensuring consistency in multilingual evaluation. The highly structured nature of conceptual minimal pair datasets, where positive and negative sentences primarily consist of two components-concepts and properties-enabled us to design a multi-step translation process that ensures high-quality multilingual data. \n\nThe construction process consists of four stages, as displayed in Figure 7. We use the GPT-4o model (GPT-4o-2024-08-06) via the OpenAI API as the translation assistant in the pipeline. In the first stage, we manually translated the original concepts and properties from English into German and Chinese using language experts. We used German and Chinese as additional seed languages to further reduce ambiguity, This multilingual seed data helped disambiguate concepts that might otherwise be unclear in translation. For example, the English word \"bat\" could refer to either the flying animal or the sports equipment. By including the German term \"Schl\u00e4ger\" and the Chinese term \"\u7403\u62cd\", which both unambiguously refer to the sports equipment, we ensured that the intended concept was accurately captured during translation. \n\nIn the second stage, we used LLMs to expand the seed data into the remaining 15 languages. LLMs were tasked with translating the concepts and properties, leveraging their multilingual machine translation capabilities. By providing seed data in three languages (English, German, and Chinese), we enhanced the LLMs' ability to generate accurate translations, as the additional context reduced the likelihood of semantic errors. \n\nIn the third stage, human experts for each target language manually reviewed and corrected the translated concepts and properties. This step ensured that the translations were accurate, culturally appropriate, and semantically aligned with the original dataset. Human intervention was particularly critical for low-resource languages, where LLMs often struggle with semantic precision in translation tasks.",
            "score": 0.5691334537700856,
            "section_title": "A Details of Multilingual Data Construction",
            "char_start_offset": 28009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 351,
                    "matchedPaperCorpusId": "264145744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "258740735",
            "title": "PaLM 2 Technical Report",
            "text": "forms Translate in three high-resource languages: Spanish, Polish and Portuguese. Surprisingly, we see improved translation quality in Italian and Japanese, but lower gender agreement scores. We also find reduced gender agreement scores in Telugu, Hindi and Arabic as compared to PaLM, as shown in Figure 33. These findings highlight the importance of evaluating specific failures with potential for harm, and that multilingual performance may vary by use, task, and prompting (e.g., Section 4.2 found improvements in Arabic and Telugu on 1-shot TyDi QA, while this evaluates zero-shot capabilities). Overall, with zero-shot prompting we observe higher potential for misgendering harms when translating out of English as compared to when translating into English.  While there are exceptions, for both PaLM and PaLM 2, in Figure 34 we observe a broad relationship between translation quality and percentage of pre-training data from web documents in that language. Most languages that are represented in over 2% of the training data show average quality scores over 2.50, with Arabic being the notable exception. This suggests that multilingual training data is one important part of mitigating potential misgendering harms in translation uses, but that this is not sufficient to ensure quality across languages.  Limitations. For gender-related errors in translation systems, evaluations do not consider differential harms to people related to expressing non-binary gender identities (Keyes, 2018;Dev et al., 2021a), or consider contested perspectives on pronouns across languages and cultures (Lee, 2019). Moreover, while gender agreement into English is amenable to automatic evaluation, evaluation of gender agreement out of English remains challenging and time-intensive. Finally, we note that our evaluations focus on only a subset of potential risks , and that our evaluations focus on model outputs without considering the wider sociotechnical context in which instruction-finetuned language models exist (Shelby et al., 2023). See Appendix D.8 for measurement quality rubric for this evaluation when translating into English.",
            "score": 0.568866732853358,
            "section_title": "D.5.2 Translating from English",
            "char_start_offset": 114381,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.131103515625
        },
        {
            "corpus_id": "270688770",
            "title": "TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement",
            "text": "Dataset Our testset mainly comes from WMT22 and WMT23, except for Icelandic from WMT213 .We evaluate 17 translation directions, including both high-resource and low-resource languages, and translations between English-centric and non-English-centric languages, totally covering 10 languages: English (En), French (Fr), German (De), Figure 4: Results for human preference study, comparing TEaR with IT, SCoT, and CT.The data for the first row of subfigures comes from WMT22 tested on GPT-3.5-turbo, while the experiments for the second row of subfigures were conducted on our WMT23 Zh-En dataset using three models (GPT-3.5-turbo,Gemini-Pro, Claude-2).\n\nCzech (Cs), Icelandic (Is), Chinese (Zh), Japanese (Ja), Russian (Ru), Ukrainian (Uk), and Hebrew (He).For each translation pair, we randomly selected 200 pairs to create our test dataset.Detailed statistics can be found in Appendix A.\n\nLLMs Due to the multilingual support and capabilities of the models, we focused on the effectiveness of TEaR in general-purpose LLMs through their APIs, including GPT-3.5-turbo4 , Claude-25 , and Gemini-Pro6 .We also explored the potential of Evaluation Methods To evaluate the quality of a translation, we utilize commonly used metrics (COMET-22 (Rei et al., 2020), COMETKiwi (Rei",
            "score": 0.5688510702907252,
            "section_title": "Experimental Setup",
            "char_start_offset": 8912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 651
                },
                {
                    "start": 653,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 888
                },
                {
                    "start": 890,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1271
                }
            ],
            "ref_mentions": [
                {
                    "start": 1237,
                    "end": 1255,
                    "matchedPaperCorpusId": "221819581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10321044921875
        },
        {
            "corpus_id": "218487823",
            "title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding",
            "text": "The evaluation of NLU models was always an integral part of their development. Even though there are many established tasks on which to evaluate newly proposed models, there is no strict standard specifying which one to choose. The difficulty of a fair comparison between models eventually led to the introduction of multi-task benchmarks that unify the evaluation. \n\nOne such benchmark is SentEval (Conneau and Kiela, 2018). It consists of seventeen established tasks used to evaluate the quality of sentence embeddings. Additionally, ten probing tasks are provided to detect what linguistic properties are retained in sentence embeddings. In all tasks, models 1 https://klejbenchmark.com take either a single sentence embedding or a pair of sentence embeddings as the input and solve a classification (or a regression) problem. The authors released a toolkit2 for model evaluation. However, they do not provide a public leaderboard to compare the results of different models. \n\nAnother benchmark for evaluating models is de-caNLP (McCann et al., 2018), which consists of ten pre-existing tasks. In contrast to SentEval, choice of tasks is much more diverse, ranging from machine translation, semantic parsing to summarization. All tasks have been automatically converted to a question answering format. \n\nFinally, the GLUE benchmark (Wang et al., 2019a) proposes a set of nine tasks. All of them are constructed from existing, well-established datasets. Authors selected tasks that are more diverse and more difficult than SentEval. Otherwise, the design of the benchmark is similar to SentEval. \n\nThe aforementioned benchmarks are limited to the English language. Noteworthy attempts at providing multi-language benchmarks include XNLI dataset (Conneau et al., 2018), with the MNLI (Williams et al., 2018) dataset translated by professional translators into 14 languages. A similar effort is XQuAD (Artetxe et al., 2019) which is a translation of the SQuAD dataset (Rajpurkar et al., 2016) into 10 languages. \n\nNone of these efforts includes Polish. Other resources to evaluate the Polish language understanding models are scarce.",
            "score": 0.5686182412555876,
            "section_title": "Related Work",
            "char_start_offset": 3238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 424,
                    "matchedPaperCorpusId": "3932228"
                },
                {
                    "start": 1785,
                    "end": 1808,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1968,
                    "end": 1992,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3759765625
        },
        {
            "corpus_id": "236447339",
            "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
            "text": "In the third place we have Russian, which a version of SQuAD [88], a dataset for open-domain QA over Wikidata [152], a boolean QA dataset [105], and datasets for cloze-style commonsense reasoning and multi-choice, multi-hop RC [95]. \n\nThe fourth best resourced language is Japanese, with a Cloze RC dataset [303], a manual translation of a part of SQuAD [13], and a commonsense reasoning resource [212]. \n\nThree more languages have their versions of SQuAD [235]: French [78,144], Vietnamese [210], and Korean [171], \n\nand there are three more small-scale evaluation sets (independently collected for Arabic [205]), human-translated to French [13]). Polish has a small dataset of open-domain questions based on Wikipedia \"Did you know...?\" data [188]. And, to the best of our knowledge, this is it: not even the relatively well-resourced languages like German necessarily have any monolingual QA/RC data. There is more data for individual languages that is part of multilingual benchmarks, but that comes with a different set of issues ( \u00a77.2). \n\nIn the absence of data, the researchers resort to machine translation of English resources. For instance, there is such SQuAD data for Spanish [43], Arabic [205], Italian [70], Korean [158]. However, this has clear limitations: machine translation comes with its own problems and artifacts, and in terms of content even the best translations could differ from the questions that would be \"naturally\" asked by the speakers of different languages. \n\nThe fact that so few languages have many high-quality QA/RC resources reflecting the idiosyncrasies and information needs of the speakers of their languages says a lot about the current distribution of funding for data development, and the NLP community appetite for publishing non-English data at top NLP conferences. There are reports of reviewer bias [248]: such work may be perceived as \"niche\" and low-impact, which makes it look like a natural candidate for second-tier venues 9 , which makes such work hard to pursue for early career researchers.",
            "score": 0.5683281236483737,
            "section_title": "Manuscript submitted to ACM",
            "char_start_offset": 54531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "227230829"
                },
                {
                    "start": 307,
                    "end": 312,
                    "matchedPaperCorpusId": "218973931"
                },
                {
                    "start": 397,
                    "end": 402,
                    "matchedPaperCorpusId": "226262292"
                },
                {
                    "start": 456,
                    "end": 461,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "218974030"
                },
                {
                    "start": 491,
                    "end": 496,
                    "matchedPaperCorpusId": "222066698"
                },
                {
                    "start": 607,
                    "end": 612,
                    "matchedPaperCorpusId": "189762189"
                },
                {
                    "start": 744,
                    "end": 749,
                    "matchedPaperCorpusId": "61855958"
                },
                {
                    "start": 1202,
                    "end": 1207,
                    "matchedPaperCorpusId": "189762189"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "201827300"
                },
                {
                    "start": 1230,
                    "end": 1235,
                    "matchedPaperCorpusId": "21702843"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13916015625
        },
        {
            "corpus_id": "236428949",
            "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
            "text": "Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios. We evaluate F1 and EM scores over the questions with answer annotations from two datasets, following the common evaluation practice in open QA (Lee et al., 2019). \n\nXOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data). XOR-TYDI QA offers both training and evaluation data. \n\nMKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA. MKQA has evaluation data only; XOR-TYDI QA and MKQA have five languages in common.",
            "score": 0.5683089080086057,
            "section_title": "Datasets and Knowledge Sources",
            "char_start_offset": 11704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1173
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 274,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "266844358",
            "title": "Building Efficient and Effective OpenQA Systems for Low-Resource Languages",
            "text": "These techniques include filtering out unnecessary passages prior to the retrieval step, consolidating retriever-reader models with a single encoder, and employing post-training compression (see also [62,63]). \n\nThe English-centric nature of research in this area is arguably holding back retriever development as well. The largest and most widely used dataset in this space is the MS MARCO Passage Ranking dataset [64,65], and it contains only English texts and queries. However, Bonifacio et al. [66] translated MS MARCO into 13 different languages using automatic translation. The result is mMARCO [66], the first multilingual MS MARCO variant. mMARCO has enabled much new research on multilingual passage retrieval. However, mMARCO does not have any labels within the text to denote answer spans, and so it cannot by itself support the development of multilingual QA systems. \n\nNeural information retrieval (IR) systems can begin from pretrained multilingual embeddings, and this can facilitate multilingual retrieval work. For example, Asai et al. [67] use DPR in the retriever step and propose a cross-lingual transfer method (XOR QA) to obtain answers for unanswerable questions in the non-English languages from English Wikipedia. In order to do that, they (1) translate the questions in non-English languages into English, (2) find relevant passages and answer spans from English Wikipedia, and (3) translate the English answer spans back to the original language. The leaderboard for their paper, XOR-TyDi [67], includes cross-lingual retrieval and OpenQA tasks. 4 XOR-TyDi has similar motivations to our work, in that it tackles issues around building OpenQA systems in non-English languages effectively, but it differs from our work in substantive ways. To achieve its goals, XOR-TyDi makes the English knowledge source available to non-English languages with the help of a cross-lingual retriever. In contrast, we propose a method to build an in-language retriever that benefits from an existing in-language knowledge source in a non-English language.",
            "score": 0.5677436535833026,
            "section_title": "Open Domain Question Answering",
            "char_start_offset": 15263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 212,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "248887228"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "260460088"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 1516,
                    "end": 1520,
                    "matchedPaperCorpusId": "225040672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46142578125
        },
        {
            "corpus_id": "268357052",
            "title": "No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks",
            "text": "(3) Current evaluation methods often focus on monolingual assessments and fail to comprehensively evaluate bilingual proficiency. Even more concerning is their lack of performance validation with data-out-of-training tasks, leading to potentially inflated evaluation metrics. (4) While existing instruction datasets including financial NLP tasks like classification, extraction, prediction and reasoning, these datasets, particularly for Chinese, show a notable lack of diversity in each task category. (5) Original and corresponding translated datasets are often overlooked in the development of comprehensive bilingual instruction datasets, making it challenging to assess the models' cross-lingual generalisation. \n\nTo tackle this issue, we introduce ICE-PIXIU, the first opensource Chinese-English bilingual framework meticulously designed to bridge the research gap. As detailed in Table 2, ICE-PIXIU has painstakingly curated 40 datasets, incorporating 1,185,076 raw data points, 603,940 fine-tuning instruction data, and 95,091 evaluation data, strategically covering a diverse range of Chinese-English bilingual financial tasks. This diverse array not only underscores our model's commitment to linguistic breadth but also to task-specific depth, ensuring ICE-INTENT's proficiency across various financial scenarios 2 . ICE-PIXIU includes the bilingual open-source multitask and Chinese-English instructional dataset (ICE-FIND) with 36 datasets spanning 18 specific tasks, the pioneering open-source Chinese-English bilingual evaluation benchmark (ICE-FLARE) with 40 datasets covering 20 specific tasks, and the open-source Chinese-English bilingual financial LLM, ICE-INTENT. \n\nTo establish a multi-task and bilingual instruction data and evaluation resources, we integrate Chinese and English financial specific tasks from 4 data categories, including (A) Chinese financial datasets, (B) English financial datasets, (C) Chinese translation datasets, and (D) out-of-training financial datasets (as outlined in Section 3.1.1), merging expert-annotated prompts with task-specific data samples. This led to the creation of Chinese and English bilingual financial instruction data called ICE-FIND.",
            "score": 0.5677301668672803,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "258865762",
            "title": "Cross-lingual QA: A Key to Unlocking In-context Cross-lingual Performance",
            "text": "In-context learning capabilities of Large Language Models (LLMs) have been extensively studied since the successful emergence of GPT-3 (Brown et al., 2020). This success extends to multilingual LLMs (MLLMs) such as BLOOM (Scao et al., 2022) and XGLM (Lin et al., 2022), which aim to enhance in-context learning performance across various multilingual tasks. However, not all languages are adequately addressed during the pre-training stage, leading to inconsistent cross-lingual task performance across different languages. \n\nPrevious approaches typically use translation to enhance task performance in the target language (Conneau et al., 2020;Liu et al., 2019;Xue et al., 2021). One of the earliest methods, translate-train, involves translating the train dataset into the target language. More recently, LLMs have utilized translated in-context examples (Ahuja et al., 2023;Asai et al., 2023) to evaluate cross-lingual task performance. However, translating entire task datasets can interfere with the integrity of the context and incur high translation costs, especially for lengthy passages. \n\nTo address these issues, we propose Cross-lingual QA prompting, which maintains the passage in the source language while translating only the question and answer pairs into the target language, as illustrated in Figure 1. We find that selectively translating certain components of the dataset is an efficient method for enhancing cross-lingual task performance. Retaining the passage in the source language and translating only the question-answer pairs in the incontext examples achieves performance comparable to fully translated counterparts. \n\nWe experiment with four multilingual benchmarks across three task categories: classification, reasoning, and question answering (QA). We evaluate XNLI (Conneau et al., 2018) for classification and XCOPA (Ponti et al., 2020) for reasoning. For QA tasks, we use XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020), which test the model's comprehension ability to answer questions using given contexts.",
            "score": 0.5670588511634596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 268,
                    "matchedPaperCorpusId": "245334784"
                },
                {
                    "start": 623,
                    "end": 645,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 662,
                    "end": 679,
                    "matchedPaperCorpusId": "225040574"
                },
                {
                    "start": 1850,
                    "end": 1870,
                    "matchedPaperCorpusId": "218470125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338623046875
        },
        {
            "corpus_id": "268987378",
            "title": "Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation",
            "text": "A.1 Supplementary Material on Datasets No Language Left Behind (NLLB) The NLLB (Costa-juss\u00e0 et al., 2022) corpus consists of translation training datasets for low-resource languages and is automatically created through the process of bitext mining.We employed a selection process based on the LASER score, where we chose the top 100,000 and 25,000 translation pairs from the selected language pair for dataset size variation.However, NLLB lacks coverage and human quality control due to the noisy nature of the entire procedure and is only suitable for training purposes.Government corpus (Gvt) The government document corpus (Fernando et al., 2020) is a multiway parallel corpus for Sinhala, Tamil, and English.It comprises a range of official Sri Lankan government documents, including annual and committee reports, content sourced from government websites, procurement-related documents, and legislative acts.Samanantar corpus (Sam) The Samanantar corpus (Ramesh et al., 2023) is the largest publicly available Parallel Corpora Collection for 11 Indic Languages.The data is derived from two sources: existing databases and new data automatically collected through web crawling and sentence alignment techniques.FLORES The FLORES dataset (Goyal et al., 2022) is a multiway multilingual translation evaluation dataset.FLORES-101 is comprised of translations from 842 unique web articles, comprising a total of 3001 sentences.Because all translations are fully aligned, the resulting dataset allows for a more accurate assessment of model quality on the long tail of LRLs, including the evaluation of many-to-many multilingual translation systems.The professional rigor and reliability of the results are strengthened by using an out-of-domain evaluation of this type, resulting in a more robust and objective evaluation of the model's translation performance across many domains.FLORES-200 expands the language coverage to twice that of FLORES-101.We used FLORES-200 (Costa-juss\u00e0 et al., 2022) for Sinhala since it is not in FLORES-101, and dev-test split for both FLORES-101 and FLORES-200.",
            "score": 0.5666237378997233,
            "section_title": "A Appendix",
            "char_start_offset": 15481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 1949
                },
                {
                    "start": 1949,
                    "end": 2092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "258823032",
            "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search",
            "text": "Open-domain question answering task assumes answering factoid questions without a predefined domain [26]. Since we propose a novel benchmark in crosslingual code understanding, we also review multilingual NLP models, where recent research has been focused on non-English question answering (QA) datasets and multilingual transfer learning, usually from English to other languages. \n\nUntil recently, appropriate training and evaluation datasets have been scarce, but in recent years many works have collected loosely aligned data obtained through automatic translation or by parsing similar multilingual sources. Thus, Ahn et al. [1] presented an early attempt to multilingual question answering by translating an answer to the target language. Two years later, Bos and Nissim [7] presented another similar system using translation. Lee and Lee [27] showed successful transfer for multilingual QA with training on English data and evaluation in Chinese. Ferr\u00e1ndez et al. [14] presented a work on multilingual knowledge base incorporating a system. Mitamura et al. [33] developed a system with the translation of keywords extracted from question to get an answer in the target language. Artetxe et al. [3] studied multilingual transfer of monolingual representations for a Transformer-based masked language model. M'hamdi et al. [32] examined a multilingual optimization-based meta-learning approach to learn to adapt to new languages for QA. Gao and Callan [15] proposed unsupervised pretraining for dense passage retrieval, although the authors concentrated on retrieval itself and ignored the multilingual nature of the data. \n\nMost approaches used extractive models, mostly due to the prevalence of datasets where, similar to SQuAD [37], labeled data includes an explicitly stated question, a passage containing an answer, and a span labeling for the answer.",
            "score": 0.5663462246501078,
            "section_title": "Multilingual NLP",
            "char_start_offset": 6162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 629,
                    "end": 632,
                    "matchedPaperCorpusId": "1275403"
                },
                {
                    "start": 776,
                    "end": 779,
                    "matchedPaperCorpusId": "7944769"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "446324"
                },
                {
                    "start": 1200,
                    "end": 1203,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404052734375
        },
        {
            "corpus_id": "269187651",
            "title": "Language Ranker: A Metric for Quantifying LLM Performance Across High and Low-Resource Languages",
            "text": "In the last section, we introduced and evaluated the Language Ranker, demonstrating its ability to quantify the multilingual capabilities of LLMs by comparing their internal representations against an English baseline. This provided a robust measure of how LLMs perform across different languages, especially highlighting the disparities between highresource and low-resource languages. \n\nBuilding on these insights, in this section, we delve deeper into the proposed metric to explore its credibility and reliability further. Specifically, we aim to answer the follow-ing questions in the following three subsections: RQ5: Is choosing English as the benchmark a wise choice? RQ6: What does the subspace of each language look like? RQ7: Is choosing cosine similarity a wise choice? \n\nWhy Using English as Baseline? (RQ5) \n\nIn the above sections, we choose English as a baseline. This is based on the a priori assumption that low-resource languages generally perform worse than high-resource languages. But if we choose other high-resource languages as baselines, will we get the same performance? In other words, how can we ensure that our metric is not affected by the English language itself? To answer this question, we divided our probing datasets into three types: High Resource-High Resource (H-H), High Resource-Low Resource (H-L), and Low Resource-Low Resource (L-L). To fulfill our requirement, we utilize Tatoeba-Challenge (Tiedemann 2020) as our dataset instead of opus-100 because the latter is an English-centric dataset which means there is no Low Resource-Low Resource language pair. Tatoeba-Challenge is a challenge set for machine translation that contains 32G translation units in 2,539 bitexts. The whole data set covers 487 languages linked to each other in 4,024 language pairs.",
            "score": 0.5662611291774065,
            "section_title": "Further Analysis of Proposed Metric",
            "char_start_offset": 14646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1799
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2220458984375
        },
        {
            "corpus_id": "273502039",
            "title": "A survey of neural-network-based methods utilising comparable data for finding translation equivalents",
            "text": "Finally, the evaluation dataset is the last aspect. Given Tables 2-6, the process of evaluation of the methods is not standardised and varies from model to model, hindering our ability to compare the results and their accurate quality. \n\nThe MUSE datasets enjoy big popularity and are widely used among researchers [Ren et al., 2020, Riley and Gildea, 2020, Jawanpuria et al., 2020, Zhao et al., 2020, Xu and Koehn, 2021, Zhang et al., 2021, Edmiston et al., 2022, Tian et al. [2020] 75.5 Zhao et al. [2020] 76.2 Marchisio et al. [2021b] 51.7 Bai et al. [2019] 70.0 Zhou et al. [2022a] 72.28 en-fr Joulin et al. [2018] (benchmark) 83.3 Ren et al. [2020] 82.9 Zhao et al. [2020] 83.0 Zhou et al. [2022a] 83.59 \n\nen-fi Artetxe et al. [2018a] (benchmark) 32.4 Vuli\u0107 et al. [2020] 37.4 Marchisio et al. [2021b] 37.1 Bai et al. [2019] 22 .2 et al., 2022, etc.], despite their criticism by Kementchedjhieva et al. [2019]. Some of the efforts combine MUSE datasets with other resources [Li et al., 2021, Shi et al., 2021, Severini et al., 2022, Bai et al., 2019, etc.]. Recent advances prefer datasets created by Glava\u0161 et al. [2019] and Vuli\u0107 et al. [2019] [Li et al., 2022, Vuli\u0107 et al., 2023, etc.] as they support various language combinations. \n\nImportantly, neither these evaluation datasets nor the methods reflect on translation and how to determine the correct translation equivalent, what senses to include or how to handle inflected word forms.",
            "score": 0.5662296926911987,
            "section_title": "Recent study",
            "char_start_offset": 95387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 332,
                    "matchedPaperCorpusId": "220048677"
                },
                {
                    "start": 356,
                    "end": 381,
                    "matchedPaperCorpusId": "215814124"
                },
                {
                    "start": 381,
                    "end": 400,
                    "matchedPaperCorpusId": "220047811"
                },
                {
                    "start": 420,
                    "end": 440,
                    "matchedPaperCorpusId": "233682330"
                },
                {
                    "start": 489,
                    "end": 507,
                    "matchedPaperCorpusId": "220047811"
                },
                {
                    "start": 513,
                    "end": 537,
                    "matchedPaperCorpusId": "237010914"
                },
                {
                    "start": 598,
                    "end": 618,
                    "matchedPaperCorpusId": "52158178"
                },
                {
                    "start": 659,
                    "end": 677,
                    "matchedPaperCorpusId": "220047811"
                },
                {
                    "start": 717,
                    "end": 739,
                    "matchedPaperCorpusId": "21728524"
                },
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "220272540"
                },
                {
                    "start": 782,
                    "end": 806,
                    "matchedPaperCorpusId": "237010914"
                },
                {
                    "start": 884,
                    "end": 914,
                    "matchedPaperCorpusId": "202565930"
                },
                {
                    "start": 979,
                    "end": 995,
                    "matchedPaperCorpusId": "233682330"
                },
                {
                    "start": 1013,
                    "end": 1036,
                    "matchedPaperCorpusId": "249209865"
                },
                {
                    "start": 1106,
                    "end": 1126,
                    "matchedPaperCorpusId": "59553563"
                },
                {
                    "start": 1131,
                    "end": 1149,
                    "matchedPaperCorpusId": "202539655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "235794845",
            "title": "Improving Low resource Reading Comprehension via Cross lingual Transposition Rethinking",
            "text": "In order to accelerate research progress in the field of multilingual extractive QA, several potentially practically-useful extractive QA datasets have been released in this year or so. Lewis et al. (2019) [25] present MLQA, a multi-way parallel extractive question answering evaluation benchmark in seven languages. Clark et al. (2020) [7] propose TYDI QA, a QA dataset covering eleven typologically diverse languages with 204K QA pairs. Artetxe et al. (2019) [1] create XQuAD, a dataset of 1190 SQuAD v1.1 QA pairs from 240 paragraphs by translating the instances into ten languages. Hu, J. et al (2020) [19] release XTREME, a massively multilingual multitask benchmark to verify and validate cross-lingual generalization capability of multilingual models. The goals of these proposed multilingual datasets and benchmarks are to encourage and speedup research on developing high-quality multilingual question answering systems to generalize across a large number of the world's languages with a wide variety of complex linguistic phenomena and to further accelerate research progress on models that generalize well across the linguistic phenomena and data scenarios of the world's languages. Due to huge challenges arising from typological diversity, generalization capabilities of multilingual models are far behind training-language performance and still have significant room for improvement.",
            "score": 0.5660834927940467,
            "section_title": "Multilingual Extractive QA Datasets",
            "char_start_offset": 7992,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 337,
                    "end": 340,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "278394758",
            "title": "LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering",
            "text": "We use 3 human-annotated datasets with verified published human references as evaluation sets. \n\nLITEVAL-CORPUS LITEVAL-CORPUS is a benchmark dataset developed for examining metric performance for literary translation. It contains paragraph-level parallel data with verified high-quality human translations across four language pairs: German-English (De-En), English-German (En-De), German-Chinese (De-Zh), and English-Chinese (En-Zh). The dataset combines source from contemporary and classic literary texts, comprising over 2k paragraphs. The corpus includes outputs from 9 MT systems: the GPT series (GPT-4o), commercial models (Google Translate and DeepL), popular smaller LLMs (LLaMA3, Qwen 2, Gemini, TowerInstruct), and previous SOTA systems (M2M, NLLB). Both human and MT translations are annotated with SOTA human evaluation scheme, Multidimensional Quality Metrics (MQM) (Freitag et al., 2021;Lommel et al., 2014), allowing us to examine how metrics correlate with human judgments. This correlation is measured using Kendall's \u03c4 and ACC-EQ with the official packages from WMT. 3 The dataset includes human translations, enabling us to evaluate metric adequacy-how well metrics rank human translations above MT following Zhang et al. (2025). We compare human translations against three cases: \n\n(1) the top 4 MT systems (GPT-4o, DeepL, Google Translate, Qwen 2) as identified by Zhang et al. (2025) (top-level adequacy, the most challenging case); (2) all 9 MT systems (overall adequacy); and \n\n(3) all MT systems but top performers (low-level adequacy, the simplest case). \n\nLITERARYTRAN LITERARYTRAN is a multilingual evaluation dataset containing source paragraphs from contemporary literature in English (En), German (De), French (Fr), Russian (Ru), Czech (Cs), and Japanese (Ja), with target translations in English, Japanese, and Polish (Pl).",
            "score": 0.5660646517004638,
            "section_title": "Evaluation dataset",
            "char_start_offset": 9825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 881,
                    "end": 903,
                    "matchedPaperCorpusId": "233444275"
                },
                {
                    "start": 903,
                    "end": 923,
                    "matchedPaperCorpusId": "32079268"
                },
                {
                    "start": 1230,
                    "end": 1249,
                    "matchedPaperCorpusId": "273549882"
                },
                {
                    "start": 1388,
                    "end": 1407,
                    "matchedPaperCorpusId": "273549882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37255859375
        },
        {
            "corpus_id": "267750685",
            "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages",
            "text": "Datasets. We evaluate the multilingual capabilities of LLMs on four representative datasets: \n\n\u2022 BELEBELE (Bandarkar et al., 2023): A widely used language understanding dataset covering 122 languages, where each question, linked to a short passage, has four multiplechoice answers. This dataset proves challenging for state-of-the-art LLMs. Accuracy is reported in our experiments. \n\n\u2022 FLORES (Goyal et al., 2022): A benchmark for machine translation with parallel text from Wikipedia for 204 languages, making up over 40,000 directions. We evaluate the bidirectional translation results between the target language and English, reporting scores using SacreBLEU (Post, 2018) and COMET score using WMT22-comet-da model (Rei et al., 2022a). \n\n\u2022 XL-SUM (Hasan et al., 2021): A multilingual abstractive summarization benchmark for 44 languages, comprising multiple long news texts requiring summarization into a single sentence. ROUGE-1 and ROUGE-L F1 scores are reported. \n\n\u2022 MKQA (Longpre et al., 2020): An opendomain question-answering dataset across 26 diverse languages, providing multiple possible short answers as ground truth for each question. We use the official evaluation script and report token overlapped F1 scores.",
            "score": 0.5660095178245235,
            "section_title": "Experiments 4.1 Setup",
            "char_start_offset": 16166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1225
                }
            ],
            "ref_mentions": [
                {
                    "start": 662,
                    "end": 674,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 718,
                    "end": 737,
                    "matchedPaperCorpusId": "256461051"
                },
                {
                    "start": 750,
                    "end": 770,
                    "matchedPaperCorpusId": "235658519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253173828125
        },
        {
            "corpus_id": "268032934",
            "title": "Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings",
            "text": "To assess the proficiency of our bilingual backbone models, we conduct evaluations on two distinct benchmarks for language models. The GLUE (General Language Understanding Evaluation) [Wang et al., 2019] benchmark is used to examine the model's capabilities in the English language. It comprises nine datasets designed to evaluate natural language understanding specifically in English. For measuring the cross-lingual performance of our model, we utilize the XTREME (Cross-lingual TRansfer Evaluation of Multilingual Encoders) benchmark [Hu et al., 2020]. XTREME is a comprehensive multi-task benchmark designed to evaluate the cross-lingual generalization capabilities of multilingual representations. For both benchmarks, the training, development, and test data splits are provided. It is worth noting that the test splits do not include labels, and the participants can evaluate their systems by submitting predictions on the test set via the designated submission servers. 6,7  In our study, we conduct a comparative analysis of our models against established multilingual models, specifically mBERT [Devlin et al., 2019] and XLM-RoBERTa [Conneau et al., 2020], which are commonly used as backbones for multilingual embedding models. The evaluation is performed on the development sets of the two benchmarks. \n\nFor evaluation on the GLUE benchmark, we follow the methodology of [Phang et al., 2018], for RTE, STS, and MRPC, and start fine-tuning from models trained on GLUE's MNLI task, rather than utilizing the pre-trained models directly. \n\nFor the XTREME benchmark, our experiments involve two distinct settings. Firstly, we concentrate on zero-shot cross-lingual transfer, employing English as the source language and German and Spanish as the target languages. In the second setting, translated train splits are available to conduct an evaluation where we train our models on the German and Spanish translations. \n\nIn both benchmarks, we train for 10 epochs with a batch size of 32 and a learning rate of 2e\u22125. For each task, we report the best performance on the development set. \n\nTable 2, and Table 3 report the results of the GLUE and XTREME benchmarks.",
            "score": 0.5658101740500658,
            "section_title": "Performance of Backbone Models",
            "char_start_offset": 19540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2092
                },
                {
                    "start": 2095,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 538,
                    "end": 555,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1144,
                    "end": 1166,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "269457064",
            "title": "Suvach - Generated Hindi QA benchmark",
            "text": "A critical challenge in the field of natural language processing (NLP) is the dearth of high-quality benchmarks for low-resource languages.Traditionally, these benchmarks were constructed by machine translating existing English benchmarks.This approach suffers from two key limitations: (1) bias towards frequently used English words, and (2) loss of the richness and diversity inherent in the target language during translation.Consequently, LLM outputs are inadvertently evaluated using flawed benchmarks, hindering accurate assessment.However, a promising solution emerges with the growing availability of large, freely accessible LLMs trained on massive datasets.These models, coupled with their extensive context capabilities, demonstrate significant potential for benchmark generation.As demonstrated using added context to maintain quality in our approach, further research is warranted to explore LLM-powered benchmark creation for various tasks and across a broader range of Indic languages.",
            "score": 0.5657713025289335,
            "section_title": "Conclusion",
            "char_start_offset": 4734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 1000
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342529296875
        },
        {
            "corpus_id": "273380704",
            "title": "Investigating strategies for lexical complexity prediction in a multilingual setting using generative language models and supervised approaches",
            "text": "Tables 2 and 3 shows F1 scores for Spanish, German, and French. For the sake of comparison, we also provide the results of the CWI 2018 official baseline, of the best systems of the shared task, and of a random baseline randomly selecting the output from {0,1}. We can derive several insights and make observations regarding the performance and trends across different types of training strategies: \n\nMultilingual learning. Generally, multilingual models trained on all languages (but French) have strong performance across all languages (Spanish : 0.800, German : 0.7911, French : 0.799).The zero-shot configuration, which involves using a model in scenarios where it hasn't been explicitly trained on the target language's data, performed reasonably well but not as well as multilingual models trained on all languages (Spanish: 0.746, German: 0.744), cf. Monolingual learning. Focused training on a single language shows competitive results but still lags slightly behind the multilingual approach: Spanish: 0.775, German: 0.761, cf. Table 2. Augmenting the data with translations from the English data tends to be useful, as shown in Table 2, especially with an augmentation of 4k training instances translated from English to the target language. Other tested sizes tend to reach lower performance. \n\nRegarding French, the LCP model does not use native training data but instead relies on data created by translating the English training dataset to French. This method shows varying performances as the data size increases (F1 scores: 0.770 with 2k instances, 0.713 with 4k, 0.751 with 10k, 0.717 with 27k, the full training set). The fluctuating performance with different dataset sizes indicates that the quality and consistency of translated data might vary significantly, impacting the model's learning and performance. Simply increasing the dataset size does not consistently improve performance. This approach highlights the challenges and limitations of relying on translated data for training language models, where nuances and context-specific elements of the original language might be lost or misrepresented in translation. \n\nBaseline and Random.",
            "score": 0.5657137169491075,
            "section_title": "Results",
            "char_start_offset": 14081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386962890625
        },
        {
            "corpus_id": "266999171",
            "title": "MiTTenS: A Dataset for Evaluating Gender Mistranslation",
            "text": "Finally, the evolving and contested nature of sociocultural norms related to gender make general purpose benchmark methods challenging to develop, particularly for expressions of non-binary gender across linguistic and cultural contexts globally (Dev et al., 2021;Lauscher et al., 2023;Hossain et al., 2023;Cao and Daum\u00e9 III, 2020;Keyes, 2018). \n\nTo address these challenges, we introduce Gender MisTranslations Test Set (MiTTenS); a new dataset with 13 evaluation sets, including 26 languages (Table 1). We address challenges with contamination by creating targeted synthetic datasets, releasing provenance of mined datasets, and marking dataset files with canaries (Srivastava et al., 2023). We address challenges with evaluation methods by precisely targeting specific error patterns, many of which can be scored automatically with simple heuristics. We additionally release evaluation sets for translating out of English, for use with human evaluation protocols similar to Anil et al. (2023). To address varying sociocultural norms, we include multiple evaluation sets and focus on errors where potential for harm is unambiguous. Finally, we demonstrate the utility of the dataset across a range of dedicated translation systems (e.g., NLLB, Team et al., 2022) and foundation models (e.g., . \n\nWe note that some languages we target such as Lingala have few existing evaluation resources. The evaluation sets we release can be expanded in future work (e.g., increasing diversity of source passages, more counterfactual variations). We also leave important challenges with mistranslation of non-binary gender expressions to future work.",
            "score": 0.5654485115597951,
            "section_title": "Introduction",
            "char_start_offset": 3260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 286,
                    "matchedPaperCorpusId": "258888039"
                },
                {
                    "start": 307,
                    "end": 331,
                    "matchedPaperCorpusId": "204961553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "276928746",
            "title": "Contextual Cues in Machine Translation: Investigating the Potential of Multi-Source Input Strategies in LLMs and NMT Systems",
            "text": "mediate languages significantly enhance translation quality for technical domains with defined terminology. The experiments also suggest this approach may be particularly beneficial for linguistically distant language pairs, though further research with additional language pairs would be needed to validate this hypothesis. When GPT-4o generates its own intermediate translations for context, performance remains consistently below baseline levels, suggesting that the model does not provide additional language-specific information comparable to gold-standard context translations. The comparison between GPT-4o and the NMT system highlights their complementary strengths: while the latter excels in direct English-to-Portuguese translations, particularly on proprietary datasets, GPT-4o shows superior performance in leveraging contextual information for domain-specific content. Furthermore, our implementation of shallow fusion in the NMT system enhances the model's performance for non-English source languages by effectively leveraging English as an auxiliary context, suggesting that multilingual models can be optimized by leveraging their strongest language pair to enhance performance across other language combinations. These findings demonstrate the potential of multi-source strategies to enhance translation accuracy across diverse scenarios. They emphasize the importance of selecting models and methods based on task-specific requirements, such as leveraging GPT-4o for its contextual adaptability and using shallow fusion in multilingual NMT systems for non-English source-target pairs.",
            "score": 0.5652811657817671,
            "section_title": "Spanish",
            "char_start_offset": 24083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1529541015625
        },
        {
            "corpus_id": "232307434",
            "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets",
            "text": "This risk of undesired downstream effects calls for future studies with a careful treatment of intertwined effects such as data size and domain, languagespecific phenomena, evaluation data and metric biases. To give the reader a brief glimpse of the impact of data quality for the example of translation, we compare the C% metric from our audit with the translation quality (sentencepiece-BLEU, spBLEU) of the multilingual translation model M2M124 for 124 languages (Goyal et al., 2021). It was trained on WikiMatrix and CCAligned, and similar data collected with the same tools, which we expect to show similar biases. Translation quality is evaluated on the trusted, humantranslated FloReS benchmark (Goyal et al., 2021). For the 21 languages present in both the audit and the FloReS benchmark, we found a positive correlation (Spearman) between the data quality scores and spBLEU of \u03c1 = 0.44 (p = 0.041). This is not as large as the correlation with data size (\u03c1 = 0.66, p = 0.00078), but it nonetheless helps to explain translation quality-the correlation between the product of C% and data size (in other words, the expected total number of good sentences in the dataset), is the highest yet, with a value of \u03c1 = 0.73 (p = 0.00013).12 Representation washing Since there are datasets which contain many low-resource languages, the community may feel a sense of progress and growing equity, despite the actual quality of the resources for these languages. Similarly, if low-quality datasets are used as benchmarks they may exaggerate model performance, making low-resource NLP appear more solved than it is-or conversely, if models perform poorly when trained with such data, it may be  Table 6: Examples of \"parallel\" data where the translation has a different meaning than the source, but the form looks the same. (We added translations of the non-English side.) Such data may encourage hallucinations of fake \"facts\". wrongly assumed that the task of learning models for these languages is harder than it actually is or infeasible given current resources.",
            "score": 0.5649319110046075,
            "section_title": "Dataset Mis-labeling",
            "char_start_offset": 28749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2061
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1961669921875
        },
        {
            "corpus_id": "258352741",
            "title": "Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models",
            "text": "Datasets We evaluate performance with the XL-WSD dataset (Pasini et al., 2021), which is comprised of 18 languages: Basque, Bulgarian, Catalan, Chinese, Croatian, Danish, Dutch, English, Estonian, French, Galician, German, Hungarian, Italian, Japanese, Korean, Slovenian, and Spanish. We use the BabelNet API 4.0.1 (Navigli and Ponzetto, 2010) as our multilingual word sense ontology to obtain translations and sense inventories of the data. \n\nWe consider five target languages for our experiments: English, Chinese, Russian, Spanish, and Finnish; we aim to consider a wide range of typologically diverse languages as targets while maintaining high coverage of the source language examples in the multilingual ontology. 2 In the case where a (non-English) evaluation example does not have at least one corresponding translation in the target language, we back off to the English translation setting as it provides full coverage over all non-English evaluation sets. When evaluating English, we instead back off to the most common sense (MCS) of the word when an example is not covered by the target language(s) in each evaluation setting. \n\nModels Picking the three most powerful PLMs from the previous section, we use the BLOOM models (Scao et al., 2022) with 3 billion parameters and 7.1 billion parameters and the GPT-NeoX model with 20 billion parameters (Black et al., 2022). While GPT-NeoX is primarily trained in English, the Bloom models are specifically pretrained on 6 out of the 18 evaluation languages of the XL-WSD dataset. \n\nBaselines We consider the Most Common Sense (MCS) method as a baseline, which predicts each word's most common sense according to BabelNet (Pasini et al., 2021). Additionally, we report the best results from the models introduced to benchmark the XL-WSD dataset in Pasini et al. (2021) as well as those in Zhang et al. (2022) and Berend (2022). Prior results are presented as a point of reference for the task scores.",
            "score": 0.5646089767126492,
            "section_title": "Experimental Setup",
            "char_start_offset": 14901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1536
                },
                {
                    "start": 1539,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 78,
                    "matchedPaperCorpusId": "231878544"
                },
                {
                    "start": 315,
                    "end": 343,
                    "matchedPaperCorpusId": "2085726"
                },
                {
                    "start": 1678,
                    "end": 1699,
                    "matchedPaperCorpusId": "231878544"
                },
                {
                    "start": 1804,
                    "end": 1824,
                    "matchedPaperCorpusId": "231878544"
                },
                {
                    "start": 1845,
                    "end": 1864,
                    "matchedPaperCorpusId": "252904712"
                },
                {
                    "start": 1869,
                    "end": 1882,
                    "matchedPaperCorpusId": "250390978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "241583550",
            "title": "SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining",
            "text": "Results for data augmentation are shown in Table 7. When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning. \n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages. \n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval. \n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average.",
            "score": 0.5644861794428911,
            "section_title": "Data Augmentation",
            "char_start_offset": 25601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5166015625
        },
        {
            "corpus_id": "274149965",
            "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
            "text": "Methodology. We compare the performance of models trained on combinations of German (target) and English (auxiliary) with varying existing English datasets based on the common crawl: mC4 (Xue, 2020), RedPajamav2 (RPJv2) (Computer, 2023), RefinedWeb (RFW) (Penedo et al., 2023), FineWeb (FW), and FineWeb-EDU (FWE) (Penedo et al., 2024). These datasets have been constructed following a pipeline of different filtering steps outlined in Figure 1a. The resulting datasets have higher quality filtering and cover different snapshots of the common crawl. They are all aimed at general language model pretraining. Further details for the datasets are available in Appendix B. For comparison with monolingual models trained only on German (target), we consider both German mC4, and a low quality version of German mC4 referred to as \"no ARC German mC4\". 2 \n\nFindings. Across all tasks in Figure 2, we observe that for English evaluations, better quality English datasets attain substantially higher performance on the English tasks. For the medium models, this corresponds to up to 5% increase between the worst and best performing English datasets. For the XL models, the performance improvement in English is up to 9%. For the same benchmarks translated into German, the performance increase is around 1% for the medium model and 2% for the XL model. Of all compared English datasets, FineWeb-EDU achieves the best average downstream performance, and similar performance to the no ARC German dataset. There are two primary factors we hypothesize contribute to FineWeb-EDU achieving better performance on downstream tasks: high-quality data filtering, and information filtering. \n\nNext, we investigate these factors in more detail.",
            "score": 0.5640682244683093,
            "section_title": "Better English Datasets",
            "char_start_offset": 9234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2208251953125
        },
        {
            "corpus_id": "265019003",
            "title": "Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval",
            "text": "Since 2020, the EU has publicly released raw debates in the form of transcribed source-language speeches with rich multilingual topic index data, along with the original video and audio recordings. This forms the basis of the Multi-EuP dataset, with additional attributes for each speaking MEP such as an image, birthplace, and nationality. Zhang et al. (2021) introduced Mr. TYDI, an evaluation benchmark dataset for dense retrieval assessment over 11 languages. This dataset is constructed from TYDI (Clark et al., 2020), a question answering dataset. For each language, annotators assign relevance scores as judgments for questions, derived from Wikipedia articles. Notably, the questions for different languages are crafted independently, and relevance judgements are provided in-language only. Based on the dataset, the authors evaluate on monolingual retrieval tasks for non-English languages using BM25 and mDPR as zero-shot baselines. However, Mr. TYDI's scope is limited in that it is not truly multilingual, in that queries in a given language are only performed over documents in that language. This is part of the void our work aims to address. \n\nMS MARCO (Nguyen et al., 2016) (Tiedemann and Thottingal, 2020) and one commercial system in the form of Google Translate. 4 Analysis of the authors' results reveals a positive correlation between translation quality and retrieval performance, with higher translation BLEU scores yielding improved retrieval MRR outcomes. However, similar to Mr. TYDI, mMARCO focuses on in-language retrieval only for multiple languages, rather than multilingual retrieval. \n\nThroughout the past few decades, numerous datasets and tasks pertaining to multilingual retrieval have been developed for evaluation, through efforts such as CLEF, TREC, and FIRE, each contributing standardized document collections and evaluation procedures. These evaluation datasets facilitate genuine multilingual IR research such as Rahimi et al. (2015) and Lawrie et al. (2023). However, the scope of these datasets is generally limited to a small number of queries. For example, in the case of CLEF 2001CLEF -2003, each edition encompasses a mere few dozen queries.",
            "score": 0.5639777944358515,
            "section_title": "Introduction",
            "char_start_offset": 4184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 1190,
                    "end": 1221,
                    "matchedPaperCorpusId": "221097277"
                },
                {
                    "start": 1955,
                    "end": 1975,
                    "matchedPaperCorpusId": "126361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "276249513",
            "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
            "text": "This appendix describes the translation scheme that was used to machine translate the datasets used in our experiments. This scheme is designed to mitigate low-quality translations, or hallucinations generated by the machine translation model, thus providing reliable datasets at the end. We specifically used a large (i.e., 1.2 billion parameters) open-access multilingual machine translation model named M2M100 (Fan et al., 2021), available on the HuggingFace6 model hub. M2M100 was trained on a range of high and low-resource languages from different families and using different scripts, achieving state-of-the-art performance across a diverse set of 100 languages. While machine translated data allows us to assess multilingual captioning metrics, the results will depend not only on the performance of the metrics but also on the quality of the translations. Low-quality translations, or hallucinations generated by the translation model, will impact the caption and break our assumption that human ratings for the English data can be transferred across languages. To address this issue, we propose to use the COMETKiwi (Rei et al., 2022(Rei et al., , 2023) ) machine translation quality estimation metric to control for translation quality, assessing the impact of low quality translations on the observed results. \n\nWe specifically began by translating the VICR dataset, followed by the other English datasets mentioned in Appendix D. VICR features English captions with human ratings and also reference captions originally from the MSCOCO and Flickr8K datasets. For each caption, whether a candidate or a reference, we return 25 translations using a beam search technique with 100 beams. Subsequently, we filtered the candidates with a language checker, to ensure proper translation into the intended language. After the language check, we selected for each instance the translation that scored higher based on a large COMETKiwi model7 .",
            "score": 0.5637537003756272,
            "section_title": "D The Machine Translation Scheme",
            "char_start_offset": 41539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 431,
                    "matchedPaperCorpusId": "224814118"
                },
                {
                    "start": 1126,
                    "end": 1143,
                    "matchedPaperCorpusId": "252222165"
                },
                {
                    "start": 1143,
                    "end": 1165,
                    "matchedPaperCorpusId": "262084083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39697265625
        },
        {
            "corpus_id": "225040672",
            "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering",
            "text": "Multilingual QA Recently, much effort has been made to create non-English QA datasets to overcome the data scarcity in non-English languages. We already discussed primary differences between our XOR-TYDI QA and existing multilingual QA datasets in \u00a72.2. MLQA (Lewis et al., 2020a) and XQuAD (Artetxe et al., 2020b) were created by translating questions from SQuAD (Rajpurkar et al., 2016) to several languages to provide parallel evaluation datasets to estimate mod-    XOR-TYDI QA provides train and evaluation datasets to foster building a multilingual QA system that is robust to diverse linguistic and cultural backgrounds. All of our questions are information-seeking questions written by native speakers, thereby reducing exploitable artifacts. Further, none of the prior multilingual QA work that involves retrieval from large scale document collections (e.g., Wikipedia) addresses the challenges of cross-lingual open-retrieval. MKQA marks examples that cannot be answered by knowledge sources in a single language (e.g., English) as \"unanswerable,\" while we find answers for those questions using another language's Wikipedia, demonstrating the benefit of largescale multilingual knowledge sources.\n\nCross-lingual Information Retrieval Crosslingual Information Retrieval (CLIR) is the task of retrieving relevant documents when the document collection is in a different language from the query language (Hull and Grefenstette, 1996;Ballesteros and Croft, 1996). The retrieval component in XOR QA is closely related to CLIR, but differs in several critical ways. Firstly, since the end goal of XOR QA is question answering, XOR QA queries always take question forms rather than key words for search. Further, while CLIR typically retrieves documents from a single (low-resource) language (Zhang et al., 2019), XOR QA considers documents from both English and the query language. In many real-world applications, we do not know a priori in which language we can find information that the user is seeking for. Lastly, our document collection is orders of magnitude bigger than typical CLIR benchmarks (Sasaki et al., 2018;",
            "score": 0.5636631599278956,
            "section_title": "Related Work",
            "char_start_offset": 25030,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 314,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1441,
                    "end": 1469,
                    "matchedPaperCorpusId": "7109112"
                },
                {
                    "start": 1796,
                    "end": 1816,
                    "matchedPaperCorpusId": "182952611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "239993316",
            "title": "An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation",
            "text": "We use three highly multilingual MT benchmarks:\n\n\u2022 TED Talks (Qi et al., 2018). An Englishcentric parallel corpus with 10M training sentences across 116 translation directions. Following Aharoni et al. (2019), we evaluate on a total of 16 language directions, while as zeroshot test we evaluate on 4 language pairs.\n\n\u2022 WMT-2018 (Bojar et al., 2018). 3 A parallel dataset provided by the WMT-2018 shared task on news translation. We use all available language pairs, i.e. 14, up to 5M training sentences for each language pair. We evaluate the models on the test sets of the shared task, i.e. newstest2018. As there are no zero-shot test sets provided by the competition, we use the test portion from the Tatoeba-challenge (Tiedemann, 2020), 4 in all possible language pair combinations included in the challenge.\n\n\u2022 OPUS-100 ). An Englishcentric multi-domain benchmark, built upon the OPUS parallel text collection (Tiedemann, 2012 Table 1: Results on the Many-to-Many TED Talks benchmark. The baselines consist of 1 our replication of the standard 6-layer Transformer model by Aharoni et al. (2019), and 2 its variant with a 1.5-entmax function on the cross attention heads as in Correia et al. (2019). The labels (a), (b), (c) denote the use of different alignment supervision (see Section 2). \"#Param.\": trainable parameter number. \"EN -> X (16)\" and \"X-> EN (16)\": average BLEU scores for English to Non-English languages and for Non-English languages to English on 16 language pairs respectively. \"BLEU zero (4)\" and \"ACC zero (4)\": average BLEU scores and target language identification accuracy over 4 zero-shot language directions. We report average BLEU and accuracy scores, plus the standard deviation over 3 training runs with different random seeds.\n\nlanguage pair. It provides supervised translation test data for 188 language pairs, and zero-shot evaluation data for 30 pairs.\n\nFollowing related work (Aharoni et al., 2019;",
            "score": 0.5628752566268114,
            "section_title": "Experimental Setup",
            "char_start_offset": 3996,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 77,
                    "matchedPaperCorpusId": "4929974"
                },
                {
                    "start": 328,
                    "end": 348,
                    "matchedPaperCorpusId": "53247198"
                },
                {
                    "start": 722,
                    "end": 739,
                    "matchedPaperCorpusId": "222310761"
                },
                {
                    "start": 915,
                    "end": 931,
                    "matchedPaperCorpusId": "15453873"
                },
                {
                    "start": 1078,
                    "end": 1099,
                    "matchedPaperCorpusId": "67855815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365966796875
        },
        {
            "corpus_id": "268201926",
            "title": "EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation",
            "text": "Dataset details. We evaluated our methods using IWSLT 2017 Cettolo et al. (2017) and Europarl v7 Koehn (2005). \n\nTable 6 provides a summary of the languages. \n\nThe IWSLT 2017 translation dataset features multilingual data derived from TED talks. We followed previous work and used a standard split for zero-shot evaluation (Dabre and Kurohashi, 2017;Liu et al., 2021). In particular, IWSLT contains English-centric training data for Italian, Dutch, and Romanian, while evaluation is performed in six zero-shot directions. IWSLT is a relatively small dataset, which tests our method's ability to generalize from few languages. \n\nEuroparl is a multilingual dataset crawled from the proceedings of the European Parliament. We again followed previous work (Liu et al., 2021) and evaluated our methods with a standard split for the zero-shot setting, containing   English-centric data for eight languages with a total of 56 zero-shot evaluation directions. We specifically adopted the non-overlapping setting: in the original corpus, a sentence may be translated into multiple languages, and the non-overlapping setup chooses only one target translation for each input. This prevents potential data leaking problems. Europarl contains more data and languages than IWSLT, which further tests our method's ability to generalize across multiple languages. \n\nImplementation details. We directly adopt the neural architecture and hyperparameters in Liu et al. (2021). In particular, we used 5-and 8-layer encoder-decoder models for IWSLT and Europarl, respectively. For both datasets, we had 512 hidden units and 8 attention heads. Our BLEU scores are based on SacreBLEU Post (2018) with the following specifications: BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1. These setups establish a controlled comparison against previous work. \n\nIn our presentation of beam search and the proposed EBBS, we describe the scorer as the multiplication of step-wise probabilities. In implementation, we used the sum of log-probabilities for numerical stability.",
            "score": 0.5627117787601693,
            "section_title": "B Details of Our Experiments",
            "char_start_offset": 31492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 80,
                    "matchedPaperCorpusId": "21675165"
                },
                {
                    "start": 97,
                    "end": 109,
                    "matchedPaperCorpusId": "38407095"
                },
                {
                    "start": 350,
                    "end": 367,
                    "matchedPaperCorpusId": "229923747"
                },
                {
                    "start": 752,
                    "end": 770,
                    "matchedPaperCorpusId": "229923747"
                },
                {
                    "start": 1439,
                    "end": 1456,
                    "matchedPaperCorpusId": "229923747"
                },
                {
                    "start": 1661,
                    "end": 1672,
                    "matchedPaperCorpusId": "13751870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242919921875
        },
        {
            "corpus_id": "258840943",
            "title": "LLM-powered Data Augmentation for Enhanced Crosslingual Performance",
            "text": "The zero-shot cross-lingual approach is commonly used when a multilingual dataset is insufficient. In this subsection, we investigate whether a synthetically generated multilingual dataset outperforms training solely in English. We choose the XCOPA dataset and explore two settings: synthetic multilingual data by asking LLMs to generate responses in the target languages directly and translating the English-generated data to target languages with Google Translate API. We exclude Dolly-v2 and StableVicuna due to their limited effectiveness in generating non-English text. Although GPT-4 exhibits the most promising performance, it is significantly costlier compared to ChatGPT. Therefore, we also consider using ChatGPT as a contrasting Table 5: Accuracy on XCOPA. ORI corresponds to the original data, GEN EN and GEN XX represents data generated in English and target languages. T rans denotes translations of the English-generated data. We show languages that are available in all settings. Improvement and decline in performance are represented with green and red shadows. \n\nexperiment under resource-constrained conditions. \n\nTable 5 shows the results for the languages that are available for all settings, excluding TR and TH (unavailable for LLM-generation, refer to \u00a74), and QU (not supported by the Google Translate API). We can see the impact of the generated data varies across different fine-tuned models and languages, aligning with the findings of Kumar et al. (2022). Training on GPT-4 synthesized data displays consistent improvement across all scenarios and languages, except the zero-shot crosslingual result on HT with XLMR-Large. \n\nMore fluctuating results can be observed with ChatGPT-generated data. A comparison between GENEN + ORI and GENXX + ORI indicates that utilising data generated in target languages generally leads to improved performance with GPT-4 generated data, as well as in base models with ChatGPT-generated data. However, for XLMR-Large, employing ChatGPT-generated data in target languages mostly yields negative outcomes. In languages such as TA and VI, training on generated data in the target languages results in more performance degradation compared to zero-shot cross-lingual transfer.",
            "score": 0.5622592457073142,
            "section_title": "Multilingual Data Generation",
            "char_start_offset": 13480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 1464,
                    "end": 1483,
                    "matchedPaperCorpusId": "250144691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164794921875
        },
        {
            "corpus_id": "250390900",
            "title": "AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models",
            "text": "Encoder-decoder models are best suited for natural language generation tasks such as summarization, question answering, machine translation, etc. Cross-lingual datasets are often used as benchmarks to evaluate multilingual pretrained models. Despite our efforts to evaluate on as many tasks as possible, many existing datasets feature few to no African languages. For example, popular crosslingual datasets such as WikiLingua (Ladhak et al., 2020), XQuAD (Artetxe et al., 2020), and Tydi QA (Clark et al., 2020) only contain Swahili. \n\nExisting machine translation systems in many low-resource languages require much larger parallel corpora to improve translation quality. Exam-ples include languages such as Yor\u00f9b\u00e1, Igbo, and Luganda. To improve such systems, there is a need for high-quality data in multiple domains. While there are existing efforts to curate parallel datasets such as JW300 (Agi\u0107 and Vuli\u0107, 2019), Yor\u00f9b\u00e1 (Adelani et al., 2021), Igbo (Ezeani et al., 2020), Fon (Emezue and Dossou, 2020), parallel corpora for bi-directional translation in Amharic, Tigrigna, Afan-Oromo, Wolaytta, and Ge'ez (Teferra Abate et al., 2018), there is a need for continued research to creating high-quality datasets to further drive advances in low-resource machine translation (Fan et al., 2021).",
            "score": 0.5621517897596096,
            "section_title": "Limitations",
            "char_start_offset": 19059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 426,
                    "end": 447,
                    "matchedPaperCorpusId": "222177239"
                },
                {
                    "start": 455,
                    "end": 477,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 491,
                    "end": 511,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 926,
                    "end": 948,
                    "matchedPaperCorpusId": "247476207"
                },
                {
                    "start": 1111,
                    "end": 1139,
                    "matchedPaperCorpusId": "53383410"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "224814118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "270737791",
            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
            "text": "The findings of our initial evaluation suggest that the use of automatic translation of the questions in LLM benchmarks causes deviations in the results due to errors in the translation.To eliminate those deviations and ensure that the results are not contaminated by translation errors, the translations should be revised by experts and, ideally, the tests should be adapted to the language being evaluated.However, given the number of questions on the LLM benchmarks and the number of languages evaluated, this requires a large effort that calls for coordinated action from the community.\n\nIn fact, there are efforts in this direction such as the validation of the Spanish Okapi benchmarks as part of the #Somos600M Project Grandury (2024).This community annotation effort uses open-source frameworks and is performed by native Spanish-speaking volunteers.During the first two months, more than 60 persons participated and together covered one-third of the total number of dataset items, which shows how time-consuming it is to manually validate and correct these translations.\n\nIn the absence of manually adapted or checked benchmarks, our methodology enables a fast evaluation of the impact of translation errors with limited manual checking.Further refinements can be introduced to detect translation errors.For example, after translating into Spanish we could translate back into English the questions with different answers in both languages and run the questions again.When the answer to this English translation is different from the one of the original English question, the translation is likely to be the culprit of the error.In general, developing strategies to identify these issues automatically would be very helpful to understand the impact of automatic translation and also to correct the translation errors.\n\nIn this work, we have focused on Spanish which is one of the most widely used languages in the world and also is typically in the top five languages with more data on the LLM training datasets.It would be interesting to study the impact of translation errors on other languages.For languages with fewer data and speakers, we would expect a larger number of translation errors but also a larger number of genuine errors in the LLM answers.Therefore, the relative impact of translation errors on the benchmark results compared to Spanish can be either larger or smaller.",
            "score": 0.5619901743288397,
            "section_title": "Discussion",
            "char_start_offset": 13911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 590
                },
                {
                    "start": 592,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1079
                },
                {
                    "start": 1081,
                    "end": 1246
                },
                {
                    "start": 1246,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1638
                },
                {
                    "start": 1638,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2106
                },
                {
                    "start": 2106,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2396
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350830078125
        },
        {
            "corpus_id": "250390624",
            "title": "Zero-shot cross-lingual open domain question answering",
            "text": "(Clark et al., 2020a). 2) Large-scale cross-lingual datasets are not available that supply passages in a diverse number of languages which can enable better training of cross-lingual retrievers. \n\nOne specific approach that has been followed for bringing multilingual QA close to English QA is that the non-English question is translated into English and the answer from the English QA system is translated back to the query language. These systems suffer from the problem of machine translation error propagating itself in the downstream question answering. And also, these systems aren't able to exploit the fact that for high resource languages like Spanish, and Chinese the evidence might lie in the target language itself which is eas-ier than two-way translation. \n\nIn this paper, we aim to extend the task of crosslingual question answering to tackle the following research questions -a) How can we adapt the retrieve-then-generate approaches for English open QA to cross-lingual QA that do not rely on machine translation? -b) How do multilingual QA models trained on a small set of languages perform in zero-shot settings? \n\nWe follow CORA (Asai et al., 2021b) which is a many-to-many multilingual QA model by following a four-stage pipeline for addressing crosslingual QA. The DPR based on mBERT (mDPR) is a bi-encoder retriever that retrieves documents cross-lingually without relying on machine translation. XLM-RoBERTA which serves as a passage reranker is trained as a cross-encoder to capture the interactions between the question and the passage on the top k documents fetched by the mDPR retriever. The reranked documents are passed through a Fusion-in-Decoder based mT5 reader module which can effectively learn to collect evidence from multiple passages to arrive at the final answer. In some cases, the predicted answer is not in the target language as desired by the user because the generator is either not able to convert entities into the target language or evidence is directly extracted from a different language passage. Further, we use a postprocessing step to map entities from Wikidata to convert the answer into the target language.",
            "score": 0.5619374583030304,
            "section_title": "Introduction",
            "char_start_offset": 1984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 1149,
                    "end": 1169,
                    "matchedPaperCorpusId": "236428949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.176513671875
        },
        {
            "corpus_id": "258048937",
            "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
            "text": "In this section, we report results on multilingual machine translation and introduce our main findings about LLMs' translation ability. \n\nThe multilingual translation capabilities of LLMs are continually involving Table 1 and Table 3 presents another clue, where we evaluate GPT-4 on French-centric and Chinese-centric translation. Compared to English-centric translation, GPT-4 faces greater challenge when it comes to non-English-centric translation, which again indicates LLM's unbalanced translation ability across languages. \n\nLLMs still lag behind the strong supervised baseline, especially on low-resource languages Figure 2 shows the translation performance of the supervised systems and GPT-4 on each language. In 40.91% translation directions, GPT-4 has achieved higher BLEU scores than NLLB, indicating the promising future of this new translation paradigm. But on long-tail low-resource languages, GPT-4 still lags behind NLLB, let alone Google Translate. \n\nData leakage issue should be considered before evaluating LLMs on public datasets. We do not include BLOOMZ's performance on FLORES-101 in our report because BLOOMZ is instructiontuned with XP3 dataset (Scao et al., 2022), which includes FLORES-200 dataset. Thus BLOOMZ may have been exposed to test cases from FLORES-101 during training. If so, the evaluation results can not precisely reflect its translation ability (Elangovan et al., 2021). \n\nTo illustrate this concern, we take 1000 English sentences from the most recent news spanning August 2023 to October 202311 , and ask human experts to translate them into Chinese and construct a bilingual no-leakage evaluation set, named NEWS2023. Figure 4 shows that BLOOMZ's performance significantly deteriorates on this no leak-age set, whereas other models maintain a consistent performance across both datasets. Through this example, we wish to draw the community's attention to the potential data leakage issue when evaluating large language models.",
            "score": 0.5619138087500106,
            "section_title": "Benchmarking LLMs for Massively Multilingual Machine Translation",
            "char_start_offset": 7739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 138,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "269214448",
            "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
            "text": "Zero-shot cross-lingual transfer. There is a long line of research on cross-lingual representation generalizability, such as with sentence embeddings (Conneau et al., 2018) or more recently, LMs (Wu andDredze, 2019, 2020;Pires et al., 2019;Siddhant et al., 2020). Commonly, a multilingual LM (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020a; i.a.) is finetuned on a task in a source language and evaluated on the task's test set in a different language. This is generally effective. Our RM transfer setup can be viewed under this framework, but we go further and show that this generalizability is useful for downstream tasks, in our case alignment. Shaham et al. (2024) and Chirkova and Nikoulina (2024) are close to us in studying cross-lingual generalizability in alignment, but only focusing on SFT and only using translated data. \n\nMultilingual Alignment. For SFT, it is common to assemble existing multilingual task datasets into instruction datasets (Muennighoff et al., 2023;Asai et al., 2023;Ahuja et al., 2023). Some have directly collected SFT data for non-English languages, either on a per-language basis (Zhang et al., 2023;Xu et al., 2023b;Ni et al., 2023;i.a.) or multilingually (Zhao et al., 2024;Singh et al., 2024), though this can be expensive. Past work has also used automatic translation for SFT (Li et al., 2023a;Lai et al., 2023;Shaham et al., 2024;i.a.) and RM data (Lai et al., 2023;Shen et al., 2024). We also use translation for SFT, but showed that crosslingual transfer outperforms translation for RM.",
            "score": 0.5613484136806128,
            "section_title": "Related Work",
            "char_start_offset": 26424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1551
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 240,
                    "end": 262,
                    "matchedPaperCorpusId": "202539969"
                },
                {
                    "start": 292,
                    "end": 313,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 313,
                    "end": 338,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 338,
                    "end": 359,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 976,
                    "end": 1002,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 1356,
                    "end": 1373,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 1411,
                    "end": 1429,
                    "matchedPaperCorpusId": "260334562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "270226445",
            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
            "text": "Visual question answering (VQA) aims to answer an open-ended question by reasoning about a given image (Agrawal et al., 2015).Despite recent advances in vision-language (VL) modeling, building proficient models across various languages is still challenging.This issue primarily arises from the limited availability of annotated datasets, which are predominantly in high-resource languages such as English.Although recent efforts in developing multilingual VL models can address this issue to some extent (Zhou et al., 2021;Qiu et al., 2022;Li et al., 2023b;Geigle et al., 2023), training on datasets in the target languages is still crucial for enhanced model performance in those languages.\n\nTo mitigate the data scarcity issue, cross-lingual transfer learning focuses on utilizing extensive datasets in a source language (typically English) to build models effective in a target language (Artetxe et al., 2020;Bugliarello et al., 2022).One of the popular approaches, namely translate-train, translates training samples into individual target languages and uses them to train models for target languages.This approach is advantageous as it does not perform translation during inference, but it requires training individual models for each target language.Furthermore, recent VL models (Singh et al., 2022;Liu et al., 2023b;Li et al., 2023a), which are mostly tailored in English, are not suitable for the translate-train approach.Another widely adopted approach, called translatetest, translates test samples written in target languages into the source language and uses VL models of the source language for the inference.These translation-based approaches have shown remarkable performance in cross-lingual tasks.\n\nDespite the effectiveness of translation systems in cross-lingual VL tasks, using machine-translated texts as input inevitably introduces a mismatch between the training and inference phases.In the translate-test approach, models are trained on human-written texts but evaluated on machinetranslated texts.This distribution shift could hurt the generalization of models to different lan-arXiv:2406.02331v1",
            "score": 0.5613036500244983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 691
                },
                {
                    "start": 693,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1623
                },
                {
                    "start": 1623,
                    "end": 1715
                },
                {
                    "start": 1717,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 2023
                },
                {
                    "start": 2023,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 125,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 912,
                    "end": 937,
                    "matchedPaperCorpusId": "40142250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3037109375
        },
        {
            "corpus_id": "246442089",
            "title": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers",
            "text": "Today, the research in the field of KGQA is strongly dependent on data, and it suffers from the lack of multilingual benchmarks [6], [7]. To the best of our knowledge, only three KGQA benchmarks exist that tackle multiple languages: QALD [3], RuBQ [4], and CWQ [5]. \n\nQALD-9 contains 558 questions incorporating information of the DBpedia knowledge base7 [8] where for each question the following is given: a textual representation in multiple languages, the corresponding SPARQL query (over DBpedia), the answer entity URI, and the answer type. The dataset has become a benchmark for many research studies in QA (e.g., [9], [1], [10], [11], [12]). \n\nRuBQ 2.0 is a KGQA dataset over Wikidata8 [13] that contains 2910 questions. The questions are represented in native Russian language and machine-translated to English language. Additionally, it contains a list of entities, relations, answer entities, SPARQL queries, answer-bearing paragraphs, and query type tags. \n\nCWQ is a recently published KGQA dataset over Wikidata that is based on CFQ data [14]. CWQ contains questions in Hebrew, Kannada, Chinese, and English languages. All the non-English questions were obtained using machine translation with several rule-based adjustments. It has a well-detailed structure including: question with highlighted entities, original SPARQL query over Freebase [15] (from CFQ), SPARQL query over Wikidata (introduced in CWQ), textual representations of a question in four aforementioned languages, and additional fields. \n\nDespite the considered benchmarks contain questions in multiple languages, these multilingual representations were either machine-translated (RuBQ 2.0, CWQ) or have doubtful quality (QALD-9, see Section III-A). In contrast, the dataset that we present is covering 9 languages (including English) and was created by native speakers.",
            "score": 0.5605103229312007,
            "section_title": "II. MULTILINGUAL KGQA BENCHMARKS",
            "char_start_offset": 4145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1847
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 131,
                    "matchedPaperCorpusId": "212848801"
                },
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "219781347"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "235271356"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "7278297"
                },
                {
                    "start": 620,
                    "end": 623,
                    "matchedPaperCorpusId": "16925174"
                },
                {
                    "start": 625,
                    "end": 628,
                    "matchedPaperCorpusId": "3953136"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "3670687"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "239349592"
                },
                {
                    "start": 1354,
                    "end": 1358,
                    "matchedPaperCorpusId": "207167677"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378662109375
        },
        {
            "corpus_id": "222141608",
            "title": "Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages",
            "text": "Approaches. Multilingual MT (Dong et al., 2015;Firat et al., 2016a,b;Wang et al., 2020) addresses the transfer of MT from high-resourced to low-resourced languages by training multilingual models for all languages at once. (Aharoni et al., 2019;Arivazhagan et al., 2019) train models to translate between English and 102 languages, for the 10 most high-resourced African languages on private data, and otherwise on public TED talks (Qi et al., 2018). Multilingual training often outperforms bilingual training, especially for low-resourced languages. However, with multilingual parallel data being also Anglo-centric, the capabilities to translate from English versus into English vastly diverge (Zhang et al., 2020).\n\nAnother recent approach, mBART (Liu et al., 2020), leverages both monolingual and parallel data and also yields improvements in translation quality for lower-resource languages such as Nepali, Sinhala and Gujarati. 3 While this provides a solution for small quantities of training data or monolingual resources, the extent to which standard BLEU evaluations reflect translation quality is not clear yet, since human evaluation studies are missing.\n\nTargeted Resource Creation. Guzm\u00e1n et al. (2019) develop evaluation datasets for lowresourced MT between English and Nepali, Sinhala, Khmer and Pashtolow. They highlight many problems with low-resourced translation: tokenization, content selection, and translation verification, illustrating increased difficulty translating from English into lowresourced languages, and highlight the ineffectiveness of accepted state-of-the-art techniques on morphologically-rich languages. Despite involving all agents of the MT process (Section 3), the study does not involve data curators or evaluators that understood the languages involved, and resorts to standard MT evaluation metrics. Additionally, how this effort-intensive approach would scale to more than a handful of languages remains an open question.",
            "score": 0.560216799279457,
            "section_title": "Multilingual",
            "char_start_offset": 4506,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 245,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 432,
                    "end": 449,
                    "matchedPaperCorpusId": "4929974"
                },
                {
                    "start": 696,
                    "end": 716,
                    "matchedPaperCorpusId": "216144650"
                },
                {
                    "start": 1196,
                    "end": 1216,
                    "matchedPaperCorpusId": "263883936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.249755859375
        },
        {
            "corpus_id": "270521667",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "text": "The datasets and models are publicly available.\n\nIn Kassner et al. (2021)'s work, the authors investigate the ability of multilingual BERT (mBERT) to serve as a multilingual knowledge base by translating English datasets (TREx and GoogleRE) into 53 languages.They propose a typed querying approach that outperforms the original fill-in-theblank method and show that mBERT exhibits varying performance across languages, with strong results for 21 languages and weak performance for 32 others.The study reveals language-specific biases in mBERT's predictions and demonstrates that pooling predictions across languages can improve performance, even surpassing monolingual English BERT.The paper highlights the importance of extending research on language models as knowledge bases to multiple languages for diversity and accessibility.Srinivasan et al. (2021) propose a method to predict the performance of multilingual models like mBERT and XLM-Roberta on languages and tasks for which evaluation datasets are unavailable.The authors train a regression model using XGBoost with features based on pretraining data size, typological features, and target-pivot language similarity to predict performance scores.They evaluate the predictor's ability to generalize to unseen languages and find that while the model performs well in predicting scores for languages seen during training, the performance drops when applied to new languages.The results suggest that improvements are needed for the predictor to replace the creation of test sets in new languages.However, the model is able to predict relative performance trends across languages and tasks effectively.XTREME-R (Ruder et al., 2021) is an improved multilingual benchmark covering 50 diverse languages and 10 challenging tasks, including retrieval from a multilingual pool.Experiments and analyses show that recent progress has been uneven across tasks, with gains concentrated on retrieval.Performance remains poor on low-resource languages and non-Latin scripts, with translate-train approaches generally performing best.XTREME-R introduces an extensible diagnostic and evaluation suite, including a massively multilingual test suite (MULTICHECKLIST) and fine-grained evaluation via EXPLAINABOARD.The benchmark aims to facilitate nuanced evaluation and comparison of multilingual models through an interactive public leader-board with detailed metadata.",
            "score": 0.5601411518432605,
            "section_title": "Multi-lingual performance evaluation",
            "char_start_offset": 55444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 49,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1944
                },
                {
                    "start": 1944,
                    "end": 2076
                },
                {
                    "start": 2076,
                    "end": 2252
                },
                {
                    "start": 2252,
                    "end": 2408
                }
            ],
            "ref_mentions": [
                {
                    "start": 1666,
                    "end": 1686,
                    "matchedPaperCorpusId": "233241004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "221641003",
            "title": "FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding",
            "text": "Cross-lingual Datasets Cross-lingual language understanding has been investigated for many NLP tasks, where the knowledge learned from a pivot language (e.g., English) is transferred to other languages indirectly, as labeled data in low-resource languages are often scarce. There exist many multilingual corpora for diverse NLP tasks. Nivre et al. (2016) released a collection of multilingual treebanks on universal dependencies for 33 languages. Pan et al. (2017) introduced cross-lingual name tagging and linking for 282 languages. Other multilingual datasets range over tasks such as document classification, natural language inference, information retrieval, paraphrase identification, and summarization (Klementiev, Titov, and Bhattarai 2012;Cer et al. 2 Our code is released at https://github.com/yuwfan/FILTER 2017; Conneau et al. 2018;Sasaki et al. 2018;Yang et al. 2019;Zhu et al. 2019).\n\nMore recent studies on open-domain question answering and machine reading comprehension also introduced cross-lingual datasets, such as MLQA (Lewis et al. 2020), XQuAD (Artetxe, Ruder, and Yogatama 2020), and Ty-DiQA (Clark et al. 2020). Most recently, XTREME (Hu et al. 2020) and XGLUE (Liang et al. 2020) released several datasets across multiple tasks, and set up public leaderboards for evaluating cross-lingual models. In this paper, we work on both XTREME (see Figure 1 for examples) and XGLUE to demonstrate the effectiveness of our proposed method.\n\nCross-lingual Models Most previous work tackles crosslingual problems in two fashions: (i) cross-lingual zeroshot transfer; and (ii) translate-train/test. For cross-lingual zero-shot transfer, models are trained on labeled data in the source language only, and directly evaluated on target languages. Early work focused on training multilingual word embeddings (Mikolov, Le, and Sutskever 2013;Faruqui and Dyer 2014;Xu et al. 2018), while more recent",
            "score": 0.5599790254017329,
            "section_title": "Related Work",
            "char_start_offset": 4969,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "17954486"
                },
                {
                    "start": 708,
                    "end": 747,
                    "matchedPaperCorpusId": "6758088"
                },
                {
                    "start": 823,
                    "end": 843,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 862,
                    "end": 879,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 879,
                    "end": 895,
                    "matchedPaperCorpusId": "202541473"
                },
                {
                    "start": 1066,
                    "end": 1101,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1115,
                    "end": 1134,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 1158,
                    "end": 1173,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "216868259",
            "title": "MLSUM: The Multilingual Summarization Corpus",
            "text": "The research community has produced several multilingual datasets for tasks other than summarization. We report two recent efforts below, noting that both i) rely on human translations, and ii) only provide evaluation data. \n\nThe Cross-Lingual NLI Corpus The SNLI corpus (Bowman et al., 2015) is a large scale dataset for natural language inference (NLI). It is composed of a collection of 570k human-written English sentence pairs, associated with their label, entailment, contradiction, or neutral. The Multi-Genre Natural Language Inference (MultiNLI) corpus is an extension of SNLI, comparable in size, but including a more diverse range of text. Conneau et al. (2018) introduced the Cross-Lingual NLI Corpus (XNLI) to evaluate transfer learning from English to other languages: based on MultiNLI, a collection of 5,000 test and 2,500 dev pairs were translated by humans in 15 languages. \n\nMLQA Given a paragraph and a question, the Question Answering (QA) task consists in providing the correct answer. Large scale datasets such as (Rajpurkar et al., 2016;Choi et al., 2018;Trischler et al., 2016) have driven fast progress.2 However, these datasets are only in English. To assess how well models perform on other languages, Lewis et al. (2019) recently proposed MLQA, an evaluation dataset for cross-lingual extractive QA composed of 5K QA instances in 7 languages. \n\nXTREME The Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark covers 40 languages over 9 tasks. The summarization task is not included in the benchmark. \n\nXGLUE In order to train and evaluate their performance across a diverse set of cross-lingual tasks, Liang et al. (2020) recently released XGLUE, covering both Natural Language Understanding and Generation scenarios. While no summarization task is included, it comprises a News Title Generation task: the data is crawled from a commercial news website and provided in form of article-title pairs for 5 languages (German, English, French, Spanish and Russian).",
            "score": 0.5598780103044106,
            "section_title": "Existing Multilingual Datasets",
            "char_start_offset": 5072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 1061,
                    "end": 1079,
                    "matchedPaperCorpusId": "202718969"
                },
                {
                    "start": 1079,
                    "end": 1102,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1647,
                    "end": 1666,
                    "matchedPaperCorpusId": "214794966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "paperId": "2ea80e07c187ab6f3b73b4d52d40143811a618c8",
            "corpusId": 271904057,
            "title": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.09701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277236689",
                    "name": "Mingda Li"
                },
                {
                    "authorId": "2277803290",
                    "name": "Abhijit Mishra"
                },
                {
                    "authorId": "1581473893",
                    "name": "Utkarsh Mujumdar"
                }
            ],
            "abstract": "The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CODELLAMA and CODEGEMMA, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and fine-tuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM's token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.",
            "corpus_id": "271904057",
            "text": "The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CODELLAMA and CODEGEMMA, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and fine-tuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM's token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.262939453125
        },
        {
            "paperId": "05b90590b1ef911703d83399ea1ff5f01faa44d5",
            "corpusId": 246285849,
            "title": "A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2022,
            "referenceCount": 48,
            "citationCount": 28,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2022/0606.pdf",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.10707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144783277",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "50251691",
                    "name": "Tao Ge"
                },
                {
                    "authorId": "2118866998",
                    "name": "Shuming Ma"
                },
                {
                    "authorId": "2109057340",
                    "name": "Jingjing Li"
                },
                {
                    "authorId": "49807919",
                    "name": "Furu Wei"
                },
                {
                    "authorId": "1781885",
                    "name": "Houfeng Wang"
                }
            ],
            "abstract": "Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pre-trained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language. \n\nOur approach creates diverse parallel GEC data without any language-specific operations by taking the non-autoregressive translation generated by PXLM and the gold translation as error-corrected sentence pairs. Then, we reuse PXLM to initialize the GEC model and pre-train it with the synthetic data generated by itself, which yields further improvement. We evaluate our approach on three public benchmarks of GEC in different languages. It achieves the state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian). Further analysis demonstrates that our data construction method is complementary to rule-based approaches.",
            "corpus_id": "246285849",
            "text": "Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pre-trained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language. \n\nOur approach creates diverse parallel GEC data without any language-specific operations by taking the non-autoregressive translation generated by PXLM and the gold translation as error-corrected sentence pairs. Then, we reuse PXLM to initialize the GEC model and pre-train it with the synthetic data generated by itself, which yields further improvement. We evaluate our approach on three public benchmarks of GEC in different languages. It achieves the state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian). Further analysis demonstrates that our data construction method is complementary to rule-based approaches.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.337158203125
        },
        {
            "paperId": "8bbea2a965fa1d66f17ac966fbd87ea38204fa15",
            "corpusId": 277562168,
            "title": "Cross-Lingual Adaptation for Vision-Language Model via Multimodal Semantic Distillation",
            "venue": "IEEE transactions on multimedia",
            "year": 2025,
            "referenceCount": 70,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2025.3557678?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2025.3557678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306845747",
                    "name": "Yu Weng"
                },
                {
                    "authorId": "2308850536",
                    "name": "Wenbin He"
                },
                {
                    "authorId": "2309753367",
                    "name": "Jun Dong"
                },
                {
                    "authorId": "2288851435",
                    "name": "Chaomurilige Wang"
                },
                {
                    "authorId": "2299282199",
                    "name": "Xuan Liu"
                },
                {
                    "authorId": "2299362413",
                    "name": "Zheng Liu"
                }
            ],
            "abstract": "Large Multimodal Models (LMMs) excel in English multimedia tasks but face challenges in adapting to other languages due to linguistic diversity, limited non-English multimodal data, and high training costs. Existing approaches rely on machine-translated multimodal corpora or multilingual large language models, yet they demand substantial resources and achieve only modest zero-shot cross-lingual transfer performance, as shown in the IGLUE benchmark. In this work, we propose SMSA, a Syntax-aware Multimodal Semantic Adaptation approach, which efficiently extends vision-language models (VLMs) to multiple languages via a lightweight adaptation module. Instead of learning from scratch, SMSA transfers multimodal knowledge from English-trained models using two key components: (1) a Syntax-aware Adapter (SAA), which restructures multilingual text representations to align better with English syntax, reducing cross-lingual misalignment; (2) a Multimodal Semantic Distillation (MSD) method, which enables the model to mimic English sequence processing and retain multimodal associations across languages. This allows efficient adaptation to new languages while preserving the original model's strong multimodal capabilities. We extend an MoE-based VLM to 8 languages using a small translation dataset. Evaluations on the IGLUE benchmark show that SMSA achieves strong zero-shot transfer, outperforming some multilingual LMMs and demonstrating its effectiveness in cross-lingual vision-language adaptation.",
            "corpus_id": "277562168",
            "text": "Large Multimodal Models (LMMs) excel in English multimedia tasks but face challenges in adapting to other languages due to linguistic diversity, limited non-English multimodal data, and high training costs. Existing approaches rely on machine-translated multimodal corpora or multilingual large language models, yet they demand substantial resources and achieve only modest zero-shot cross-lingual transfer performance, as shown in the IGLUE benchmark. In this work, we propose SMSA, a Syntax-aware Multimodal Semantic Adaptation approach, which efficiently extends vision-language models (VLMs) to multiple languages via a lightweight adaptation module. Instead of learning from scratch, SMSA transfers multimodal knowledge from English-trained models using two key components: (1) a Syntax-aware Adapter (SAA), which restructures multilingual text representations to align better with English syntax, reducing cross-lingual misalignment; (2) a Multimodal Semantic Distillation (MSD) method, which enables the model to mimic English sequence processing and retain multimodal associations across languages. This allows efficient adaptation to new languages while preserving the original model's strong multimodal capabilities. We extend an MoE-based VLM to 8 languages using a small translation dataset. Evaluations on the IGLUE benchmark show that SMSA achieves strong zero-shot transfer, outperforming some multilingual LMMs and demonstrating its effectiveness in cross-lingual vision-language adaptation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.345947265625
        },
        {
            "paperId": "70a79ded7818ba8ae807102b00643e331e344ee8",
            "corpusId": 232478575,
            "title": "UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2021,
            "referenceCount": 55,
            "citationCount": 92,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2104.00332",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.00332, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152176855",
                    "name": "Mingyang Zhou"
                },
                {
                    "authorId": "2116644664",
                    "name": "Luowei Zhou"
                },
                {
                    "authorId": "2992833",
                    "name": "Shuohang Wang"
                },
                {
                    "authorId": "153655416",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "2107923860",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2116680369",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "46700348",
                    "name": "Jingjing Liu"
                }
            ],
            "abstract": "Vision-and-language pre-training has achieved impressive success in learning multimodal representations between vision and language. To generalize this success to non-English languages, we introduce UC2, the first machine translation-augmented framework for cross-lingual cross-modal representation learning. To tackle the scarcity problem of multilingual captions for image datasets, we first augment existing English-only datasets with other languages via machine translation (MT). Then we extend the standard Masked Language Modeling and Image-Text Matching training objectives to multilingual setting, where alignment between different languages is captured through shared visual context (i.e., using image as pivot). To facilitate the learning of a joint embedding space of images and all languages of interest, we further propose two novel pre-training tasks, namely Masked Region-to-Token Modeling (MRTM) and Visual Translation Language Modeling (VTLM), leveraging MT-enhanced translated data. Evaluation on multilingual image-text retrieval and multilingual visual question answering benchmarks demonstrates that our proposed framework achieves new state of the art on diverse non-English benchmarks while maintaining comparable performance to monolingual pre-trained models on English tasks.",
            "corpus_id": "232478575",
            "text": "Vision-and-language pre-training has achieved impressive success in learning multimodal representations between vision and language. To generalize this success to non-English languages, we introduce UC2, the first machine translation-augmented framework for cross-lingual cross-modal representation learning. To tackle the scarcity problem of multilingual captions for image datasets, we first augment existing English-only datasets with other languages via machine translation (MT). Then we extend the standard Masked Language Modeling and Image-Text Matching training objectives to multilingual setting, where alignment between different languages is captured through shared visual context (i.e., using image as pivot). To facilitate the learning of a joint embedding space of images and all languages of interest, we further propose two novel pre-training tasks, namely Masked Region-to-Token Modeling (MRTM) and Visual Translation Language Modeling (VTLM), leveraging MT-enhanced translated data. Evaluation on multilingual image-text retrieval and multilingual visual question answering benchmarks demonstrates that our proposed framework achieves new state of the art on diverse non-English benchmarks while maintaining comparable performance to monolingual pre-trained models on English tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.493408203125
        },
        {
            "paperId": "c95eddb6dddeec0d2b643985deecbdf5985548e2",
            "corpusId": 270620206,
            "title": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "referenceCount": 38,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66794253",
                    "name": "G. Ghazaryan"
                },
                {
                    "authorId": "2159004",
                    "name": "Erik Arakelyan"
                },
                {
                    "authorId": "2294362638",
                    "name": "Pasquale Minervini"
                },
                {
                    "authorId": "1736067",
                    "name": "Isabelle Augenstein"
                }
            ],
            "abstract": "Question Answering (QA) datasets have been instrumental in developing and evaluating Large Language Model (LLM) capabilities. However, such datasets are scarce for languages other than English due to the cost and difficulties of collection and manual annotation. This means that producing novel models and measuring the performance of multilingual LLMs in low-resource languages is challenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a method for generating and validating QA datasets for low-resource languages. We utilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs between English and the target language. We use the English data as context to $\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality. Combining these with their designated non-English $\\textit{human-curated}$ paragraphs form the final QA dataset. The method allows to maintain the content quality, reduces the likelihood of factual errors, and circumvents the need for costly annotation. To test the method, we created a QA dataset with $1.2$K samples for the Armenian language. The human evaluation shows that $98\\%$ of the generated English data maintains quality and diversity in the question types and topics, while the translation validation pipeline can filter out $\\sim70\\%$ of data with poor quality. We use the dataset to benchmark state-of-the-art LLMs, showing their inability to achieve human accuracy with some model performances closer to random chance. This shows that the generated dataset is non-trivial and can be used to evaluate reasoning capabilities in low-resource language.",
            "corpus_id": "270620206",
            "text": "Question Answering (QA) datasets have been instrumental in developing and evaluating Large Language Model (LLM) capabilities. However, such datasets are scarce for languages other than English due to the cost and difficulties of collection and manual annotation. This means that producing novel models and measuring the performance of multilingual LLMs in low-resource languages is challenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a method for generating and validating QA datasets for low-resource languages. We utilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs between English and the target language. We use the English data as context to $\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality. Combining these with their designated non-English $\\textit{human-curated}$ paragraphs form the final QA dataset. The method allows to maintain the content quality, reduces the likelihood of factual errors, and circumvents the need for costly annotation. To test the method, we created a QA dataset with $1.2$K samples for the Armenian language. The human evaluation shows that $98\\%$ of the generated English data maintains quality and diversity in the question types and topics, while the translation validation pipeline can filter out $\\sim70\\%$ of data with poor quality. We use the dataset to benchmark state-of-the-art LLMs, showing their inability to achieve human accuracy with some model performances closer to random chance. This shows that the generated dataset is non-trivial and can be used to evaluate reasoning capabilities in low-resource language.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.415283203125
        },
        {
            "paperId": "af14f732e59955e6208c316b9e307c26d7a94265",
            "corpusId": 279392012,
            "title": "Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation",
            "venue": "",
            "year": 2025,
            "referenceCount": 36,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.11820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356794252",
                    "name": "Xintong Wang"
                },
                {
                    "authorId": "2294143394",
                    "name": "Jingheng Pan"
                },
                {
                    "authorId": "2362614757",
                    "name": "Yixiao Liu"
                },
                {
                    "authorId": "2333958548",
                    "name": "Xiaohu Zhao"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2327995656",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2259967124",
                    "name": "Christian Biemann"
                },
                {
                    "authorId": "2302342302",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "2334573168",
                    "name": "Linlong Xu"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "Vision-Language Translation (VLT) is a challenging task that requires accurately recognizing multilingual text embedded in images and translating it into the target language with the support of visual context. While recent Large Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual understanding capabilities, there is a lack of systematic evaluation and understanding of their performance on VLT. In this work, we present a comprehensive study of VLT from three key perspectives: data quality, model architecture, and evaluation metrics. (1) We identify critical limitations in existing datasets, particularly in semantic and cultural fidelity, and introduce AibTrans -- a multilingual, parallel, human-verified dataset with OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6 state-of-the-art open-source models across end-to-end and cascaded architectures, revealing their OCR dependency and contrasting generation versus reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric reliability issues under varying contextual complexity, introducing the DA Score as a more robust measure of translation quality. Building upon these findings, we establish a new evaluation benchmark for VLT. Notably, we observe that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual performance, and we propose a balanced multilingual fine-tuning strategy that effectively adapts LVLMs to VLT without sacrificing their generalization ability.",
            "corpus_id": "279392012",
            "text": "Vision-Language Translation (VLT) is a challenging task that requires accurately recognizing multilingual text embedded in images and translating it into the target language with the support of visual context. While recent Large Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual understanding capabilities, there is a lack of systematic evaluation and understanding of their performance on VLT. In this work, we present a comprehensive study of VLT from three key perspectives: data quality, model architecture, and evaluation metrics. (1) We identify critical limitations in existing datasets, particularly in semantic and cultural fidelity, and introduce AibTrans -- a multilingual, parallel, human-verified dataset with OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6 state-of-the-art open-source models across end-to-end and cascaded architectures, revealing their OCR dependency and contrasting generation versus reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric reliability issues under varying contextual complexity, introducing the DA Score as a more robust measure of translation quality. Building upon these findings, we establish a new evaluation benchmark for VLT. Notably, we observe that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual performance, and we propose a balanced multilingual fine-tuning strategy that effectively adapts LVLMs to VLT without sacrificing their generalization ability.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.53076171875
        },
        {
            "paperId": "929ba5466d41359264e27345bae5a89a410ab2fe",
            "corpusId": 278695226,
            "title": "A Novel Approach to Continual Knowledge Transfer in Multilingual Neural Machine Translation Using Autoregressive and Non-Autoregressive Models for Indic Languages",
            "venue": "IEEE Access",
            "year": 2025,
            "referenceCount": 94,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3570699?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3570699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "120293160",
                    "name": "Shailashree K. Sheshadri"
                },
                {
                    "authorId": "2267712848",
                    "name": "Deepa Gupta"
                },
                {
                    "authorId": "2361955822",
                    "name": "Biswajit Paul"
                },
                {
                    "authorId": "2363738767",
                    "name": "J. S. Bhavani"
                }
            ],
            "abstract": "Recent progress in multilingual pre-trained models has significantly improved translation quality for Indic languages. However, extending these models to new languages via fine-tuning or retraining remains computationally costly and often leads to parameter interference, degrading performance on previously learned or typologically distant languages. While continual learning offers a promising alternative for incremental language addition, its application in Indic contexts is still limited and faces challenges in generalization across diverse linguistic settings. To overcome these issues, we propose a Continual Knowledge Transfer (CKT) framework for efficient and scalable multilingual adaptation. CKT is realized in both autoregressive (MNMT) and non-autoregressive (Switch-GLAT) architectures, yielding two variants: MNMT+CKT and Switch-GLAT+CKT. Rather than retraining the entire model, CKT freezes the multilingual base and updates only parameters relevant to the newly added language. Key innovations include gradient-based knowledge pruning, sequential teacher integration, and dynamic vocabulary expansion for minimizing interference and maximizing cross-lingual retention. Comprehensive evaluations on the IN22-Conv and IN22-Gen benchmark datasets demonstrate that both MNMT+CKT and Switch-GLAT+CKT consistently outperform established baselines, such as IndicTrans2, Google Translate, GPT-4-32K, LLaMA-2-17B, and NLIP-LAB-IITH. The proposed multi-step distillation approach, MNMT+CKT, consistently outperforms conventional fine-tuning and Knowledge Transfer (MNMT+KT) strategies for incremental adaptation of linguistically diverse Indic languages. On IN22-Conv, BLEU improvements range from +4.93 (Kashmiri <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> English) to +11.48 (Assamese <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> English) and similar improvements are seen for IN22-Gen. The method also achieves substantial reductions in trainable parameters\u201419.80% (Nepali) to 66.87% (Kannada)\u2014while enabling up to <inline-formula> <tex-math notation=\"LaTeX\">$4x$ </tex-math></inline-formula> faster inference when integrated with the Switch-GLAT architecture. Among the two, Switch- GLAT+CKT achieves the highest BLEU scores across all language pairs. In the English <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> Indic translation direction, BLEU gains range from +9.12 (Kannada) to +26.00 (Nepali), while in the Indic <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> English direction, gains range from +0.07 (Odia) to +3.78 (Assamese). Furthermore, ablation studies and sequential integration of multilingual teacher models reveal that CKT significantly reduces the number of trainable parameters required for each incremental step.",
            "corpus_id": "278695226",
            "text": "Recent progress in multilingual pre-trained models has significantly improved translation quality for Indic languages. However, extending these models to new languages via fine-tuning or retraining remains computationally costly and often leads to parameter interference, degrading performance on previously learned or typologically distant languages. While continual learning offers a promising alternative for incremental language addition, its application in Indic contexts is still limited and faces challenges in generalization across diverse linguistic settings. To overcome these issues, we propose a Continual Knowledge Transfer (CKT) framework for efficient and scalable multilingual adaptation. CKT is realized in both autoregressive (MNMT) and non-autoregressive (Switch-GLAT) architectures, yielding two variants: MNMT+CKT and Switch-GLAT+CKT. Rather than retraining the entire model, CKT freezes the multilingual base and updates only parameters relevant to the newly added language. Key innovations include gradient-based knowledge pruning, sequential teacher integration, and dynamic vocabulary expansion for minimizing interference and maximizing cross-lingual retention. Comprehensive evaluations on the IN22-Conv and IN22-Gen benchmark datasets demonstrate that both MNMT+CKT and Switch-GLAT+CKT consistently outperform established baselines, such as IndicTrans2, Google Translate, GPT-4-32K, LLaMA-2-17B, and NLIP-LAB-IITH. The proposed multi-step distillation approach, MNMT+CKT, consistently outperforms conventional fine-tuning and Knowledge Transfer (MNMT+KT) strategies for incremental adaptation of linguistically diverse Indic languages. On IN22-Conv, BLEU improvements range from +4.93 (Kashmiri <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> English) to +11.48 (Assamese <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> English) and similar improvements are seen for IN22-Gen. The method also achieves substantial reductions in trainable parameters\u201419.80% (Nepali) to 66.87% (Kannada)\u2014while enabling up to <inline-formula> <tex-math notation=\"LaTeX\">$4x$ </tex-math></inline-formula> faster inference when integrated with the Switch-GLAT architecture. Among the two, Switch- GLAT+CKT achieves the highest BLEU scores across all language pairs. In the English <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> Indic translation direction, BLEU gains range from +9.12 (Kannada) to +26.00 (Nepali), while in the Indic <inline-formula> <tex-math notation=\"LaTeX\">$\\rightarrow $ </tex-math></inline-formula> English direction, gains range from +0.07 (Odia) to +3.78 (Assamese). Furthermore, ablation studies and sequential integration of multilingual teacher models reveal that CKT significantly reduces the number of trainable parameters required for each incremental step.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.17626953125
        },
        {
            "paperId": "0d5124d1fb7f21aa2efc0ae234feab97e8a23208",
            "corpusId": 258841008,
            "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 63,
            "citationCount": 46,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13632",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2159539050",
                    "name": "Yifu Qiu"
                },
                {
                    "authorId": "7264689",
                    "name": "Yftah Ziser"
                },
                {
                    "authorId": "145762466",
                    "name": "A. Korhonen"
                },
                {
                    "authorId": "3381663",
                    "name": "E. Ponti"
                },
                {
                    "authorId": "40146204",
                    "name": "Shay B. Cohen"
                }
            ],
            "abstract": "Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed loss weighting method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ.",
            "corpus_id": "258841008",
            "text": "Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed loss weighting method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.311279296875
        },
        {
            "paperId": "df126fff32094fccc0ddd3d39929845bd3e199d3",
            "corpusId": 273877524,
            "title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code",
            "venue": "",
            "year": 2024,
            "referenceCount": 89,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.04794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267332991",
                    "name": "Yuxin Zuo"
                },
                {
                    "authorId": "2329593105",
                    "name": "Wenxuan Jiang"
                },
                {
                    "authorId": "2265516393",
                    "name": "Wenxuan Liu"
                },
                {
                    "authorId": "46947005",
                    "name": "Zixuan Li"
                },
                {
                    "authorId": "2075398318",
                    "name": "Long Bai"
                },
                {
                    "authorId": "2329617551",
                    "name": "Hanbin Wang"
                },
                {
                    "authorId": "122290781",
                    "name": "Yutao Zeng"
                },
                {
                    "authorId": "2149111400",
                    "name": "Xiaolong Jin"
                },
                {
                    "authorId": "1777025",
                    "name": "J. Guo"
                },
                {
                    "authorId": "2244825947",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\\% and SoTA by 20.03\\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder",
            "corpus_id": "273877524",
            "text": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\\% and SoTA by 20.03\\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.342041015625
        },
        {
            "paperId": "457987f1e7c22e193a8d0dd3704781b3a51b977f",
            "corpusId": 249017870,
            "title": "DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "referenceCount": 57,
            "citationCount": 8,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.emnlp-main.532.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.12215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1897770594",
                    "name": "Gabriele Sarti"
                },
                {
                    "authorId": "3242253",
                    "name": "Arianna Bisazza"
                },
                {
                    "authorId": "1899559",
                    "name": "Ana Guerberof Arenas"
                },
                {
                    "authorId": "2065048323",
                    "name": "Antonio Toral"
                }
            ],
            "abstract": "We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.",
            "corpus_id": "249017870",
            "text": "We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1435546875
        },
        {
            "paperId": "9455c82d26b4761ba5ee26a5d6674ba692aa0fb4",
            "corpusId": 270440174,
            "title": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive Evaluation Framework with CUDRT",
            "venue": "",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299329349",
                    "name": "Zhen Tao"
                },
                {
                    "authorId": "2299389906",
                    "name": "Yanfang Chen"
                },
                {
                    "authorId": "2128365150",
                    "name": "Dinghao Xi"
                },
                {
                    "authorId": "2279173209",
                    "name": "Zhiyu Li"
                },
                {
                    "authorId": "2261892814",
                    "name": "Wei Xu"
                }
            ],
            "abstract": "The increasing prevalence of large language models (LLMs) has significantly advanced text generation, but the human-like quality of LLM outputs presents major challenges in reliably distinguishing between human-authored and LLM-generated texts. Existing detection benchmarks are constrained by their reliance on static datasets, scenario-specific tasks (e.g., question answering and text refinement), and a primary focus on English, overlooking the diverse linguistic and operational subtleties of LLMs. To address these gaps, we propose CUDRT, a comprehensive evaluation framework and bilingual benchmark in Chinese and English, categorizing LLM activities into five key operations: Create, Update, Delete, Rewrite, and Translate. CUDRT provides extensive datasets tailored to each operation, featuring outputs from state-of-the-art LLMs to assess the reliability of LLM-generated text detectors. This framework supports scalable, reproducible experiments and enables in-depth analysis of how operational diversity, multilingual training sets, and LLM architectures influence detection performance. Our extensive experiments demonstrate the framework's capacity to optimize detection systems, providing critical insights to enhance reliability, cross-linguistic adaptability, and detection accuracy. By advancing robust methodologies for identifying LLM-generated texts, this work contributes to the development of intelligent systems capable of meeting real-world multilingual detection challenges. Source code and dataset are available at GitHub.",
            "corpus_id": "270440174",
            "text": "The increasing prevalence of large language models (LLMs) has significantly advanced text generation, but the human-like quality of LLM outputs presents major challenges in reliably distinguishing between human-authored and LLM-generated texts. Existing detection benchmarks are constrained by their reliance on static datasets, scenario-specific tasks (e.g., question answering and text refinement), and a primary focus on English, overlooking the diverse linguistic and operational subtleties of LLMs. To address these gaps, we propose CUDRT, a comprehensive evaluation framework and bilingual benchmark in Chinese and English, categorizing LLM activities into five key operations: Create, Update, Delete, Rewrite, and Translate. CUDRT provides extensive datasets tailored to each operation, featuring outputs from state-of-the-art LLMs to assess the reliability of LLM-generated text detectors. This framework supports scalable, reproducible experiments and enables in-depth analysis of how operational diversity, multilingual training sets, and LLM architectures influence detection performance. Our extensive experiments demonstrate the framework's capacity to optimize detection systems, providing critical insights to enhance reliability, cross-linguistic adaptability, and detection accuracy. By advancing robust methodologies for identifying LLM-generated texts, this work contributes to the development of intelligent systems capable of meeting real-world multilingual detection challenges. Source code and dataset are available at GitHub.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.494384765625
        },
        {
            "paperId": "e156e15dc1bf50986267e79420f69300da09d2f9",
            "corpusId": 269502611,
            "title": "UQA: Corpus for Urdu Question Answering",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2024,
            "referenceCount": 26,
            "citationCount": 7,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299329602",
                    "name": "Samee Arif"
                },
                {
                    "authorId": "2299328342",
                    "name": "Sualeha Farid"
                },
                {
                    "authorId": "2299330526",
                    "name": "Awais Athar"
                },
                {
                    "authorId": "2046780",
                    "name": "Agha Ali Raza"
                }
            ],
            "abstract": "This paper introduces UQA, a novel dataset for question answering and text comprehension in Urdu, a low-resource language with over 70 million native speakers. UQA is generated by translating the Stanford Question Answering Dataset (SQuAD2.0), a large-scale English QA dataset, using a technique called EATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in the translated context paragraphs. The paper describes the process of selecting and evaluating the best translation model among two candidates: Google Translator and Seamless M4T. The paper also benchmarks several state-of-the-art multilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and reports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and 74.56 EM. UQA is a valuable resource for developing and testing multilingual NLP systems for Urdu and for enhancing the cross-lingual transferability of existing models. Further, the paper demonstrates the effectiveness of EATS for creating high-quality datasets for other languages and domains. The UQA dataset and the code are publicly available at www.github.com/sameearif/UQA",
            "corpus_id": "269502611",
            "text": "This paper introduces UQA, a novel dataset for question answering and text comprehension in Urdu, a low-resource language with over 70 million native speakers. UQA is generated by translating the Stanford Question Answering Dataset (SQuAD2.0), a large-scale English QA dataset, using a technique called EATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in the translated context paragraphs. The paper describes the process of selecting and evaluating the best translation model among two candidates: Google Translator and Seamless M4T. The paper also benchmarks several state-of-the-art multilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and reports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and 74.56 EM. UQA is a valuable resource for developing and testing multilingual NLP systems for Urdu and for enhancing the cross-lingual transferability of existing models. Further, the paper demonstrates the effectiveness of EATS for creating high-quality datasets for other languages and domains. The UQA dataset and the code are publicly available at www.github.com/sameearif/UQA",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.318359375
        },
        {
            "paperId": "8b57632b19df9ebd3e57e3adbef7fc6ec93bc506",
            "corpusId": 268856712,
            "title": "HyperCLOVA X Technical Report",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 77,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31760501",
                    "name": "Kang Min Yoo"
                },
                {
                    "authorId": "2294597771",
                    "name": "Jaegeun Han"
                },
                {
                    "authorId": "80937486",
                    "name": "Sookyo In"
                },
                {
                    "authorId": "2294572963",
                    "name": "Heewon Jeon"
                },
                {
                    "authorId": "2115678107",
                    "name": "Jisu Jeong"
                },
                {
                    "authorId": "35518563",
                    "name": "Jaewook Kang"
                },
                {
                    "authorId": "2294667967",
                    "name": "Hyunwook Kim"
                },
                {
                    "authorId": "2294668918",
                    "name": "Kyung-Min Kim"
                },
                {
                    "authorId": "2294601212",
                    "name": "Munhyong Kim"
                },
                {
                    "authorId": "2144275606",
                    "name": "Sungju Kim"
                },
                {
                    "authorId": "10469987",
                    "name": "Donghyun Kwak"
                },
                {
                    "authorId": "3434480",
                    "name": "Hanock Kwak"
                },
                {
                    "authorId": "12693169",
                    "name": "S. Kwon"
                },
                {
                    "authorId": "2294682578",
                    "name": "Bado Lee"
                },
                {
                    "authorId": "2294510209",
                    "name": "Dongsoo Lee"
                },
                {
                    "authorId": "26419196",
                    "name": "Gichang Lee"
                },
                {
                    "authorId": "2295294595",
                    "name": "Jooho Lee"
                },
                {
                    "authorId": "120751934",
                    "name": "Baeseong Park"
                },
                {
                    "authorId": "2271068659",
                    "name": "Seongjin Shin"
                },
                {
                    "authorId": "2294305074",
                    "name": "Joonsang Yu"
                },
                {
                    "authorId": "2218730229",
                    "name": "Seolki Baek"
                },
                {
                    "authorId": "2294570168",
                    "name": "Sumin Byeon"
                },
                {
                    "authorId": "2294573188",
                    "name": "Eungsup Cho"
                },
                {
                    "authorId": "2294571485",
                    "name": "Dooseok Choe"
                },
                {
                    "authorId": "2294597338",
                    "name": "Jeesung Han"
                },
                {
                    "authorId": "2294672195",
                    "name": "Youngkyun Jin"
                },
                {
                    "authorId": "2294567225",
                    "name": "Hyein Jun"
                },
                {
                    "authorId": "2294684040",
                    "name": "Jaeseung Jung"
                },
                {
                    "authorId": "2294853769",
                    "name": "Chanwoong Kim"
                },
                {
                    "authorId": "2294598511",
                    "name": "Jinhong Kim"
                },
                {
                    "authorId": "2294595143",
                    "name": "Jinuk Kim"
                },
                {
                    "authorId": "2294682239",
                    "name": "Dokyeong Lee"
                },
                {
                    "authorId": "2294812715",
                    "name": "Dongwook Park"
                },
                {
                    "authorId": "2294572181",
                    "name": "Jeong Min Sohn"
                },
                {
                    "authorId": "2294669308",
                    "name": "Sujung Han"
                },
                {
                    "authorId": "2150286766",
                    "name": "J. Heo"
                },
                {
                    "authorId": "2294684441",
                    "name": "S. Hong"
                },
                {
                    "authorId": "2294563035",
                    "name": "Mina Jeon"
                },
                {
                    "authorId": "2294601197",
                    "name": "Hyunhoon Jung"
                },
                {
                    "authorId": "2294680138",
                    "name": "Jungeun Jung"
                },
                {
                    "authorId": "2197078471",
                    "name": "Wangkyo Jung"
                },
                {
                    "authorId": "2294675746",
                    "name": "Chungjoon Kim"
                },
                {
                    "authorId": "2294837934",
                    "name": "Hyeri Kim"
                },
                {
                    "authorId": "2267507172",
                    "name": "Jonghyun Kim"
                },
                {
                    "authorId": "2291456466",
                    "name": "Min Young Kim"
                },
                {
                    "authorId": "2294597229",
                    "name": "Soeun Lee"
                },
                {
                    "authorId": "2243044802",
                    "name": "Joonhee Park"
                },
                {
                    "authorId": "2116387969",
                    "name": "Jieun Shin"
                },
                {
                    "authorId": "2294679997",
                    "name": "Sojin Yang"
                },
                {
                    "authorId": "2264532196",
                    "name": "Jung-Ki Yoon"
                },
                {
                    "authorId": "2271303845",
                    "name": "Hwaran Lee"
                },
                {
                    "authorId": "152846184",
                    "name": "Sanghwan Bae"
                },
                {
                    "authorId": "2294563209",
                    "name": "Jeehwan Cha"
                },
                {
                    "authorId": "2213970589",
                    "name": "Dong-Ui Ham"
                },
                {
                    "authorId": "145424106",
                    "name": "Youngki Hong"
                },
                {
                    "authorId": "2294676853",
                    "name": "Yunki Hong"
                },
                {
                    "authorId": "2294563857",
                    "name": "Myunggeun Ji"
                },
                {
                    "authorId": "2294682449",
                    "name": "Yeguk Jin"
                },
                {
                    "authorId": "2294562740",
                    "name": "Chansong Jo"
                },
                {
                    "authorId": "2165899518",
                    "name": "Shinyoung Joo"
                },
                {
                    "authorId": "2110516576",
                    "name": "S. Jung"
                },
                {
                    "authorId": "2294812754",
                    "name": "Hyomin Kim"
                },
                {
                    "authorId": "2283442113",
                    "name": "Jungwhan Kim"
                },
                {
                    "authorId": "2294597574",
                    "name": "Minkyoung Kim"
                },
                {
                    "authorId": "2294601557",
                    "name": "Minseung Kim"
                },
                {
                    "authorId": "2829848",
                    "name": "Sungdong Kim"
                },
                {
                    "authorId": "2294676152",
                    "name": "Yonghee Kim"
                },
                {
                    "authorId": "2257352064",
                    "name": "Youngjun Kim"
                },
                {
                    "authorId": "17733849",
                    "name": "Donghyeon Ko"
                },
                {
                    "authorId": "2294679181",
                    "name": "Dughyun Lee"
                },
                {
                    "authorId": "2283510187",
                    "name": "Jaehong Lee"
                },
                {
                    "authorId": "2294674979",
                    "name": "Jieun Lee"
                },
                {
                    "authorId": "2294674839",
                    "name": "Jongjin Lee"
                },
                {
                    "authorId": "2294848361",
                    "name": "M. Lee"
                },
                {
                    "authorId": "2294717413",
                    "name": "Yehbin Lee"
                },
                {
                    "authorId": "2276522922",
                    "name": "Taehong Min"
                },
                {
                    "authorId": "2276522858",
                    "name": "Kiyoon Moon"
                },
                {
                    "authorId": "2294838361",
                    "name": "Jaesun Park"
                },
                {
                    "authorId": "2294750231",
                    "name": "Kyuyon Park"
                },
                {
                    "authorId": "2294563567",
                    "name": "Seunghyun Seo"
                },
                {
                    "authorId": "2294563480",
                    "name": "Gyubin Son"
                },
                {
                    "authorId": "2294569329",
                    "name": "Wonjoon Yoo"
                },
                {
                    "authorId": "2294564794",
                    "name": "Myungin You"
                },
                {
                    "authorId": "2294566256",
                    "name": "Doheon Ahn"
                },
                {
                    "authorId": "2294570118",
                    "name": "Homin Ahn"
                },
                {
                    "authorId": "2294564678",
                    "name": "Joohee Ahn"
                },
                {
                    "authorId": "2294775278",
                    "name": "Seongmin Ahn"
                },
                {
                    "authorId": "2294569916",
                    "name": "Chanwoo An"
                },
                {
                    "authorId": "2294572192",
                    "name": "Hyeryun An"
                },
                {
                    "authorId": "2295293249",
                    "name": "Junho An"
                },
                {
                    "authorId": "2294604293",
                    "name": "Sang-Min An"
                },
                {
                    "authorId": "2294565047",
                    "name": "Boram Byun"
                },
                {
                    "authorId": "2294563211",
                    "name": "Jongho Cha"
                },
                {
                    "authorId": "10240121",
                    "name": "M. Chang"
                },
                {
                    "authorId": "2287810891",
                    "name": "Seunggyu Chang"
                },
                {
                    "authorId": "2294679474",
                    "name": "Haesong Cho"
                },
                {
                    "authorId": "2294809237",
                    "name": "Youngdo Cho"
                },
                {
                    "authorId": "2294680680",
                    "name": "Dalnim Choi"
                },
                {
                    "authorId": "2294680682",
                    "name": "Daseul Choi"
                },
                {
                    "authorId": "2294670793",
                    "name": "Hyoseok Choi"
                },
                {
                    "authorId": "2184321837",
                    "name": "M. Choi"
                },
                {
                    "authorId": "2276316802",
                    "name": "Sangho Choi"
                },
                {
                    "authorId": "2155629887",
                    "name": "Seongjae Choi"
                },
                {
                    "authorId": "2295115680",
                    "name": "Wooyong Choi"
                },
                {
                    "authorId": "2294564651",
                    "name": "Sewhan Chun"
                },
                {
                    "authorId": "2294564327",
                    "name": "Dong Young Go"
                },
                {
                    "authorId": "1594024058",
                    "name": "Chiheon Ham"
                },
                {
                    "authorId": "2294607020",
                    "name": "Danbi Han"
                },
                {
                    "authorId": "2294597773",
                    "name": "Jaemin Han"
                },
                {
                    "authorId": "2294563673",
                    "name": "Mihak Hong"
                },
                {
                    "authorId": "2294563675",
                    "name": "Moonyoung Hong"
                },
                {
                    "authorId": "2294684441",
                    "name": "S. Hong"
                },
                {
                    "authorId": "2261425908",
                    "name": "S. Hwang"
                },
                {
                    "authorId": "2294569821",
                    "name": "Eunbin Hyun"
                },
                {
                    "authorId": "2294569577",
                    "name": "Jinbae Im"
                },
                {
                    "authorId": "2295236471",
                    "name": "Jaehyung Jang"
                },
                {
                    "authorId": "2295236473",
                    "name": "Jaeni Jang"
                },
                {
                    "authorId": "2282452119",
                    "name": "Si-Hyong Jang"
                },
                {
                    "authorId": "2177452975",
                    "name": "Sung-Kook Jang"
                },
                {
                    "authorId": "2294564383",
                    "name": "Joonha Jeon"
                },
                {
                    "authorId": "144159178",
                    "name": "Yujin Jeon"
                },
                {
                    "authorId": "2268841115",
                    "name": "Daun Jeong"
                },
                {
                    "authorId": "2273708723",
                    "name": "Joonhyun Jeong"
                },
                {
                    "authorId": "2283309282",
                    "name": "Kyeongseok Jeong"
                },
                {
                    "authorId": "2294570575",
                    "name": "Mini Jeong"
                },
                {
                    "authorId": "2294756987",
                    "name": "Yeji Jeong"
                },
                {
                    "authorId": "2294814701",
                    "name": "Sol Jin"
                },
                {
                    "authorId": "2294570504",
                    "name": "Hanbyeol Jo"
                },
                {
                    "authorId": "2294570506",
                    "name": "Hanju Jo"
                },
                {
                    "authorId": "2287924544",
                    "name": "Minjung Jo"
                },
                {
                    "authorId": "2100860030",
                    "name": "Lee Jonghyun"
                },
                {
                    "authorId": "2294566359",
                    "name": "Chaeyoon Jung"
                },
                {
                    "authorId": "2274097733",
                    "name": "Hyungsik Jung"
                },
                {
                    "authorId": "2294684042",
                    "name": "Jaeuk Jung"
                },
                {
                    "authorId": "2294680140",
                    "name": "Juyoung Jung"
                },
                {
                    "authorId": "2295078867",
                    "name": "Kwangsun Jung"
                },
                {
                    "authorId": "1753223653",
                    "name": "Seungjae Jung"
                },
                {
                    "authorId": "31650913",
                    "name": "Soonwon Ka"
                },
                {
                    "authorId": "2295073404",
                    "name": "Donghan Kang"
                },
                {
                    "authorId": "2149085001",
                    "name": "Soyoung Kang"
                },
                {
                    "authorId": "2244620205",
                    "name": "Taeho Kil"
                },
                {
                    "authorId": "2295255498",
                    "name": "Areum Kim"
                },
                {
                    "authorId": "2115287422",
                    "name": "Beomyoung Kim"
                },
                {
                    "authorId": "46239568",
                    "name": "Byeongwook Kim"
                },
                {
                    "authorId": "2244640050",
                    "name": "Daehee Kim"
                },
                {
                    "authorId": "2294677148",
                    "name": "Donggook Kim"
                },
                {
                    "authorId": "2294677148",
                    "name": "Donggook Kim"
                },
                {
                    "authorId": "2294684232",
                    "name": "Donghyun Kim"
                },
                {
                    "authorId": "2294670624",
                    "name": "Euna Kim"
                },
                {
                    "authorId": "2294670846",
                    "name": "Eunchul Kim"
                },
                {
                    "authorId": "2244820077",
                    "name": "Geewook Kim"
                },
                {
                    "authorId": "2294668676",
                    "name": "Gyu Ri Kim"
                },
                {
                    "authorId": "2218839914",
                    "name": "Hanbyul Kim"
                },
                {
                    "authorId": "2284448872",
                    "name": "Heesu Kim"
                },
                {
                    "authorId": "2294681681",
                    "name": "Isaac Kim"
                },
                {
                    "authorId": "2294657765",
                    "name": "Jeonghoon Kim"
                },
                {
                    "authorId": "2294602962",
                    "name": "Jihye Kim"
                },
                {
                    "authorId": "2265618046",
                    "name": "Joonghoon Kim"
                },
                {
                    "authorId": "2294602950",
                    "name": "Minjae Kim"
                },
                {
                    "authorId": "2116507605",
                    "name": "Minsub Kim"
                },
                {
                    "authorId": "2294561659",
                    "name": "Pil Hwan Kim"
                },
                {
                    "authorId": "2294667764",
                    "name": "Sammy Kim"
                },
                {
                    "authorId": "2294673815",
                    "name": "Seokhun Kim"
                },
                {
                    "authorId": "2279044682",
                    "name": "Seonghyeon Kim"
                },
                {
                    "authorId": "2267573770",
                    "name": "Soojin Kim"
                },
                {
                    "authorId": "2294726584",
                    "name": "Soong Kim"
                },
                {
                    "authorId": "2294673220",
                    "name": "Soyoon Kim"
                },
                {
                    "authorId": "2294673045",
                    "name": "Sunyoung Kim"
                },
                {
                    "authorId": "2294665685",
                    "name": "Taeho Kim"
                },
                {
                    "authorId": "2294646883",
                    "name": "Wonho Kim"
                },
                {
                    "authorId": "2257354402",
                    "name": "Yoonsik Kim"
                },
                {
                    "authorId": "2294681815",
                    "name": "You Jin Kim"
                },
                {
                    "authorId": "2294810189",
                    "name": "Yuri Kim"
                },
                {
                    "authorId": "2248267179",
                    "name": "Beomseok Kwon"
                },
                {
                    "authorId": "2294571369",
                    "name": "Ohsung Kwon"
                },
                {
                    "authorId": "2294586641",
                    "name": "Yoo-Hwan Kwon"
                },
                {
                    "authorId": "2294679992",
                    "name": "Anna Lee"
                },
                {
                    "authorId": "2294671670",
                    "name": "Byungwook Lee"
                },
                {
                    "authorId": "2294766544",
                    "name": "Changho Lee"
                },
                {
                    "authorId": "2294706755",
                    "name": "Daun Lee"
                },
                {
                    "authorId": "2284307404",
                    "name": "DongJae Lee"
                },
                {
                    "authorId": "2294681304",
                    "name": "Ha-Ram Lee"
                },
                {
                    "authorId": "2294716538",
                    "name": "Hodong Lee"
                },
                {
                    "authorId": "2294675768",
                    "name": "Hwiyeong Lee"
                },
                {
                    "authorId": "2294668858",
                    "name": "Hyunmi Lee"
                },
                {
                    "authorId": "2294680720",
                    "name": "Injae Lee"
                },
                {
                    "authorId": "2294668451",
                    "name": "Jaeung Lee"
                },
                {
                    "authorId": "2294884852",
                    "name": "Jeongsang Lee"
                },
                {
                    "authorId": "2294681061",
                    "name": "Jisoo Lee"
                },
                {
                    "authorId": "2295294597",
                    "name": "Joongjae Lee"
                },
                {
                    "authorId": "2294681529",
                    "name": "Juhan Lee"
                },
                {
                    "authorId": "2294665413",
                    "name": "Jung Hyun Lee"
                },
                {
                    "authorId": "2294674956",
                    "name": "Junghoon Lee"
                },
                {
                    "authorId": "2294673632",
                    "name": "Junwoo Lee"
                },
                {
                    "authorId": "2294604539",
                    "name": "Se Yun Lee"
                },
                {
                    "authorId": "2294604349",
                    "name": "Sujin Lee"
                },
                {
                    "authorId": "2177406663",
                    "name": "Sungjae Lee"
                },
                {
                    "authorId": "2294829531",
                    "name": "Sungwoo Lee"
                },
                {
                    "authorId": "2294845608",
                    "name": "Wonjae Lee"
                },
                {
                    "authorId": "2294570219",
                    "name": "Zoo Hyun Lee"
                },
                {
                    "authorId": "2294677209",
                    "name": "Jong Kun Lim"
                },
                {
                    "authorId": "2294994731",
                    "name": "Kun Lim"
                },
                {
                    "authorId": "2294566206",
                    "name": "Taemin Lim"
                },
                {
                    "authorId": "2294565999",
                    "name": "Yuri Min"
                },
                {
                    "authorId": "2294567076",
                    "name": "Nuri Na"
                },
                {
                    "authorId": "2000600815",
                    "name": "JeongYeon Nam"
                },
                {
                    "authorId": "10521251",
                    "name": "Kyeong-Min Nam"
                },
                {
                    "authorId": "2294565404",
                    "name": "Yeonseog Noh"
                },
                {
                    "authorId": "2294572038",
                    "name": "Biro Oh"
                },
                {
                    "authorId": "2294690209",
                    "name": "Hyangnam Oh"
                },
                {
                    "authorId": "2284573373",
                    "name": "Jungsik Oh"
                },
                {
                    "authorId": "2294668576",
                    "name": "Solgil Oh"
                },
                {
                    "authorId": "2294751876",
                    "name": "Yeontaek Oh"
                },
                {
                    "authorId": "2295290692",
                    "name": "Boyoun Park"
                },
                {
                    "authorId": "2284731913",
                    "name": "Cheonbok Park"
                },
                {
                    "authorId": "13453892",
                    "name": "Dongju Park"
                },
                {
                    "authorId": "2110832174",
                    "name": "Hyeon-ju Park"
                },
                {
                    "authorId": "2110376119",
                    "name": "Hyunjung Park"
                },
                {
                    "authorId": "2110376119",
                    "name": "Hyunjung Park"
                },
                {
                    "authorId": "2294595929",
                    "name": "Jihye Park"
                },
                {
                    "authorId": "2294602712",
                    "name": "Jooseok Park"
                },
                {
                    "authorId": "2294746638",
                    "name": "Junghwan Park"
                },
                {
                    "authorId": "2294746640",
                    "name": "Jungsoo Park"
                },
                {
                    "authorId": "2294560153",
                    "name": "Miru Park"
                },
                {
                    "authorId": "2294559118",
                    "name": "S. Park"
                },
                {
                    "authorId": "2294669681",
                    "name": "Seunghyun Park"
                },
                {
                    "authorId": "2294572849",
                    "name": "Taerim Park"
                },
                {
                    "authorId": "2294715748",
                    "name": "Wonkyeong Park"
                },
                {
                    "authorId": "2294564943",
                    "name": "Hyunjoon Ryu"
                },
                {
                    "authorId": "2294801035",
                    "name": "Jeonghun Ryu"
                },
                {
                    "authorId": "2126415383",
                    "name": "Nahyeon Ryu"
                },
                {
                    "authorId": "2283304906",
                    "name": "Soonshin Seo"
                },
                {
                    "authorId": "2294563569",
                    "name": "Suk Min Seo"
                },
                {
                    "authorId": "2294563537",
                    "name": "Yoonjeong Shim"
                },
                {
                    "authorId": "2810739",
                    "name": "Kyuyong Shin"
                },
                {
                    "authorId": "2294572046",
                    "name": "Wonkwang Shin"
                },
                {
                    "authorId": "2294566961",
                    "name": "Hyun Sim"
                },
                {
                    "authorId": "2294564305",
                    "name": "Mihyun Sim"
                },
                {
                    "authorId": "2281318046",
                    "name": "W. Sim"
                },
                {
                    "authorId": "2294557639",
                    "name": "Hyejin Soh"
                },
                {
                    "authorId": "65842988",
                    "name": "Bokyung Son"
                },
                {
                    "authorId": "2294564517",
                    "name": "Hyunjun Son"
                },
                {
                    "authorId": "2294570439",
                    "name": "Seulah Son"
                },
                {
                    "authorId": "2152601230",
                    "name": "Chiyoung Song"
                },
                {
                    "authorId": "2152601230",
                    "name": "Chiyoung Song"
                },
                {
                    "authorId": "2294677519",
                    "name": "Ka Yeon Song"
                },
                {
                    "authorId": "2294606838",
                    "name": "Minchul Song"
                },
                {
                    "authorId": "2294686151",
                    "name": "Seungmin Song"
                },
                {
                    "authorId": "2294668346",
                    "name": "Jisung Wang"
                },
                {
                    "authorId": "2294571774",
                    "name": "Matt Yeo"
                },
                {
                    "authorId": "2126416362",
                    "name": "Y. Yeo"
                },
                {
                    "authorId": "16295192",
                    "name": "Myeong Yeon Yi"
                },
                {
                    "authorId": "2120159539",
                    "name": "Moonbin Yim"
                },
                {
                    "authorId": "2294562745",
                    "name": "Taehwan Yoo"
                },
                {
                    "authorId": "2294362158",
                    "name": "Y. Yoo"
                },
                {
                    "authorId": "2268180407",
                    "name": "S. Yoon"
                },
                {
                    "authorId": "2294564837",
                    "name": "Young Jin Yoon"
                },
                {
                    "authorId": "2294578555",
                    "name": "Hangyeol Yu"
                },
                {
                    "authorId": "2294564464",
                    "name": "Ui Seon Yu"
                },
                {
                    "authorId": "49996817",
                    "name": "Xingdong Zuo"
                },
                {
                    "authorId": "2187296055",
                    "name": "Jeongin Bae"
                },
                {
                    "authorId": "2294556240",
                    "name": "Joungeun Bae"
                },
                {
                    "authorId": "2148935112",
                    "name": "Hyun-Woong Cho"
                },
                {
                    "authorId": "2294675491",
                    "name": "Seonghyun Cho"
                },
                {
                    "authorId": "2294809239",
                    "name": "Yongjin Cho"
                },
                {
                    "authorId": "2284685346",
                    "name": "Taekyoon Choi"
                },
                {
                    "authorId": "2294671638",
                    "name": "Yera Choi"
                },
                {
                    "authorId": "2004821977",
                    "name": "Jiwan Chung"
                },
                {
                    "authorId": "2294816762",
                    "name": "Zhenghui Han"
                },
                {
                    "authorId": "3086596",
                    "name": "Byeongho Heo"
                },
                {
                    "authorId": "2294563293",
                    "name": "Euisuk Hong"
                },
                {
                    "authorId": "2119518956",
                    "name": "T. Hwang"
                },
                {
                    "authorId": "2294570402",
                    "name": "Seonyeol Im"
                },
                {
                    "authorId": "2294564934",
                    "name": "Sumin Jegal"
                },
                {
                    "authorId": "2294564551",
                    "name": "Sumin Jeon"
                },
                {
                    "authorId": "2294756989",
                    "name": "Yelim Jeong"
                },
                {
                    "authorId": "2279930540",
                    "name": "Yonghyun Jeong"
                },
                {
                    "authorId": "2294717585",
                    "name": "Can Jiang"
                },
                {
                    "authorId": "2294682530",
                    "name": "Juyong Jiang"
                },
                {
                    "authorId": "2165259166",
                    "name": "Jiho Jin"
                },
                {
                    "authorId": "2294564309",
                    "name": "Ara Jo"
                },
                {
                    "authorId": "2294556897",
                    "name": "Younghyun Jo"
                },
                {
                    "authorId": "2294599373",
                    "name": "Hoyoun Jung"
                },
                {
                    "authorId": "2294680140",
                    "name": "Juyoung Jung"
                },
                {
                    "authorId": "2244640050",
                    "name": "Daehee Kim"
                },
                {
                    "authorId": "2294681826",
                    "name": "Ginam Kim"
                },
                {
                    "authorId": "2294770608",
                    "name": "Hangyeol Kim"
                },
                {
                    "authorId": "2115191328",
                    "name": "Heeseung Kim"
                },
                {
                    "authorId": "2294812756",
                    "name": "Hyojin Kim"
                },
                {
                    "authorId": "2294812758",
                    "name": "Hyojun Kim"
                },
                {
                    "authorId": "2294666313",
                    "name": "Hyun-Ah Kim"
                },
                {
                    "authorId": "2294595392",
                    "name": "Jeehye Kim"
                },
                {
                    "authorId": "2294598511",
                    "name": "Jinhong Kim"
                },
                {
                    "authorId": "2294595219",
                    "name": "Jiseon Kim"
                },
                {
                    "authorId": "2294596833",
                    "name": "Jonghak Kim"
                },
                {
                    "authorId": "2294746256",
                    "name": "Jung Yoon Kim"
                },
                {
                    "authorId": "2146948145",
                    "name": "Rak Yeong Kim"
                },
                {
                    "authorId": "2294681170",
                    "name": "Seoyoon Kim"
                },
                {
                    "authorId": "2294679211",
                    "name": "Sewon Kim"
                },
                {
                    "authorId": "2294726586",
                    "name": "Sooyoung Kim"
                },
                {
                    "authorId": "2294668219",
                    "name": "Sukyoung Kim"
                },
                {
                    "authorId": "2294671349",
                    "name": "Taeyong Kim"
                },
                {
                    "authorId": "2014024020",
                    "name": "Naeun Ko"
                },
                {
                    "authorId": "2294565319",
                    "name": "Bonseung Koo"
                },
                {
                    "authorId": "2286879839",
                    "name": "Heeyoung Kwak"
                },
                {
                    "authorId": "2294809733",
                    "name": "Haena Kwon"
                },
                {
                    "authorId": "2294586643",
                    "name": "Youngjin Kwon"
                },
                {
                    "authorId": "2294562368",
                    "name": "Boram Lee"
                },
                {
                    "authorId": "2258793964",
                    "name": "Bruce W. Lee"
                },
                {
                    "authorId": "2294706757",
                    "name": "Dagyeong Lee"
                },
                {
                    "authorId": "2294669657",
                    "name": "Erin Lee"
                },
                {
                    "authorId": "2294669930",
                    "name": "Euijin Lee"
                },
                {
                    "authorId": "2294674462",
                    "name": "Ha Gyeong Lee"
                },
                {
                    "authorId": "2291077343",
                    "name": "Hyojin Lee"
                },
                {
                    "authorId": "2294681569",
                    "name": "Hyunjeong Lee"
                },
                {
                    "authorId": "2294681813",
                    "name": "Jeeyoon Lee"
                },
                {
                    "authorId": "2294884854",
                    "name": "Jeonghyun Lee"
                },
                {
                    "authorId": "2294675477",
                    "name": "Jongheok Lee"
                },
                {
                    "authorId": "2295294599",
                    "name": "Joonhyung Lee"
                },
                {
                    "authorId": "2294669173",
                    "name": "Junhyuk Lee"
                },
                {
                    "authorId": "2294848361",
                    "name": "M. Lee"
                },
                {
                    "authorId": "2294585365",
                    "name": "Nayeon Lee"
                },
                {
                    "authorId": "2284690114",
                    "name": "Sangkyu Lee"
                },
                {
                    "authorId": "2294604541",
                    "name": "Se Young Lee"
                },
                {
                    "authorId": "2257369035",
                    "name": "Seulgi Lee"
                },
                {
                    "authorId": "2291154774",
                    "name": "Seung Jin Lee"
                },
                {
                    "authorId": "2294599007",
                    "name": "Suhyeon Lee"
                },
                {
                    "authorId": "2294717415",
                    "name": "Yeonjae Lee"
                },
                {
                    "authorId": "2294717417",
                    "name": "Yesol Lee"
                },
                {
                    "authorId": "2294670801",
                    "name": "Youngbeom Lee"
                },
                {
                    "authorId": "2294651764",
                    "name": "Yujin Lee"
                },
                {
                    "authorId": "2294638159",
                    "name": "Shaodong Li"
                },
                {
                    "authorId": "2295070420",
                    "name": "Tianyu Liu"
                },
                {
                    "authorId": "2294758017",
                    "name": "Seong-Eun Moon"
                },
                {
                    "authorId": "2239232931",
                    "name": "Taehong Moon"
                },
                {
                    "authorId": "2294563993",
                    "name": "Max-Lasse Nihlenramstroem"
                },
                {
                    "authorId": "2288892172",
                    "name": "Wonseok Oh"
                },
                {
                    "authorId": "2294751878",
                    "name": "Yuri Oh"
                },
                {
                    "authorId": "2294675387",
                    "name": "Hongbeen Park"
                },
                {
                    "authorId": "2294676994",
                    "name": "Hyekyung Park"
                },
                {
                    "authorId": "2266467701",
                    "name": "Nohil Park"
                },
                {
                    "authorId": "2294559118",
                    "name": "S. Park"
                },
                {
                    "authorId": "2294801037",
                    "name": "Jiwon Ryu"
                },
                {
                    "authorId": "2294566912",
                    "name": "Miru Ryu"
                },
                {
                    "authorId": "2218819038",
                    "name": "Simo Ryu"
                },
                {
                    "authorId": "2294565005",
                    "name": "Ahreum Seo"
                },
                {
                    "authorId": "2252980570",
                    "name": "H. Seo"
                },
                {
                    "authorId": "2294564567",
                    "name": "Kangdeok Seo"
                },
                {
                    "authorId": "2271013514",
                    "name": "Jamin Shin"
                },
                {
                    "authorId": "2295237749",
                    "name": "Seungyoun Shin"
                },
                {
                    "authorId": "2294565017",
                    "name": "Heetae Sin"
                },
                {
                    "authorId": "2294670999",
                    "name": "Jiangping Wang"
                },
                {
                    "authorId": "2294750656",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2294570090",
                    "name": "Ning Xiang"
                },
                {
                    "authorId": "2294621978",
                    "name": "Longxiang Xiao"
                },
                {
                    "authorId": "2294808747",
                    "name": "Jing Xu"
                },
                {
                    "authorId": "2295109079",
                    "name": "Seonyeong Yi"
                },
                {
                    "authorId": "2294562807",
                    "name": "Haanju Yoo"
                },
                {
                    "authorId": "2294562809",
                    "name": "Haneul Yoo"
                },
                {
                    "authorId": "2294562811",
                    "name": "Hwanhee Yoo"
                },
                {
                    "authorId": "2294675955",
                    "name": "Liang Yu"
                },
                {
                    "authorId": "2284703785",
                    "name": "Youngjae Yu"
                },
                {
                    "authorId": "2293656004",
                    "name": "Weijie Yuan"
                },
                {
                    "authorId": "2294560094",
                    "name": "Bo Zeng"
                },
                {
                    "authorId": "2294676886",
                    "name": "Qian Zhou"
                },
                {
                    "authorId": "2279965735",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "2264669923",
                    "name": "J. Ha"
                },
                {
                    "authorId": "2266672702",
                    "name": "Joonsuk Park"
                },
                {
                    "authorId": "2291094763",
                    "name": "J. Hwang"
                },
                {
                    "authorId": "2277581926",
                    "name": "H. Kwon"
                },
                {
                    "authorId": "2295304783",
                    "name": "Soonyong Kwon"
                },
                {
                    "authorId": "2207020228",
                    "name": "Jungyeon Lee"
                },
                {
                    "authorId": "2294608712",
                    "name": "Seungho Lee"
                },
                {
                    "authorId": "2291128236",
                    "name": "Se-Eun Choi"
                },
                {
                    "authorId": "2261393694",
                    "name": "Sang-Woo Lee"
                },
                {
                    "authorId": "2294679296",
                    "name": "Jung Hwa Lim"
                },
                {
                    "authorId": "40188877",
                    "name": "Nako Sung"
                }
            ],
            "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.",
            "corpus_id": "268856712",
            "text": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1444091796875
        }
    ],
    "quotes": {
        "cost": 0.24241200000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[204734128 | Lewis et al. | 2019 | Citations: 509]",
                "snippets": "Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "The MLQA corpus",
                        "pdf_hash": "",
                        "start": 288,
                        "end": 1073,
                        "sentence_offsets": [
                            {
                                "start": 288,
                                "end": 377
                            },
                            {
                                "start": 378,
                                "end": 467
                            },
                            {
                                "start": 468,
                                "end": 590
                            },
                            {
                                "start": 591,
                                "end": 682
                            },
                            {
                                "start": 685,
                                "end": 809
                            },
                            {
                                "start": 810,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 1073
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[215548041 | Artetxe et al. | 2020 | Citations: 120]",
                "snippets": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training....Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204901567 | Artetxe et al. | 2019 | Citations: 799]": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
                    "[52271711 | Conneau et al. | 2018 | Citations: 1388]": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 747,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 617
                            },
                            {
                                "start": 618,
                                "end": 744
                            }
                        ],
                        "ref_mentions": [
                            "52271711",
                            "204901567"
                        ],
                        "quote": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1973,
                        "end": 2103,
                        "sentence_offsets": [
                            {
                                "start": 1973,
                                "end": 2101
                            },
                            {
                                "start": 2102,
                                "end": 2413
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[218470125 | Ponti et al. | 2020 | Citations: 327]",
                "snippets": "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Annotation Design",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1030,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 18
                            },
                            {
                                "start": 19,
                                "end": 303
                            },
                            {
                                "start": 304,
                                "end": 673
                            },
                            {
                                "start": 676,
                                "end": 1031
                            }
                        ],
                        "ref_mentions": [
                            "9497990"
                        ],
                        "quote": "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[220871404 | Longpre et al. | 2020 | Citations: 160]",
                "snippets": "Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[225067135 | Riabi et al. | 2020 | Citations: 53]",
                "snippets": "XQuAD (Artetxe et al., 2020) is a human translation of the SQuAD en development set in 10 languages (Arabic, Chinese, German, Greek, Hindi, Russian, Spanish, Thai, Turkish, and Vietnamese), with 1k QA pairs for each language.\n\nMLQA (Lewis et al., 2020) is an evaluation dataset in 7 languages (English, Arabic, Chinese, German, Hindi, and Spanish). The dataset is built from aligned Wikipedia sentences across at least two languages (full alignment between all languages being impossible), with the goal of providing natural rather than translated paragraphs. The QA pairs are manually annotated on the English sentences and then human translated on the aligned sentences. The dataset contains about 46K aligned QA pairs in total.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": []
            },
            {
                "idx": 5,
                "key": "[226262367 | Hardalov et al. | 2020 | Citations: 54]",
                "snippets": "We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade.\n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks...E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade.\n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1340,
                        "end": 1743,
                        "sentence_offsets": [
                            {
                                "start": 1340,
                                "end": 1421
                            },
                            {
                                "start": 1422,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1573
                            },
                            {
                                "start": 1574,
                                "end": 1742
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[235097287 | Wang et al. | 2021 | Citations: 28]",
                "snippets": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data Augmentation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 471,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 133
                            },
                            {
                                "start": 134,
                                "end": 312
                            },
                            {
                                "start": 313,
                                "end": 472
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[235358129 | Goyal et al. | 2021 | Citations: 587]",
                "snippets": "Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 145,
                        "end": 342,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[237364084 | Bonifacio et al. | 2021 | Citations: 113]",
                "snippets": "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages....researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets....we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 180,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 178
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 196,
                        "end": 584,
                        "sentence_offsets": [
                            {
                                "start": 181,
                                "end": 460
                            },
                            {
                                "start": 461,
                                "end": 581
                            },
                            {
                                "start": 582,
                                "end": 807
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets"
                    },
                    {
                        "quote": ".we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[237532506 | Aggarwal et al. | 2021 | Citations: 6]",
                "snippets": "Multi-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28](Rotman et al., 2018) rely on direct translations of English captions while others (Gella et al., 2017)(Huang et al., 2019) have used independent image and language text pairs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[19396711 | Gella et al. | 2017 | Citations: 78]": "In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.",
                    "[202772083 | Huang et al. | 2019 | Citations: 25]": "With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new objective function which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our model yields a significant performance gain over other methods in all of the three tasks.",
                    "[21688999 | Rotman et al. | 2018 | Citations: 26]": "We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 328,
                        "end": 677,
                        "sentence_offsets": [
                            {
                                "start": 328,
                                "end": 531
                            },
                            {
                                "start": 532,
                                "end": 677
                            }
                        ],
                        "ref_mentions": [
                            "21688999",
                            "19396711",
                            "202772083"
                        ],
                        "quote": "Multi-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28](Rotman et al., 2018) rely on direct translations of English captions while others (Gella et al., 2017)(Huang et al., 2019) have used independent image and language text pairs."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[246294502 | Bugliarello et al. | 2022 | Citations: 64]",
                "snippets": "We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 129,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 129
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[248780386 | Ahuja et al. | 2022 | Citations: 16]",
                "snippets": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 741,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            },
                            {
                                "start": 114,
                                "end": 428
                            },
                            {
                                "start": 429,
                                "end": 592
                            },
                            {
                                "start": 593,
                                "end": 741
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[252568105 | Jeronymo et al. | 2022 | Citations: 3]",
                "snippets": "A key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL.\n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222310837 | Lin et al. | 2020 | Citations: 627]": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly.",
                    "[233296851 | Craswell et al. | 2021 | Citations: 52]": "The TREC Deep Learning (DL) Track studies ad hoc search in the large data regime, meaning that a large set of human-labeled training data is available. Results so far indicate that the best models with large data may be deep neural networks. This paper supports the reuse of the TREC DL test collections in three ways. First we describe the data sets in detail, documenting clearly and in one place some details that are otherwise scattered in track guidelines, overview papers and in our associated MS MARCO leaderboard pages. We intend this description to make it easy for newcomers to use the TREC DL data. Second, because there is some risk of iteration and selection bias when reusing a data set, we describe the best practices for writing a paper using TREC DL data, without overfitting. We provide some illustrative analysis. Finally we address a number of issues around the TREC DL data, including an analysis of reusability.",
                    "[237213465 | Zhang et al. | 2021 | Citations: 115]": "We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data. As a starting point, we provide zero-shot baselines for this new dataset based on a multi-lingual adaptation of DPR that we call \u201cmDPR\u201d. Experiments show that although the effectiveness of mDPR is much lower than BM25, dense representations nevertheless appear to provide valuable relevance signals, improving BM25 results in sparse\u2013dense hybrids. In addition to analyses of our results, we also discuss future challenges and present a research agenda in multi-lingual dense retrieval. Mr. TyDi can be downloaded at https://github.com/castorini/mr.tydi.",
                    "[246275873 | Lawrie et al. | 2022 | Citations: 30]": "HC4 is a new suite of test collections for ad hoc Cross-Language Information Retrieval (CLIR), with Common Crawl News documents in Chinese, Persian, and Russian, topics in English and in the document languages, and graded relevance judgments. New test collections are needed because existing CLIR test collections built using pooling of traditional CLIR runs have systematic gaps in their relevance judgments when used to evaluate neural CLIR methods. The HC4 collections contain 60 topics and about half a million documents for each of Chinese and Persian, and 54 topics and five million documents for Russian. Active learning was used to determine which documents to annotate after being seeded using interactive search and judgment. Documents were judged on a three-grade relevance scale. This paper describes the design and construction of the new test collections and provides baseline results for demonstrating their utility for evaluating systems.",
                    "[248817938 | Voorhees et al. | 2022 | Citations: 23]": "This paper presents the lessons regarding the construction and use of large Cranfield-style test collections learned from the TREC 2021 Deep Learning track. The corpus used in the 2021 edition of the track was much bigger than the corpus used previously and it contains many more relevant documents. The process used to select documents to judge that had been used in earlier years of the track failed to produce a reliable collection because most topics have too many relevant documents. Judgment budgets were exceeded before an adequate sample of the relevant set could be found, so there are likely many unknown relevant documents in the unjudged portion of the corpus. As a result, the collection is not reusable, and furthermore, recall-based measures are unreliable even for the retrieval systems that were used to build the collection. Yet, early-precision measures cannot distinguish among system results because the maximum score is easily obtained for many topics. And since the existing tools for appraising the quality of test collections depend on systems' scores, they also fail when there are too many relevant documents. Collection builders will need new strategies and tools for building reliable test collections for continued use of the Cranfield paradigm on ever-larger corpora. Ensuring that the definition of 'relevant' truly reflects the desired systems' rankings is a provisional strategy for continued collection building.",
                    "[42162974 | Peters et al. | 2001 | Citations: 78]": "The goals of the CLEF (Cross-Language Evaluation Forum) series of evaluation campaigns for information retrieval systems operating on European languages are described. The difficulties of organizing an activity which aims at an objective evaluation of systems running on and over a number of different languages are examined. The discussion includes an analysis of the first results and proposals for possible developments in the future."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1296,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 115
                            },
                            {
                                "start": 116,
                                "end": 165
                            },
                            {
                                "start": 166,
                                "end": 301
                            },
                            {
                                "start": 302,
                                "end": 477
                            },
                            {
                                "start": 478,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 710
                            },
                            {
                                "start": 711,
                                "end": 881
                            },
                            {
                                "start": 882,
                                "end": 1012
                            },
                            {
                                "start": 1015,
                                "end": 1265
                            },
                            {
                                "start": 1266,
                                "end": 1297
                            }
                        ],
                        "ref_mentions": [
                            "233296851",
                            "248817938",
                            "42162974",
                            "38347697",
                            "262629903",
                            "1720123",
                            "246275873",
                            "237213465",
                            "222310837",
                            "59910946"
                        ],
                        "quote": "A key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL.\n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[258187051 | Chung et al. | 2023 | Citations: 57]",
                "snippets": "In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "EVALUATION TASKS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 607,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 72
                            },
                            {
                                "start": 73,
                                "end": 233
                            },
                            {
                                "start": 234,
                                "end": 280
                            },
                            {
                                "start": 281,
                                "end": 381
                            },
                            {
                                "start": 382,
                                "end": 607
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[258557740 | Hennig et al. | 2023 | Citations: 8]",
                "snippets": "We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[258865558 | Asai et al. | 2023 | Citations: 62]",
                "snippets": "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218470125 | Ponti et al. | 2020 | Citations: 327]": "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL.",
                    "[216914383 | Artetxe et al. | 2020 | Citations: 72]": "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."
                },
                "metadata": [
                    {
                        "section_title": "Design Principles",
                        "pdf_hash": "",
                        "start": 885,
                        "end": 1368,
                        "sentence_offsets": [
                            {
                                "start": 885,
                                "end": 923
                            },
                            {
                                "start": 924,
                                "end": 1087
                            },
                            {
                                "start": 1088,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1368
                            }
                        ],
                        "ref_mentions": [
                            "218470125",
                            "216914383"
                        ],
                        "quote": "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[259095896 | Li et al. | 2023 | Citations: 120]",
                "snippets": "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Stage IV: Key Datasets Translation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 891,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 293
                            },
                            {
                                "start": 294,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 427
                            },
                            {
                                "start": 428,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 558
                            },
                            {
                                "start": 559,
                                "end": 803
                            },
                            {
                                "start": 804,
                                "end": 891
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names)."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[259224700 | Rodr'iguez-Cantelar et al. | 2023 | Citations: 10]",
                "snippets": "Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and costintensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. \n\nFurthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art Englishonly dialogue systems into highly capable multilingual systems. Although multilingual pre-trained language models may exist and can be potentially used for training multilingual dialogue systems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some lowresource languages. To address this problem, we take advantage of recent advances in neural machine translation and paraphrasing systems. Using existing high-quality services and models, it is possible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Due to the lack of robust automatic evaluation metrics (Mehri and Eskenazi, 2020a), researchers have to resort to the time-consuming and costintensive human evaluation process to analyze the performance of their model and benchmark their proposed methods against baselines. \n\nFurthermore, to the best of our knowledge, none of the existing metrics have been thoroughly tested in a multilingual setting. Metric generalization across different languages is highly desirable, as it helps the transformation of state-of-the-art Englishonly dialogue systems into highly capable multilingual systems. Although multilingual pre-trained language models may exist and can be potentially used for training multilingual dialogue systems, human-annotations or high-quality dialogue datasets for languages other than English are very scarce or even nonexistent in the case of some lowresource languages. To address this problem, we take advantage of recent advances in neural machine translation and paraphrasing systems. Using existing high-quality services and models, it is possible to create new datasets for different languages and perform back-translation or paraphrasing to create additional data in the original language to improve and evaluate the robustness of existing metrics.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[268357672 | Kim et al. | 2024 | Citations: 23]",
                "snippets": "Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, (Ma et al., 2022) emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247762223 | Ma et al. | 2022 | Citations: 8]": "While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. Through language modeling (LM) evaluations and manual analyses, we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks. Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research.",
                    "[250562882 | Seo et al. | 2022 | Citations: 12]": "Recent natural language understanding (NLU) research on the Korean language has been vigorously maturing with the advancements of pretrained language models and datasets. However, Korean pretrained language models still struggle to generate a short sentence with a given condition based on compositionality and commonsense reasoning (i.e., generative commonsense reasoning). The two ma-jor challenges are inadequate data resources to develop generative commonsense reasoning regarding Korean linguistic features and to evaluate language models which are necessary for natural language generation (NLG). To solve these problems, we propose a text-generation dataset for Korean generative commonsense reasoning and language model evaluation. In this work, a semi-automatic dataset construction approach filters out contents in-explicable to commonsense, ascertains quality, and reduces the cost of building the dataset. We also present an in-depth analysis of the generation results of language models with various evaluation metrics along with human-annotated scores. The whole dataset is publicly available at ( https://aihub.or. kr/opendata/korea-university )."
                },
                "metadata": [
                    {
                        "section_title": "Multilingual and Commonsense",
                        "pdf_hash": "",
                        "start": 1194,
                        "end": 2073,
                        "sentence_offsets": [
                            {
                                "start": 1194,
                                "end": 1438
                            },
                            {
                                "start": 1438,
                                "end": 1626
                            },
                            {
                                "start": 1626,
                                "end": 1781
                            },
                            {
                                "start": 1781,
                                "end": 1849
                            },
                            {
                                "start": 1849,
                                "end": 2073
                            }
                        ],
                        "ref_mentions": [
                            "250562882",
                            "3523957",
                            "247762223"
                        ],
                        "quote": "Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, (Ma et al., 2022) emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[268819377 | Xu et al. | 2024 | Citations: 43]",
                "snippets": "The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English (Lauscher et al., 2019), [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[135465247 | Lauscher et al. | 2019 | Citations: 55]": "Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques."
                },
                "metadata": [
                    {
                        "section_title": "B. Bias Benchmark",
                        "pdf_hash": "",
                        "start": 1673,
                        "end": 2150,
                        "sentence_offsets": [
                            {
                                "start": 1673,
                                "end": 1799
                            },
                            {
                                "start": 1800,
                                "end": 1892
                            },
                            {
                                "start": 1893,
                                "end": 2001
                            },
                            {
                                "start": 2002,
                                "end": 2150
                            }
                        ],
                        "ref_mentions": [
                            "135465247"
                        ],
                        "quote": "The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English (Lauscher et al., 2019), [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[269362363 | Singh et al. | 2024 | Citations: 28]",
                "snippets": "We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1395,
                        "end": 1759,
                        "sentence_offsets": [
                            {
                                "start": 1395,
                                "end": 1532
                            },
                            {
                                "start": 1535,
                                "end": 1896
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization"
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[269362697 | Moon et al. | 2024 | Citations: 0]",
                "snippets": "For validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[214794966 | Liang et al. | 2020 | Citations: 350]": "In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 643,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 145
                            },
                            {
                                "start": 146,
                                "end": 326
                            },
                            {
                                "start": 327,
                                "end": 492
                            },
                            {
                                "start": 493,
                                "end": 643
                            }
                        ],
                        "ref_mentions": [
                            "214794966"
                        ],
                        "quote": "For validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[270285718 | Sakai et al. | 2024 | Citations: 5]",
                "snippets": "Multilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 637,
                        "end": 1369,
                        "sentence_offsets": [
                            {
                                "start": 637,
                                "end": 826
                            },
                            {
                                "start": 828,
                                "end": 1369
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Multilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[270380088 | Etxaniz et al. | 2024 | Citations: 11]",
                "snippets": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 569,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 267
                            },
                            {
                                "start": 268,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 569
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273229002 | Kargaran et al. | 2024 | Citations: 7]",
                "snippets": "These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation (Lai et al., 2023). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235658519 | Hasan et al. | 2021 | Citations: 372]": "Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present XL-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model, with XL-Sum and experiment on multilingual and low-resource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at \\url{https://github.com/csebuetnlp/xl-sum}.",
                    "[260334562 | Lai et al. | 2023 | Citations: 150]": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi."
                },
                "metadata": [
                    {
                        "section_title": "A.2 MULTILINGUAL EVALUATION BENCHMARKS",
                        "pdf_hash": "",
                        "start": 775,
                        "end": 1178,
                        "sentence_offsets": [
                            {
                                "start": 775,
                                "end": 986
                            },
                            {
                                "start": 987,
                                "end": 1178
                            }
                        ],
                        "ref_mentions": [
                            "260334562",
                            "235658519"
                        ],
                        "quote": "These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation (Lai et al., 2023). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273323209 | Thellmann et al. | 2024 | Citations: 10]",
                "snippets": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246680398 | Meng et al. | 2022 | Citations: 235]": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1842,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 413
                            },
                            {
                                "start": 414,
                                "end": 601
                            },
                            {
                                "start": 602,
                                "end": 787
                            },
                            {
                                "start": 790,
                                "end": 1008
                            },
                            {
                                "start": 1009,
                                "end": 1074
                            },
                            {
                                "start": 1075,
                                "end": 1135
                            },
                            {
                                "start": 1136,
                                "end": 1286
                            },
                            {
                                "start": 1287,
                                "end": 1460
                            },
                            {
                                "start": 1463,
                                "end": 1559
                            },
                            {
                                "start": 1560,
                                "end": 1680
                            },
                            {
                                "start": 1681,
                                "end": 1842
                            }
                        ],
                        "ref_mentions": [
                            "246680398"
                        ],
                        "quote": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273532366 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Evaluation Tasks",
                        "pdf_hash": "",
                        "start": 1131,
                        "end": 1559,
                        "sentence_offsets": [
                            {
                                "start": 1131,
                                "end": 1263
                            },
                            {
                                "start": 1264,
                                "end": 1443
                            },
                            {
                                "start": 1444,
                                "end": 1559
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[273532388 | Son et al. | 2024 | Citations: 13]",
                "snippets": "However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminary",
                        "pdf_hash": "",
                        "start": 952,
                        "end": 2121,
                        "sentence_offsets": [
                            {
                                "start": 952,
                                "end": 1288
                            },
                            {
                                "start": 1289,
                                "end": 1417
                            },
                            {
                                "start": 1418,
                                "end": 1529
                            },
                            {
                                "start": 1532,
                                "end": 1557
                            },
                            {
                                "start": 1558,
                                "end": 1699
                            },
                            {
                                "start": 1702,
                                "end": 1728
                            },
                            {
                                "start": 1729,
                                "end": 1893
                            },
                            {
                                "start": 1894,
                                "end": 1953
                            },
                            {
                                "start": 1956,
                                "end": 1997
                            },
                            {
                                "start": 1998,
                                "end": 2121
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273662078 | Tourni et al. | 2024 | Citations: 0]",
                "snippets": "Multilingual and Cross-lingual QA have emerged as critical areas of NLP research, driven by the advance of LLMs and the creation of diverse multilingual QA datasets. Benchmarks such as TYDI QA (Clark et al., 2020), MLQA (Lewis et al., 2019), XQuAD (Artetxe et al., 2019), andMKQA (Longpre et al., 2021) have been instrumental in enabling evaluation across a wide range of languages, representing different language families and typologies. Most of these multilingual datasets, however, are still limited to evaluation on benchmarking datasets and not yet tested in real-world settings. \n\nVarious works have focused on generating synthetic multilingual QA data for training downstream QA models (Riabi et al., 2020)Shakeri et al., 2020). Notable contributions include PAXQA (Li and Callison-Burch, 2023), generating crosslingual QA datasets by leveraging parallel corpora and without requiring annotated non-English QA data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[225067135 | Riabi et al. | 2020 | Citations: 53]": "Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).",
                    "[212657414 | Clark et al. | 2020 | Citations: 612]": "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation."
                },
                "metadata": [
                    {
                        "section_title": "Multilingual QA",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 923,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 165
                            },
                            {
                                "start": 166,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 585
                            },
                            {
                                "start": 588,
                                "end": 736
                            },
                            {
                                "start": 737,
                                "end": 923
                            }
                        ],
                        "ref_mentions": [
                            "212657414",
                            "225067135"
                        ],
                        "quote": "Multilingual and Cross-lingual QA have emerged as critical areas of NLP research, driven by the advance of LLMs and the creation of diverse multilingual QA datasets. Benchmarks such as TYDI QA (Clark et al., 2020), MLQA (Lewis et al., 2019), XQuAD (Artetxe et al., 2019), andMKQA (Longpre et al., 2021) have been instrumental in enabling evaluation across a wide range of languages, representing different language families and typologies. Most of these multilingual datasets, however, are still limited to evaluation on benchmarking datasets and not yet tested in real-world settings. \n\nVarious works have focused on generating synthetic multilingual QA data for training downstream QA models (Riabi et al., 2020)Shakeri et al., 2020). Notable contributions include PAXQA (Li and Callison-Burch, 2023), generating crosslingual QA datasets by leveraging parallel corpora and without requiring annotated non-English QA data."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[273950544 | Azime et al. | 2024 | Citations: 2]",
                "snippets": "To create evaluation benchmarks that are multilingual, researchers (Koto et al., 2024); Li et al. (2023); Son et al. (2024) introduced benchmark datasets for different languages by translating a subset of the MMLU dataset. Beyond research efforts, translating existing benchmarks into different languages is an effective strategy to evaluate the multilingual capabilities of closed-source LLMs. These benchmarks evaluate multilingual understanding of models by presenting a range of extensive world knowledge tasks in the language of interest. While combining different subjects in a benchmark dataset may seem beneficial, it does not always provide a clear picture of the model's shortcomings. For example, using MMLU in different languages tests language and subject understanding simultaneously (Koto et al., 2024). There should be evaluation benchmarks that disentangle language understanding and specific subject knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267760288 | Koto et al. | 2024 | Citations: 44]": "The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present \\datasetname{}, the first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA) and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLaMA2, and Falcon struggle to achieve a score of 50%, while even the top-performing Arabic-centric model only achieves a score of 62.3%."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1066,
                        "end": 1998,
                        "sentence_offsets": [
                            {
                                "start": 1066,
                                "end": 1287
                            },
                            {
                                "start": 1288,
                                "end": 1459
                            },
                            {
                                "start": 1460,
                                "end": 1608
                            },
                            {
                                "start": 1609,
                                "end": 1759
                            },
                            {
                                "start": 1760,
                                "end": 1888
                            },
                            {
                                "start": 1889,
                                "end": 1998
                            }
                        ],
                        "ref_mentions": [
                            "267760288",
                            "267760288"
                        ],
                        "quote": "To create evaluation benchmarks that are multilingual, researchers (Koto et al., 2024); Li et al. (2023); Son et al. (2024) introduced benchmark datasets for different languages by translating a subset of the MMLU dataset. Beyond research efforts, translating existing benchmarks into different languages is an effective strategy to evaluate the multilingual capabilities of closed-source LLMs. These benchmarks evaluate multilingual understanding of models by presenting a range of extensive world knowledge tasks in the language of interest. While combining different subjects in a benchmark dataset may seem beneficial, it does not always provide a clear picture of the model's shortcomings. For example, using MMLU in different languages tests language and subject understanding simultaneously (Koto et al., 2024). There should be evaluation benchmarks that disentangle language understanding and specific subject knowledge."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276249513 | Gomes et al. | 2025 | Citations: 1]",
                "snippets": "Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusions",
                        "pdf_hash": "",
                        "start": 183,
                        "end": 494,
                        "sentence_offsets": [
                            {
                                "start": 183,
                                "end": 494
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276259236 | Huang et al. | 2025 | Citations: 4]",
                "snippets": "To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 492,
                        "end": 778,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[276408022 | Wu et al. | 2025 | Citations: 1]",
                "snippets": "Existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date...The dataset is constructed through an automatic translation process with human review to ensure quality...Zhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer...we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1021411 | Pontiki et al. | 2016 | Citations: 1422]": "This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams."
                },
                "metadata": [
                    {
                        "quote": "Existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "quote": "The dataset is constructed through an automatic translation process with human review to ensure quality",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 425,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 171
                            },
                            {
                                "start": 172,
                                "end": 425
                            }
                        ],
                        "ref_mentions": [
                            "1021411"
                        ],
                        "quote": "Zhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 928,
                        "end": 1249,
                        "sentence_offsets": [
                            {
                                "start": 914,
                                "end": 1083
                            },
                            {
                                "start": 1084,
                                "end": 1248
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[276408508 | Banerjee et al. | 2025 | Citations: 1]",
                "snippets": "XSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A Additional experiment",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 705,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 101
                            },
                            {
                                "start": 102,
                                "end": 303
                            },
                            {
                                "start": 304,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 567
                            },
                            {
                                "start": 568,
                                "end": 705
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "XSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[276421738 | Barth et al. | 2025 | Citations: 0]",
                "snippets": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2023). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265445838 | Tao et al. | 2023 | Citations: 87]": "Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."
                },
                "metadata": [
                    {
                        "section_title": "Challenge 3: Cultural Bias",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1193,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 60
                            },
                            {
                                "start": 61,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 328
                            },
                            {
                                "start": 329,
                                "end": 444
                            },
                            {
                                "start": 445,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 820
                            },
                            {
                                "start": 821,
                                "end": 943
                            },
                            {
                                "start": 946,
                                "end": 1018
                            },
                            {
                                "start": 1019,
                                "end": 1193
                            }
                        ],
                        "ref_mentions": [
                            "265445838"
                        ],
                        "quote": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2023). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[276647924 | He et al. | 2025 | Citations: 1]",
                "snippets": "Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, (Zhang et al., 2023) introduced MELA to assess multilingual linguistic acceptability, while (Mueller et al., 2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2019). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020)(Ladhak et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204734128 | Lewis et al. | 2019 | Citations: 509]": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
                    "[201698093 | Yang et al. | 2019 | Citations: 368]": "Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.",
                    "[214641214 | Hu et al. | 2020 | Citations: 976]": "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.",
                    "[214794966 | Liang et al. | 2020 | Citations: 350]": "In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.",
                    "[216868259 | Scialom et al. | 2020 | Citations: 177]": "We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.",
                    "[218470403 | Mueller et al. | 2020 | Citations: 57]": "A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models\u2019 ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.",
                    "[222177239 | Ladhak et al. | 2020 | Citations: 202]": "We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct cross-lingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.",
                    "[231740666 | Kassner et al. | 2021 | Citations: 141]": "Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as \u201cParis is the capital of [MASK]\u201d are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT\u2019s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.",
                    "[264145744 | Qi et al. | 2023 | Citations: 76]": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
                    "[265213331 | Zhang et al. | 2023 | Citations: 3]": "In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability -- MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language -- Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks. Our data is available at https://github.com/sjtu-compling/MELA.",
                    "[52271711 | Conneau et al. | 2018 | Citations: 1388]": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
                },
                "metadata": [
                    {
                        "section_title": "Multilingual Evaluation Benchmark",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1319,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 136
                            },
                            {
                                "start": 137,
                                "end": 263
                            },
                            {
                                "start": 264,
                                "end": 474
                            },
                            {
                                "start": 475,
                                "end": 739
                            },
                            {
                                "start": 742,
                                "end": 949
                            },
                            {
                                "start": 950,
                                "end": 1142
                            },
                            {
                                "start": 1143,
                                "end": 1319
                            }
                        ],
                        "ref_mentions": [
                            "265213331",
                            "218470403",
                            "231740666",
                            "264145744",
                            "214641214",
                            "214794966",
                            "52271711",
                            "201698093",
                            "204734128",
                            "216868259",
                            "222177239"
                        ],
                        "quote": "Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, (Zhang et al., 2023) introduced MELA to assess multilingual linguistic acceptability, while (Mueller et al., 2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2019). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020)(Ladhak et al., 2020)."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[277621694 | Luo et al. | 2025 | Citations: 0]",
                "snippets": "Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[277780370 | Hupkes et al. | 2025 | Citations: 0]",
                "snippets": "Translation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by (Brown et al., 2020), but has not been common since...Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "E.2 Multilingual evaluation options for LLMs",
                        "pdf_hash": "",
                        "start": 643,
                        "end": 907,
                        "sentence_offsets": [
                            {
                                "start": 643,
                                "end": 821
                            },
                            {
                                "start": 822,
                                "end": 908
                            }
                        ],
                        "ref_mentions": [
                            "218971783"
                        ],
                        "quote": "Translation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by (Brown et al., 2020), but has not been common since"
                    },
                    {
                        "quote": "Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[277993848 | Wu et al. | 2025 | Citations: 5]",
                "snippets": "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)...when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221516475 | Hendrycks et al. | 2020 | Citations: 4568]": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
                    "[252735112 | Shi et al. | 2022 | Citations: 369]": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",
                    "[260334562 | Lai et al. | 2023 | Citations: 150]": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.",
                    "[263829211 | Koto et al. | 2023 | Citations: 46]": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",
                    "[271245129 | Yuksel et al. | 2024 | Citations: 14]": "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                },
                "metadata": [
                    {
                        "section_title": "Domain",
                        "pdf_hash": "",
                        "start": 282,
                        "end": 911,
                        "sentence_offsets": [
                            {
                                "start": 282,
                                "end": 360
                            },
                            {
                                "start": 361,
                                "end": 569
                            },
                            {
                                "start": 570,
                                "end": 664
                            },
                            {
                                "start": 665,
                                "end": 912
                            }
                        ],
                        "ref_mentions": [
                            "252735112",
                            "260334562",
                            "221516475",
                            "263829211",
                            "271245129"
                        ],
                        "quote": "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1290,
                        "end": 1860,
                        "sentence_offsets": [
                            {
                                "start": 1282,
                                "end": 1511
                            },
                            {
                                "start": 1512,
                                "end": 1859
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.034875,
        "cot": "Looking at the user query, I need to organize the key advantages, challenges, and pitfalls of building multilingual evaluation datasets through translation of English datasets. After reviewing the quotes, I can identify several dimensions that would help structure a comprehensive answer:\n\n1. \"Background on Multilingual Evaluation Datasets\" - This section should provide context on the current state of multilingual evaluation datasets and why they're important. This will be a synthesis paragraph since it's providing foundational information to help the user understand the rest of the content.\n\n2. \"Advantages of Translating English Datasets\" - This section will cover the key benefits of using translation to create multilingual evaluation datasets. A synthesis format is appropriate here to coherently explain the interconnected advantages.\n\n3. \"Challenges in Building Translation-Based Datasets\" - This section will outline the difficulties in creating multilingual datasets through translation. A synthesis paragraph makes sense to explain these interrelated challenges.\n\n4. \"Quality and Reliability Issues\" - This section addresses concerns about the reliability of cross-lingual evaluations based on translated datasets. This will be a synthesis to effectively analyze and explain these interconnected issues.\n\n5. \"Alternative Approaches\" - This section will present alternative methods for creating multilingual evaluation datasets beyond simple translation. A list format works well here to clearly enumerate different approaches.\n\n6. \"Best Practices for Translation-Based Datasets\" - This section will provide recommendations for improving the quality of translated datasets. A list format is appropriate to clearly separate distinct practices.\n\nThe order of these dimensions follows a logical flow: providing background first, then discussing advantages, followed by challenges and reliability issues, before presenting alternatives and best practices.",
        "plan": {
            "Background on Multilingual Evaluation Datasets (synthesis)": [
                1,
                7,
                13,
                22,
                23,
                24,
                28,
                35,
                36
            ],
            "Advantages of Translating English Datasets (synthesis)": [
                1,
                3,
                6,
                8,
                9,
                10,
                12,
                20,
                21,
                25,
                30
            ],
            "Challenges in Building Translation-Based Datasets (synthesis)": [
                0,
                2,
                11,
                14,
                16,
                17,
                18,
                19,
                25,
                26,
                27
            ],
            "Quality and Reliability Issues (synthesis)": [
                2,
                15,
                18,
                23,
                25,
                27,
                34,
                37,
                38
            ],
            "Alternative Approaches (list)": [
                0,
                1,
                5,
                22,
                25,
                29,
                34,
                38
            ],
            "Best Practices for Translation-Based Datasets (list)": [
                2,
                14,
                16,
                27,
                31,
                32,
                33
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Background on Multilingual Evaluation Datasets",
                "tldr": "Multilingual evaluation datasets are essential for assessing language models across diverse languages, with most created through translation from English, independent collection, or compilation of similar tasks. While translation-based approaches offer parallel evaluation sets for measuring cross-lingual transfer capabilities, they introduce biases related to the source language and culture. (20 sources)",
                "text": "\nMultilingual evaluation datasets have become increasingly important for assessing the performance of language models across diverse languages. However, there remains a significant imbalance in resource availability, with most NLP resources being English-specific <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Current evaluation benchmarks often lack good coverage of low-resource languages, are restricted to specific domains, or suffer from quality issues due to semi-automatic construction methods <Paper corpusId=\"235358129\" paperTitle=\"(Goyal et al., 2021)\" isShortName></Paper>. This disproportionate focus on English and a handful of high-resource languages limits our understanding of how language models perform in multilingual and lower-resource scenarios <Paper corpusId=\"277621694\" paperTitle=\"(Luo et al., 2025)\" isShortName></Paper>.\n\nResearchers have developed multilingual datasets primarily through three approaches <Paper corpusId=\"270285718\" paperTitle=\"(Sakai et al., 2024)\" isShortName></Paper>:\n\n1. **Translation from existing datasets**: Many datasets are created by translating English resources into other languages, such as XNLI <Paper corpusId=\"52271711\" paperTitle=\"(Conneau et al., 2018)\" isShortName></Paper>, XQuAD <Paper corpusId=\"204901567\" paperTitle=\"(Artetxe et al., 2019)\" isShortName></Paper>, and MLQA <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>. These translations can be human-verified, as in AfriXNLI, or rely solely on machine translation <Paper corpusId=\"273229002\" paperTitle=\"(Kargaran et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260334562\" paperTitle=\"(Lai et al., 2023)\" isShortName></Paper>.\n\n2. **Independent collection across languages**: Some datasets like TyDi QA <Paper corpusId=\"212657414\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper> and XLSum <Paper corpusId=\"235658519\" paperTitle=\"(Hasan et al., 2021)\" isShortName></Paper> are created by collecting and annotating data in multiple languages separately, resulting in non-parallel datasets where the size may vary between languages.\n\n3. **Compilation of similar tasks**: This approach involves gathering similar tasks across multiple languages to create comprehensive benchmarks like XTREME <Paper corpusId=\"214641214\" paperTitle=\"(Hu et al., 2020)\" isShortName></Paper> and XGLUE <Paper corpusId=\"214794966\" paperTitle=\"(Liang et al., 2020)\" isShortName></Paper>.\n\nTranslation-based approaches offer two significant advantages: they leverage existing professional translation services rather than requiring expertise in multiple languages, and they create parallel evaluation sets that provide meaningful measures of cross-lingual transfer gaps between models <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. However, this approach also introduces biases related to translations and cultural representation, potentially affecting experimental conclusions by reflecting the origin culture <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>.\n\nRecent efforts have expanded the range of multilingual benchmarks beyond translation-based approaches. These include probing benchmarks for assessing linguistic properties like MELA <Paper corpusId=\"265213331\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> and multilingual knowledge probing datasets such as MLAMA <Paper corpusId=\"231740666\" paperTitle=\"(Kassner et al., 2021)\" isShortName></Paper> and BMIKE-53 <Paper corpusId=\"276647924\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>. Additionally, researchers have developed datasets for natural language generation tasks like summarization <Paper corpusId=\"216868259\" paperTitle=\"(Scialom et al., 2020)\" isShortName></Paper> <Paper corpusId=\"218470403\" paperTitle=\"(Mueller et al., 2020)\" isShortName></Paper>.\n\nSome researchers argue for avoiding benchmarks where English plays a special role, including datasets constructed in English and translated to other languages, as well as zero-shot transfer tasks where English is the sole source language <Paper corpusId=\"258187051\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>. This perspective emphasizes the need for linguistically diverse evaluation tasks that are free from language bias and well-balanced across languages.",
                "citations": [
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training",
                            ".Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.85693359375
                    },
                    {
                        "id": "(Goyal et al., 2021)",
                        "snippets": [
                            "Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures."
                        ],
                        "paper": {
                            "corpus_id": 235358129,
                            "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
                            "authors": [
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "2107063269",
                                    "name": "Cynthia Gao"
                                },
                                {
                                    "authorId": "113810201",
                                    "name": "Vishrav Chaudhary"
                                },
                                {
                                    "authorId": "2158170998",
                                    "name": "Peng-Jen Chen"
                                },
                                {
                                    "authorId": "2293203",
                                    "name": "Guillaume Wenzek"
                                },
                                {
                                    "authorId": "3092435",
                                    "name": "Da Ju"
                                },
                                {
                                    "authorId": "1491706730",
                                    "name": "Sanjan Krishnan"
                                },
                                {
                                    "authorId": "1706809",
                                    "name": "Marc'Aurelio Ranzato"
                                },
                                {
                                    "authorId": "144204682",
                                    "name": "Francisco Guzm\u00e1n"
                                },
                                {
                                    "authorId": "144270981",
                                    "name": "Angela Fan"
                                }
                            ],
                            "year": 2021,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 587
                        },
                        "score": 0.7666015625
                    },
                    {
                        "id": "(Luo et al., 2025)",
                        "snippets": [
                            "Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios."
                        ],
                        "paper": {
                            "corpus_id": 277621694,
                            "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2325478503",
                                    "name": "Hengyu Luo"
                                },
                                {
                                    "authorId": "2312330893",
                                    "name": "Zihao Li"
                                },
                                {
                                    "authorId": "2308478088",
                                    "name": "Joseph Attieh"
                                },
                                {
                                    "authorId": "2354172398",
                                    "name": "Sawal Devkota"
                                },
                                {
                                    "authorId": "51436367",
                                    "name": "Ona de Gibert"
                                },
                                {
                                    "authorId": "2312353271",
                                    "name": "Shaoxiong Ji"
                                },
                                {
                                    "authorId": "2266791225",
                                    "name": "Peiqin Lin"
                                },
                                {
                                    "authorId": "2354176011",
                                    "name": "Bhavani Sai Praneeth Varma Mantina"
                                },
                                {
                                    "authorId": "2329094725",
                                    "name": "Ananda Sreenidhi"
                                },
                                {
                                    "authorId": "2257040447",
                                    "name": "Ra'ul V'azquez"
                                },
                                {
                                    "authorId": "2355403639",
                                    "name": "Mengjie Wang"
                                },
                                {
                                    "authorId": "2354177102",
                                    "name": "Samea Yusofi"
                                },
                                {
                                    "authorId": "2237430242",
                                    "name": "J\u00f6rg Tiedemann"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.76171875
                    },
                    {
                        "id": "(Sakai et al., 2024)",
                        "snippets": [
                            "Multilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 270285718,
                            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
                            "authors": [
                                {
                                    "authorId": "2204051777",
                                    "name": "Yusuke Sakai"
                                },
                                {
                                    "authorId": "2300756",
                                    "name": "Hidetaka Kamigaito"
                                },
                                {
                                    "authorId": "2266807418",
                                    "name": "Taro Watanabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 5
                        },
                        "score": 0.7333984375
                    },
                    {
                        "id": "(Conneau et al., 2018)",
                        "snippets": [
                            "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
                        ],
                        "paper": {
                            "corpus_id": 52271711,
                            "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
                            "authors": [
                                {
                                    "authorId": "2480903",
                                    "name": "Alexis Conneau"
                                },
                                {
                                    "authorId": "1830914",
                                    "name": "Guillaume Lample"
                                },
                                {
                                    "authorId": "1905713",
                                    "name": "Ruty Rinott"
                                },
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                },
                                {
                                    "authorId": "144518416",
                                    "name": "Holger Schwenk"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1388
                        },
                        "score": 0
                    },
                    {
                        "id": "(Artetxe et al., 2019)",
                        "snippets": [
                            "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators."
                        ],
                        "paper": {
                            "corpus_id": 204901567,
                            "title": "On the Cross-lingual Transferability of Monolingual Representations",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 799
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2019)",
                        "snippets": [
                            "Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."
                        ],
                        "paper": {
                            "corpus_id": 204734128,
                            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "1905713",
                                    "name": "Ruty Rinott"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "144518416",
                                    "name": "Holger Schwenk"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 509
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Kargaran et al., 2024)",
                        "snippets": [
                            "These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation (Lai et al., 2023). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages."
                        ],
                        "paper": {
                            "corpus_id": 273229002,
                            "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
                            "authors": [
                                {
                                    "authorId": "98623604",
                                    "name": "Amir Hossein Kargaran"
                                },
                                {
                                    "authorId": "2054744",
                                    "name": "Ali Modarressi"
                                },
                                {
                                    "authorId": "2210982230",
                                    "name": "Nafiseh Nikeghbal"
                                },
                                {
                                    "authorId": "2261085110",
                                    "name": "Jana Diesner"
                                },
                                {
                                    "authorId": "2302797923",
                                    "name": "Franccois Yvon"
                                },
                                {
                                    "authorId": "2130001188",
                                    "name": "Hinrich Schutze"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.8271484375
                    },
                    {
                        "id": "(Lai et al., 2023)",
                        "snippets": [
                            "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi."
                        ],
                        "paper": {
                            "corpus_id": 260334562,
                            "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
                            "authors": [
                                {
                                    "authorId": "1405279380",
                                    "name": "Viet Dac Lai"
                                },
                                {
                                    "authorId": "2186540882",
                                    "name": "Chien Van Nguyen"
                                },
                                {
                                    "authorId": "1692755523",
                                    "name": "Nghia Trung Ngo"
                                },
                                {
                                    "authorId": "2116085512",
                                    "name": "Thuat Nguyen"
                                },
                                {
                                    "authorId": "2462276",
                                    "name": "Franck Dernoncourt"
                                },
                                {
                                    "authorId": "2066337266",
                                    "name": "Ryan A. Rossi"
                                },
                                {
                                    "authorId": "1811211",
                                    "name": "Thien Huu Nguyen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 150
                        },
                        "score": 0
                    },
                    {
                        "id": "(Clark et al., 2020)",
                        "snippets": [
                            "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation."
                        ],
                        "paper": {
                            "corpus_id": 212657414,
                            "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
                            "authors": [
                                {
                                    "authorId": "144797264",
                                    "name": "J. Clark"
                                },
                                {
                                    "authorId": "2890423",
                                    "name": "Eunsol Choi"
                                },
                                {
                                    "authorId": "123052390",
                                    "name": "Michael Collins"
                                },
                                {
                                    "authorId": "2758616",
                                    "name": "Dan Garrette"
                                },
                                {
                                    "authorId": "15652489",
                                    "name": "T. Kwiatkowski"
                                },
                                {
                                    "authorId": "48942032",
                                    "name": "Vitaly Nikolaev"
                                },
                                {
                                    "authorId": "52578817",
                                    "name": "J. Palomaki"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 612
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hasan et al., 2021)",
                        "snippets": [
                            "Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present XL-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model, with XL-Sum and experiment on multilingual and low-resource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at \\url{https://github.com/csebuetnlp/xl-sum}."
                        ],
                        "paper": {
                            "corpus_id": 235658519,
                            "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages",
                            "authors": [
                                {
                                    "authorId": "1400373232",
                                    "name": "Tahmid Hasan"
                                },
                                {
                                    "authorId": "49785688",
                                    "name": "Abhik Bhattacharjee"
                                },
                                {
                                    "authorId": null,
                                    "name": "Md Saiful Islam"
                                },
                                {
                                    "authorId": "1955443619",
                                    "name": "Kazi Samin Mubasshir"
                                },
                                {
                                    "authorId": "4495301",
                                    "name": "Yuan-Fang Li"
                                },
                                {
                                    "authorId": "145059666",
                                    "name": "Yong-Bin Kang"
                                },
                                {
                                    "authorId": "2218125433",
                                    "name": "M. Rahman"
                                },
                                {
                                    "authorId": "2046603",
                                    "name": "Rifat Shahriyar"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 372
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hu et al., 2020)",
                        "snippets": [
                            "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks."
                        ],
                        "paper": {
                            "corpus_id": 214641214,
                            "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
                            "authors": [
                                {
                                    "authorId": "2149221827",
                                    "name": "Junjie Hu"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "9356387",
                                    "name": "Aditya Siddhant"
                                },
                                {
                                    "authorId": "1700325",
                                    "name": "Graham Neubig"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "145657834",
                                    "name": "Melvin Johnson"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 976
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liang et al., 2020)",
                        "snippets": [
                            "In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison."
                        ],
                        "paper": {
                            "corpus_id": 214794966,
                            "title": "XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "3887469",
                                    "name": "Yaobo Liang"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "2068345080",
                                    "name": "Ning Wu"
                                },
                                {
                                    "authorId": "2239234",
                                    "name": "Fenfei Guo"
                                },
                                {
                                    "authorId": "15629561",
                                    "name": "Weizhen Qi"
                                },
                                {
                                    "authorId": "50175330",
                                    "name": "Ming Gong"
                                },
                                {
                                    "authorId": "24962156",
                                    "name": "Linjun Shou"
                                },
                                {
                                    "authorId": "71790825",
                                    "name": "Daxin Jiang"
                                },
                                {
                                    "authorId": "3320836",
                                    "name": "Guihong Cao"
                                },
                                {
                                    "authorId": "48025819",
                                    "name": "Xiaodong Fan"
                                },
                                {
                                    "authorId": "2116121771",
                                    "name": "Bruce Zhang"
                                },
                                {
                                    "authorId": "2054897723",
                                    "name": "Rahul Agrawal"
                                },
                                {
                                    "authorId": "144530394",
                                    "name": "Edward Cui"
                                },
                                {
                                    "authorId": "2111326903",
                                    "name": "Sining Wei"
                                },
                                {
                                    "authorId": "1490606819",
                                    "name": "Taroon Bharti"
                                },
                                {
                                    "authorId": "2056923187",
                                    "name": "Ying Qiao"
                                },
                                {
                                    "authorId": "33926030",
                                    "name": "Jiun-Hung Chen"
                                },
                                {
                                    "authorId": "2110027452",
                                    "name": "Winnie Wu"
                                },
                                {
                                    "authorId": "2108002297",
                                    "name": "Shuguang Liu"
                                },
                                {
                                    "authorId": "145338263",
                                    "name": "Fan Yang"
                                },
                                {
                                    "authorId": "144081089",
                                    "name": "Daniel Fernando Campos"
                                },
                                {
                                    "authorId": "32431940",
                                    "name": "Rangan Majumder"
                                },
                                {
                                    "authorId": "92660691",
                                    "name": "Ming Zhou"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 350
                        },
                        "score": 0
                    },
                    {
                        "id": "(Etxaniz et al., 2024)",
                        "snippets": [
                            "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."
                        ],
                        "paper": {
                            "corpus_id": 270380088,
                            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                            "authors": [
                                {
                                    "authorId": "2226458991",
                                    "name": "Julen Etxaniz"
                                },
                                {
                                    "authorId": "2481918",
                                    "name": "Gorka Azkune"
                                },
                                {
                                    "authorId": "2260104163",
                                    "name": "A. Soroa"
                                },
                                {
                                    "authorId": "2251043402",
                                    "name": "Oier L\u00f3pez de Lacalle"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 11
                        },
                        "score": 0.8037109375
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability -- MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language -- Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks. Our data is available at https://github.com/sjtu-compling/MELA."
                        ],
                        "paper": {
                            "corpus_id": 265213331,
                            "title": "MELA: Multilingual Evaluation of Linguistic Acceptability",
                            "authors": [
                                {
                                    "authorId": "2267343117",
                                    "name": "Ziyin Zhang"
                                },
                                {
                                    "authorId": "2214607250",
                                    "name": "Yikang Liu"
                                },
                                {
                                    "authorId": "2000111554",
                                    "name": "Wei Huang"
                                },
                                {
                                    "authorId": "2266752587",
                                    "name": "Junyu Mao"
                                },
                                {
                                    "authorId": "2266813948",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "2118878378",
                                    "name": "Hai Hu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kassner et al., 2021)",
                        "snippets": [
                            "Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as \"Paris is the capital of [MASK]\" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT\u2019s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin."
                        ],
                        "paper": {
                            "corpus_id": 231740666,
                            "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "9529535",
                                    "name": "Nora Kassner"
                                },
                                {
                                    "authorId": "35501453",
                                    "name": "Philipp Dufter"
                                },
                                {
                                    "authorId": "144418438",
                                    "name": "Hinrich Sch\u00fctze"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 141
                        },
                        "score": 0
                    },
                    {
                        "id": "(He et al., 2025)",
                        "snippets": [
                            "Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, (Zhang et al., 2023) introduced MELA to assess multilingual linguistic acceptability, while (Mueller et al., 2020) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA (Kassner et al., 2021), BMLAMA (Qi et al., 2023), and BMIKE-53 (Nie et al., 2024) have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency. \n\nBeyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020)   have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), and MLQA (Lewis et al., 2019). On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization (Scialom et al., 2020)(Ladhak et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 276647924,
                            "title": "XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs",
                            "authors": [
                                {
                                    "authorId": "2257338995",
                                    "name": "Linyang He"
                                },
                                {
                                    "authorId": "2197254657",
                                    "name": "Ercong Nie"
                                },
                                {
                                    "authorId": "2294424733",
                                    "name": "Sukru Samet Dindar"
                                },
                                {
                                    "authorId": "2321723374",
                                    "name": "Arsalan Firoozi"
                                },
                                {
                                    "authorId": "2311441706",
                                    "name": "Adrian Florea"
                                },
                                {
                                    "authorId": "2349460444",
                                    "name": "Van Nguyen"
                                },
                                {
                                    "authorId": "2174811177",
                                    "name": "Corentin Puffay"
                                },
                                {
                                    "authorId": "2347533359",
                                    "name": "Riki Shimizu"
                                },
                                {
                                    "authorId": "2278833873",
                                    "name": "Haotian Ye"
                                },
                                {
                                    "authorId": "2293392156",
                                    "name": "Jonathan Brennan"
                                },
                                {
                                    "authorId": "2256998691",
                                    "name": "Helmut Schmid"
                                },
                                {
                                    "authorId": "2261745622",
                                    "name": "Hinrich Schutze"
                                },
                                {
                                    "authorId": "1686269",
                                    "name": "N. Mesgarani"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.767578125
                    },
                    {
                        "id": "(Scialom et al., 2020)",
                        "snippets": [
                            "We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset."
                        ],
                        "paper": {
                            "corpus_id": 216868259,
                            "title": "MLSUM: The Multilingual Summarization Corpus",
                            "authors": [
                                {
                                    "authorId": "90745780",
                                    "name": "Thomas Scialom"
                                },
                                {
                                    "authorId": "1502219404",
                                    "name": "Paul-Alexis Dray"
                                },
                                {
                                    "authorId": "1782552",
                                    "name": "S. Lamprier"
                                },
                                {
                                    "authorId": "1703777",
                                    "name": "Benjamin Piwowarski"
                                },
                                {
                                    "authorId": "1767493",
                                    "name": "Jacopo Staiano"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 177
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mueller et al., 2020)",
                        "snippets": [
                            "A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models\u2019 ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages."
                        ],
                        "paper": {
                            "corpus_id": 218470403,
                            "title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models",
                            "authors": [
                                {
                                    "authorId": "49355602",
                                    "name": "Aaron Mueller"
                                },
                                {
                                    "authorId": "40156252",
                                    "name": "Garrett Nicolai"
                                },
                                {
                                    "authorId": "1432608177",
                                    "name": "Panayiota Petrou-Zeniou"
                                },
                                {
                                    "authorId": "147714277",
                                    "name": "N. Talmina"
                                },
                                {
                                    "authorId": "2467508",
                                    "name": "Tal Linzen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 57
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chung et al., 2023)",
                        "snippets": [
                            "In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language."
                        ],
                        "paper": {
                            "corpus_id": 258187051,
                            "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining",
                            "authors": [
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "40832517",
                                    "name": "Noah Constant"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 57
                        },
                        "score": 0.75341796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Advantages of Translating English Datasets",
                "tldr": "Translating existing English datasets offers a cost-effective way to create multilingual evaluation resources with parallel data across languages, enabling direct cross-lingual performance comparisons and addressing the scarcity of resources in low-resource languages. (12 sources)",
                "text": "\nTranslating existing English-language datasets into other languages has emerged as a prevalent approach for creating multilingual evaluation resources due to several key advantages. First, translation leverages existing professional translation services rather than requiring expertise in multiple languages, making it a practical and accessible method for expanding language coverage <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. This approach is significantly more cost-effective and time-efficient compared to creating new datasets from scratch with human annotators across multiple languages <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper>.\n\nPerhaps most importantly, translation creates parallel evaluation sets that provide meaningful measures of cross-lingual transfer capabilities between models <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. This parallel structure allows researchers to attribute performance differences to language understanding versus task knowledge, isolating the factors that affect cross-lingual generalization <Paper corpusId=\"269362363\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>. For instance, machine-translated test sets enable direct evaluation of \"translate test\" cross-lingual transfer approaches <Paper corpusId=\"246294502\" paperTitle=\"(Bugliarello et al., 2022)\" isShortName></Paper>.\n\nTranslation-based approaches also help address the significant challenge of labeled data scarcity in non-English languages. In real-world applications, labeled data is often available primarily in high-resource languages like English but sparse or nonexistent in others <Paper corpusId=\"235097287\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. By translating existing high-quality English benchmarks, researchers can rapidly expand evaluation resources to multiple languages <Paper corpusId=\"276249513\" paperTitle=\"(Gomes et al., 2025)\" isShortName></Paper>.\n\nThe translation approach has proven particularly valuable for specific tasks like information retrieval, where a common problem with multilingual datasets is their low number of judgments per query. Translating densely annotated English benchmarks, such as TREC's Robust 2004, to multiple languages helps mitigate this issue, providing higher-quality evaluation resources <Paper corpusId=\"252568105\" paperTitle=\"(Jeronymo et al., 2022)\" isShortName></Paper>.\n\nAdditionally, translation-based datasets have demonstrated utility in improving multilingual model performance. Researchers have shown that using machine translation as a form of cross-lingual data augmentation can significantly enhance results on multilingual benchmarks <Paper corpusId=\"235097287\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. In retrieval tasks, studies have found a positive correlation between translation quality and retrieval effectiveness, suggesting that improvements in translation methods directly contribute to better multilingual information retrieval performance <Paper corpusId=\"237364084\" paperTitle=\"(Bonifacio et al., 2021)\" isShortName></Paper>.\n\nThe translation approach has been successfully applied across various benchmark types, from natural language inference tasks like XNLI <Paper corpusId=\"269362697\" paperTitle=\"(Moon et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52271711\" paperTitle=\"(Conneau et al., 2018)\" isShortName></Paper> to open-domain question answering evaluations like MKQA, which comprises 10,000 question-answer pairs aligned across 26 typologically diverse languages <Paper corpusId=\"220871404\" paperTitle=\"(Longpre et al., 2020)\" isShortName></Paper>. In multimodal contexts, translated English captions have been widely used to build multilingual datasets for tasks like image retrieval <Paper corpusId=\"237532506\" paperTitle=\"(Aggarwal et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training",
                            ".Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.85693359375
                    },
                    {
                        "id": "(Thellmann et al., 2024)",
                        "snippets": [
                            "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 273323209,
                            "title": "Towards Multilingual LLM Evaluation for European Languages",
                            "authors": [
                                {
                                    "authorId": "2269539",
                                    "name": "Klaudia Thellmann"
                                },
                                {
                                    "authorId": "2325726089",
                                    "name": "Bernhard Stadler"
                                },
                                {
                                    "authorId": "2258551638",
                                    "name": "Michael Fromm"
                                },
                                {
                                    "authorId": "2258550838",
                                    "name": "Jasper Schulze Buschhoff"
                                },
                                {
                                    "authorId": "2325726070",
                                    "name": "Alex Jude"
                                },
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2258552277",
                                    "name": "Johannes Leveling"
                                },
                                {
                                    "authorId": "2347259072",
                                    "name": "Nicolas Flores-Herr"
                                },
                                {
                                    "authorId": "2330411614",
                                    "name": "Joachim K\u00f6hler"
                                },
                                {
                                    "authorId": "2330412312",
                                    "name": "Ren\u00e9 J\u00e4kel"
                                },
                                {
                                    "authorId": "2258668067",
                                    "name": "Mehdi Ali"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Singh et al., 2024)",
                        "snippets": [
                            "We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization"
                        ],
                        "paper": {
                            "corpus_id": 269362363,
                            "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
                            "authors": [
                                {
                                    "authorId": "2119151340",
                                    "name": "Harman Singh"
                                },
                                {
                                    "authorId": "2285178",
                                    "name": "Nitish Gupta"
                                },
                                {
                                    "authorId": "2136381352",
                                    "name": "Shikhar Bharadwaj"
                                },
                                {
                                    "authorId": "2298270541",
                                    "name": "Dinesh Tewari"
                                },
                                {
                                    "authorId": "2298271272",
                                    "name": "Partha Talukdar"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.7060546875
                    },
                    {
                        "id": "(Bugliarello et al., 2022)",
                        "snippets": [
                            "We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer."
                        ],
                        "paper": {
                            "corpus_id": 246294502,
                            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
                            "authors": [
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "144097210",
                                    "name": "Fangyu Liu"
                                },
                                {
                                    "authorId": "153733568",
                                    "name": "Jonas Pfeiffer"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 64
                        },
                        "score": 0.77392578125
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples."
                        ],
                        "paper": {
                            "corpus_id": 235097287,
                            "title": "Practical Transformer-based Multilingual Text Classification",
                            "authors": [
                                {
                                    "authorId": "2108725400",
                                    "name": "Cindy Wang"
                                },
                                {
                                    "authorId": "2339397",
                                    "name": "Michele Banko"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.70166015625
                    },
                    {
                        "id": "(Gomes et al., 2025)",
                        "snippets": [
                            "Using a machine translation scheme with quality filtering, we can cost-effectively extend well-established Englishcentric benchmarks to multiple languages, without compromising benchmark quality and validity, which can be very beneficial for the finetuning and evaluation of new, multilingual evaluation models."
                        ],
                        "paper": {
                            "corpus_id": 276249513,
                            "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
                            "authors": [
                                {
                                    "authorId": "2282539942",
                                    "name": "Gonccalo Gomes"
                                },
                                {
                                    "authorId": "36259430",
                                    "name": "Chrysoula Zerva"
                                },
                                {
                                    "authorId": "2344751390",
                                    "name": "Bruno Martins"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Jeronymo et al., 2022)",
                        "snippets": [
                            "A key ingredient in the development of information retrieval algorithms are reusable evaluation datasets [7,19,20]. For English, there are a number of such datasets. For other languages, there are initiatives such TREC CLIR [17], CLEF [13,[4][5][6], FIRE [12,11], NTCIR [16] and more recently HC4 [9]. A common problem with these multilingual IR datasets is their low number of judgments per query, that is, the number of documents marked as relevant or not relevant per query. For example, in the multilingual datasets mMARCO [3] and Mr.Tydi [21], there is only one or two documents manually marked as relevant per query. These \"sparse\" annotations, we argue, prevent correct evaluations of retrieval methods. For example, the RM3 query expansion method evaluated on the MS MARCO benchmark [1], which uses sparse annotations, shows no improvement over baselines such as BM25 [10]. However, the same method shows significant improvements over BM25 when evaluated on densely annotated benchmarks, such as TREC-DL.\n\nIn this work, to mitigate the issue with sparse annotations on multilingual IR datasets, we translate the TREC's Robust 2004 benchmark [18], an English dataset with a high number of judgments, to 8 languages using a high-quality automatic translator. We call this dataset mRobust04."
                        ],
                        "paper": {
                            "corpus_id": 252568105,
                            "title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
                            "authors": [
                                {
                                    "authorId": "2167031295",
                                    "name": "Vitor Jeronymo"
                                },
                                {
                                    "authorId": "2186302660",
                                    "name": "Mauricio Nascimento"
                                },
                                {
                                    "authorId": "2066179820",
                                    "name": "R. Lotufo"
                                },
                                {
                                    "authorId": "143744603",
                                    "name": "Rodrigo Nogueira"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.80810546875
                    },
                    {
                        "id": "(Bonifacio et al., 2021)",
                        "snippets": [
                            "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages",
                            ".researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets",
                            ".we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval."
                        ],
                        "paper": {
                            "corpus_id": 237364084,
                            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
                            "authors": [
                                {
                                    "authorId": "2003019597",
                                    "name": "L. Bonifacio"
                                },
                                {
                                    "authorId": "1472908948",
                                    "name": "Israel Campiotti"
                                },
                                {
                                    "authorId": "1809633",
                                    "name": "R. Lotufo"
                                },
                                {
                                    "authorId": "143744603",
                                    "name": "Rodrigo Nogueira"
                                }
                            ],
                            "year": 2021,
                            "venue": "",
                            "n_citations": 113
                        },
                        "score": 0.78759765625
                    },
                    {
                        "id": "(Moon et al., 2024)",
                        "snippets": [
                            "For validation, we select multilingual benchmark tasks in which the maintenance of the interrelation among data components plays a critical role. Specifically, we adopt the XNLI dataset (Conneau et al., 2018) and select two tasks in an XGLUE benchmark (Liang et al., 2020): Web Page Ranking (WPR) and Question Generation (QG). We construct training data for up to five languages (German, French, Chinese, Hindi, and Vietnamese) by translating the English dataset existing within each dataset. Subsequently, by evaluating the performance of the models trained on each translated data, we estimate the validity of each data translation strategy."
                        ],
                        "paper": {
                            "corpus_id": 269362697,
                            "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
                            "authors": [
                                {
                                    "authorId": "2101318920",
                                    "name": "Hyeonseok Moon"
                                },
                                {
                                    "authorId": "2275193495",
                                    "name": "Seungyoon Lee"
                                },
                                {
                                    "authorId": "2298449941",
                                    "name": "Seongtae Hong"
                                },
                                {
                                    "authorId": "2144498836",
                                    "name": "Seungjun Lee"
                                },
                                {
                                    "authorId": "2115195904",
                                    "name": "Chanjun Park"
                                },
                                {
                                    "authorId": "83056580",
                                    "name": "Heu-Jeoung Lim"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.7421875
                    },
                    {
                        "id": "(Conneau et al., 2018)",
                        "snippets": [
                            "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
                        ],
                        "paper": {
                            "corpus_id": 52271711,
                            "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
                            "authors": [
                                {
                                    "authorId": "2480903",
                                    "name": "Alexis Conneau"
                                },
                                {
                                    "authorId": "1830914",
                                    "name": "Guillaume Lample"
                                },
                                {
                                    "authorId": "1905713",
                                    "name": "Ruty Rinott"
                                },
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                },
                                {
                                    "authorId": "144518416",
                                    "name": "Holger Schwenk"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1388
                        },
                        "score": 0
                    },
                    {
                        "id": "(Longpre et al., 2020)",
                        "snippets": [
                            "Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages."
                        ],
                        "paper": {
                            "corpus_id": 220871404,
                            "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "2143774328",
                                    "name": "Yi Lu"
                                },
                                {
                                    "authorId": "1696803",
                                    "name": "Joachim Daiber"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 160
                        },
                        "score": 0.8408203125
                    },
                    {
                        "id": "(Aggarwal et al., 2021)",
                        "snippets": [
                            "Multi-lingual dataset collection has always been a major hurdle when it comes to building models in a one-model-fits-all style that can provide good results for image retrieval across multiple languages. Most methods [22,28](Rotman et al., 2018) rely on direct translations of English captions while others (Gella et al., 2017)(Huang et al., 2019) have used independent image and language text pairs."
                        ],
                        "paper": {
                            "corpus_id": 237532506,
                            "title": "Towards Zero-shot Cross-lingual Image Retrieval and Tagging",
                            "authors": [
                                {
                                    "authorId": "30141435",
                                    "name": "Pranav Aggarwal"
                                },
                                {
                                    "authorId": "51251519",
                                    "name": "Ritiz Tambi"
                                },
                                {
                                    "authorId": "37493415",
                                    "name": "Ajinkya Kale"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.7958984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges in Building Translation-Based Datasets",
                "tldr": "Translation-based multilingual datasets face challenges including quality issues from machine translation errors, cultural biases that compromise authenticity, and difficulties transferring labels for specific NLP tasks, all of which can affect evaluation reliability. (12 sources)",
                "text": "\nDespite its advantages, creating multilingual evaluation datasets through translation introduces several significant challenges. One fundamental concern is the quality and naturalness of the translated content. Translation can introduce artifacts such as reduced fluency, unnatural phrasing, and \"translationese\" that may inadvertently affect model performance and evaluation reliability <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. When machine translation is used, the quality heavily depends on the capabilities of the language model employed, with particular concerns for medium and low-resource languages where translation accuracy may be compromised <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259095896\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe translation process often fails to capture cultural nuances and language-specific expressions, potentially overlooking subtle linguistic or cultural differences <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. As Kim et al. demonstrate, language models often fail to capture biases in different languages due to cultural insensitivity <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This limitation is particularly problematic as research has shown that integrating cultural knowledge can significantly improve model performance <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247762223\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nTranslation-based approaches tend to bend the target language to match the structural and lexical properties of the source language, compromising the naturalness and idiomaticity of the resulting text <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. This issue can lead to evaluation benchmarks that fail to adequately measure system performance in the target language <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. Even when human translators are employed, achieving high-quality translations that maintain both cross-lingual alignment and linguistic authenticity requires careful selection of translators and allowing for necessary target-language adjustments <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>.\n\nAnother significant challenge lies in transferring labels or annotations across languages. While translation works well for some tasks, it can be problematic for others, particularly structure-dependent tasks like part-of-speech tagging and named entity recognition <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Specific linguistic features of target languages, such as pronoun-dropping, compounding, and inflection, can degrade dataset quality and model performance <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\nFor certain tasks, finding naturally parallel documents is essential to avoid the unnatural quality that can result from manual translation at scale <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>. However, such naturally occurring parallel content may not be available for all language pairs or domains.\n\nThe reliability of translation-based evaluation can be further compromised when models are trained on translated data, potentially leading to artificially inflated performance metrics <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Xu et al. note that translated benchmarks may introduce additional biases due to translation errors and cultural differences <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"135465247\" paperTitle=\"(Lauscher et al., 2019)\" isShortName></Paper>.\n\nThese challenges highlight that naively translating existing evaluation benchmarks can introduce artifacts that affect judgment and overlook multilingual-specific nuances such as code-switching or unique linguistic patterns <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. Recent research suggests that translation-based methods may generate less natural, less representative target language content, which can affect the validity of evaluations <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, studies have shown that machine translation can be a viable strategy for transferring certain tasks across languages, with native speakers judging more than 83% of machine-translated relation extraction instances to be linguistically and semantically acceptable <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>. However, achieving this level of quality requires careful quality control measures and an awareness of the specific linguistic features that may affect translation success.",
                "citations": [
                    {
                        "id": "(Son et al., 2024)",
                        "snippets": [
                            "However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."
                        ],
                        "paper": {
                            "corpus_id": 273532388,
                            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
                            "authors": [
                                {
                                    "authorId": "2321452295",
                                    "name": "Guijin Son"
                                },
                                {
                                    "authorId": "29830817",
                                    "name": "Dongkeun Yoon"
                                },
                                {
                                    "authorId": "2299329316",
                                    "name": "Juyoung Suk"
                                },
                                {
                                    "authorId": "2301578911",
                                    "name": "Javier Aula-Blasco"
                                },
                                {
                                    "authorId": "2327215494",
                                    "name": "Mano Aslan"
                                },
                                {
                                    "authorId": "2327216625",
                                    "name": "Vu Trong Kim"
                                },
                                {
                                    "authorId": "2232783785",
                                    "name": "Shayekh Bin Islam"
                                },
                                {
                                    "authorId": "2327215436",
                                    "name": "Jaume Prats-Cristi\u00e0"
                                },
                                {
                                    "authorId": "2327217057",
                                    "name": "Luc\u00eda Tormo-Ba\u00f1uelos"
                                },
                                {
                                    "authorId": "2184037220",
                                    "name": "Seungone Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.72900390625
                    },
                    {
                        "id": "(Thellmann et al., 2024)",
                        "snippets": [
                            "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 273323209,
                            "title": "Towards Multilingual LLM Evaluation for European Languages",
                            "authors": [
                                {
                                    "authorId": "2269539",
                                    "name": "Klaudia Thellmann"
                                },
                                {
                                    "authorId": "2325726089",
                                    "name": "Bernhard Stadler"
                                },
                                {
                                    "authorId": "2258551638",
                                    "name": "Michael Fromm"
                                },
                                {
                                    "authorId": "2258550838",
                                    "name": "Jasper Schulze Buschhoff"
                                },
                                {
                                    "authorId": "2325726070",
                                    "name": "Alex Jude"
                                },
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2258552277",
                                    "name": "Johannes Leveling"
                                },
                                {
                                    "authorId": "2347259072",
                                    "name": "Nicolas Flores-Herr"
                                },
                                {
                                    "authorId": "2330411614",
                                    "name": "Joachim K\u00f6hler"
                                },
                                {
                                    "authorId": "2330412312",
                                    "name": "Ren\u00e9 J\u00e4kel"
                                },
                                {
                                    "authorId": "2258668067",
                                    "name": "Mehdi Ali"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names)."
                        ],
                        "paper": {
                            "corpus_id": 259095896,
                            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2109472880",
                                    "name": "Yuwei Yin"
                                },
                                {
                                    "authorId": "2117947438",
                                    "name": "Shicheng Li"
                                },
                                {
                                    "authorId": "2146034504",
                                    "name": "Liang Chen"
                                },
                                {
                                    "authorId": "144202874",
                                    "name": "Peiyi Wang"
                                },
                                {
                                    "authorId": "1906099",
                                    "name": "Shuhuai Ren"
                                },
                                {
                                    "authorId": "2027599235",
                                    "name": "Mukai Li"
                                },
                                {
                                    "authorId": "14646700",
                                    "name": "Yazheng Yang"
                                },
                                {
                                    "authorId": "47883405",
                                    "name": "Jingjing Xu"
                                },
                                {
                                    "authorId": "2116530295",
                                    "name": "Xu Sun"
                                },
                                {
                                    "authorId": "47648549",
                                    "name": "Lingpeng Kong"
                                },
                                {
                                    "authorId": "2144831944",
                                    "name": "Qi Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 120
                        },
                        "score": 0.74072265625
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, (Ma et al., 2022) emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages."
                        ],
                        "paper": {
                            "corpus_id": 268357672,
                            "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
                            "authors": [
                                {
                                    "authorId": "2283821804",
                                    "name": "Eunsu Kim"
                                },
                                {
                                    "authorId": "2299329316",
                                    "name": "Juyoung Suk"
                                },
                                {
                                    "authorId": "2262214736",
                                    "name": "Philhoon Oh"
                                },
                                {
                                    "authorId": "47111345",
                                    "name": "Haneul Yoo"
                                },
                                {
                                    "authorId": "2262214700",
                                    "name": "James Thorne"
                                },
                                {
                                    "authorId": "2283768962",
                                    "name": "Alice Oh"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 23
                        },
                        "score": 0.7412109375
                    },
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate, manually validate, and benchmark EnCBP, a finer-grained news-based cultural background prediction dataset in English. Through language modeling (LM) evaluations and manual analyses, we confirm that there are noticeable differences in linguistic expressions among five English-speaking countries and across four states in the US. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic (PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2, Emotion, and Go-Emotions) show that, while introducing cultural background information does not benefit the Go-Emotions task due to text domain conflicts, it noticeably improves deep learning (DL) model performance on other tasks. Our findings strongly support the importance of cultural background modeling to a wide variety of NLP tasks and demonstrate the applicability of EnCBP in culture-related research."
                        ],
                        "paper": {
                            "corpus_id": 247762223,
                            "title": "EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in English",
                            "authors": [
                                {
                                    "authorId": "2227771",
                                    "name": "Weicheng Ma"
                                },
                                {
                                    "authorId": "2160540667",
                                    "name": "Samiha Datta"
                                },
                                {
                                    "authorId": "2117930921",
                                    "name": "Lili Wang"
                                },
                                {
                                    "authorId": "1918441",
                                    "name": "Soroush Vosoughi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Findings",
                            "n_citations": 8
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ponti et al., 2020)",
                        "snippets": [
                            "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."
                        ],
                        "paper": {
                            "corpus_id": 218470125,
                            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "50383712",
                                    "name": "Qianchu Liu"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 327
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."
                        ],
                        "paper": {
                            "corpus_id": 248780386,
                            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "NLPPOWER",
                            "n_citations": 16
                        },
                        "score": 0.765625
                    },
                    {
                        "id": "(Hennig et al., 2023)",
                        "snippets": [
                            "We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."
                        ],
                        "paper": {
                            "corpus_id": 258557740,
                            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
                            "authors": [
                                {
                                    "authorId": "36943315",
                                    "name": "Leonhard Hennig"
                                },
                                {
                                    "authorId": "143680610",
                                    "name": "Philippe E. Thomas"
                                },
                                {
                                    "authorId": "145733288",
                                    "name": "Sebastian M\u00f6ller"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.7333984375
                    },
                    {
                        "id": "(Lewis et al., 2019)",
                        "snippets": [
                            "Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."
                        ],
                        "paper": {
                            "corpus_id": 204734128,
                            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "1905713",
                                    "name": "Ruty Rinott"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "144518416",
                                    "name": "Holger Schwenk"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 509
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English (Lauscher et al., 2019), [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."
                        ],
                        "paper": {
                            "corpus_id": 268819377,
                            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                            "authors": [
                                {
                                    "authorId": "2257136845",
                                    "name": "Yuemei Xu"
                                },
                                {
                                    "authorId": "2258334185",
                                    "name": "Ling Hu"
                                },
                                {
                                    "authorId": "2294513520",
                                    "name": "Jiayi Zhao"
                                },
                                {
                                    "authorId": "2294361104",
                                    "name": "Zihan Qiu"
                                },
                                {
                                    "authorId": "2294363807",
                                    "name": "Yuqi Ye"
                                },
                                {
                                    "authorId": "2294933103",
                                    "name": "Hanwen Gu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Comput. Sci.",
                            "n_citations": 43
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Lauscher et al., 2019)",
                        "snippets": [
                            "Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques."
                        ],
                        "paper": {
                            "corpus_id": 135465247,
                            "title": "Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors",
                            "authors": [
                                {
                                    "authorId": "29891652",
                                    "name": "Anne Lauscher"
                                },
                                {
                                    "authorId": "2472657",
                                    "name": "Goran Glavas"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Workshop on Semantic Evaluation",
                            "n_citations": 55
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation."
                        ],
                        "paper": {
                            "corpus_id": 273532366,
                            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                            "authors": [
                                {
                                    "authorId": "2327246234",
                                    "name": "Junhua Liu"
                                },
                                {
                                    "authorId": "2325881142",
                                    "name": "Bin Fu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.90771484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Quality and Reliability Issues",
                "tldr": "Translation-based multilingual datasets face significant reliability challenges including translation artifacts that affect model evaluations, cultural bias that compromises authenticity, and overestimation of model performance when both training and evaluation use translated content. (13 sources)",
                "text": "\nThe reliability of translation-based multilingual evaluation datasets is compromised by several interrelated issues. A primary concern is the presence of translation artifacts\u2014including reduced fluency, unnatural phrasing, and \"translationese\"\u2014that inadvertently affect the judgment of evaluator models <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. These artifacts can introduce subtle errors that may invalidate original labels and distort evaluation outcomes. Recent research demonstrates that naively translating existing meta-evaluation benchmarks can introduce translation artifacts that affect judgment and overlook key multilingual-specific nuances such as code-switching or linguistic patterns <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>.\n\nCultural bias presents another significant challenge for translation-based approaches. Many multilingual benchmarks are translated from English, introducing biases related to translations and cultural representation that affect experimental conclusions by reflecting the origin culture <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>. This issue is particularly relevant as language models often fail to capture biases in different languages due to their cultural insensitivity <Paper corpusId=\"268357672\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>. Cultural biases in multilingual datasets present substantial obstacles regarding their scope, validity, and reliability as global benchmarks, and these biases are not mitigated if benchmarks are automatically translated from English into other languages <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nThe translation process often compromises the naturalness and idiomaticity of the target language text, bending it to match the structural and lexical properties of the source language <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. This results in evaluation benchmarks that fail to adequately measure system performance in the target language. Translation quality heavily depends on the capability of the language model used, with particular concerns for medium and low-resource languages where translation accuracy may be compromised <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246680398\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\nComparative studies reveal that simply translating English benchmarks proves insufficient for robust evaluation. Localized benchmarks show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. Research has also found that using local versus English-translated data can result in differences of more than 20 percentage points for the best-performing models, drastically changing the estimated difficulty of some languages <Paper corpusId=\"277780370\" paperTitle=\"(Hupkes et al., 2025)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>.\n\nThese reliability issues may also lead to misleading evaluation results. Translated datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper> <Paper corpusId=\"216914383\" paperTitle=\"(Artetxe et al._1, 2020)\" isShortName></Paper>. Researchers highlight that when models are trained on translated data and then evaluated on translated benchmarks, their performance may be artificially inflated, giving a distorted picture of their actual multilingual capabilities.\n\nTo address these quality and reliability concerns, researchers recommend controlled translation approaches that avoid machine translation of entire preference datasets <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>, thorough quality checks by native speakers, and development of benchmarks based on regional resources rather than translating benchmarks with inherent cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>. For instance, the Include benchmark was created based on local exam sources instead of translating benchmarks with inherent cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Son et al., 2024)",
                        "snippets": [
                            "However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."
                        ],
                        "paper": {
                            "corpus_id": 273532388,
                            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
                            "authors": [
                                {
                                    "authorId": "2321452295",
                                    "name": "Guijin Son"
                                },
                                {
                                    "authorId": "29830817",
                                    "name": "Dongkeun Yoon"
                                },
                                {
                                    "authorId": "2299329316",
                                    "name": "Juyoung Suk"
                                },
                                {
                                    "authorId": "2301578911",
                                    "name": "Javier Aula-Blasco"
                                },
                                {
                                    "authorId": "2327215494",
                                    "name": "Mano Aslan"
                                },
                                {
                                    "authorId": "2327216625",
                                    "name": "Vu Trong Kim"
                                },
                                {
                                    "authorId": "2232783785",
                                    "name": "Shayekh Bin Islam"
                                },
                                {
                                    "authorId": "2327215436",
                                    "name": "Jaume Prats-Cristi\u00e0"
                                },
                                {
                                    "authorId": "2327217057",
                                    "name": "Luc\u00eda Tormo-Ba\u00f1uelos"
                                },
                                {
                                    "authorId": "2184037220",
                                    "name": "Seungone Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.72900390625
                    },
                    {
                        "id": "(Etxaniz et al., 2024)",
                        "snippets": [
                            "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."
                        ],
                        "paper": {
                            "corpus_id": 270380088,
                            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                            "authors": [
                                {
                                    "authorId": "2226458991",
                                    "name": "Julen Etxaniz"
                                },
                                {
                                    "authorId": "2481918",
                                    "name": "Gorka Azkune"
                                },
                                {
                                    "authorId": "2260104163",
                                    "name": "A. Soroa"
                                },
                                {
                                    "authorId": "2251043402",
                                    "name": "Oier L\u00f3pez de Lacalle"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 11
                        },
                        "score": 0.8037109375
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, (Ma et al., 2022) emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages."
                        ],
                        "paper": {
                            "corpus_id": 268357672,
                            "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
                            "authors": [
                                {
                                    "authorId": "2283821804",
                                    "name": "Eunsu Kim"
                                },
                                {
                                    "authorId": "2299329316",
                                    "name": "Juyoung Suk"
                                },
                                {
                                    "authorId": "2262214736",
                                    "name": "Philhoon Oh"
                                },
                                {
                                    "authorId": "47111345",
                                    "name": "Haneul Yoo"
                                },
                                {
                                    "authorId": "2262214700",
                                    "name": "James Thorne"
                                },
                                {
                                    "authorId": "2283768962",
                                    "name": "Alice Oh"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 23
                        },
                        "score": 0.7412109375
                    },
                    {
                        "id": "(Tao et al., 2023)",
                        "snippets": [
                            "Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."
                        ],
                        "paper": {
                            "corpus_id": 265445838,
                            "title": "Cultural bias and cultural alignment of large language models",
                            "authors": [
                                {
                                    "authorId": "2105711243",
                                    "name": "Yan Tao"
                                },
                                {
                                    "authorId": "2269470454",
                                    "name": "Olga Viberg"
                                },
                                {
                                    "authorId": "2255317189",
                                    "name": "Ryan S. Baker"
                                },
                                {
                                    "authorId": "2246853606",
                                    "name": "Ren\u00e9 F. Kizilcec"
                                }
                            ],
                            "year": 2023,
                            "venue": "PNAS Nexus",
                            "n_citations": 87
                        },
                        "score": 0
                    },
                    {
                        "id": "(Barth et al., 2025)",
                        "snippets": [
                            "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2023). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations."
                        ],
                        "paper": {
                            "corpus_id": 276421738,
                            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                            "authors": [
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2302558975",
                                    "name": "Georg Rehm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.77392578125
                    },
                    {
                        "id": "(Ponti et al., 2020)",
                        "snippets": [
                            "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."
                        ],
                        "paper": {
                            "corpus_id": 218470125,
                            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "50383712",
                                    "name": "Qianchu Liu"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 327
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Thellmann et al., 2024)",
                        "snippets": [
                            "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 273323209,
                            "title": "Towards Multilingual LLM Evaluation for European Languages",
                            "authors": [
                                {
                                    "authorId": "2269539",
                                    "name": "Klaudia Thellmann"
                                },
                                {
                                    "authorId": "2325726089",
                                    "name": "Bernhard Stadler"
                                },
                                {
                                    "authorId": "2258551638",
                                    "name": "Michael Fromm"
                                },
                                {
                                    "authorId": "2258550838",
                                    "name": "Jasper Schulze Buschhoff"
                                },
                                {
                                    "authorId": "2325726070",
                                    "name": "Alex Jude"
                                },
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2258552277",
                                    "name": "Johannes Leveling"
                                },
                                {
                                    "authorId": "2347259072",
                                    "name": "Nicolas Flores-Herr"
                                },
                                {
                                    "authorId": "2330411614",
                                    "name": "Joachim K\u00f6hler"
                                },
                                {
                                    "authorId": "2330412312",
                                    "name": "Ren\u00e9 J\u00e4kel"
                                },
                                {
                                    "authorId": "2258668067",
                                    "name": "Mehdi Ali"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class."
                        ],
                        "paper": {
                            "corpus_id": 246680398,
                            "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding",
                            "authors": [
                                {
                                    "authorId": "145391513",
                                    "name": "Yu Meng"
                                },
                                {
                                    "authorId": "3488341",
                                    "name": "Jiaxin Huang"
                                },
                                {
                                    "authorId": "49891156",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "2111759643",
                                    "name": "Jiawei Han"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 235
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)",
                            "when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources."
                        ],
                        "paper": {
                            "corpus_id": 277993848,
                            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                            "authors": [
                                {
                                    "authorId": "2327995656",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "2258789284",
                                    "name": "Weixuan Wang"
                                },
                                {
                                    "authorId": "2349802912",
                                    "name": "Sinuo Liu"
                                },
                                {
                                    "authorId": "2331767275",
                                    "name": "Huifeng Yin"
                                },
                                {
                                    "authorId": "2356794252",
                                    "name": "Xintong Wang"
                                },
                                {
                                    "authorId": "2331686159",
                                    "name": "Yu Zhao"
                                },
                                {
                                    "authorId": "2266387313",
                                    "name": "Chenyang Lyu"
                                },
                                {
                                    "authorId": "2302342302",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Hupkes et al., 2025)",
                        "snippets": [
                            "Translation as a proxy for multilinguality Another, more implicit method to assess multilinguality in LLMs is to evaluate their ability to translate from one language to another. This approach was famously used by (Brown et al., 2020), but has not been common since",
                            "Lastly, we find that using local vs English-translated data can result in differences more than 20 points for the best performing models, drastically change the estimated difficulty of some languages. For using machines instead of human translations, we find a weaker effect on ordering of language difficulty, a larger difference in model rankings, and a substantial drop in estimated performance for all models."
                        ],
                        "paper": {
                            "corpus_id": 277780370,
                            "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
                            "authors": [
                                {
                                    "authorId": "3449411",
                                    "name": "Dieuwke Hupkes"
                                },
                                {
                                    "authorId": "3444222",
                                    "name": "Nikolay Bogoychev"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.72900390625
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."
                        ],
                        "paper": {
                            "corpus_id": 258865558,
                            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "35871436",
                                    "name": "Sneha Kudugunta"
                                },
                                {
                                    "authorId": "2118211280",
                                    "name": "Xinyan Velocity Yu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "1557386977",
                                    "name": "Machel Reid"
                                },
                                {
                                    "authorId": "2073587169",
                                    "name": "Yulia Tsvetkov"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 62
                        },
                        "score": 0.70166015625
                    },
                    {
                        "id": "(Artetxe et al._1, 2020)",
                        "snippets": [
                            "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."
                        ],
                        "paper": {
                            "corpus_id": 216914383,
                            "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                },
                                {
                                    "authorId": "3255091",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 72
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Alternative Approaches",
                "tldr": "To overcome translation limitations, researchers have developed multilingual datasets through native content collection, expert-created benchmarks tailored to specific cultures, and compilation of similar tasks across languages, providing more authentic evaluation resources. (10 sources)",
                "text": "\nRather than relying on translation-based approaches, researchers have developed several alternative methods for creating multilingual evaluation datasets:\n\n1. **Native language collection and annotation** - Datasets like TyDi QA collect and annotate data in multiple languages separately, resulting in non-parallel datasets that better represent linguistic characteristics native to each language <Paper corpusId=\"270285718\" paperTitle=\"(Sakai et al., 2024)\" isShortName></Paper>. This approach addresses the concerns about unnaturalness found in translated content by using documents that are authentically written in target languages <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>.\n\n2. **Expert-created benchmarks for specific languages/cultures** - Recent efforts have focused on developing benchmarks based on regional resources rather than translating benchmarks with inherent cultural bias. The Include benchmark was created using local exam sources to maintain cultural authenticity <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>. Similarly, the E\u03c7\u03b1\u00b5s dataset was collected from multilingual high school examinations where questions were written by educational experts rather than non-expert annotators, making evaluation results comparable to a top-performing student <Paper corpusId=\"226262367\" paperTitle=\"(Hardalov et al., 2020)\" isShortName></Paper>.\n\n3. **Human-annotated multilingual datasets from scratch** - Though time and resource-intensive, creating multilingual datasets with human annotators of multiple languages can achieve higher quality and cultural relevance compared to translation approaches <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper>.\n\n4. **Task-specific collection across languages** - Researchers have collected similar tasks across multiple languages to create comprehensive benchmarks like XTREME and XGLUE <Paper corpusId=\"270285718\" paperTitle=\"(Sakai et al., 2024)\" isShortName></Paper>. This approach ensures task consistency while maintaining language authenticity.\n\n5. **Culturally-sensitive multilingual benchmarks** - Some benchmarks engage professional and community annotators to label samples as culturally-sensitive or culturally-agnostic, improving the quality of multilingual evaluation <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.\n\n6. **National exam questions in target languages** - Inspired by Hendrycks et al.'s approach with MMLU <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>, researchers have collected human exam questions from specific regions in target languages, such as IndoMMLU for Indonesian <Paper corpusId=\"263829211\" paperTitle=\"(Koto et al., 2023)\" isShortName></Paper> and TurkishMMLU with over 10,000 questions covering 9 different subjects from Turkish high-school curricula <Paper corpusId=\"271245129\" paperTitle=\"(Yuksel et al., 2024)\" isShortName></Paper>.\n\n7. **Parallel document discovery** - Rather than manually translating documents, some researchers find naturally-parallel documents across languages, providing high-quality content without requiring manual translation <Paper corpusId=\"204734128\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>.\n\nResearch comparing these approaches with translation-based methods shows that localized benchmarks demonstrate substantially higher correlation with human judgments (0.68) compared to translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sakai et al., 2024)",
                        "snippets": [
                            "Multilingual datasets When focusing on the evaluation of multilingual performance of LMs,  the evaluation datasets are almost exclusively created through three methods, as shown in Table 2:\n\n(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 270285718,
                            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
                            "authors": [
                                {
                                    "authorId": "2204051777",
                                    "name": "Yusuke Sakai"
                                },
                                {
                                    "authorId": "2300756",
                                    "name": "Hidetaka Kamigaito"
                                },
                                {
                                    "authorId": "2266807418",
                                    "name": "Taro Watanabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 5
                        },
                        "score": 0.7333984375
                    },
                    {
                        "id": "(Lewis et al., 2019)",
                        "snippets": [
                            "Parallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation."
                        ],
                        "paper": {
                            "corpus_id": 204734128,
                            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "1905713",
                                    "name": "Ruty Rinott"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "144518416",
                                    "name": "Holger Schwenk"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 509
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Barth et al., 2025)",
                        "snippets": [
                            "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2023). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations."
                        ],
                        "paper": {
                            "corpus_id": 276421738,
                            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                            "authors": [
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2302558975",
                                    "name": "Georg Rehm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.77392578125
                    },
                    {
                        "id": "(Hardalov et al., 2020)",
                        "snippets": [
                            "We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. \u2022 We collect a new challenging dataset E\u03c7\u03b1\u00b5s from multilingual high school examinations, which offers several advantages over existing datasets: (i) it covers various domains, (ii) it is nearly three times larger than pre-existing Science QA datasets, (iii) it extends multilingual QA tasks to more languages, (iv) the questions are written by experts, rather than translated or crowdsourced, (v) the questions are harder since they are from matriculation exams rather than 4-8th grade.\n\nRecently, some multilingual datasets, were released to the public. MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020) use translations by professionals and extend the monolingual SQuAD (Rajpurkar et al., 2016) to 7 and 11 languages, respectively, thus forming cross-lingual evaluation benchmarks",
                            "E\u03c7\u03b1\u00b5s differs from the aforementioned multilingual benchmarks in several aspects. First, we extend the multilingual QA efforts to a different, more challenging domain (Clark et al., 2018). Second, our datasets support more languages. Next, the questions in E\u03c7\u03b1\u00b5s are written by educational experts rather than non-expert annotators, making the evaluation results comparable to a top-performing student."
                        ],
                        "paper": {
                            "corpus_id": 226262367,
                            "title": "EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering",
                            "authors": [
                                {
                                    "authorId": "3255454",
                                    "name": "Momchil Hardalov"
                                },
                                {
                                    "authorId": "39980906",
                                    "name": "Todor Mihaylov"
                                },
                                {
                                    "authorId": "46190940",
                                    "name": "Dimitrina Zlatkova"
                                },
                                {
                                    "authorId": "1379925776",
                                    "name": "Yoan Dinkov"
                                },
                                {
                                    "authorId": "52553663",
                                    "name": "Ivan Koychev"
                                },
                                {
                                    "authorId": "1683562",
                                    "name": "Preslav Nakov"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 54
                        },
                        "score": 0.7666015625
                    },
                    {
                        "id": "(Thellmann et al., 2024)",
                        "snippets": [
                            "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 273323209,
                            "title": "Towards Multilingual LLM Evaluation for European Languages",
                            "authors": [
                                {
                                    "authorId": "2269539",
                                    "name": "Klaudia Thellmann"
                                },
                                {
                                    "authorId": "2325726089",
                                    "name": "Bernhard Stadler"
                                },
                                {
                                    "authorId": "2258551638",
                                    "name": "Michael Fromm"
                                },
                                {
                                    "authorId": "2258550838",
                                    "name": "Jasper Schulze Buschhoff"
                                },
                                {
                                    "authorId": "2325726070",
                                    "name": "Alex Jude"
                                },
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2258552277",
                                    "name": "Johannes Leveling"
                                },
                                {
                                    "authorId": "2347259072",
                                    "name": "Nicolas Flores-Herr"
                                },
                                {
                                    "authorId": "2330411614",
                                    "name": "Joachim K\u00f6hler"
                                },
                                {
                                    "authorId": "2330412312",
                                    "name": "Ren\u00e9 J\u00e4kel"
                                },
                                {
                                    "authorId": "2258668067",
                                    "name": "Mehdi Ali"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Tao et al., 2023)",
                        "snippets": [
                            "Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."
                        ],
                        "paper": {
                            "corpus_id": 265445838,
                            "title": "Cultural bias and cultural alignment of large language models",
                            "authors": [
                                {
                                    "authorId": "2105711243",
                                    "name": "Yan Tao"
                                },
                                {
                                    "authorId": "2269470454",
                                    "name": "Olga Viberg"
                                },
                                {
                                    "authorId": "2255317189",
                                    "name": "Ryan S. Baker"
                                },
                                {
                                    "authorId": "2246853606",
                                    "name": "Ren\u00e9 F. Kizilcec"
                                }
                            ],
                            "year": 2023,
                            "venue": "PNAS Nexus",
                            "n_citations": 87
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hendrycks et al., 2020)",
                        "snippets": [
                            "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
                        ],
                        "paper": {
                            "corpus_id": 221516475,
                            "title": "Measuring Massive Multitask Language Understanding",
                            "authors": [
                                {
                                    "authorId": "3422872",
                                    "name": "Dan Hendrycks"
                                },
                                {
                                    "authorId": "90909974",
                                    "name": "Collin Burns"
                                },
                                {
                                    "authorId": "104444594",
                                    "name": "Steven Basart"
                                },
                                {
                                    "authorId": "1380103052",
                                    "name": "Andy Zou"
                                },
                                {
                                    "authorId": "16787428",
                                    "name": "Mantas Mazeika"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "5164568",
                                    "name": "J. Steinhardt"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4568
                        },
                        "score": 0
                    },
                    {
                        "id": "(Koto et al., 2023)",
                        "snippets": [
                            "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."
                        ],
                        "paper": {
                            "corpus_id": 263829211,
                            "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
                            "authors": [
                                {
                                    "authorId": "2789148",
                                    "name": "Fajri Koto"
                                },
                                {
                                    "authorId": "2256987672",
                                    "name": "Nurul Aisyah"
                                },
                                {
                                    "authorId": "49404498",
                                    "name": "Haonan Li"
                                },
                                {
                                    "authorId": "2256987316",
                                    "name": "Timothy Baldwin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 46
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yuksel et al., 2024)",
                        "snippets": [
                            "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                        ],
                        "paper": {
                            "corpus_id": 271245129,
                            "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
                            "authors": [
                                {
                                    "authorId": "2311698426",
                                    "name": "Arda Yuksel"
                                },
                                {
                                    "authorId": "1999179692",
                                    "name": "Abdullatif K\u00f6ksal"
                                },
                                {
                                    "authorId": "2126865294",
                                    "name": "Lutfi Kerem cSenel"
                                },
                                {
                                    "authorId": "2311700614",
                                    "name": "Anna Korhonen"
                                },
                                {
                                    "authorId": "2130001188",
                                    "name": "Hinrich Schutze"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)",
                            "when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources."
                        ],
                        "paper": {
                            "corpus_id": 277993848,
                            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                            "authors": [
                                {
                                    "authorId": "2327995656",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "2258789284",
                                    "name": "Weixuan Wang"
                                },
                                {
                                    "authorId": "2349802912",
                                    "name": "Sinuo Liu"
                                },
                                {
                                    "authorId": "2331767275",
                                    "name": "Huifeng Yin"
                                },
                                {
                                    "authorId": "2356794252",
                                    "name": "Xintong Wang"
                                },
                                {
                                    "authorId": "2331686159",
                                    "name": "Yu Zhao"
                                },
                                {
                                    "authorId": "2266387313",
                                    "name": "Chenyang Lyu"
                                },
                                {
                                    "authorId": "2302342302",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.82080078125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Best Practices for Translation-Based Datasets",
                "tldr": "Despite inherent challenges, researchers have developed effective practices for creating reliable translation-based datasets, including using native translators, incorporating thorough quality verification processes, and allowing for cultural adaptations. (7 sources)",
                "text": "\nDespite the limitations of translation-based approaches, they remain widely used due to their efficiency and scalability. Researchers have identified several best practices to maximize the quality and reliability of translation-based multilingual evaluation datasets:\n\n1. **Use native speakers for translation and verification** - Employing carefully selected native-speaking translators for each target language helps ensure linguistic naturalness and idiomaticity <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. When machine translation is used, having multiple native-speaking annotators independently verify each translated sample significantly improves quality <Paper corpusId=\"276259236\" paperTitle=\"(Huang et al., 2025)\" isShortName></Paper>.\n\n2. **Allow for cultural and linguistic adaptations** - Rather than forcing direct translations, provide translators enough flexibility to make necessary target-language adjustments, including substitutions with culture-specific concepts and multi-word paraphrases where the original text cannot be directly translated <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>.\n\n3. **Implement controlled translation processes** - Machine translation of entire preference datasets should be avoided to prevent artifacts that can introduce subtle errors and invalidate original labels. Instead, use a more controlled approach with rigorous verification <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>.\n\n4. **Conduct thorough quality verification** - Each translated item should undergo careful validation to ensure clear distinctions between correct and incorrect responses. Subsets that don't meet quality standards should be discarded <Paper corpusId=\"273532388\" paperTitle=\"(Son et al., 2024)\" isShortName></Paper>. For instance, M-ABSA implemented efficient automatic data quality verification followed by manual inspection when necessary <Paper corpusId=\"276408022\" paperTitle=\"(Wu et al._1, 2025)\" isShortName></Paper>.\n\n5. **Select appropriate translation models** - When using machine translation, employ state-of-the-art multilingual translation models and apply automatic filtering mechanisms to ensure translation quality, such as keeping only languages with translation BLEU scores above certain thresholds <Paper corpusId=\"259095896\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n6. **Consider linguistic features of target languages** - Be aware of specific linguistic features like pronoun-dropping, compounding, and inflection that can affect translation quality and model performance <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\n7. **Ensure cross-lingual consistency** - When building safety benchmarks or other sensitive datasets, verification by annotators should focus on maintaining cross-lingual consistency while preserving language-specific nuances <Paper corpusId=\"276408508\" paperTitle=\"(Banerjee et al., 2025)\" isShortName></Paper>.\n\n8. **Combine translation with native language data** - For optimal performance, consider using a combination of translated and native language data when training multilingual models, as this approach can outperform models trained solely on either source <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\nThese practices help mitigate many of the challenges associated with translation-based approaches while preserving their advantages in terms of cross-lingual alignment and efficiency.",
                "citations": [
                    {
                        "id": "(Ponti et al., 2020)",
                        "snippets": [
                            "Design Objectives. The principal objectives in XCOPA creation were: 1) to create a genuinely typologically diverse multilingual dataset, aligned across target languages in order to make performance scores comparable, and 2) to ensure high quality, naturalness and idiomacity of each monolingual dataset. While the commonly used translation approach addresses the former objective, it is prone to compromise the latter goal, bending the target language to the structural and lexical properties of the source language: the resulting evaluation benchmarks thus fail to measure system performance adequately (Koppel and Ordan, 2011;Volansky et al., 2015;Artetxe et al., 2020a). \n\nIn order to avoid these pitfalls, we: (i) entrusted the translation task to a single (but carefully selected) translator for each target language, and (ii) offered enough leeway for necessary target-language adjustments (e.g., substitutions with culture-specific concepts and multi-word paraphrases, wherever the original text eluded direct translation)."
                        ],
                        "paper": {
                            "corpus_id": 218470125,
                            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "50383712",
                                    "name": "Qianchu Liu"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 327
                        },
                        "score": 0.83740234375
                    },
                    {
                        "id": "(Huang et al., 2025)",
                        "snippets": [
                            "To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction."
                        ],
                        "paper": {
                            "corpus_id": 276259236,
                            "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2345018196",
                                    "name": "Xu Huang"
                                },
                                {
                                    "authorId": "2279840735",
                                    "name": "Wenhao Zhu"
                                },
                                {
                                    "authorId": "2345123408",
                                    "name": "Hanxu Hu"
                                },
                                {
                                    "authorId": "2351236188",
                                    "name": "Conghui He"
                                },
                                {
                                    "authorId": "2151532469",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2339084258",
                                    "name": "Shujian Huang"
                                },
                                {
                                    "authorId": "2279544481",
                                    "name": "Fei Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.865234375
                    },
                    {
                        "id": "(Son et al., 2024)",
                        "snippets": [
                            "However, we find that naive translation of existing meta-evaluation benchmarks can (i) introduce translation artifacts (mistranslation, degraded fluency, \"translationese\", etc.) that inadvertently affect the judgement of evaluator LLMs, and (ii) overlook key multilingual-specific nuances such as code-switching, or linguistic patterns. Together, these issues underscore the importance of careful, multilingual-tailored design rather than naive machine translation. Accordingly, we propose the following design choices to ensure reliable and truly multilingual meta-evaluation: \n\n\u2022 Controlled Translation. Machine Translation of entire preference datasets should be avoided, as artifacts can introduce subtle errors and invalidate original labels. \n\n\u2022 Thorough Quality Checks. Each entry (prompt-chosen-rejected) triplet should undergo careful validation to ensure a clear while non-trivial distinction between chosen and rejected responses. Subsets that do not meet this standard should be discarded. \n\n\u2022 Objectively Correct or Human-Preferred. Whenever possible, the \"chosen\" response should be objectively (or verifiably) correct or reflect existing human judgments."
                        ],
                        "paper": {
                            "corpus_id": 273532388,
                            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
                            "authors": [
                                {
                                    "authorId": "2321452295",
                                    "name": "Guijin Son"
                                },
                                {
                                    "authorId": "29830817",
                                    "name": "Dongkeun Yoon"
                                },
                                {
                                    "authorId": "2299329316",
                                    "name": "Juyoung Suk"
                                },
                                {
                                    "authorId": "2301578911",
                                    "name": "Javier Aula-Blasco"
                                },
                                {
                                    "authorId": "2327215494",
                                    "name": "Mano Aslan"
                                },
                                {
                                    "authorId": "2327216625",
                                    "name": "Vu Trong Kim"
                                },
                                {
                                    "authorId": "2232783785",
                                    "name": "Shayekh Bin Islam"
                                },
                                {
                                    "authorId": "2327215436",
                                    "name": "Jaume Prats-Cristi\u00e0"
                                },
                                {
                                    "authorId": "2327217057",
                                    "name": "Luc\u00eda Tormo-Ba\u00f1uelos"
                                },
                                {
                                    "authorId": "2184037220",
                                    "name": "Seungone Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.72900390625
                    },
                    {
                        "id": "(Wu et al._1, 2025)",
                        "snippets": [
                            "Existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date",
                            "The dataset is constructed through an automatic translation process with human review to ensure quality",
                            "Zhang et al. (2021a) construct a multilingual dataset by automatically translating the SemEval-2016 dataset (Pontiki et al., 2016), covering five languages for evaluation. However, there is no assessment of the translation quality, and most importantly -the number of languages in this dataset is limited, preventing researchers from conducting a strictly controlled evaluation of the effectiveness of cross-lingual transfer",
                            "we use existing high-quality English datasets from multiple domains and construct a dataset (by manually annotating an English corpus) from another domain. These datasets are then automatically translated into 20 languages, followed by an efficient automatic data quality verification and manual inspection if necessary."
                        ],
                        "paper": {
                            "corpus_id": 276408022,
                            "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis",
                            "authors": [
                                {
                                    "authorId": "2335666192",
                                    "name": "Chengyan Wu"
                                },
                                {
                                    "authorId": "2335825370",
                                    "name": "Bolei Ma"
                                },
                                {
                                    "authorId": "2107995084",
                                    "name": "Yihong Liu"
                                },
                                {
                                    "authorId": "2144388883",
                                    "name": "Zheyu Zhang"
                                },
                                {
                                    "authorId": "2266470510",
                                    "name": "Ningyuan Deng"
                                },
                                {
                                    "authorId": "2345863610",
                                    "name": "Yanshu Li"
                                },
                                {
                                    "authorId": "2346275292",
                                    "name": "Baolan Chen"
                                },
                                {
                                    "authorId": "2345832186",
                                    "name": "Yi Zhang"
                                },
                                {
                                    "authorId": "2284982105",
                                    "name": "Barbara Plank"
                                },
                                {
                                    "authorId": "2335829308",
                                    "name": "Yun Xue"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7392578125
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names)."
                        ],
                        "paper": {
                            "corpus_id": 259095896,
                            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2109472880",
                                    "name": "Yuwei Yin"
                                },
                                {
                                    "authorId": "2117947438",
                                    "name": "Shicheng Li"
                                },
                                {
                                    "authorId": "2146034504",
                                    "name": "Liang Chen"
                                },
                                {
                                    "authorId": "144202874",
                                    "name": "Peiyi Wang"
                                },
                                {
                                    "authorId": "1906099",
                                    "name": "Shuhuai Ren"
                                },
                                {
                                    "authorId": "2027599235",
                                    "name": "Mukai Li"
                                },
                                {
                                    "authorId": "14646700",
                                    "name": "Yazheng Yang"
                                },
                                {
                                    "authorId": "47883405",
                                    "name": "Jingjing Xu"
                                },
                                {
                                    "authorId": "2116530295",
                                    "name": "Xu Sun"
                                },
                                {
                                    "authorId": "47648549",
                                    "name": "Lingpeng Kong"
                                },
                                {
                                    "authorId": "2144831944",
                                    "name": "Qi Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 120
                        },
                        "score": 0.74072265625
                    },
                    {
                        "id": "(Hennig et al., 2023)",
                        "snippets": [
                            "We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."
                        ],
                        "paper": {
                            "corpus_id": 258557740,
                            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
                            "authors": [
                                {
                                    "authorId": "36943315",
                                    "name": "Leonhard Hennig"
                                },
                                {
                                    "authorId": "143680610",
                                    "name": "Philippe E. Thomas"
                                },
                                {
                                    "authorId": "145733288",
                                    "name": "Sebastian M\u00f6ller"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.7333984375
                    },
                    {
                        "id": "(Banerjee et al., 2025)",
                        "snippets": [
                            "XSafety: This is a multilingual safety benchmark designed to evaluate LLMs across multiple languages. It consists of 2,800 manually translated instances covering 14 safety categories in 10 widely spoken languages: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German. Built from existing monolingual safety datasets, XSafety was translated and verified by annotators, ensuring cross-lingual consistency. The benchmark reveals significant safety gaps in non-English responses, emphasizing the need for multilingual safety alignment. For our experiments, we use google translate8 to translate English queries into other languages when they are not present in the dataset."
                        ],
                        "paper": {
                            "corpus_id": 276408508,
                            "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
                            "authors": [
                                {
                                    "authorId": "2246317582",
                                    "name": "Somnath Banerjee"
                                },
                                {
                                    "authorId": "2273400615",
                                    "name": "Sayan Layek"
                                },
                                {
                                    "authorId": "2345820876",
                                    "name": "Pratyush Chatterjee"
                                },
                                {
                                    "authorId": "2286311932",
                                    "name": "Animesh Mukherjee"
                                },
                                {
                                    "authorId": "2273549702",
                                    "name": "Rima Hazra"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.74462890625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.21624600000000002
    }
}