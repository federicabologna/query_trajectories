{
    "query": "Are there any papers effectiveness quality filtering of LLM pre training data advanced models such BERT found many prior studies fasttext it filtering models weak",
    "user_id": "lib_user",
    "task_id": "49369b02-d99f-4834-a34e-8cb109eeccc1",
    "timestamp": "2025-06-23T21:04:42.016746",
    "n_retrieval": 256,
    "n_retrieved": 264,
    "n_candidates": 19,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.280146,
    "decomposed_query": {
        "rewritten_query": "Effectiveness of quality filtering of LLM pre-training data for advanced models such as BERT compared to prior studies using fasttext for filtering models.",
        "keyword_query": "quality filtering LLM pre-training data advanced models BERT prior studies fasttext filtering models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009978,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08310, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262451401",
                    "name": "Ruihang Li"
                },
                {
                    "authorId": "2107995927",
                    "name": "Yixuan Wei"
                },
                {
                    "authorId": "2273515255",
                    "name": "Miaosen Zhang"
                },
                {
                    "authorId": "2316146999",
                    "name": "Nenghai Yu"
                },
                {
                    "authorId": "2262465375",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2243685256",
                    "name": "Houwen Peng"
                }
            ],
            "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.",
            "corpus_id": 271874495,
            "sentences": [
                {
                    "corpus_id": "271874495",
                    "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
                    "text": "The success of large language models (LLMs) is significantly influenced by the quality and quantity of the pre-training corpus. Researchers have developed various data curation pipelines to enhance dataset quality, focusing on raw web crawling, text extraction, repetition and toxic content removal, and, notably, quality filtering (Brown et al., 2020;Rae et al., 2021;Penedo et al., 2023). \n\nQuality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020;Gao et al., 2020;Chowdhery et al., 2023) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models. These methods mitigate the biases introduced by reference datasets but encounter challenges due to the indirect relationship between absolute perplexity and document quality. This indirect relationship inadvertently favor data with simple and repetitive content. Although such content is easier for models to predict, it contributes minimally to learning diversity and complexity (Wettig et al., 2024). \n\nTo address these issues, we introduce a simple yet effective quality filtering approach named Scal-ingFilter, which inversely leverages recent scaling laws in generative modeling to assess data quality. The core idea is to analyze the perplexity differences between two pre-trained models on the same data and assess the data quality based on these differences. We find a positive correlation between data quality and perplexity differences by inversely deriving Chinchilla scaling law (Hoffmann et al., 2022).",
                    "score": 0.4796261713787634,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 390
                        },
                        {
                            "start": 393,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1703
                        },
                        {
                            "start": 1706,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 332,
                            "end": 352,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 369,
                            "end": 389,
                            "matchedPaperCorpusId": "259063761"
                        },
                        {
                            "start": 724,
                            "end": 744,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 761,
                            "end": 784,
                            "matchedPaperCorpusId": "247951931"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.849609375
                }
            ],
            "relevance_judgement": 0.849609375,
            "relevance_judgment_input_expanded": "# Title: ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Ruihang Li, Yixuan Wei, Miaosen Zhang, Nenghai Yu, Han Hu, Houwen Peng\n## Abstract\nHigh-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.\n## Introduction\nThe success of large language models (LLMs) is significantly influenced by the quality and quantity of the pre-training corpus. Researchers have developed various data curation pipelines to enhance dataset quality, focusing on raw web crawling, text extraction, repetition and toxic content removal, and, notably, quality filtering (Brown et al., 2020;Rae et al., 2021;Penedo et al., 2023). \n\nQuality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020;Gao et al., 2020;Chowdhery et al., 2023) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models. These methods mitigate the biases introduced by reference datasets but encounter challenges due to the indirect relationship between absolute perplexity and document quality. This indirect relationship inadvertently favor data with simple and repetitive content. Although such content is easier for models to predict, it contributes minimally to learning diversity and complexity (Wettig et al., 2024). \n\nTo address these issues, we introduce a simple yet effective quality filtering approach named Scal-ingFilter, which inversely leverages recent scaling laws in generative modeling to assess data quality. The core idea is to analyze the perplexity differences between two pre-trained models on the same data and assess the data quality based on these differences. We find a positive correlation between data quality and perplexity differences by inversely deriving Chinchilla scaling law (Hoffmann et al., 2022).",
            "reference_string": "[271874495 | Li et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 61,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.05427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312864296",
                    "name": "Yudong Wang"
                },
                {
                    "authorId": "2354260586",
                    "name": "Zixuan Fu"
                },
                {
                    "authorId": "2295809950",
                    "name": "Jie Cai"
                },
                {
                    "authorId": "2359636324",
                    "name": "Peijun Tang"
                },
                {
                    "authorId": "2359634927",
                    "name": "Hongya Lyu"
                },
                {
                    "authorId": "2295846809",
                    "name": "Yewei Fang"
                },
                {
                    "authorId": "2295929465",
                    "name": "Zhi Zheng"
                },
                {
                    "authorId": "2295789325",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "1398454307",
                    "name": "Guoyang Zeng"
                },
                {
                    "authorId": "51131083",
                    "name": "Chaojun Xiao"
                },
                {
                    "authorId": "2324934386",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2316519794",
                    "name": "Zhiyuan Liu"
                }
            ],
            "abstract": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.",
            "corpus_id": 278394813,
            "sentences": [
                {
                    "corpus_id": "278394813",
                    "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
                    "text": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.",
                    "score": 0.5912050896145489,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81103515625
                },
                {
                    "corpus_id": "278394813",
                    "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
                    "text": "Thus, modeldriven data filtering, which employs a neural classifier to select high-quality content, has emerged as a better choice (Gunasekar et al., 2023;Shao et al., 2024). Notably, datasets such as FineWeb-edu (Penedo et al., 2024), Chinese FineWeb-edu (Yu et al., 2025), CCI3-HQ (Wang et al., 2024), and DCLM (Li et al., 2024) have demonstrated the efficacy of this paradigm by incorporating model-based classifiers following preprocessing stages, achieving not only substantial improvements in dataset quality but also measurable enhancements in downstream LLM performance across various benchmark tasks. Nevertheless, existing model-driven filtering methods still suffer from two main challenges: (1) There is a lack of efficient validation to quickly verify the filtering results, typically requiring large-scale training to observe the effect, resulting in high costs and low efficiency. (2) They heavily rely on manually-selected seed data, and the data for training classifiers often depends on human expertise, introducing significant subjectivity. \n\nTo address these challenges, we design an efficient data filtering pipeline based on an efficient verification strategy. This verification approach enables impartial seed data selection and facilitates iterative data filtering processes. Specifically, in contrast to conventional approaches that verify data quality by training LLMs from scratch using candidate corpora, our proposed efficient verification strategy leverages a nearly-trained LLM as a foundation. We incorporate candidate corpora during the final training steps and utilize the resulting performance improvement as a metric for assessing data quality. This verification strategy significantly enhances evaluation efficiency while maintaining quality assessment accuracy. Based on our efficient verification strategy, we can impartially select high-quality seed data for classifier training. Building upon the assumption that \"high-quality seed data is beneficial for LLM training\", we develop and optimize the strategy for selecting classifier training seeds and recipes, while carefully curating balanced sets of both positive and negative samples to ensure classifier quality and robustness. In addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016).",
                    "score": 0.48592984976400616,
                    "section_title": "Introduction",
                    "char_start_offset": 1245,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 1059
                        },
                        {
                            "start": 1062,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1525
                        },
                        {
                            "start": 1526,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2222
                        },
                        {
                            "start": 2223,
                            "end": 2352
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78271484375
                },
                {
                    "corpus_id": "278394813",
                    "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
                    "text": "In this paper, we construct a higher-quality Ultra-FineWeb dataset (including English data Ultra-FineWeben, approximately 1T tokens, and Chinese data Ultra-FineWeb-zh, approximately 120B tokens, totaling approximately 1.1T tokens). This dataset is based on the FineWeb and Chinese FineWeb datasets, utilizing our proposed efficient data filtering pipeline. Through rigorous experimental evaluations, we demonstrate that Ultra-FineWeb-en and Ultra-FineWeb-zh outperform FineWeb-edu and Chinese FineWeb-edu-v2 when used for small-scale model training from scratch. Additionally, we show the effectiveness of the high-quality data filtered by our classifier on the DCLM-Pool and MAP-CC datasets, further confirming the reliability and effectiveness of our proposed pipeline. These results indicate that classifiers based on our efficient data filtering pipeline can select higher-quality data with reduced computational cost, thereby improving model training performance. We provide a detailed description of the implementation of our efficient data filtering pipeline, especially the efficient verification strategy driven by classifiers in the pipeline. This strategy enables reliable assessment of training data impact on LLM performance while maintaining minimal computational requirements. Furthermore, we present detailed methodologies for classifier seed data selection, training recipes, and FastText model training configuration, ensuring experimental reproducibility and result transparency. This study aims to provide novel insights and methodologies for high-quality data filtering, offering valuable references for data quality optimization in future LLM training processes, and contributing to the further development of LLMs.",
                    "score": 0.4450163698234618,
                    "section_title": "Conclusion",
                    "char_start_offset": 28827,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1737
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.46337890625
                },
                {
                    "corpus_id": "278394813",
                    "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
                    "text": "In addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016). Compared to LLM-based classifiers (Penedo et al., 2024), the fastText-based classifier demonstrates superior inference efficiency, enabling both the filtering of higher-quality training data for LLMs and significantly accelerating the high-quality data filtration pipeline. We apply the proposed data filtering pipeline to the FineWeb (Penedo et al., 2024) and Chinese FineWeb (Yu et al., 2025) datasets (source data from Chinese FineWeb-edu-v2, which includes IndustryCorpus2 (Shi et al., 2024), MiChao (Liu et al., 2023), WuDao (BAAI, 2023), SkyPile (Wei et al., 2023), WanJuan (Qiu et al., 2024), ChineseWebText (Chen et al., 2023), TeleChat (He et al., 2024), and CCI3 (Wang et al., 2024)), resulting in the creation of higher-quality Ultra-FineWeb-en and Ultra-FineWeb-zh datasets, collectively referred to as Ultra-FineWeb. Experimental results show that LLMs trained on Ultra-FineWeb perform excellently across multiple benchmark tasks, providing empirical validation for the effectiveness of our high-quality data filtering pipeline and its efficiency in reducing computational costs. \n\nOur main contributions are as follows. The datasets and classifier will be released to facilitate the development of LLMs. \n\n\u2022 Efficient Verification Strategy: We propose a computationally efficient verification strategy that enables rapid evaluation of the impact of data on LLM training performance with minimal computational cost, significantly improving the efficiency of high-quality data filtering experiments.",
                    "score": 0.48949493317549403,
                    "section_title": "Introduction",
                    "char_start_offset": 3468,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1222
                        },
                        {
                            "start": 1225,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1347
                        },
                        {
                            "start": 1350,
                            "end": 1641
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.450927734375
                },
                {
                    "corpus_id": "278394813",
                    "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
                    "text": "Current high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency. Notably, most of our large-scale experiments are conducted in a distributed manner using a Spark4 cluster. For data preprocessing, we implement several key steps, including removing redundant empty lines and extra spaces, stripping diacritics, and converting all English text to lowercase. Additionally, we adopt the DeepSeek-V2 tokenizer (Liu et al., 2024), which outperforms traditional tokenization methods (such as space-based tokenization for English and Jieba5 for Chinese). Meanwhile, we preserve structural information such as \\n, \\t, and \\r. To ensure dataset integrity and balance, the final training set comprised 600K samples, evenly split between positive and negative examples. \n\nFor training details, we trained a fastText classifier with a vector dimension of 256, a learning rate of 0.1, a maximum word n-gram length of 3, a minimum word occurrence threshold of 5, and a total of 3 training epochs. Additionally, during inference, we maintain the default threshold of 0.5 to simplify operations and ensure experimental consistency, avoiding the need for additional tuning steps.",
                    "score": 0.4245897601978594,
                    "section_title": "FastText-based Quality Filtering",
                    "char_start_offset": 14544,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 214,
                            "end": 304
                        },
                        {
                            "start": 305,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1317
                        },
                        {
                            "start": 1318,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1528
                        },
                        {
                            "start": 1531,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.3515625
                }
            ],
            "relevance_judgement": 0.81103515625,
            "relevance_judgment_input_expanded": "# Title: Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data\n# Venue: arXiv.org\n# Authors: Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, Xu Han, Zhiyuan Liu\n## Abstract\nData quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.\n## Introduction\nThus, modeldriven data filtering, which employs a neural classifier to select high-quality content, has emerged as a better choice (Gunasekar et al., 2023;Shao et al., 2024). Notably, datasets such as FineWeb-edu (Penedo et al., 2024), Chinese FineWeb-edu (Yu et al., 2025), CCI3-HQ (Wang et al., 2024), and DCLM (Li et al., 2024) have demonstrated the efficacy of this paradigm by incorporating model-based classifiers following preprocessing stages, achieving not only substantial improvements in dataset quality but also measurable enhancements in downstream LLM performance across various benchmark tasks. Nevertheless, existing model-driven filtering methods still suffer from two main challenges: (1) There is a lack of efficient validation to quickly verify the filtering results, typically requiring large-scale training to observe the effect, resulting in high costs and low efficiency. (2) They heavily rely on manually-selected seed data, and the data for training classifiers often depends on human expertise, introducing significant subjectivity. \n\nTo address these challenges, we design an efficient data filtering pipeline based on an efficient verification strategy. This verification approach enables impartial seed data selection and facilitates iterative data filtering processes. Specifically, in contrast to conventional approaches that verify data quality by training LLMs from scratch using candidate corpora, our proposed efficient verification strategy leverages a nearly-trained LLM as a foundation. We incorporate candidate corpora during the final training steps and utilize the resulting performance improvement as a metric for assessing data quality. This verification strategy significantly enhances evaluation efficiency while maintaining quality assessment accuracy. Based on our efficient verification strategy, we can impartially select high-quality seed data for classifier training. Building upon the assumption that \"high-quality seed data is beneficial for LLM training\", we develop and optimize the strategy for selecting classifier training seeds and recipes, while carefully curating balanced sets of both positive and negative samples to ensure classifier quality and robustness. In addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016).\n...\nIn addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016). Compared to LLM-based classifiers (Penedo et al., 2024), the fastText-based classifier demonstrates superior inference efficiency, enabling both the filtering of higher-quality training data for LLMs and significantly accelerating the high-quality data filtration pipeline. We apply the proposed data filtering pipeline to the FineWeb (Penedo et al., 2024) and Chinese FineWeb (Yu et al., 2025) datasets (source data from Chinese FineWeb-edu-v2, which includes IndustryCorpus2 (Shi et al., 2024), MiChao (Liu et al., 2023), WuDao (BAAI, 2023), SkyPile (Wei et al., 2023), WanJuan (Qiu et al., 2024), ChineseWebText (Chen et al., 2023), TeleChat (He et al., 2024), and CCI3 (Wang et al., 2024)), resulting in the creation of higher-quality Ultra-FineWeb-en and Ultra-FineWeb-zh datasets, collectively referred to as Ultra-FineWeb. Experimental results show that LLMs trained on Ultra-FineWeb perform excellently across multiple benchmark tasks, providing empirical validation for the effectiveness of our high-quality data filtering pipeline and its efficiency in reducing computational costs. \n\nOur main contributions are as follows. The datasets and classifier will be released to facilitate the development of LLMs. \n\n\u2022 Efficient Verification Strategy: We propose a computationally efficient verification strategy that enables rapid evaluation of the impact of data on LLM training performance with minimal computational cost, significantly improving the efficiency of high-quality data filtering experiments.\n\n## FastText-based Quality Filtering\nCurrent high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency. Notably, most of our large-scale experiments are conducted in a distributed manner using a Spark4 cluster. For data preprocessing, we implement several key steps, including removing redundant empty lines and extra spaces, stripping diacritics, and converting all English text to lowercase. Additionally, we adopt the DeepSeek-V2 tokenizer (Liu et al., 2024), which outperforms traditional tokenization methods (such as space-based tokenization for English and Jieba5 for Chinese). Meanwhile, we preserve structural information such as \\n, \\t, and \\r. To ensure dataset integrity and balance, the final training set comprised 600K samples, evenly split between positive and negative examples. \n\nFor training details, we trained a fastText classifier with a vector dimension of 256, a learning rate of 0.1, a maximum word n-gram length of 3, a minimum word occurrence threshold of 5, and a total of 3 training epochs. Additionally, during inference, we maintain the default threshold of 0.5 to simplify operations and ensure experimental consistency, avoiding the need for additional tuning steps.\n\n## Conclusion\nIn this paper, we construct a higher-quality Ultra-FineWeb dataset (including English data Ultra-FineWeben, approximately 1T tokens, and Chinese data Ultra-FineWeb-zh, approximately 120B tokens, totaling approximately 1.1T tokens). This dataset is based on the FineWeb and Chinese FineWeb datasets, utilizing our proposed efficient data filtering pipeline. Through rigorous experimental evaluations, we demonstrate that Ultra-FineWeb-en and Ultra-FineWeb-zh outperform FineWeb-edu and Chinese FineWeb-edu-v2 when used for small-scale model training from scratch. Additionally, we show the effectiveness of the high-quality data filtered by our classifier on the DCLM-Pool and MAP-CC datasets, further confirming the reliability and effectiveness of our proposed pipeline. These results indicate that classifiers based on our efficient data filtering pipeline can select higher-quality data with reduced computational cost, thereby improving model training performance. We provide a detailed description of the implementation of our efficient data filtering pipeline, especially the efficient verification strategy driven by classifiers in the pipeline. This strategy enables reliable assessment of training data impact on LLM performance while maintaining minimal computational requirements. Furthermore, we present detailed methodologies for classifier seed data selection, training recipes, and FastText model training configuration, ensuring experimental reproducibility and result transparency. This study aims to provide novel insights and methodologies for high-quality data filtering, offering valuable references for data quality optimization in future LLM training processes, and contributing to the further development of LLMs.",
            "reference_string": "[278394813 | Wang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 22,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.05816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1500242049",
                    "name": "Tristan Thrush"
                },
                {
                    "authorId": "2279335958",
                    "name": "Christopher Potts"
                },
                {
                    "authorId": "2214604036",
                    "name": "Tatsunori Hashimoto"
                }
            ],
            "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier. We have now also updated this paper to include results from preregistered experiments with new pretraining data on an aggregation of 22 benchmarks up to the 1.4B scale, showing increasing improvements of our method over others with more scale. A pip package with full documentation can be found here: https://github.com/TristanThrush/perplexity-correlations.",
            "corpus_id": 272524632,
            "sentences": [
                {
                    "corpus_id": "272524632",
                    "title": "Improving Pretraining Data Using Perplexity Correlations",
                    "text": "In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
                    "score": 0.4648686348308666,
                    "section_title": "INITIAL PRETRAINING EXPERIMENTS",
                    "char_start_offset": 26288,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1327
                        },
                        {
                            "start": 1330,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2203
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7861328125
                },
                {
                    "corpus_id": "272524632",
                    "title": "Improving Pretraining Data Using Perplexity Correlations",
                    "text": "Validation losses on samples of text corpora are commonly used as a proxy for downstream performance when comparing LLMs pretrained on the same data distribution (Kaplan et al., 2020;Hoffmann et al., 2022;Wei et al., 2022), even if they have different architectures (Poli et al., 2023;Peng et al., 2023;Gu & Dao, 2024). \n\nAccording to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering. Are we missing important information that we can efficiently extract from a diverse collection of already trained models, larger and more diverse than any single organization is likely to produce? We show evidence supporting this hypothesissimple loss-performance correlation coefficients are effective when used for data selection.",
                    "score": 0.4152416981955893,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 319
                        },
                        {
                            "start": 322,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1024
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.333740234375
                }
            ],
            "relevance_judgement": 0.7861328125,
            "relevance_judgment_input_expanded": "# Title: Improving Pretraining Data Using Perplexity Correlations\n# Venue: International Conference on Learning Representations\n# Authors: Tristan Thrush, Christopher Potts, Tatsunori Hashimoto\n## Abstract\nQuality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier. We have now also updated this paper to include results from preregistered experiments with new pretraining data on an aggregation of 22 benchmarks up to the 1.4B scale, showing increasing improvements of our method over others with more scale. A pip package with full documentation can be found here: https://github.com/TristanThrush/perplexity-correlations.\n## RELATED WORK\nValidation losses on samples of text corpora are commonly used as a proxy for downstream performance when comparing LLMs pretrained on the same data distribution (Kaplan et al., 2020;Hoffmann et al., 2022;Wei et al., 2022), even if they have different architectures (Poli et al., 2023;Peng et al., 2023;Gu & Dao, 2024). \n\nAccording to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering. Are we missing important information that we can efficiently extract from a diverse collection of already trained models, larger and more diverse than any single organization is likely to produce? We show evidence supporting this hypothesissimple loss-performance correlation coefficients are effective when used for data selection.\n\n## INITIAL PRETRAINING EXPERIMENTS\nIn addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
            "reference_string": "[272524632 | Thrush et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Data Management For Training Large Language Models: A Survey",
            "venue": "",
            "year": 2023,
            "reference_count": 153,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238165141",
                    "name": "Zige Wang"
                },
                {
                    "authorId": "2249763710",
                    "name": "Wanjun Zhong"
                },
                {
                    "authorId": "46395829",
                    "name": "Yufei Wang"
                },
                {
                    "authorId": "2269768949",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2258717400",
                    "name": "Fei Mi"
                },
                {
                    "authorId": "2239032344",
                    "name": "Baojun Wang"
                },
                {
                    "authorId": "2238661808",
                    "name": "Lifeng Shang"
                },
                {
                    "authorId": "2257942536",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "2249841180",
                    "name": "Qun Liu"
                }
            ],
            "abstract": "Data plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the underlying mechanism of current prominent practices are still unknown. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey aims to provide a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various aspects of data management strategy design. Looking into the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through efficient data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM.",
            "corpus_id": 265609639,
            "sentences": [
                {
                    "corpus_id": "265609639",
                    "title": "Data Management For Training Large Language Models: A Survey",
                    "text": "Public datasets like Common Crawl2 and multilingual datasets (Kreutzer et al., 2022) usually contain low-quality data that hampers the training of LLMs. Hence, existing works usually perform quality filtering using hand-crafted heuristics (Yang et al., 2019;Raffel et al., 2020;Nijkamp et al., 2022), a trained classifier (Brown et al., 2020;Gao et al., 2020;Du et al., 2022;Touvron et al., 2023a;Wettig et al., 2024), metric thresholding (Wenzek et al., 2020;Muennighoff et al., 2023) or combinations of these techniques. Besides instance-level filtering, embedding clustering is also adopted to filter one cluster at a time (Kaddour, 2023). \n\nDespite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023b). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration.",
                    "score": 0.4227106823263234,
                    "section_title": "Quality Filtering",
                    "char_start_offset": 10941,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 642
                        },
                        {
                            "start": 645,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1426
                        },
                        {
                            "start": 1427,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1685
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 61,
                            "end": 84,
                            "matchedPaperCorpusId": "232307434"
                        },
                        {
                            "start": 239,
                            "end": 258,
                            "matchedPaperCorpusId": "195069387"
                        },
                        {
                            "start": 258,
                            "end": 278,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 278,
                            "end": 299,
                            "matchedPaperCorpusId": "252668917"
                        },
                        {
                            "start": 322,
                            "end": 342,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 359,
                            "end": 375,
                            "matchedPaperCorpusId": "245124124"
                        },
                        {
                            "start": 439,
                            "end": 460,
                            "matchedPaperCorpusId": "207870323"
                        },
                        {
                            "start": 771,
                            "end": 794,
                            "matchedPaperCorpusId": "258832491"
                        },
                        {
                            "start": 983,
                            "end": 1003,
                            "matchedPaperCorpusId": "268096300"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77734375
                }
            ],
            "relevance_judgement": 0.77734375,
            "relevance_judgment_input_expanded": "# Title: Data Management For Training Large Language Models: A Survey\n# Venue: \n# Authors: Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Lifeng Shang, Xin Jiang, Qun Liu\n## Abstract\nData plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the underlying mechanism of current prominent practices are still unknown. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey aims to provide a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various aspects of data management strategy design. Looking into the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through efficient data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM.\n## Quality Filtering\nPublic datasets like Common Crawl2 and multilingual datasets (Kreutzer et al., 2022) usually contain low-quality data that hampers the training of LLMs. Hence, existing works usually perform quality filtering using hand-crafted heuristics (Yang et al., 2019;Raffel et al., 2020;Nijkamp et al., 2022), a trained classifier (Brown et al., 2020;Gao et al., 2020;Du et al., 2022;Touvron et al., 2023a;Wettig et al., 2024), metric thresholding (Wenzek et al., 2020;Muennighoff et al., 2023) or combinations of these techniques. Besides instance-level filtering, embedding clustering is also adopted to filter one cluster at a time (Kaddour, 2023). \n\nDespite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023b). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration.",
            "reference_string": "[265609639 | Wang et al. | 2023 | Citations: 14]"
        },
        {
            "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366068443",
                    "name": "Xinlin Zhuang"
                },
                {
                    "authorId": "2233445161",
                    "name": "Jiahui Peng"
                },
                {
                    "authorId": "2299118979",
                    "name": "Ren Ma"
                },
                {
                    "authorId": "2352285941",
                    "name": "Yinfan Wang"
                },
                {
                    "authorId": "2318978696",
                    "name": "Tianyi Bai"
                },
                {
                    "authorId": "2298376663",
                    "name": "Xingjian Wei"
                },
                {
                    "authorId": "2289911484",
                    "name": "Jiantao Qiu"
                },
                {
                    "authorId": "2325489246",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2356585875",
                    "name": "Ying Qian"
                },
                {
                    "authorId": "2346476781",
                    "name": "Conghui He"
                }
            ],
            "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.",
            "corpus_id": 277955900,
            "sentences": [
                {
                    "corpus_id": "277955900",
                    "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
                    "text": "As the scale of training corpora continues to grow and data-centric AI evolves, there is an increasing need for systematic approaches to select highquality pre-training data. This need has spurred the development of comprehensive pre-training data processing pipelines (Penedo et al., 2023;He et al., 2023He et al., , 2024a)), and data selection methods. Existing pre-training data selection methods can be categorized into three primary approaches: natural language quality-based methods, diversity-based methods, and classifier-based methods. \n\nFor natural language quality-based methods, Gopher (Rae et al., 2021) and RedPajama (Weber et al., 2024) propose empirical rules like controlling the ratio of word and number token in texts to improve language modeling. Additionally, previous works (Muennighoff et al., 2024;Wenzek et al., 2020) have shown that selecting data with perplexity (PPL) scores on validation datasets can lead to superior performance on downstream tasks compared to using the entire dataset. Another notable method is DSIR (Xie et al., 2023b), which streamlines the selection process by employing hashed N-gram features (named as data importance scores) to efficiently identify high-quality data within large datasets. Meanwhile, another line of works utilize clustering (Zhang et al., 2025) or deduplication (Abbas et al., 2023;He et al., 2024b) to enhance diversity of pre-training datasets. \n\nMore recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data.",
                    "score": 0.4517848867445222,
                    "section_title": "Related Work",
                    "char_start_offset": 4197,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 544
                        },
                        {
                            "start": 547,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1418
                        },
                        {
                            "start": 1421,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1686
                        },
                        {
                            "start": 1687,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 1963
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 822,
                            "end": 842,
                            "matchedPaperCorpusId": "207870323"
                        },
                        {
                            "start": 1048,
                            "end": 1067,
                            "matchedPaperCorpusId": "256627727"
                        },
                        {
                            "start": 1334,
                            "end": 1354,
                            "matchedPaperCorpusId": "257557221"
                        },
                        {
                            "start": 1696,
                            "end": 1717,
                            "matchedPaperCorpusId": "267681974"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72216796875
                }
            ],
            "relevance_judgement": 0.72216796875,
            "relevance_judgment_input_expanded": "# Title: Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models\n# Venue: arXiv.org\n# Authors: Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He\n## Abstract\nThe composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.\n## Related Work\nAs the scale of training corpora continues to grow and data-centric AI evolves, there is an increasing need for systematic approaches to select highquality pre-training data. This need has spurred the development of comprehensive pre-training data processing pipelines (Penedo et al., 2023;He et al., 2023He et al., , 2024a)), and data selection methods. Existing pre-training data selection methods can be categorized into three primary approaches: natural language quality-based methods, diversity-based methods, and classifier-based methods. \n\nFor natural language quality-based methods, Gopher (Rae et al., 2021) and RedPajama (Weber et al., 2024) propose empirical rules like controlling the ratio of word and number token in texts to improve language modeling. Additionally, previous works (Muennighoff et al., 2024;Wenzek et al., 2020) have shown that selecting data with perplexity (PPL) scores on validation datasets can lead to superior performance on downstream tasks compared to using the entire dataset. Another notable method is DSIR (Xie et al., 2023b), which streamlines the selection process by employing hashed N-gram features (named as data importance scores) to efficiently identify high-quality data within large datasets. Meanwhile, another line of works utilize clustering (Zhang et al., 2025) or deduplication (Abbas et al., 2023;He et al., 2024b) to enhance diversity of pre-training datasets. \n\nMore recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data.",
            "reference_string": "[277955900 | Zhuang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 177,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.14149",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.14149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1944690382",
                    "name": "Kaiyuan Gao"
                },
                {
                    "authorId": "2112345574",
                    "name": "Su He"
                },
                {
                    "authorId": "2152990262",
                    "name": "Zhenyu He"
                },
                {
                    "authorId": null,
                    "name": "Jiacheng Lin"
                },
                {
                    "authorId": "2171652249",
                    "name": "Qizhi Pei"
                },
                {
                    "authorId": "2234370833",
                    "name": "Jie Shao"
                },
                {
                    "authorId": "2256597384",
                    "name": "Wei Zhang"
                }
            ],
            "abstract": "Generative pre-trained transformer (GPT) models have revolutionized the field of natural language processing (NLP) with remarkable performance in various tasks and also extend their power to multimodal domains. Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage. The need for user-friendly, relatively small, and open-sourced alternative GPT models arises from the desire to overcome these limitations while retaining high performance. In this survey paper, we provide an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility. Through this extensive survey, we aim to equip researchers, practitioners, and enthusiasts with a thorough understanding of user-friendly and relatively small open-sourced models of large GPTs, their current state, challenges, and future research directions, inspiring the development of more efficient, accessible, and versatile GPT models that cater to the broader scientific community and advance the field of general artificial intelligence. The source contents are continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",
            "corpus_id": 261243909,
            "sentences": [
                {
                    "corpus_id": "261243909",
                    "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
                    "text": "Pre-training typically requires a vast amount of data to achieve high performance, especially when the LLM parameter scale increases dramatically [2,39,101]. Larger datasets enable models to learn more robust and nuanced language representations. Along the history of the large language model development, we can see that the data scale increased significantly, same as the model parameters, for example, from billion tokens to tens of trillion tokens. However, it is crucial to emphasize that solely high-quality data are essential for performance improvement. Merely augmenting the dataset with low-quality data can result in a decline in performance [102]. Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training. \n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data. \n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples. \n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training. \n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality.",
                    "score": 0.5405186384475906,
                    "section_title": "Data Quantity and Quality",
                    "char_start_offset": 23736,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 802
                        },
                        {
                            "start": 805,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1413
                        },
                        {
                            "start": 1416,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1654
                        },
                        {
                            "start": 1657,
                            "end": 1822
                        },
                        {
                            "start": 1825,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2152
                        },
                        {
                            "start": 2153,
                            "end": 2311
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 146,
                            "end": 149,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 653,
                            "end": 658,
                            "matchedPaperCorpusId": "248986979"
                        },
                        {
                            "start": 1187,
                            "end": 1192,
                            "matchedPaperCorpusId": "245124124"
                        },
                        {
                            "start": 1192,
                            "end": 1196,
                            "matchedPaperCorpusId": "258832491"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71533203125
                }
            ],
            "relevance_judgement": 0.71533203125,
            "relevance_judgment_input_expanded": "# Title: Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models\n# Venue: arXiv.org\n# Authors: Kaiyuan Gao, Su He, Zhenyu He, Jiacheng Lin, Qizhi Pei, Jie Shao, Wei Zhang\n## Abstract\nGenerative pre-trained transformer (GPT) models have revolutionized the field of natural language processing (NLP) with remarkable performance in various tasks and also extend their power to multimodal domains. Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage. The need for user-friendly, relatively small, and open-sourced alternative GPT models arises from the desire to overcome these limitations while retaining high performance. In this survey paper, we provide an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility. Through this extensive survey, we aim to equip researchers, practitioners, and enthusiasts with a thorough understanding of user-friendly and relatively small open-sourced models of large GPTs, their current state, challenges, and future research directions, inspiring the development of more efficient, accessible, and versatile GPT models that cater to the broader scientific community and advance the field of general artificial intelligence. The source contents are continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.\n## Data Quantity and Quality\nPre-training typically requires a vast amount of data to achieve high performance, especially when the LLM parameter scale increases dramatically [2,39,101]. Larger datasets enable models to learn more robust and nuanced language representations. Along the history of the large language model development, we can see that the data scale increased significantly, same as the model parameters, for example, from billion tokens to tens of trillion tokens. However, it is crucial to emphasize that solely high-quality data are essential for performance improvement. Merely augmenting the dataset with low-quality data can result in a decline in performance [102]. Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training. \n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data. \n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples. \n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training. \n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality.",
            "reference_string": "[261243909 | Gao et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Investigating Web Corpus Filtering Methods for Language Model Development in Japanese",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 13,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.naacl-srw.18, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306632898",
                    "name": "Rintaro Enomoto"
                },
                {
                    "authorId": "153616317",
                    "name": "A. Tolmachev"
                },
                {
                    "authorId": "2301579122",
                    "name": "Takuro Niitsuma"
                },
                {
                    "authorId": "2306632538",
                    "name": "Shuhei Kurita"
                },
                {
                    "authorId": "2306632595",
                    "name": "Daisuke Kawahara"
                }
            ],
            "abstract": "The development of large language models (LLMs) is becoming increasingly significant, and there is a demand for high-quality, large-scale corpora for their pretraining.The quality of a web corpus is especially essential to improve the performance of LLMs because it accounts for a large proportion of the whole corpus. However, filtering methods for Web corpora have yet to be established.In this paper, we present empirical studies to reveal which filtering methods are indeed effective and analyze why they are.We build classifiers and language models in Japanese that can process large amounts of corpora rapidly enough for pretraining LLMs in limited computational resources. By evaluating these filtering methods based on a Web corpus quality evaluation benchmark, we reveal that the most accurate method is the N-gram language model. Indeed, we empirically present that strong filtering methods can rather lead to lesser performance in downstream tasks.We also report that the proportion of some specific topics in the processed documents decreases significantly during the filtering process.",
            "corpus_id": 270514462,
            "sentences": [
                {
                    "corpus_id": "270514462",
                    "title": "Investigating Web Corpus Filtering Methods for Language Model Development in Japanese",
                    "text": "The quality of the pretraining data significantly impacts the performance of large language models (LLMs) (Longpre et al., 2023;Gunasekar et al., 2023).The training data mostly comprise Web documents, and therefore it is essential to remove low-quality documents or paragraphs from them efficiently.However, no quality filtering method has been established for these documents.\n\nRule-based filtering can quickly remove documents with many unnecessary alphabets, symbols, and specific repetitive sentences.However, they have no comprehensive understanding of documents to be removed and can overdo or overlook certain types of documents.On the other hand, learning-based filtering methods can remove some low-quality documents that seem to maintain a level of quality and therefore bypass the rule-based filters.However, it is not verified yet which filtering methods are better and what types of documents are removed by such filters.\n\nIn empirical experiments, We examine learningbased filtering methods to remove low-quality documents in a Web corpus.We focus on Japanese corpora in this paper because there has been little study on methods for filtering Japanese corpora for developing LLMs, while many models have been developed in recent years, especially for the Japanese language.We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora.The experimental results show that the perplexity filtering method based on an N-gram language model is the best.We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022).The results show that massively strong filtering results in performance deterioration.Furthermore, topic analysis on the Web corpus shows that the proportion of specific topics decreases during the filtering process.",
                    "score": 0.5253102771891003,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 152,
                            "end": 299
                        },
                        {
                            "start": 299,
                            "end": 377
                        },
                        {
                            "start": 379,
                            "end": 505
                        },
                        {
                            "start": 505,
                            "end": 636
                        },
                        {
                            "start": 636,
                            "end": 811
                        },
                        {
                            "start": 811,
                            "end": 934
                        },
                        {
                            "start": 936,
                            "end": 1053
                        },
                        {
                            "start": 1053,
                            "end": 1287
                        },
                        {
                            "start": 1287,
                            "end": 1402
                        },
                        {
                            "start": 1402,
                            "end": 1515
                        },
                        {
                            "start": 1515,
                            "end": 1724
                        },
                        {
                            "start": 1724,
                            "end": 1810
                        },
                        {
                            "start": 1810,
                            "end": 1940
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1537,
                            "end": 1558,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1700,
                            "end": 1723,
                            "matchedPaperCorpusId": "249687727"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67919921875
                }
            ],
            "relevance_judgement": 0.67919921875,
            "relevance_judgment_input_expanded": "# Title: Investigating Web Corpus Filtering Methods for Language Model Development in Japanese\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Rintaro Enomoto, A. Tolmachev, Takuro Niitsuma, Shuhei Kurita, Daisuke Kawahara\n## Abstract\nThe development of large language models (LLMs) is becoming increasingly significant, and there is a demand for high-quality, large-scale corpora for their pretraining.The quality of a web corpus is especially essential to improve the performance of LLMs because it accounts for a large proportion of the whole corpus. However, filtering methods for Web corpora have yet to be established.In this paper, we present empirical studies to reveal which filtering methods are indeed effective and analyze why they are.We build classifiers and language models in Japanese that can process large amounts of corpora rapidly enough for pretraining LLMs in limited computational resources. By evaluating these filtering methods based on a Web corpus quality evaluation benchmark, we reveal that the most accurate method is the N-gram language model. Indeed, we empirically present that strong filtering methods can rather lead to lesser performance in downstream tasks.We also report that the proportion of some specific topics in the processed documents decreases significantly during the filtering process.\n## Introduction\nThe quality of the pretraining data significantly impacts the performance of large language models (LLMs) (Longpre et al., 2023;Gunasekar et al., 2023).The training data mostly comprise Web documents, and therefore it is essential to remove low-quality documents or paragraphs from them efficiently.However, no quality filtering method has been established for these documents.\n\nRule-based filtering can quickly remove documents with many unnecessary alphabets, symbols, and specific repetitive sentences.However, they have no comprehensive understanding of documents to be removed and can overdo or overlook certain types of documents.On the other hand, learning-based filtering methods can remove some low-quality documents that seem to maintain a level of quality and therefore bypass the rule-based filters.However, it is not verified yet which filtering methods are better and what types of documents are removed by such filters.\n\nIn empirical experiments, We examine learningbased filtering methods to remove low-quality documents in a Web corpus.We focus on Japanese corpora in this paper because there has been little study on methods for filtering Japanese corpora for developing LLMs, while many models have been developed in recent years, especially for the Japanese language.We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora.The experimental results show that the perplexity filtering method based on an N-gram language model is the best.We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022).The results show that massively strong filtering results in performance deterioration.Furthermore, topic analysis on the Web corpus shows that the proportion of specific topics decreases during the filtering process.",
            "reference_string": "[270514462 | Enomoto et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "venue": "Neurocomputing",
            "year": 2024,
            "reference_count": 203,
            "citation_count": 74,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.02038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116426849",
                    "name": "Yi-Hsueh Liu"
                },
                {
                    "authorId": "2155082967",
                    "name": "Haoyang He"
                },
                {
                    "authorId": "2184719751",
                    "name": "Tianle Han"
                },
                {
                    "authorId": "2273584640",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2210636248",
                    "name": "Mengyuan Liu"
                },
                {
                    "authorId": "2257433902",
                    "name": "Jiaming Tian"
                },
                {
                    "authorId": "2257095790",
                    "name": "Yutong Zhang"
                },
                {
                    "authorId": "2110238778",
                    "name": "Jiaqi Wang"
                },
                {
                    "authorId": "2277869261",
                    "name": "Xiaohui Gao"
                },
                {
                    "authorId": "2215167446",
                    "name": "Tianyang Zhong"
                },
                {
                    "authorId": "2221032216",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2211904452",
                    "name": "Shaochen Xu"
                },
                {
                    "authorId": "2263593041",
                    "name": "Zihao Wu"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2257586495",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2277750447",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1742535",
                    "name": "Xintao Hu"
                },
                {
                    "authorId": "49104946",
                    "name": "Tuo Zhang"
                },
                {
                    "authorId": "2251076040",
                    "name": "Ning Qiang"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2257302793",
                    "name": "Bao Ge"
                }
            ],
            "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.",
            "corpus_id": 266755678,
            "sentences": [
                {
                    "corpus_id": "266755678",
                    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                    "text": "Once an adequate corpus of data is collected, the subsequent step is data preprocessing. The quality of data preprocessing directly impacts the model's performance and security. The specific preprocessing steps involve filtering low-quality text, including eliminating toxic and biased content to ensure the model aligns with human ethical standards. It also includes deduplication, removing duplicates in the training set, and excluding redundant content in the test set to maintain the sample distribution balance. Privacy scrubbing is applied to ensure the model's security, preventing information leakage or other privacy-related concerns. Additionally, if fine-tuning LLMs is considered, expanding the vocabulary should also be considered. On the other hand, LLaMA 2 models [10] represent a notable exception. These models forego filtering in their pretraining corpus, as aggressive filtration might accidentally filter out some demographic groups. This approach enhances the generalizability of the base LLaMA 2 models, making them more adept across a range of downstream tasks, such as hate speech detection and privacy de-identification. Observations indicate that abstaining from additional filtering in the pretraining data enables the base model to achieve reasonable safety alignment with fewer examples [10]. While this increases both generalizability and safety alignment efficiency, the implementation of additional safety mitigations is still imperative prior to public deployment, as further discussed in Section 3.5.4. \n\nQuality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText [85] to filter out low-quality datasets. \n\nDeduplication: Language models may sometimes repetitively generate the same content during text generation, potentially due to a high degree of repetition in the training data. Extensive repetition can lead to training instability, resulting in a decline in the performance of LLMs [86].",
                    "score": 0.4963321290948211,
                    "section_title": "Data preprocessing",
                    "char_start_offset": 29962,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 953
                        },
                        {
                            "start": 954,
                            "end": 1145
                        },
                        {
                            "start": 1146,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1536
                        },
                        {
                            "start": 1539,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1965
                        },
                        {
                            "start": 1966,
                            "end": 2103
                        },
                        {
                            "start": 2106,
                            "end": 2282
                        },
                        {
                            "start": 2283,
                            "end": 2393
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2063,
                            "end": 2067,
                            "matchedPaperCorpusId": "160025533"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63134765625
                }
            ],
            "relevance_judgement": 0.63134765625,
            "relevance_judgment_input_expanded": "# Title: Understanding LLMs: A Comprehensive Overview from Training to Inference\n# Venue: Neurocomputing\n# Authors: Yi-Hsueh Liu, Haoyang He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zheng Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge\n## Abstract\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.\n## Data preprocessing\nOnce an adequate corpus of data is collected, the subsequent step is data preprocessing. The quality of data preprocessing directly impacts the model's performance and security. The specific preprocessing steps involve filtering low-quality text, including eliminating toxic and biased content to ensure the model aligns with human ethical standards. It also includes deduplication, removing duplicates in the training set, and excluding redundant content in the test set to maintain the sample distribution balance. Privacy scrubbing is applied to ensure the model's security, preventing information leakage or other privacy-related concerns. Additionally, if fine-tuning LLMs is considered, expanding the vocabulary should also be considered. On the other hand, LLaMA 2 models [10] represent a notable exception. These models forego filtering in their pretraining corpus, as aggressive filtration might accidentally filter out some demographic groups. This approach enhances the generalizability of the base LLaMA 2 models, making them more adept across a range of downstream tasks, such as hate speech detection and privacy de-identification. Observations indicate that abstaining from additional filtering in the pretraining data enables the base model to achieve reasonable safety alignment with fewer examples [10]. While this increases both generalizability and safety alignment efficiency, the implementation of additional safety mitigations is still imperative prior to public deployment, as further discussed in Section 3.5.4. \n\nQuality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText [85] to filter out low-quality datasets. \n\nDeduplication: Language models may sometimes repetitively generate the same content during text generation, potentially due to a high degree of repetition in the training data. Extensive repetition can lead to training instability, resulting in a decline in the performance of LLMs [86].",
            "reference_string": "[266755678 | Liu et al. | 2024 | Citations: 74]"
        },
        {
            "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260817933",
                    "name": "Jiazheng Li"
                },
                {
                    "authorId": "2350870800",
                    "name": "Lu Yu"
                },
                {
                    "authorId": "2279851906",
                    "name": "Qing Cui"
                },
                {
                    "authorId": "2344807347",
                    "name": "Zhiqiang Zhang"
                },
                {
                    "authorId": "2344948641",
                    "name": "Jun Zhou"
                },
                {
                    "authorId": "2093920413",
                    "name": "Yanfang Ye"
                },
                {
                    "authorId": "2117879943",
                    "name": "Chuxu Zhang"
                }
            ],
            "abstract": "High-quality data plays a critical role in the pretraining and fine-tuning of large language models (LLMs), even determining their performance ceiling to some degree. Consequently, numerous data selection methods have been proposed to identify subsets of data that can effectively and efficiently enhance model performance. However, most of these methods focus on general data selection and tend to overlook the specific nuances of domain-related data. In this paper, we introduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using the \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning domain. By taking into account the unique characteristics of mathematics and reasoning, we construct a skill graph that captures the mathematical skills and their interrelations from a reference dataset. This skill graph guides us in assigning quality scores to the target dataset, enabling us to select the top-ranked subset which is further used to pretrain LLMs. Experimental results demonstrate the efficiency and effectiveness of MASS across different model sizes (1B and 7B) and pretraining datasets (web data and synthetic data). Specifically, in terms of efficiency, models trained on subsets selected by MASS can achieve similar performance to models trained on the original datasets, with a significant reduction in the number of trained tokens - ranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained on the same amount of tokens, models trained on the data selected by MASS outperform those trained on the original datasets by 3.3\\% to 5.9\\%. These results underscore the potential of MASS to improve both the efficiency and effectiveness of pretraining LLMs.",
            "corpus_id": 277113645,
            "sentences": [
                {
                    "corpus_id": "277113645",
                    "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
                    "text": "The success of large language models (LLMs) is closely tied to the scaling up of model size, training data, and computational resources (Brown, 2020;Hoffmann et al., 2022;Qwen Team, 2024). Among these three factors, training data serves as the foundation of LLMs' rich knowledge and capabilities, enabling them to excel in creative writing, complex reasoning, and even agentic planning. Recently, there is mounting evidence that pretraining on high-quality and diverse corpora can significantly enhance LLM performance (Penedo et al., 2024;Li et al., 2024a). For example, Microsoft's Phi series of models has been renowned for their pretraining corpus's quality and efficiency. The latest Phi-4 model (Abdin et al., 2024), trained on 10T tokens, has surpassed other models of the same size, including Qwen 2.5 (Qwen Team, 2024), which was trained on 18 trillion tokens. This has led many researchers to focus on LLM data curation methodologies, which include the selection of high-quality sub-datasets (Wettig et al., 2024), deduplication of extensive datasets (Tirumala et al., 2023), and the balancing of data from various domains (Xie et al., 2024), among other techniques. \n\nIn terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value.",
                    "score": 0.5036974694388894,
                    "section_title": "Introduction",
                    "char_start_offset": 2053,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1176
                        },
                        {
                            "start": 1179,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1962
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 519,
                            "end": 540,
                            "matchedPaperCorpusId": "270711474"
                        },
                        {
                            "start": 540,
                            "end": 557,
                            "matchedPaperCorpusId": "270560330"
                        },
                        {
                            "start": 1002,
                            "end": 1023,
                            "matchedPaperCorpusId": "267681974"
                        },
                        {
                            "start": 1133,
                            "end": 1151,
                            "matchedPaperCorpusId": "258741043"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59228515625
                }
            ],
            "relevance_judgement": 0.59228515625,
            "relevance_judgment_input_expanded": "# Title: MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models\n# Venue: arXiv.org\n# Authors: Jiazheng Li, Lu Yu, Qing Cui, Zhiqiang Zhang, Jun Zhou, Yanfang Ye, Chuxu Zhang\n## Abstract\nHigh-quality data plays a critical role in the pretraining and fine-tuning of large language models (LLMs), even determining their performance ceiling to some degree. Consequently, numerous data selection methods have been proposed to identify subsets of data that can effectively and efficiently enhance model performance. However, most of these methods focus on general data selection and tend to overlook the specific nuances of domain-related data. In this paper, we introduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using the \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning domain. By taking into account the unique characteristics of mathematics and reasoning, we construct a skill graph that captures the mathematical skills and their interrelations from a reference dataset. This skill graph guides us in assigning quality scores to the target dataset, enabling us to select the top-ranked subset which is further used to pretrain LLMs. Experimental results demonstrate the efficiency and effectiveness of MASS across different model sizes (1B and 7B) and pretraining datasets (web data and synthetic data). Specifically, in terms of efficiency, models trained on subsets selected by MASS can achieve similar performance to models trained on the original datasets, with a significant reduction in the number of trained tokens - ranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained on the same amount of tokens, models trained on the data selected by MASS outperform those trained on the original datasets by 3.3\\% to 5.9\\%. These results underscore the potential of MASS to improve both the efficiency and effectiveness of pretraining LLMs.\n## Introduction\nThe success of large language models (LLMs) is closely tied to the scaling up of model size, training data, and computational resources (Brown, 2020;Hoffmann et al., 2022;Qwen Team, 2024). Among these three factors, training data serves as the foundation of LLMs' rich knowledge and capabilities, enabling them to excel in creative writing, complex reasoning, and even agentic planning. Recently, there is mounting evidence that pretraining on high-quality and diverse corpora can significantly enhance LLM performance (Penedo et al., 2024;Li et al., 2024a). For example, Microsoft's Phi series of models has been renowned for their pretraining corpus's quality and efficiency. The latest Phi-4 model (Abdin et al., 2024), trained on 10T tokens, has surpassed other models of the same size, including Qwen 2.5 (Qwen Team, 2024), which was trained on 18 trillion tokens. This has led many researchers to focus on LLM data curation methodologies, which include the selection of high-quality sub-datasets (Wettig et al., 2024), deduplication of extensive datasets (Tirumala et al., 2023), and the balancing of data from various domains (Xie et al., 2024), among other techniques. \n\nIn terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value.",
            "reference_string": "[277113645 | Li et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.15484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2061741965",
                    "name": "Aaron Levine"
                },
                {
                    "authorId": "2293356757",
                    "name": "Connie Huang"
                },
                {
                    "authorId": "2293351792",
                    "name": "Chenguang Wang"
                },
                {
                    "authorId": "2293313172",
                    "name": "Eduardo Batista"
                },
                {
                    "authorId": "2293311576",
                    "name": "Ewa Szymanska"
                },
                {
                    "authorId": "2293527287",
                    "name": "Hongyi Ding"
                },
                {
                    "authorId": "2121371065",
                    "name": "Houwei Chou"
                },
                {
                    "authorId": "2099242119",
                    "name": "Jean-Fran\u00e7ois Pessiot"
                },
                {
                    "authorId": "2293313121",
                    "name": "Johanes Effendi"
                },
                {
                    "authorId": "2293313467",
                    "name": "Justin Chiu"
                },
                {
                    "authorId": "147240287",
                    "name": "Kai Torben Ohlhus"
                },
                {
                    "authorId": "2293312430",
                    "name": "Karan Chopra"
                },
                {
                    "authorId": "40466108",
                    "name": "Keiji Shinzato"
                },
                {
                    "authorId": "2313561595",
                    "name": "Koji Murakami"
                },
                {
                    "authorId": "2293314500",
                    "name": "Lee Xiong"
                },
                {
                    "authorId": "2293403169",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2293314132",
                    "name": "Maki Kubota"
                },
                {
                    "authorId": "2295735234",
                    "name": "Maksim Tkatchenko"
                },
                {
                    "authorId": "2293317836",
                    "name": "Miroku Lee"
                },
                {
                    "authorId": "2293314089",
                    "name": "Naoki Takahashi"
                },
                {
                    "authorId": "35640774",
                    "name": "Prathyusha Jwalapuram"
                },
                {
                    "authorId": "2293313287",
                    "name": "Ryutaro Tatsushima"
                },
                {
                    "authorId": "2165228954",
                    "name": "Saurabh Jain"
                },
                {
                    "authorId": "49596302",
                    "name": "Sunil Kumar Yadav"
                },
                {
                    "authorId": "2293312307",
                    "name": "Ting Cai"
                },
                {
                    "authorId": "2271252858",
                    "name": "Wei-Te Chen"
                },
                {
                    "authorId": "3456546",
                    "name": "Yandi Xia"
                },
                {
                    "authorId": "2293302761",
                    "name": "Yuki Nakayama"
                },
                {
                    "authorId": "2293313297",
                    "name": "Yutaka Higashiyama"
                }
            ],
            "abstract": "We introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.",
            "corpus_id": 268680360,
            "sentences": [
                {
                    "corpus_id": "268680360",
                    "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
                    "text": "Previous research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;Dodge et al., 2021;Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English.We remove any personally identifiable information in the datasets such as phone numbers, email addresses etc.\n\nThe filtering process comprises of multiple stages, including normalization, deduplication and classification, that helps us to distinguish between high and low-quality data.This approach ensures that our model is trained on a curated dataset, enhancing its ability to generate coherent and relevant outputs.We train the released models on approximately 175 billion tokens of filtered data.",
                    "score": 0.48700591881936484,
                    "section_title": "Foundation Model Training",
                    "char_start_offset": 3237,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 189,
                            "end": 378
                        },
                        {
                            "start": 378,
                            "end": 487
                        },
                        {
                            "start": 489,
                            "end": 663
                        },
                        {
                            "start": 663,
                            "end": 797
                        },
                        {
                            "start": 797,
                            "end": 879
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 146,
                            "end": 165,
                            "matchedPaperCorpusId": "237568724"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57763671875
                }
            ],
            "relevance_judgement": 0.57763671875,
            "relevance_judgment_input_expanded": "# Title: RakutenAI-7B: Extending Large Language Models for Japanese\n# Venue: arXiv.org\n# Authors: Aaron Levine, Connie Huang, Chenguang Wang, Eduardo Batista, Ewa Szymanska, Hongyi Ding, Houwei Chou, Jean-Fran\u00e7ois Pessiot, Johanes Effendi, Justin Chiu, Kai Torben Ohlhus, Karan Chopra, Keiji Shinzato, Koji Murakami, Lee Xiong, Lei Chen, Maki Kubota, Maksim Tkatchenko, Miroku Lee, Naoki Takahashi, Prathyusha Jwalapuram, Ryutaro Tatsushima, Saurabh Jain, Sunil Kumar Yadav, Ting Cai, Wei-Te Chen, Yandi Xia, Yuki Nakayama, Yutaka Higashiyama\n## Abstract\nWe introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.\n## Foundation Model Training\nPrevious research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;Dodge et al., 2021;Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English.We remove any personally identifiable information in the datasets such as phone numbers, email addresses etc.\n\nThe filtering process comprises of multiple stages, including normalization, deduplication and classification, that helps us to distinguish between high and low-quality data.This approach ensures that our model is trained on a curated dataset, enhancing its ability to generate coherent and relevant outputs.We train the released models on approximately 175 billion tokens of filtered data.",
            "reference_string": "[268680360 | Levine et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Fewer Truncations Improve Language Modeling",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 14,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2113455281",
                    "name": "Hantian Ding"
                },
                {
                    "authorId": "2259065741",
                    "name": "Zijian Wang"
                },
                {
                    "authorId": "2296990653",
                    "name": "Giovanni Paolini"
                },
                {
                    "authorId": "40574366",
                    "name": "Varun Kumar"
                },
                {
                    "authorId": "1713801",
                    "name": "Anoop Deoras"
                },
                {
                    "authorId": "2258962983",
                    "name": "Dan Roth"
                },
                {
                    "authorId": "2264070792",
                    "name": "Stefano Soatto"
                }
            ],
            "abstract": "In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.",
            "corpus_id": 269187631,
            "sentences": [
                {
                    "corpus_id": "269187631",
                    "title": "Fewer Truncations Improve Language Modeling",
                    "text": "Pre-training Data Pre-training data is pivotal to the quality of language models.There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2020b), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2022;Marion et al., 2023;Chen et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023a;Raffel et al., 2020a)) propose various filtering strategies to improve data quality.Our work broadly applies on top of these pre-training datasets.\n\nData grouping in language model training Recent transformer language models have adopted different strategies to group training data into batched sequences in order to tackle the variable document length problem.For encoder-only models, the choice of data formatting was first studied in RoBERTa (Liu et al., 2019), which shows that concatenating sentences from more than one documents in the same training sequence results in very little performance degradation.Krell et al. (2021) proposed an approximation-based combinatorial packing method to accelerate BERT training (Devlin et al., 2019), yet without improving downstream performance.It is worth mentioning that document truncation is less of a concern for encoder models for two reasons: first, they are usually trained on relatively short text spans of 128-512 tokens that only respect sentence boundary.In such case, document-wise truncation is inevitable given the limited context size.Second, they are not intended for openended generation, and thus, hallucination is not an issue.",
                    "score": 0.45035597502185076,
                    "section_title": "Related Work",
                    "char_start_offset": 23240,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 81
                        },
                        {
                            "start": 81,
                            "end": 354
                        },
                        {
                            "start": 354,
                            "end": 579
                        },
                        {
                            "start": 579,
                            "end": 642
                        },
                        {
                            "start": 644,
                            "end": 856
                        },
                        {
                            "start": 856,
                            "end": 1107
                        },
                        {
                            "start": 1107,
                            "end": 1284
                        },
                        {
                            "start": 1284,
                            "end": 1506
                        },
                        {
                            "start": 1506,
                            "end": 1590
                        },
                        {
                            "start": 1590,
                            "end": 1686
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 184,
                            "end": 206,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 394,
                            "end": 412,
                            "matchedPaperCorpusId": "235829052"
                        },
                        {
                            "start": 450,
                            "end": 473,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 495,
                            "end": 516,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1216,
                            "end": 1236,
                            "matchedPaperCorpusId": "226096901"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.576171875
                }
            ],
            "relevance_judgement": 0.576171875,
            "relevance_judgment_input_expanded": "# Title: Fewer Truncations Improve Language Modeling\n# Venue: International Conference on Machine Learning\n# Authors: Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, Stefano Soatto\n## Abstract\nIn large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.\n## Related Work\nPre-training Data Pre-training data is pivotal to the quality of language models.There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2020b), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2022;Marion et al., 2023;Chen et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023a;Raffel et al., 2020a)) propose various filtering strategies to improve data quality.Our work broadly applies on top of these pre-training datasets.\n\nData grouping in language model training Recent transformer language models have adopted different strategies to group training data into batched sequences in order to tackle the variable document length problem.For encoder-only models, the choice of data formatting was first studied in RoBERTa (Liu et al., 2019), which shows that concatenating sentences from more than one documents in the same training sequence results in very little performance degradation.Krell et al. (2021) proposed an approximation-based combinatorial packing method to accelerate BERT training (Devlin et al., 2019), yet without improving downstream performance.It is worth mentioning that document truncation is less of a concern for encoder models for two reasons: first, they are usually trained on relatively short text spans of 128-512 tokens that only respect sentence boundary.In such case, document-wise truncation is inevitable given the limited context size.Second, they are not intended for openended generation, and thus, hallucination is not an issue.",
            "reference_string": "[269187631 | Ding et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219037377",
                    "name": "Bettina Messmer"
                },
                {
                    "authorId": "2350460665",
                    "name": "Vinko Sabolcec"
                },
                {
                    "authorId": "2328413457",
                    "name": "Martin Jaggi"
                }
            ],
            "abstract": "Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.",
            "corpus_id": 276394897,
            "sentences": [
                {
                    "corpus_id": "276394897",
                    "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
                    "text": "Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.",
                    "score": 0.4107176685152487,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56005859375
                }
            ],
            "relevance_judgement": 0.56005859375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Multilingual LLM Pretraining with Model-Based Data Selection\n# Venue: arXiv.org\n# Authors: Bettina Messmer, Vinko Sabolcec, Martin Jaggi\n## Abstract\nDataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.\n",
            "reference_string": "[276394897 | Messmer et al. | 2025 | Citations: 2]"
        },
        {
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.01149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264139789",
                    "name": "Jianghao Chen"
                },
                {
                    "authorId": "2329739061",
                    "name": "Pu Jian"
                },
                {
                    "authorId": "2264961304",
                    "name": "Tengxiao Xi"
                },
                {
                    "authorId": "2265214644",
                    "name": "Yidong Yi"
                },
                {
                    "authorId": "8134471",
                    "name": "Qianlong Du"
                },
                {
                    "authorId": "2264336906",
                    "name": "Chenglin Ding"
                },
                {
                    "authorId": "2894321",
                    "name": "Guibo Zhu"
                },
                {
                    "authorId": "2064100826",
                    "name": "Chengqing Zong"
                },
                {
                    "authorId": "1519293616",
                    "name": "Jinqiao Wang"
                },
                {
                    "authorId": "2124819243",
                    "name": "Jiajun Zhang"
                }
            ],
            "abstract": "During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we can easily utilize an appropriate threshold to select the high-quality pre-training data for Chinese. Using our proposed approach, we release the largest and latest large-scale high-quality Chinese web text ChineseWebText, which consists of 1.42 TB and each text is associated with a quality score, facilitating the LLM researchers to choose the data according to the desired quality thresholds. We also release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90%.",
            "corpus_id": 264935645,
            "sentences": [
                {
                    "corpus_id": "264935645",
                    "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
                    "text": "In order to extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score. With the proposed approach, we release the latest and largest Chinese dataset of 1.4 TB, each of which is associated with a quality score, facilitating the LLMs researchers to re-filter the data with desired quality thresholds. We further release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90% by human evaluation. We also release the complete tool-chain that processes the raw data into the clean texts. \n\nIn the future, we will continue to enlarge the Chinese dataset with newly incoming web data. Meanwhile, we are going to explore better algorithms and strategies for data filtering. For example, we can design quality evaluation models for each kind of data noise.",
                    "score": 0.49579864991447486,
                    "section_title": "Conclusions and Future Work",
                    "char_start_offset": 32436,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 953
                        },
                        {
                            "start": 956,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1218
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55908203125
                }
            ],
            "relevance_judgement": 0.55908203125,
            "relevance_judgment_input_expanded": "# Title: ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model\n# Venue: arXiv.org\n# Authors: Jianghao Chen, Pu Jian, Tengxiao Xi, Yidong Yi, Qianlong Du, Chenglin Ding, Guibo Zhu, Chengqing Zong, Jinqiao Wang, Jiajun Zhang\n## Abstract\nDuring the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we can easily utilize an appropriate threshold to select the high-quality pre-training data for Chinese. Using our proposed approach, we release the largest and latest large-scale high-quality Chinese web text ChineseWebText, which consists of 1.42 TB and each text is associated with a quality score, facilitating the LLM researchers to choose the data according to the desired quality thresholds. We also release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90%.\n## Conclusions and Future Work\nIn order to extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score. With the proposed approach, we release the latest and largest Chinese dataset of 1.4 TB, each of which is associated with a quality score, facilitating the LLMs researchers to re-filter the data with desired quality thresholds. We further release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90% by human evaluation. We also release the complete tool-chain that processes the raw data into the clean texts. \n\nIn the future, we will continue to enlarge the Chinese dataset with newly incoming web data. Meanwhile, we are going to explore better algorithms and strategies for data filtering. For example, we can design quality evaluation models for each kind of data noise.",
            "reference_string": "[264935645 | Chen et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185631323",
                    "name": "Kun Li"
                },
                {
                    "authorId": "2146333115",
                    "name": "Tianhua Zhang"
                },
                {
                    "authorId": "2155851529",
                    "name": "Yunxiang Li"
                },
                {
                    "authorId": "1944274",
                    "name": "Hongyin Luo"
                },
                {
                    "authorId": "2348476303",
                    "name": "Abdalla Moustafa"
                },
                {
                    "authorId": "2107999711",
                    "name": "Xixin Wu"
                },
                {
                    "authorId": "2303402875",
                    "name": "James Glass"
                },
                {
                    "authorId": "2273659859",
                    "name": "Helen M. Meng"
                }
            ],
            "abstract": "Improving context faithfulness in large language models is essential for developing trustworthy retrieval augmented generation systems and mitigating hallucinations, especially in long-form question answering (LFQA) tasks or scenarios involving knowledge conflicts. Existing methods either intervene LLMs only at inference without addressing their inherent limitations or overlook the potential for self-improvement. In this paper, we introduce GenDiE (Generate, Discriminate, Evolve), a novel self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. GenDiE combines both generative and discriminative training, equipping LLMs with self-generation and self-scoring capabilities to facilitate iterative self-evolution. This supports both data construction for model alignment and score-guided search during inference. Furthermore, by treating each sentence in a response as an independent optimization unit, GenDiE effectively addresses the limitations of previous approaches that optimize at the holistic answer level, which may miss unfaithful details. Experiments on ASQA (in-domain LFQA) and ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE surpasses various baselines in both faithfulness and correctness, and exhibits robust performance for domain adaptation.",
            "corpus_id": 276776523,
            "sentences": [
                {
                    "corpus_id": "276776523",
                    "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution",
                    "text": "A.1 Quality Control Due to extensive pre-training on massive corpora, LLMs encapsulate substantial world knowledge within their parameters (Yu et al., 2023;Wang et al., 2023). As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control. Given an input sequence x, we denote the (length-normalized) negative log-likelihood (NLL) loss in generating the output sequence y with m tokens as follows: \n\nFor each candidate training instance {(q, A \u227aa , a, a \u2032 )}, we compare the NLL loss reduction ratio in generating a candidate with and without input passages to select valid negative sentences: \n\nwhere [ * ] denotes the concatenation. The motivation is to ensure that the selected negative samples a \u2032 remain less faithful to the input question than the positive answer a, even when evidence passages are provided. This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4. For each training item, we use model \u03b8 0 to sample six negative sentence candidates, as described in \u00a72.2.1. After filtering via Eq. 4, we retain at most two of the most negative sentences, ranked by the NLL loss reduction ratio, for filtered setting (some items may contain only one valid negative sentence after filtering). In contrast, the non-filtered setting uses two randomly selected sentences from the six candidates. As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics.",
                    "score": 0.441288060021433,
                    "section_title": "A Method Details",
                    "char_start_offset": 30354,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 825
                        },
                        {
                            "start": 828,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1171
                        },
                        {
                            "start": 1172,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1832
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54931640625
                }
            ],
            "relevance_judgement": 0.54931640625,
            "relevance_judgment_input_expanded": "# Title: Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution\n# Venue: arXiv.org\n# Authors: Kun Li, Tianhua Zhang, Yunxiang Li, Hongyin Luo, Abdalla Moustafa, Xixin Wu, James Glass, Helen M. Meng\n## Abstract\nImproving context faithfulness in large language models is essential for developing trustworthy retrieval augmented generation systems and mitigating hallucinations, especially in long-form question answering (LFQA) tasks or scenarios involving knowledge conflicts. Existing methods either intervene LLMs only at inference without addressing their inherent limitations or overlook the potential for self-improvement. In this paper, we introduce GenDiE (Generate, Discriminate, Evolve), a novel self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. GenDiE combines both generative and discriminative training, equipping LLMs with self-generation and self-scoring capabilities to facilitate iterative self-evolution. This supports both data construction for model alignment and score-guided search during inference. Furthermore, by treating each sentence in a response as an independent optimization unit, GenDiE effectively addresses the limitations of previous approaches that optimize at the holistic answer level, which may miss unfaithful details. Experiments on ASQA (in-domain LFQA) and ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE surpasses various baselines in both faithfulness and correctness, and exhibits robust performance for domain adaptation.\n## A Method Details\nA.1 Quality Control Due to extensive pre-training on massive corpora, LLMs encapsulate substantial world knowledge within their parameters (Yu et al., 2023;Wang et al., 2023). As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control. Given an input sequence x, we denote the (length-normalized) negative log-likelihood (NLL) loss in generating the output sequence y with m tokens as follows: \n\nFor each candidate training instance {(q, A \u227aa , a, a \u2032 )}, we compare the NLL loss reduction ratio in generating a candidate with and without input passages to select valid negative sentences: \n\nwhere [ * ] denotes the concatenation. The motivation is to ensure that the selected negative samples a \u2032 remain less faithful to the input question than the positive answer a, even when evidence passages are provided. This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4. For each training item, we use model \u03b8 0 to sample six negative sentence candidates, as described in \u00a72.2.1. After filtering via Eq. 4, we retain at most two of the most negative sentences, ranked by the NLL loss reduction ratio, for filtered setting (some items may contain only one valid negative sentence after filtering). In contrast, the non-filtered setting uses two randomly selected sentences from the six candidates. As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics.",
            "reference_string": "[276776523 | Li et al. | 2025 | Citations: 2]"
        },
        {
            "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 17,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.06408, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "15983089",
                    "name": "Li Lucy"
                },
                {
                    "authorId": "40895369",
                    "name": "Suchin Gururangan"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "2064411219",
                    "name": "David Bamman"
                },
                {
                    "authorId": "2279335512",
                    "name": "Lauren Klein"
                },
                {
                    "authorId": "34176020",
                    "name": "Jesse Dodge"
                }
            ],
            "abstract": "Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten\"quality\"and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.",
            "corpus_id": 266977204,
            "sentences": [
                {
                    "corpus_id": "266977204",
                    "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
                    "text": "Appendix B includes additional details on the reproduction of each filter. Descriptions of pretraining data curation are sometimes too vague or non-existent to allow for exact replication (OpenAI, 2023), but multiple recent and prominent LLMs still allude to the use of model-and heuristic-based data filters (Touvron et al., 2023a;Gemini Team et al., 2023;Chowdhery et al., 2023). \n\nModel-based quality. We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2020;Lauren\u00e7on et al., 2022;Muennighoff et al., 2023;Marion et al., 2023). \n\nHeuristic-based quality. Another quality filtering approach for web text applies rule-based heuristics (Raffel et al., 2023;Rae et al., 2021). We examine 19 document-level heuristics and thresholds from Gopher (Raffel et al., 2023). These heuristics remove documents that do not meet thresholds pertaining to document and word length, textual repetition, and frequencies of symbols and common English words (Appendix B). \n\nEnglish langID. Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a).",
                    "score": 0.43983257103779994,
                    "section_title": "Pretraining Data Filters",
                    "char_start_offset": 13304,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 381
                        },
                        {
                            "start": 384,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1621
                        },
                        {
                            "start": 1624,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1749
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1108,
                            "end": 1129,
                            "matchedPaperCorpusId": "207870323"
                        },
                        {
                            "start": 1129,
                            "end": 1152,
                            "matchedPaperCorpusId": "257378329"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.529296875
                }
            ],
            "relevance_judgement": 0.529296875,
            "relevance_judgment_input_expanded": "# Title: AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David Bamman, Lauren Klein, Jesse Dodge\n## Abstract\nLarge language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten\"quality\"and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.\n## Pretraining Data Filters\nAppendix B includes additional details on the reproduction of each filter. Descriptions of pretraining data curation are sometimes too vague or non-existent to allow for exact replication (OpenAI, 2023), but multiple recent and prominent LLMs still allude to the use of model-and heuristic-based data filters (Touvron et al., 2023a;Gemini Team et al., 2023;Chowdhery et al., 2023). \n\nModel-based quality. We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2020;Lauren\u00e7on et al., 2022;Muennighoff et al., 2023;Marion et al., 2023). \n\nHeuristic-based quality. Another quality filtering approach for web text applies rule-based heuristics (Raffel et al., 2023;Rae et al., 2021). We examine 19 document-level heuristics and thresholds from Gopher (Raffel et al., 2023). These heuristics remove documents that do not meet thresholds pertaining to document and word length, textual repetition, and frequencies of symbols and common English words (Appendix B). \n\nEnglish langID. Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a).",
            "reference_string": "[266977204 | Lucy et al. | 2024 | Citations: 17]"
        },
        {
            "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.19668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333395980",
                    "name": "Wanyue Zhang"
                },
                {
                    "authorId": "2333247185",
                    "name": "Ziyong Li"
                },
                {
                    "authorId": "2218735807",
                    "name": "Wen Yang"
                },
                {
                    "authorId": "2333234263",
                    "name": "Chunlin Leng"
                },
                {
                    "authorId": "2333317901",
                    "name": "Yinan Bai"
                },
                {
                    "authorId": "8134471",
                    "name": "Qianlong Du"
                },
                {
                    "authorId": "2064100826",
                    "name": "Chengqing Zong"
                },
                {
                    "authorId": "2283356334",
                    "name": "Jiajun Zhang"
                }
            ],
            "abstract": "During the development of large language models (LLMs), pre-training data play a critical role in shaping LLMs' capabilities. In recent years several large-scale and high-quality pre-training datasets have been released to accelerate the research of LLMs, including ChineseWebText1.0, C4, Pile, WanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has increasingly shifted to domain-specific capabilities and safety concerns, making those previous coarse-grained texts insufficient for meeting training requirements. Furthermore, fine-grained information, such as quality, domain and toxicity, is becoming increasingly important in building powerful and reliable LLMs for various scenarios. To address these challenges, in this paper we propose a new tool-chain called MDFG-tool for constructing large-scale and high-quality Chinese datasets with multi-dimensional and fine-grained information. First, we employ manually crafted rules to discard explicit noisy texts from raw contents. Second, the quality evaluation model, domain classifier, and toxicity evaluation model are well-designed to assess the remaining cleaned data respectively. Finally, we integrate these three types of fine-grained information for each text. With this approach, we release the largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which consists of 3.8TB and each text is associated with a quality score, domain labels, a toxicity label and a toxicity score, facilitating the LLM researchers to select data based on various types of fine-grained information. The data, codes and the tool-chain are available on this website https://github.com/CASIA-LM/ChineseWebText-2.0",
            "corpus_id": 274422859,
            "sentences": [
                {
                    "corpus_id": "274422859",
                    "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
                    "text": "This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances. Similar to quality evaluation methods, domain classification approaches commonly utilize models such as logistic regression, BERT, and FastText to assign domain labels to each text. Among these, FastText is a neural network-based approach similar to CBOW [22]. Compared to other methods, FastText offers efficient and rapid training on large-scale datasets while maintaining comparable classification performance. As a result, this paper employs the FastText model to classify domain labels for each text. \n\nToxicity Evaluation. Toxic texts are generally defined as those containing offensive or adult content [23,24]. Offensive content can be classified into three levels of severity: offensive text, abusive text, and hate text [25]. Adult content typically includes themes related to pornography or violence. Due to differences in cultural values and political perspectives, the analysis of toxic texts varies across countries. During the development of LLMs, the presence of toxic content in pre-training datasets would compromise the safety of LLMs. Consequently, toxicity evaluation has become increasingly important for the construction of pre-training datasets. In recent years, Caselli et al. [25] construct a toxicity dataset, RAL-E, using data from banned Reddit communities. They then train a BERT-based toxicity evaluation model with this dataset. Hartvigsen et al. [26] leverage GPT-3 to generate a dataset containing both subtle toxic texts and benign examples through adversarial techniques. In contrast, Deng et al. [27] construct a Chinese toxicity dataset with data from Zhihu and Weibo, and annotate each text with the help of human experts and LLMs. However, previous work often suffers from small data scales and insufficient coverage, limiting its ability to effectively evaluate the toxicity of LLMs. To address the above issues, this paper aims to construct a more robust toxicity evaluation model and release a larger-scale, higher-coverage toxicity dataset, thereby enhancing toxicity evaluation capabilities and improving the safety of LLMs. \n\nDatasets for Pre-training. As the foundation of LLMs, large-scale pre-training datasets play a crucial role in enhancing the capabilities of these models.",
                    "score": 0.41265639263157416,
                    "section_title": "Related Work",
                    "char_start_offset": 3922,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 664
                        },
                        {
                            "start": 667,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1445
                        },
                        {
                            "start": 1446,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1829
                        },
                        {
                            "start": 1830,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2228
                        },
                        {
                            "start": 2231,
                            "end": 2257
                        },
                        {
                            "start": 2258,
                            "end": 2385
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 769,
                            "end": 773,
                            "matchedPaperCorpusId": "214051295"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5185546875
                },
                {
                    "corpus_id": "274422859",
                    "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
                    "text": "Text Filtering. During the construction of pre-training datasets, most raw data from the web contains various forms of noise, such as violence, pornography, advertisements, corrupted characters, and poorly formatted text. To extract high-quality data from these raw contents, numerous text filtering methods have been proposed to automatically remove undesirable elements. Among these methods, handcrafted rules [9,17] are commonly used to filter out explicit noise, followed by deduplication techniques [18] to eliminate duplicate text from different sources. Building on this prior work, this paper introduces a rule-based preprocessing module designed to extract high-quality text from raw web content. \n\nQuality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities. In this paper, we adopt the BERT architecture to build a robust quality evaluation model for text. Domain Classification. The Pre-training of LLMs on vast amounts of diverse text data often results in a generalized understanding of language. However, to enhance their effectiveness for specific applications, it is advantageous to ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and Fine-grained Information classify and curate data from various domains, such as medicine, law, and finance. This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances.",
                    "score": 0.5500698816918924,
                    "section_title": "Related Work",
                    "char_start_offset": 1704,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 15
                        },
                        {
                            "start": 16,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 705
                        },
                        {
                            "start": 708,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1811
                        },
                        {
                            "start": 1812,
                            "end": 1931
                        },
                        {
                            "start": 1932,
                            "end": 2217
                        },
                        {
                            "start": 2218,
                            "end": 2376
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51220703125
                }
            ],
            "relevance_judgement": 0.5185546875,
            "relevance_judgment_input_expanded": "# Title: ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information\n# Venue: arXiv.org\n# Authors: Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, Jiajun Zhang\n## Abstract\nDuring the development of large language models (LLMs), pre-training data play a critical role in shaping LLMs' capabilities. In recent years several large-scale and high-quality pre-training datasets have been released to accelerate the research of LLMs, including ChineseWebText1.0, C4, Pile, WanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has increasingly shifted to domain-specific capabilities and safety concerns, making those previous coarse-grained texts insufficient for meeting training requirements. Furthermore, fine-grained information, such as quality, domain and toxicity, is becoming increasingly important in building powerful and reliable LLMs for various scenarios. To address these challenges, in this paper we propose a new tool-chain called MDFG-tool for constructing large-scale and high-quality Chinese datasets with multi-dimensional and fine-grained information. First, we employ manually crafted rules to discard explicit noisy texts from raw contents. Second, the quality evaluation model, domain classifier, and toxicity evaluation model are well-designed to assess the remaining cleaned data respectively. Finally, we integrate these three types of fine-grained information for each text. With this approach, we release the largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which consists of 3.8TB and each text is associated with a quality score, domain labels, a toxicity label and a toxicity score, facilitating the LLM researchers to select data based on various types of fine-grained information. The data, codes and the tool-chain are available on this website https://github.com/CASIA-LM/ChineseWebText-2.0\n## Related Work\nText Filtering. During the construction of pre-training datasets, most raw data from the web contains various forms of noise, such as violence, pornography, advertisements, corrupted characters, and poorly formatted text. To extract high-quality data from these raw contents, numerous text filtering methods have been proposed to automatically remove undesirable elements. Among these methods, handcrafted rules [9,17] are commonly used to filter out explicit noise, followed by deduplication techniques [18] to eliminate duplicate text from different sources. Building on this prior work, this paper introduces a rule-based preprocessing module designed to extract high-quality text from raw web content. \n\nQuality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities. In this paper, we adopt the BERT architecture to build a robust quality evaluation model for text. Domain Classification. The Pre-training of LLMs on vast amounts of diverse text data often results in a generalized understanding of language. However, to enhance their effectiveness for specific applications, it is advantageous to ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and Fine-grained Information classify and curate data from various domains, such as medicine, law, and finance. This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances.\n...\nThis process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances. Similar to quality evaluation methods, domain classification approaches commonly utilize models such as logistic regression, BERT, and FastText to assign domain labels to each text. Among these, FastText is a neural network-based approach similar to CBOW [22]. Compared to other methods, FastText offers efficient and rapid training on large-scale datasets while maintaining comparable classification performance. As a result, this paper employs the FastText model to classify domain labels for each text. \n\nToxicity Evaluation. Toxic texts are generally defined as those containing offensive or adult content [23,24]. Offensive content can be classified into three levels of severity: offensive text, abusive text, and hate text [25]. Adult content typically includes themes related to pornography or violence. Due to differences in cultural values and political perspectives, the analysis of toxic texts varies across countries. During the development of LLMs, the presence of toxic content in pre-training datasets would compromise the safety of LLMs. Consequently, toxicity evaluation has become increasingly important for the construction of pre-training datasets. In recent years, Caselli et al. [25] construct a toxicity dataset, RAL-E, using data from banned Reddit communities. They then train a BERT-based toxicity evaluation model with this dataset. Hartvigsen et al. [26] leverage GPT-3 to generate a dataset containing both subtle toxic texts and benign examples through adversarial techniques. In contrast, Deng et al. [27] construct a Chinese toxicity dataset with data from Zhihu and Weibo, and annotate each text with the help of human experts and LLMs. However, previous work often suffers from small data scales and insufficient coverage, limiting its ability to effectively evaluate the toxicity of LLMs. To address the above issues, this paper aims to construct a more robust toxicity evaluation model and release a larger-scale, higher-coverage toxicity dataset, thereby enhancing toxicity evaluation capabilities and improving the safety of LLMs. \n\nDatasets for Pre-training. As the foundation of LLMs, large-scale pre-training datasets play a crucial role in enhancing the capabilities of these models.",
            "reference_string": "[274422859 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Large Language Model-guided Document Selection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291470563",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "2238621478",
                    "name": "Tom Gunter"
                },
                {
                    "authorId": "2238621132",
                    "name": "Ruoming Pang"
                }
            ],
            "abstract": "Large Language Model (LLM) pre-training exhausts an ever growing compute budget, yet recent research has demonstrated that careful document selection enables comparable model quality with only a fraction of the FLOPs. Inspired by efforts suggesting that domain-specific training document selection is in fact an interpretable process [Gunasekar et al., 2023], as well as research showing that instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et al.,2023], we explore a promising direction for scalable general-domain document selection; employing a prompted LLM as a document grader, we distill quality labels into a classifier model, which is applied at scale to a large, and already heavily-filtered, web-crawl-derived corpus autonomously. Following the guidance of this classifier, we drop 75% of the corpus and train LLMs on the remaining data. Results across multiple benchmarks show that: 1. Filtering allows us to quality-match a model trained on the full corpus across diverse benchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and classifier models lead to better results that are less sensitive to the labeler's prompt, 3. In-context learning helps to boost the performance of less-capable labeling models. In all cases we use open-source datasets, models, recipes, and evaluation frameworks, so that results can be reproduced by the community.",
            "corpus_id": 270357359,
            "sentences": [
                {
                    "corpus_id": "270357359",
                    "title": "Large Language Model-guided Document Selection",
                    "text": "The goal of data filtering is to determine the optimal subset of training data to train on, where optimal is typically measured via trained model performance on a diverse suite of benchmarks.It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023].\n\nWe believe that the most closely related work to our own is Sachdeva et al. [2024], (developed concurrently), where the authors employ a T5 Raffel et al. [2020] model to produce quality labels before training new T5 models on the filtered data via a process they term \"Ask-LLM\".Compared to Ask-LLM, we focus on decoder-only LLMs, demonstrating that the technique works for much larger models and compute budgets (7B parameters trained for 1T tokens), as well as tougher benchmarks (MMLU), and also works when starting with a stronger baseline dataset in RPJ-CC (versus C4Raffel et al. [2020]).Furthermore, we ablate the impact of the prompt as well as the choice of labeler and distillation model, showing that more capable labelers are both more effective and less sensitive to the choice of prompt.\n\nIn what follows we describe more broadly approaches for data selection by quality, with a focus on efforts applied to language model training.\n\nEffect of data selection on pre-training stage: Training large language models typically involves utilizing extensive text datasets sourced from massive and diverse origins such as CommonCrawl and GitHub.However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b]).",
                    "score": 0.5144061236304199,
                    "section_title": "Related Work",
                    "char_start_offset": 16396,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 191,
                            "end": 384
                        },
                        {
                            "start": 384,
                            "end": 412
                        },
                        {
                            "start": 414,
                            "end": 692
                        },
                        {
                            "start": 692,
                            "end": 1007
                        },
                        {
                            "start": 1007,
                            "end": 1214
                        },
                        {
                            "start": 1216,
                            "end": 1358
                        },
                        {
                            "start": 1360,
                            "end": 1564
                        },
                        {
                            "start": 1564,
                            "end": 1848
                        },
                        {
                            "start": 1848,
                            "end": 2089
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 554,
                            "end": 574,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51806640625
                }
            ],
            "relevance_judgement": 0.51806640625,
            "relevance_judgment_input_expanded": "# Title: Large Language Model-guided Document Selection\n# Venue: arXiv.org\n# Authors: Xiang Kong, Tom Gunter, Ruoming Pang\n## Abstract\nLarge Language Model (LLM) pre-training exhausts an ever growing compute budget, yet recent research has demonstrated that careful document selection enables comparable model quality with only a fraction of the FLOPs. Inspired by efforts suggesting that domain-specific training document selection is in fact an interpretable process [Gunasekar et al., 2023], as well as research showing that instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et al.,2023], we explore a promising direction for scalable general-domain document selection; employing a prompted LLM as a document grader, we distill quality labels into a classifier model, which is applied at scale to a large, and already heavily-filtered, web-crawl-derived corpus autonomously. Following the guidance of this classifier, we drop 75% of the corpus and train LLMs on the remaining data. Results across multiple benchmarks show that: 1. Filtering allows us to quality-match a model trained on the full corpus across diverse benchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and classifier models lead to better results that are less sensitive to the labeler's prompt, 3. In-context learning helps to boost the performance of less-capable labeling models. In all cases we use open-source datasets, models, recipes, and evaluation frameworks, so that results can be reproduced by the community.\n## Related Work\nThe goal of data filtering is to determine the optimal subset of training data to train on, where optimal is typically measured via trained model performance on a diverse suite of benchmarks.It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023].\n\nWe believe that the most closely related work to our own is Sachdeva et al. [2024], (developed concurrently), where the authors employ a T5 Raffel et al. [2020] model to produce quality labels before training new T5 models on the filtered data via a process they term \"Ask-LLM\".Compared to Ask-LLM, we focus on decoder-only LLMs, demonstrating that the technique works for much larger models and compute budgets (7B parameters trained for 1T tokens), as well as tougher benchmarks (MMLU), and also works when starting with a stronger baseline dataset in RPJ-CC (versus C4Raffel et al. [2020]).Furthermore, we ablate the impact of the prompt as well as the choice of labeler and distillation model, showing that more capable labelers are both more effective and less sensitive to the choice of prompt.\n\nIn what follows we describe more broadly approaches for data selection by quality, with a focus on efforts applied to language model training.\n\nEffect of data selection on pre-training stage: Training large language models typically involves utilizing extensive text datasets sourced from massive and diverse origins such as CommonCrawl and GitHub.However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b]).",
            "reference_string": "[270357359 | Kong et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Towards Data Governance of Frontier AI Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 112,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03824, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258553730",
                    "name": "Jason Hausenloy"
                },
                {
                    "authorId": "2333897525",
                    "name": "Duncan McClements"
                },
                {
                    "authorId": "2333900015",
                    "name": "Madhavendra Thakur"
                }
            ],
            "abstract": "Data is essential to train and fine-tune today's frontier artificial intelligence (AI) models and to develop future ones. To date, academic, legal, and regulatory work has primarily addressed how data can directly harm consumers and creators, such as through privacy breaches, copyright infringements, and bias and discrimination. Our work, instead, focuses on the comparatively neglected question of how data can enable new governance capacities for frontier AI models. This approach for\"frontier data governance\"opens up new avenues for monitoring and mitigating risks from advanced AI models, particularly as they scale and acquire specific dangerous capabilities. Still, frontier data governance faces challenges that stem from the fundamental properties of data itself: data is non-rival, often non-excludable, easily replicable, and increasingly synthesizable. Despite these inherent difficulties, we propose a set of policy mechanisms targeting key actors along the data supply chain, including data producers, aggregators, model developers, and data vendors. We provide a brief overview of 15 governance mechanisms, of which we centrally introduce five, underexplored policy recommendations. These include developing canary tokens to detect unauthorized use for producers; (automated) data filtering to remove malicious content for pre-training and post-training datasets; mandatory dataset reporting requirements for developers and vendors; improved security for datasets and data generation algorithms; and know-your-customer requirements for vendors. By considering data not just as a source of potential harm, but as a critical governance lever, this work aims to equip policymakers with a new tool for the governance and regulation of frontier AI models.",
            "corpus_id": 274514936,
            "sentences": [
                {
                    "corpus_id": "274514936",
                    "title": "Towards Data Governance of Frontier AI Models",
                    "text": "Mandatory data filtering is a regulatory requirement for model developers to implement automated processes that detect and remove malicious, harmful, or unsafe content from training datasets before model training begins. Large language model (LLM) powered filtering of unsafe content in pre-training data could greatly reduce the risk of both the model passively absorbing unsafe content, thus eliminating the model's native potential to be unsafe, as well as targeted data poisoning attacks. This approach addresses the vulnerable nature of data to attacks by filtering out these attacks before training, while likely preserving the quality and safety of the information used to train AI models. \n\nImplementation: We propose model developers and other data processors should integrate filtering mechanisms for safety into data preprocessing stages. Government safety agencies would provide the specific safety criteria, and auditors with access can verify their correct implementation and effectiveness (potentially in combination with mandatory reporting requirements below). Although LLM-based methods for data filtering exist, they are not widely used for model safety, but instead to support performance. For example, 25% of Llama-3's pretraining data mix consisted of mathematics and coding tasks [32]. Classifiers can still be used to recognize specific types of unsafe or low-quality content. \n\nExisting Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32]. \n\nChallenges and mitigation: Implementing LLM-based filtering at scale can be expensive. This can be mitigated by the training/fine-tuning of specialised-models. Rather than a mandate, governments could partner with model developers, as they partnered with cloud service providers for transparency measures of cloud services usage [53]. A mandate, in principle, already has some regulatory footing, such as in the European Union's AI Act, which sets minimum quality requirements for pre-training datasets.",
                    "score": 0.5397276847806599,
                    "section_title": "Mandatory data filtering",
                    "char_start_offset": 31752,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 696
                        },
                        {
                            "start": 699,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1400
                        },
                        {
                            "start": 1403,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1631
                        },
                        {
                            "start": 1632,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1940
                        },
                        {
                            "start": 1943,
                            "end": 2029
                        },
                        {
                            "start": 2030,
                            "end": 2102
                        },
                        {
                            "start": 2103,
                            "end": 2277
                        },
                        {
                            "start": 2278,
                            "end": 2446
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.509765625
                }
            ],
            "relevance_judgement": 0.509765625,
            "relevance_judgment_input_expanded": "# Title: Towards Data Governance of Frontier AI Models\n# Venue: arXiv.org\n# Authors: Jason Hausenloy, Duncan McClements, Madhavendra Thakur\n## Abstract\nData is essential to train and fine-tune today's frontier artificial intelligence (AI) models and to develop future ones. To date, academic, legal, and regulatory work has primarily addressed how data can directly harm consumers and creators, such as through privacy breaches, copyright infringements, and bias and discrimination. Our work, instead, focuses on the comparatively neglected question of how data can enable new governance capacities for frontier AI models. This approach for\"frontier data governance\"opens up new avenues for monitoring and mitigating risks from advanced AI models, particularly as they scale and acquire specific dangerous capabilities. Still, frontier data governance faces challenges that stem from the fundamental properties of data itself: data is non-rival, often non-excludable, easily replicable, and increasingly synthesizable. Despite these inherent difficulties, we propose a set of policy mechanisms targeting key actors along the data supply chain, including data producers, aggregators, model developers, and data vendors. We provide a brief overview of 15 governance mechanisms, of which we centrally introduce five, underexplored policy recommendations. These include developing canary tokens to detect unauthorized use for producers; (automated) data filtering to remove malicious content for pre-training and post-training datasets; mandatory dataset reporting requirements for developers and vendors; improved security for datasets and data generation algorithms; and know-your-customer requirements for vendors. By considering data not just as a source of potential harm, but as a critical governance lever, this work aims to equip policymakers with a new tool for the governance and regulation of frontier AI models.\n## Mandatory data filtering\nMandatory data filtering is a regulatory requirement for model developers to implement automated processes that detect and remove malicious, harmful, or unsafe content from training datasets before model training begins. Large language model (LLM) powered filtering of unsafe content in pre-training data could greatly reduce the risk of both the model passively absorbing unsafe content, thus eliminating the model's native potential to be unsafe, as well as targeted data poisoning attacks. This approach addresses the vulnerable nature of data to attacks by filtering out these attacks before training, while likely preserving the quality and safety of the information used to train AI models. \n\nImplementation: We propose model developers and other data processors should integrate filtering mechanisms for safety into data preprocessing stages. Government safety agencies would provide the specific safety criteria, and auditors with access can verify their correct implementation and effectiveness (potentially in combination with mandatory reporting requirements below). Although LLM-based methods for data filtering exist, they are not widely used for model safety, but instead to support performance. For example, 25% of Llama-3's pretraining data mix consisted of mathematics and coding tasks [32]. Classifiers can still be used to recognize specific types of unsafe or low-quality content. \n\nExisting Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32]. \n\nChallenges and mitigation: Implementing LLM-based filtering at scale can be expensive. This can be mitigated by the training/fine-tuning of specialised-models. Rather than a mandate, governments could partner with model developers, as they partnered with cloud service providers for transparency measures of cloud services usage [53]. A mandate, in principle, already has some regulatory footing, such as in the European Union's AI Act, which sets minimum quality requirements for pre-training datasets.",
            "reference_string": "[274514936 | Hausenloy et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Data Processing for the OpenGPT-X Model Family",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 85,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108867346",
                    "name": "Nicolo\u2019 Brandizzi"
                },
                {
                    "authorId": "2899331",
                    "name": "Hammam Abdelwahab"
                },
                {
                    "authorId": "2239019329",
                    "name": "Anirban Bhowmick"
                },
                {
                    "authorId": "2313826462",
                    "name": "Lennard Helmer"
                },
                {
                    "authorId": "2325731547",
                    "name": "Benny Stein"
                },
                {
                    "authorId": "2325730238",
                    "name": "Pavel Denisov"
                },
                {
                    "authorId": "2210670041",
                    "name": "Qasid Saleem"
                },
                {
                    "authorId": "2258551638",
                    "name": "Michael Fromm"
                },
                {
                    "authorId": "2258668067",
                    "name": "Mehdi Ali"
                },
                {
                    "authorId": "2258550842",
                    "name": "Richard Rutmann"
                },
                {
                    "authorId": "2325731187",
                    "name": "Farzad Naderi"
                },
                {
                    "authorId": "2325730227",
                    "name": "Mohamad Saif Agy"
                },
                {
                    "authorId": "2325730246",
                    "name": "Alexander Schwirjow"
                },
                {
                    "authorId": "2332085973",
                    "name": "Fabian K\u00fcch"
                },
                {
                    "authorId": "2335570562",
                    "name": "Luzian Hahn"
                },
                {
                    "authorId": "2123349725",
                    "name": "Malte Ostendorff"
                },
                {
                    "authorId": "2302563988",
                    "name": "Pedro Ortiz Suarez"
                },
                {
                    "authorId": "77903998",
                    "name": "Georg Rehm"
                },
                {
                    "authorId": "2313822383",
                    "name": "Dennis Wegener"
                },
                {
                    "authorId": "2347259072",
                    "name": "Nicolas Flores-Herr"
                },
                {
                    "authorId": "2330411614",
                    "name": "Joachim K\u00f6hler"
                },
                {
                    "authorId": "2258552277",
                    "name": "Johannes Leveling"
                }
            ],
            "abstract": "This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.",
            "corpus_id": 273323858,
            "sentences": [
                {
                    "corpus_id": "273323858",
                    "title": "Data Processing for the OpenGPT-X Model Family",
                    "text": "The definition of high-quality data remains a complex and evolving topic in the field of LLMs, which has gained more traction as these models scale and diversify across languages and domains. Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]. \n\nTraditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,59,95]. One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53,31]. \n\nHowever, this approach comes with several drawbacks. First, despite the promising results, a significant limitation is the lack of transparency in the LLM's mapping between defined criteria and system output in defining high-quality content. There is often no clear link between the model's concept of quality and concrete linguistic characteristics, such as sentence structure, readability, or word length. Additionally, running LLM inference on large datasets substantially increases computational demands, making this approach accessible only to those with significant resources. Finally, using LLMs to filter or label data may introduce licensing challenges, particularly for commercial applications. We therefore recommend prioritizing more interpretable quality signals in data filtering. \n\nThese signals are directly mappable to tangible document characteristics, making them transparent and easy to justify. While LLM-based filtering techniques show promise, we believe there is a strong need for further research into how these models define high-quality data. At the very least, a hybrid approach combining both traditional and AI-based filtering could balance between efficiency, interpretability, and scalability.",
                    "score": 0.5354163798996299,
                    "section_title": "Quality Assessment",
                    "char_start_offset": 46766,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 376
                        },
                        {
                            "start": 379,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 952
                        },
                        {
                            "start": 955,
                            "end": 1007
                        },
                        {
                            "start": 1008,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1749
                        },
                        {
                            "start": 1752,
                            "end": 1870
                        },
                        {
                            "start": 1871,
                            "end": 2024
                        },
                        {
                            "start": 2025,
                            "end": 2180
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 804,
                            "end": 807,
                            "matchedPaperCorpusId": "211987786"
                        },
                        {
                            "start": 807,
                            "end": 810,
                            "matchedPaperCorpusId": "233296100"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5
                }
            ],
            "relevance_judgement": 0.5,
            "relevance_judgment_input_expanded": "# Title: Data Processing for the OpenGPT-X Model Family\n# Venue: arXiv.org\n# Authors: Nicolo\u2019 Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer, Benny Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian K\u00fcch, Luzian Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas Flores-Herr, Joachim K\u00f6hler, Johannes Leveling\n## Abstract\nThis paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.\n## Quality Assessment\nThe definition of high-quality data remains a complex and evolving topic in the field of LLMs, which has gained more traction as these models scale and diversify across languages and domains. Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]. \n\nTraditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,59,95]. One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53,31]. \n\nHowever, this approach comes with several drawbacks. First, despite the promising results, a significant limitation is the lack of transparency in the LLM's mapping between defined criteria and system output in defining high-quality content. There is often no clear link between the model's concept of quality and concrete linguistic characteristics, such as sentence structure, readability, or word length. Additionally, running LLM inference on large datasets substantially increases computational demands, making this approach accessible only to those with significant resources. Finally, using LLMs to filter or label data may introduce licensing challenges, particularly for commercial applications. We therefore recommend prioritizing more interpretable quality signals in data filtering. \n\nThese signals are directly mappable to tangible document characteristics, making them transparent and easy to justify. While LLM-based filtering techniques show promise, we believe there is a strong need for further research into how these models define high-quality data. At the very least, a hybrid approach combining both traditional and AI-based filtering could balance between efficiency, interpretability, and scalability.",
            "reference_string": "[273323858 | Brandizzi et al. | 2024 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "277043827",
            "title": "Data Caricatures: On the Representation of African American Language in Pretraining Corpora",
            "text": "Descriptions of each of the 16 data filters evaluated are included in Table 9. Language filters are typically neural models used to filter datasets to a particular language (e.g., English in the case of C4 and others). Classifier-based filters train small classification models (typically using fastText (Joulin et al., 2017)) or n-gram models (Heafield, 2011) to model text quality either through prediction of similarity to designated high-quality sources, prediction of toxicity, or n-gram model perplexity. Finally, LLM-Based filters leverage LLMs to directly predict aspects of data quality or annotate data to train a distilled model to evaluate quality. Table 9: List of the 16 evaluated pretraining corpora filters, including an example dataset or study employing each filter and a brief description. * denotes that the filter was evaluated in the development of the cited dataset, but not used in a resulting corpora. \u2020 indicates filters that were replicated; all other filters are taken from their respective cited sources.",
            "score": 0.6168753956057837,
            "section_title": "I.1 Filter Details",
            "char_start_offset": 37598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1033
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 325,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 344,
                    "end": 360,
                    "matchedPaperCorpusId": "8313873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296142578125
        },
        {
            "corpus_id": "272689526",
            "title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora",
            "text": "Classifier-based filtering. FastText (Joulin et al., 2016) is widely used to distinguish the quality of data (Computer, 2023;Wei et al., 2023;Li et al., 2024). Similar to KenLM, FastText is also an efficient model that operates on CPUs. However, as detailed in Section 4, KenLM demonstrated superior performance compared to FastText when both were trained on the same dataset. Furthermore, recent research (Gunasekar et al., 2023;Li et al., 2024;Penedo et al., 2024) has focused on finetuning pre-trained embedding models to serve as classifiers for quality filtering. Especially, Fineweb demonstrated that training relatively small-sized LLMs (1.82 billion parameters) on data filtered by a trained classifier (resulting in 350 billion tokens), rather than on unfiltered data, led to performance improvements across various benchmarks. However, these methods are impractical for processing large web corpora due to their high computational costs, which necessitate significant GPU resources.",
            "score": 0.6046388506275214,
            "section_title": "Introduction",
            "char_start_offset": 1709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 992
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "270562468",
            "title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models",
            "text": "We believe that such data is not beneficial for model training because the model cannot obtain much valuable information through autoregressive learning.Therefore, we design a quality scoring regression model based on LLM to score data quality and further filter out low-quality content.Specifically, we extract 20k data from the rule-based filtered data, score them twice using the GPT4, ranging from 0 to 6, and remove the data with a difference of about 2 points between the two scores, and finally obtained 15k training data.Then we train a scoring model based on the Bert multilingual pre-trained model, using batch-size of 128, learning rate of 3e-4, and train epoch of 10.We set a threshold for high-quality data filtering.",
            "score": 0.6014320978879232,
            "section_title": "Data Collection and Decontamination",
            "char_start_offset": 7243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 730
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.198974609375
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.",
            "score": 0.5912050896145489,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "276558383",
            "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale",
            "text": "FastText [22] family of binary classifiers have been used in prior datasets [9,11] for identifying highquality pre-training documents. Recently, [11] showed that fastText classifier trained on carefully selected data can outperform sophisticated model-based filtering approaches such as AskLLM (prompting an LLM to ask if a document is helpful). Inspired by their effectiveness coupled with the computational efficiency of fastText classifiers, we use fastText classifiers for quality annotations. \n\nWe employ two fastText classifiers: (i) the fastText classifier from [11] trained on a mix of instruction-formatted data (OpenHermes-2.5 [35]) and high-scoring posts from ELI5 subreddit [36] and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value. \n\nSpecifically, we use the supervised fastText package from [22] to train a classifier on 400k documents, equality split between positive (i.e., high-quality) and negative (i.e., low-quality) classes, selected as follows. \n\n\u2022 Positive documents: \n\n-190k synthetic documents randomly sampled from the Cosmopedia dataset -an open synthetic dataset consisting of textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1 [37]. -10k documents with high educational value selected as follows: we annotated 600k random documents from FineWeb-V1.1.0 asking Mixtral-8x22B-Instruct to score each document between 1 to 5 for its educational quality (with 5 being the highest quality), using a prompt similar to the one used by FineWeb-Edu. Next, we selected 10k random documents from the documents with scores \u2265 4. \n\n\u2022 Negative documents: 200k random documents out of the 600k Mixtral-annotated documents with scores \u2264 2. \n\nWe denote the DCLM-fastText as \u03d5 DCLM and our custom fastText as \u03d5 Cosmo .",
            "score": 0.576518783353101,
            "section_title": "FastText Classifiers",
            "char_start_offset": 10794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 13,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "274140947"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410400390625
        },
        {
            "corpus_id": "273798700",
            "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
            "text": "Several studies suggest using trainable small models for data filtering, moving beyond rule-based or interpreter-driven methods. Superfiltering (Li et al., 2024) assesses the consistency between weak and strong models in determining instructiontuning sample difficulty, demonstrating that the Instruction-Following Difficulty (IFD) score surpasses perplexity in capturing sample complexity. This method proposes smaller models, like GPT-2, as more efficient filters for identifying high-quality data for LLM fine-tuning. Similarly, Cao et al. (2024b) leverage natural language indicators to predict inference loss, offering a more efficient evaluation of data than fine-tuning LLMs. For code filtering, Zhou et al. (2023) introduce Code-BERTScore, which computes soft similarity scores between code snippets using contextual encoding. Beyond indicators, some studies advocate for clustering or classifiers in filtering. Chen et al. (2023b); Yu et al. (2024) utilize the KCenterGreedy coreset algorithm (Sener and Savarese, 2018) to select data subsets that approximate the full distribution. Dubey et al. (2024) further implement model-based classifiers, using fasttext (Joulin et al., 2017) and resource-heavy Roberta-based models (Liu et al., 2019), to identify high-quality tokens.",
            "score": 0.562852044626672,
            "section_title": "Small Model-based Filtering",
            "char_start_offset": 23543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1284
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 161,
                    "matchedPaperCorpusId": "267365346"
                },
                {
                    "start": 703,
                    "end": 721,
                    "matchedPaperCorpusId": "256827797"
                },
                {
                    "start": 920,
                    "end": 939,
                    "matchedPaperCorpusId": "258866038"
                },
                {
                    "start": 941,
                    "end": 957,
                    "matchedPaperCorpusId": "270258158"
                },
                {
                    "start": 1002,
                    "end": 1028,
                    "matchedPaperCorpusId": "3383786"
                },
                {
                    "start": 1170,
                    "end": 1191,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1232,
                    "end": 1250,
                    "matchedPaperCorpusId": "258866038"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "272753509",
            "title": "InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning",
            "text": "In this section, we examine the efficacy of two classifiers-LLM-based and fastText-based-focusing on high-precision mathematical content filtering. The comparison utilizes the DeepSeek-Coder 1.3B model, which we trained on a dataset previously introduced in Sec. 3.1.3 with a sequence length of 4096. This model was trained to score documents based on their relevance to mathematical content on a scale from 0 to 10. \n\nWe conduct the continue pretraining of the DeepSeekCoder 1.3B model using datasets filtered by both the LLM-and fastText-based classifiers. Table 7 shows the performance results. The results highlight a length bias in the LLM-based method, which tends to favor longer documents, averaging 2,500 tokens, compared to 1,700 tokens for the FastText filter. The length bias associated with the LLM-based classifier has adversely impacted the dataset's performance on the GSM8K dataset. As indicated in the table, the LLM-filtered dataset achieved lower accuracy (17.5%) on the GSM8K dataset compared to the fastText-filtered dataset (20.2%). This decrease in performance indicates that the LLM's preference for longer documents may not align well with the requirements of datasets like GSM8K, which demand concise and precise mathematical descriptions. \n\nGiven these insights, we have decided to continue utilizing the fastText classifier for high-precision filtering in our ongoing research. Nonetheless, the implications of the LLM-based classifier require further investigation to fully understand and address its biases.",
            "score": 0.5587183531752382,
            "section_title": "B Ablation Studies on High-Precision Mathematical Content Filtering",
            "char_start_offset": 33914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154296875
        },
        {
            "corpus_id": "274422859",
            "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
            "text": "Text Filtering. During the construction of pre-training datasets, most raw data from the web contains various forms of noise, such as violence, pornography, advertisements, corrupted characters, and poorly formatted text. To extract high-quality data from these raw contents, numerous text filtering methods have been proposed to automatically remove undesirable elements. Among these methods, handcrafted rules [9,17] are commonly used to filter out explicit noise, followed by deduplication techniques [18] to eliminate duplicate text from different sources. Building on this prior work, this paper introduces a rule-based preprocessing module designed to extract high-quality text from raw web content. \n\nQuality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities. In this paper, we adopt the BERT architecture to build a robust quality evaluation model for text. Domain Classification. The Pre-training of LLMs on vast amounts of diverse text data often results in a generalized understanding of language. However, to enhance their effectiveness for specific applications, it is advantageous to ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and Fine-grained Information classify and curate data from various domains, such as medicine, law, and finance. This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances.",
            "score": 0.5500698816918924,
            "section_title": "Related Work",
            "char_start_offset": 1704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "272550411",
            "title": "NOVI : Chatbot System for University Novice with BERT and LLMs",
            "text": "We compared the performance of the three best-performing models to classify the dataset, which consisted of 5,128 useful (1) and 7,283 not useful (0) labeled entries. \n\nBy implementing this methodology, future manual labeling tasks can be significantly streamlined, as this approach provides a clear framework to follow, enhancing both the efficiency and effectiveness of the filtering process. We expected the model to produce results similar to those filtered by humans. Therefore, we tested 100 completely human-generated test cases using the BERT + CNN model, but the accuracy was only 0.686. Consequently, we decided to train the LLM using data that we labeled ourselves this time. By adjusting hyperparameters and implementing this method in the future, manual labeling tasks can be significantly streamlined, as this approach provides a clear framework to follow, enhancing both the efficiency and effectiveness of the filtering process.",
            "score": 0.544940853568056,
            "section_title": "Model Comparison",
            "char_start_offset": 16136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 944
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0792236328125
        },
        {
            "corpus_id": "261243909",
            "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
            "text": "Pre-training typically requires a vast amount of data to achieve high performance, especially when the LLM parameter scale increases dramatically [2,39,101]. Larger datasets enable models to learn more robust and nuanced language representations. Along the history of the large language model development, we can see that the data scale increased significantly, same as the model parameters, for example, from billion tokens to tens of trillion tokens. However, it is crucial to emphasize that solely high-quality data are essential for performance improvement. Merely augmenting the dataset with low-quality data can result in a decline in performance [102]. Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training. \n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data. \n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples. \n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training. \n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality.",
            "score": 0.5405186384475906,
            "section_title": "Data Quantity and Quality",
            "char_start_offset": 23736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1822
                },
                {
                    "start": 1825,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 653,
                    "end": 658,
                    "matchedPaperCorpusId": "248986979"
                },
                {
                    "start": 1187,
                    "end": 1192,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "258832491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71533203125
        },
        {
            "corpus_id": "274514936",
            "title": "Towards Data Governance of Frontier AI Models",
            "text": "Mandatory data filtering is a regulatory requirement for model developers to implement automated processes that detect and remove malicious, harmful, or unsafe content from training datasets before model training begins. Large language model (LLM) powered filtering of unsafe content in pre-training data could greatly reduce the risk of both the model passively absorbing unsafe content, thus eliminating the model's native potential to be unsafe, as well as targeted data poisoning attacks. This approach addresses the vulnerable nature of data to attacks by filtering out these attacks before training, while likely preserving the quality and safety of the information used to train AI models. \n\nImplementation: We propose model developers and other data processors should integrate filtering mechanisms for safety into data preprocessing stages. Government safety agencies would provide the specific safety criteria, and auditors with access can verify their correct implementation and effectiveness (potentially in combination with mandatory reporting requirements below). Although LLM-based methods for data filtering exist, they are not widely used for model safety, but instead to support performance. For example, 25% of Llama-3's pretraining data mix consisted of mathematics and coding tasks [32]. Classifiers can still be used to recognize specific types of unsafe or low-quality content. \n\nExisting Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32]. \n\nChallenges and mitigation: Implementing LLM-based filtering at scale can be expensive. This can be mitigated by the training/fine-tuning of specialised-models. Rather than a mandate, governments could partner with model developers, as they partnered with cloud service providers for transparency measures of cloud services usage [53]. A mandate, in principle, already has some regulatory footing, such as in the European Union's AI Act, which sets minimum quality requirements for pre-training datasets.",
            "score": 0.5397276847806599,
            "section_title": "Mandatory data filtering",
            "char_start_offset": 31752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.509765625
        },
        {
            "corpus_id": "267312030",
            "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
            "text": "Xie et al. ( 2023) proposed a method, DoReMi, to select the best data mixture for pre-training language models by reweighting data from various domains. Concurrently, Abbas et al. (2023) showed that de-duplicating pre-training data can improve pre-training efficiency. Recently several methods were proposed for automatic filtering of low-quality data for faster fine-tuning of LLMs (Chen et al., 2023;Solaiman & Dennison, 2021;Zhou et al., 2023). Simultaneously, in the realm of image-language models such as CLIP (Radford et al., 2021), the Datacomp benchmark (Gadre et al., 2023) and recent entries (Maini et al., 2023;Yu et al., 2023) have developed approaches at filtering out low-quality subsets from pre-training datasets like LAION (Schuhmann et al., 2022), or from scrapes of the common crawl. Eldan & Li (2023) showed that a synthetically generated dataset in the form of stories that toddlers can understand allows training a small language model that can generate coherent sentences. Gunasekar et al. (2023) showed that textbook quality (synthetic) data alone helps models achieve state-of-the-art performance on reasoning and coding tasks. Similar approaches are used in concurrent work for enhancing coding and mathematical reasoning abilities while finetuning Liu et al. (2023a); Wei et al. (2023). Shumailov et al. (2023) show that training on synthetic data can actually be harmful for model performance, especially when we do multiple rounds of pre-training an LLM and then training the next LLM on data generated by the previous one. On the other hand, some other works have shown that such a strategy can actually be useful. Li et al. (2023b) and K \u00f6ksal et al. ( 2023) discuss how a model can generate instruction data and then fine-tune on its own generated data to improve performance.",
            "score": 0.5372061799343015,
            "section_title": "Related Work",
            "char_start_offset": 6833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 428,
                    "matchedPaperCorpusId": "235489789"
                },
                {
                    "start": 515,
                    "end": 537,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 740,
                    "end": 764,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46240234375
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "Rule-based Text Filtering. Rule-based text filtering methods are the dominant paradigm to identify content-rich and semantically coherent data from collected raw datasets with handcrafted rules. During the collection of pre-training data, there are a large number of text data on the web. However, these data include a lot of noise, such as violence, pornographic, advertisement and error characters. Consequently, in order to extract high-quality data, several rule-based methods have been proposed to explore how to automatically filter undesired content from noisy web data. In these work, deduplication [10] methods are employed to remove duplicate text from the data, while some handcrafted rules [11; 12] are adopted to filter out violence, pornographic, advertisement and other explicit noisy data. Besides, perplexity [13] is also usually used to evaluate the fluency of the texts. However, these work mainly focus on English and lack a complete tool-chain for Chinese. \n\nText Classification Model. Different from rule-based text filtering methods, text classification model is an alternative approach to identify high-quality data with a well-designed classifier. The simplest text classification model is logistic regression [5], which uses the logistic function to calculate the probability values for each text, and then classifies them into positive or negative with a designed threshold. Currently, BERT [14] and FastText [15] are both commonly used text classification models. BERT is a transformer-based [16] pre-training language model that has achieved remarkable performance in various text classification and understanding tasks. Through pre-training on masked language model and next sentence prediction tasks with a large dataset, this model learns powerful language understanding and representation abilities, which makes it perform well on text classification tasks. FastText [15] is also a neural network based approach which is similar to CBOW [17]. It is characterized by its ability to train efficiently and quickly on large-scale data, while achieving competitive classification performance. In this paper, both of these two approaches will be employed to evaluate the qualities of the Chinese texts. \n\nDatasets for Pre-training. In recent years as the scale of pre-trained language models expands, there is a concomitant increase in the demand for large-scale pre-training datasets.",
            "score": 0.5358263387565891,
            "section_title": "Related Work",
            "char_start_offset": 3950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2412
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1900,
                    "end": 1904,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "5959482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "corpus_id": "273323858",
            "title": "Data Processing for the OpenGPT-X Model Family",
            "text": "The definition of high-quality data remains a complex and evolving topic in the field of LLMs, which has gained more traction as these models scale and diversify across languages and domains. Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]. \n\nTraditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,59,95]. One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53,31]. \n\nHowever, this approach comes with several drawbacks. First, despite the promising results, a significant limitation is the lack of transparency in the LLM's mapping between defined criteria and system output in defining high-quality content. There is often no clear link between the model's concept of quality and concrete linguistic characteristics, such as sentence structure, readability, or word length. Additionally, running LLM inference on large datasets substantially increases computational demands, making this approach accessible only to those with significant resources. Finally, using LLMs to filter or label data may introduce licensing challenges, particularly for commercial applications. We therefore recommend prioritizing more interpretable quality signals in data filtering. \n\nThese signals are directly mappable to tangible document characteristics, making them transparent and easy to justify. While LLM-based filtering techniques show promise, we believe there is a strong need for further research into how these models define high-quality data. At the very least, a hybrid approach combining both traditional and AI-based filtering could balance between efficiency, interpretability, and scalability.",
            "score": 0.5354163798996299,
            "section_title": "Quality Assessment",
            "char_start_offset": 46766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "211987786"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "233296100"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5
        },
        {
            "corpus_id": "274192757",
            "title": "Exploring Accuracy-Fairness Trade-off in Large Language Models",
            "text": "The pre-trained BERT-Base-Uncased (Devlin et al., 2019) model is considered as the individual model for our framework. BERT-Base-Uncased is widely used for natural language understanding tasks due to its robust performance and versatility. Comparison methods. Six common methods for mitigating LLM's unfairness are compared to verify the effectiveness of our framework: vanilla, CDA (Qian et al., 2022), oversampling, undersampling, oversampling with CDA (Chen et al., 2024a), and undersampling with CDA (Chen et al., 2024a). The implementation of these methods including hyper-parameters was based on open-source code available at GitHub repository1 associated with the work (Chen et al., 2024a). Each of these methods involves training a single model, offering a specific balance between \u2206 T P R and accuracy. This highlights the benefits of our framework in achieving a balance between accuracy and fairness, as well as the advantages of providing a diverse set of LLMs. \n\nThe vanilla method involves directly training an LLM with the original data. Oversampling and undersampling were chosen as comparison methods since prior research (De-Arteaga et al., 2019;Chen et al., 2024a) indicates that imbalanced data contributes significantly to unfairness in trained LLMs. These methods involve adjusting the data distribution by modifying the proportions of the predicted classes before training the model. Furthermore, recent studies (Chen et al., 2024a) suggest that the pre-processing method which combines CDA with either oversampling or undersampling can further improve the fairness of LLMs. Therefore, our study aims to evaluate these methods to determine the most effective approach for mitigating bias. Parameter settings. The population set is set to 50. The maximum generation is set to 20. The number of independent experiments for each method is set to 10. The crossover, i.e., merge method, follows the approach used by the method (Wortsman et al., 2022), where the number of models to be merged is two. The mutation strategy is based on the method (Wu et al., 2022), with the lambda parameter set to 0.02.",
            "score": 0.5353689956750582,
            "section_title": "LLM model.",
            "char_start_offset": 17350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 55,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 383,
                    "end": 402,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 455,
                    "end": 475,
                    "matchedPaperCorpusId": "268819590"
                },
                {
                    "start": 504,
                    "end": 524,
                    "matchedPaperCorpusId": "268819590"
                },
                {
                    "start": 676,
                    "end": 696,
                    "matchedPaperCorpusId": "268819590"
                },
                {
                    "start": 1139,
                    "end": 1164,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1164,
                    "end": 1183,
                    "matchedPaperCorpusId": "268819590"
                },
                {
                    "start": 1435,
                    "end": 1455,
                    "matchedPaperCorpusId": "268819590"
                },
                {
                    "start": 1945,
                    "end": 1968,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2063,
                    "end": 2080,
                    "matchedPaperCorpusId": "247084085"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146240234375
        },
        {
            "corpus_id": "270514462",
            "title": "Investigating Web Corpus Filtering Methods for Language Model Development in Japanese",
            "text": "The quality of the pretraining data significantly impacts the performance of large language models (LLMs) (Longpre et al., 2023;Gunasekar et al., 2023).The training data mostly comprise Web documents, and therefore it is essential to remove low-quality documents or paragraphs from them efficiently.However, no quality filtering method has been established for these documents.\n\nRule-based filtering can quickly remove documents with many unnecessary alphabets, symbols, and specific repetitive sentences.However, they have no comprehensive understanding of documents to be removed and can overdo or overlook certain types of documents.On the other hand, learning-based filtering methods can remove some low-quality documents that seem to maintain a level of quality and therefore bypass the rule-based filters.However, it is not verified yet which filtering methods are better and what types of documents are removed by such filters.\n\nIn empirical experiments, We examine learningbased filtering methods to remove low-quality documents in a Web corpus.We focus on Japanese corpora in this paper because there has been little study on methods for filtering Japanese corpora for developing LLMs, while many models have been developed in recent years, especially for the Japanese language.We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora.The experimental results show that the perplexity filtering method based on an N-gram language model is the best.We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022).The results show that massively strong filtering results in performance deterioration.Furthermore, topic analysis on the Web corpus shows that the proportion of specific topics decreases during the filtering process.",
            "score": 0.5253102771891003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 377
                },
                {
                    "start": 379,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 934
                },
                {
                    "start": 936,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1402
                },
                {
                    "start": 1402,
                    "end": 1515
                },
                {
                    "start": 1515,
                    "end": 1724
                },
                {
                    "start": 1724,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 1537,
                    "end": 1558,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1700,
                    "end": 1723,
                    "matchedPaperCorpusId": "249687727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "244895506",
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "text": "The pre-trained BERT model's performance, accuracy and approach motivated the authors to experiment with this model for customer sentiment analysis. The main objectives and contributions are as follows; \n\n\uf0b7 Overcome known challenges of SA model performance, accuracy and context-based prediction. \n\n\uf0b7 Train BERT-base-cased model on Twitter, IMDB Movie Reviews, Yelp, and Amazon customer reviews datasets to improve the accuracy and performance of the model. \n\n\uf0b7 Evaluate the custom deep learning sequential model of BiLSTM, hybrid fastText-BiLSTM model and linear models of LSVM, fastText models using the same datasets. \n\n\uf0b7 Compare the results of the BERT model with the results of the deep learning sequential and linear model. \n\n\uf0b7 Customize the data pre-processing steps for hybrid and linear model training. \n\n\uf0b7 Fine-tune the hyperparameters for fastText-BiLSTM models. \n\nThis paper presented with several recent BERT and SA related research papers reviews and major contributions from various researchers in Section II. The literature reviews of BERT, fastText, BiLSTM, and LSVM models are presented in Section III. The experimental setting and model evaluation results are discussed in the Section IV. In Section V, concluded this paper with model results, findings of this research work and future work.",
            "score": 0.5244154344880607,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0180206298828125
        },
        {
            "corpus_id": "275931897",
            "title": "A comparison of data filtering techniques for English-Polish LLM-based machine translation in the biomedical domain",
            "text": "LLM-based MT models depend on large quantities of high-quality data for domain adaptation (Koehn et al., 2018). To enhance the quality of web-scraped corpora, various automated filtering methods have been explored within the MT field, including outlier detection (Taghipour et al., 2011), discriminator models (Xu and Koehn, 2017), graph-based unsupervised models (Cui et al., 2013), and LLMbased classifiers or scorers (A\u00e7ar\u00e7i\u00e7ek et al., 2020). \n\nWith the advent of LLMs, language-agnostic encoders such as LASER, LaBSE, and MUSE have enabled the direct scoring of bilingual sentence similarity for data set filtering, proving competitive with more complex classifier-based models (Chaudhary et al., 2019). \n\nThe research by Bane and Zaretskaya (2021) evaluated the effectiveness of filtering in English-Japanese and English-German sentence pairs using models like Marian-scorer, LASER, MUSE and XLM-R. The findings showed limited reduction of the dataset (54%-73%) with BLEU scores comparable to random selection, although Marian-based filtering consistently outperformed random downsizing, and MUSE showed variable performance by language. The latter indicates that filtering results might not be universally applicable across different pairs or topics of languages, which motivated our study design to specifically investigate the Polish-English translation. \n\nFurther exploration by Bane et al. (2022) assessed the strengths and weaknesses of specific filtering methods. They developed a dataset with ten types of noise or errors to test these methods. Results indicated that a custom-trained Marian-Scorer had the best cleaning performance, while embedding-based methods like XLM-R, MUSE, and LASER, although less effective, still performed adequately and were particularly effective at identifying issues like number mismatches and spelling errors without requiring the computationally costly calibration needed for the Marian-Scorer.",
            "score": 0.5166393528856849,
            "section_title": "Related Work",
            "char_start_offset": 2302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 110,
                    "matchedPaperCorpusId": "53242563"
                },
                {
                    "start": 263,
                    "end": 287,
                    "matchedPaperCorpusId": "41382625"
                },
                {
                    "start": 310,
                    "end": 330,
                    "matchedPaperCorpusId": "216812284"
                },
                {
                    "start": 364,
                    "end": 382,
                    "matchedPaperCorpusId": "14544096"
                },
                {
                    "start": 1388,
                    "end": 1406,
                    "matchedPaperCorpusId": "252186406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330322265625
        },
        {
            "corpus_id": "270357359",
            "title": "Large Language Model-guided Document Selection",
            "text": "The goal of data filtering is to determine the optimal subset of training data to train on, where optimal is typically measured via trained model performance on a diverse suite of benchmarks.It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023].\n\nWe believe that the most closely related work to our own is Sachdeva et al. [2024], (developed concurrently), where the authors employ a T5 Raffel et al. [2020] model to produce quality labels before training new T5 models on the filtered data via a process they term \"Ask-LLM\".Compared to Ask-LLM, we focus on decoder-only LLMs, demonstrating that the technique works for much larger models and compute budgets (7B parameters trained for 1T tokens), as well as tougher benchmarks (MMLU), and also works when starting with a stronger baseline dataset in RPJ-CC (versus C4Raffel et al. [2020]).Furthermore, we ablate the impact of the prompt as well as the choice of labeler and distillation model, showing that more capable labelers are both more effective and less sensitive to the choice of prompt.\n\nIn what follows we describe more broadly approaches for data selection by quality, with a focus on efforts applied to language model training.\n\nEffect of data selection on pre-training stage: Training large language models typically involves utilizing extensive text datasets sourced from massive and diverse origins such as CommonCrawl and GitHub.However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b]).",
            "score": 0.5144061236304199,
            "section_title": "Related Work",
            "char_start_offset": 16396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 412
                },
                {
                    "start": 414,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1214
                },
                {
                    "start": 1216,
                    "end": 1358
                },
                {
                    "start": 1360,
                    "end": 1564
                },
                {
                    "start": 1564,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 574,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "270870116",
            "title": "Memory3: Language Modeling with Explicit Memory",
            "text": "The raw data is filtered with three steps: deduplication, rule-based filtering, and model-based filtering.\n\nFirst, deduplication is performed with MinHash for most of the datasets.One exception is RedPaja-maV2, which already comes with deduplication labels.\n\nSecond, we devise heuristic, rule-based filters analogous to the ones from [76,92,25].The purpose is to eliminate texts that are ostensibly unsuitable for training, such as ones that only contain webpage source codes, random numbers, or incomprehensible shards.Our filters remove documents with less than 50 words, documents whose mean word lengths exceed 10 characters, documents with 70% of context being non-alphabetic characters, documents whose fractions of unique words are disproportionately high, documents whose entropy of unigrams is excessively low, and so on.\n\nFinally, we select the subset of data with highest \"quality\", a score produced by a finetuned BERT model.Specially, we sample ten thousand documents and grade them by the XinYu-70B model [65,68] with prompt-guided generation.The prompt asks the model to determine whether the input text is informative and produce a score between 0 and 5.Then, these scores are used to finetune the Tiny-BERT model [57], which has only 14M parameters.The hyperparameters of this finetuning are optimized with respect to a held-out validation set.After that, we use this lightweight BERT to grade the entire dataset.Remark 6. Recall from Section 3.5 that the pretraining data of Memory 3 should emphasize abstract knowledges and minimize specific knowledges.The purpose is to not only obtain a lightweight LLM with an ideal distribution of knowledges in accordance with the memory hierarchy (Figure 8), but also prevent the specific knowledges from hindering the learning process of the model.The focus of our prompt on \"informativeness\" might be contradictory to this goal, since the selected texts that are rich in information content may contain too many specific knowledges.For future versions of Memory 3 , we will switch to a model-based filter favoring texts that exhibit more reasoning and less specifics.",
            "score": 0.5141180215396572,
            "section_title": "| Filtering",
            "char_start_offset": 59821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 108,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 257
                },
                {
                    "start": 259,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 520
                },
                {
                    "start": 520,
                    "end": 830
                },
                {
                    "start": 832,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1266
                },
                {
                    "start": 1266,
                    "end": 1361
                },
                {
                    "start": 1361,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1572
                },
                {
                    "start": 1572,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1992
                },
                {
                    "start": 1992,
                    "end": 2127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31298828125
        },
        {
            "corpus_id": "234813543",
            "title": "Filtered BERT: Similarity Filter-Based Augmentation with Bidirectional Transfer Learning for Protected Health Information Prediction in Clinical Documents",
            "text": "We applied an optimized pre-training model for filtered BERT-based data augmentation. Bio-Clinical BERT [31], which was pretrained using bio and clinical data, was applied to the BERT model that predicted the masked word vector. It was confirmed in a previous study that the BERT model pretrained with a corpus suitable for the data showed good performance. To calculate the cosine similarity at the filter stage, fastText-based embedding was used when vectorization was performed by the embedding vector. In this case, fastText embedding, trained using the BioWordVec corpora [32], was used.",
            "score": 0.5068359320110998,
            "section_title": "Filtered BERT-Based Clinical Documents Augmentation",
            "char_start_offset": 13636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "271088966",
            "title": "A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training",
            "text": "Due to their raw nature, web-mined corpora are characterized by low-quality text data -insufficient for direct use in training LLMs [6,15].Available in the literature, analyses of corpora derived from the Common Crawl indicate a positive impact of LLMs training on largely filtered data considered to be of low quality [6,31,35,46].The text quality assessment and data cleaning methods used in the works analyzed include: 1) rules and heuristic methods for filtering poor quality texts, 2) pre-trained text quality classifiers, 3) use of statistical models for automatic language identification, 4) filtering of poor quality content based on n-grams, lightweight language models.\n\nRules and Heuristic Methods for Filtering Poor Quality Texts.These methods for filtering low-quality texts described in the literature focus on cleaning the data based on a set of established, explainable rules for assessing text quality.They usually involve filtering content with poor punctuation accuracy, documents that are too short or long (in terms of the number of characters), filtering texts containing HTML tags or JavaScript scripts and \"lorem ipsum\" phrases, removing privacy policy content such as: \"terms of use\", \"privacy policy\", \"cookie policy\", \"uses cookies\", \"use of cookies\", \"use cookies\", removing content containing words from the lists of prohibited words; as well as rules such as removing documents in which more than 90% of lines start with bullets, more than 30% of sentences end with ellipses, etc. [11,20,35,37,46].Methods are also used to define sets of text quality features (called quality signals), such as: text length, share of capital letters, share of non-alphanumeric characters, share of unique words, share of source code in the text, etc., followed by filtering low-quality text based on these defined characteristics [15,37,46,48].\n\nPre-Trained Text Quality Classifiers.The use of pre-trained text quality classification/regression models is a common element of low-quality content filtration methods described in the literature [6,27,35,46].",
            "score": 0.5039519388360061,
            "section_title": "Data Quality and Noise",
            "char_start_offset": 6959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 679
                },
                {
                    "start": 681,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1528
                },
                {
                    "start": 1528,
                    "end": 1857
                },
                {
                    "start": 1859,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1521,
                    "end": 1524,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1524,
                    "end": 1527,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1847,
                    "end": 1850,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1850,
                    "end": 1853,
                    "matchedPaperCorpusId": "207870323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328857421875
        },
        {
            "corpus_id": "268249158",
            "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
            "text": "In this section, we evaluate the effectiveness of adopting fine-tuned MLMs as high-quality image-text data filters. We compare the performance of vision-language models pre-trained on datasets filtered using a baseline filter with their performance using our MLM filter. We select two different VLM Table 3: Zero-shot performance of CLIP models pre-trained using baseline filtering methods and proposed MLM-FILTER on Large scale pools of the DataComp benchmark. \n\nEvaluation Setup. We select the DataComp benchmark to evaluate the effectiveness of adopting fine-tuned MLM as data filter. The evaluation process involves the data filtering stage and evaluation stage, which are shown in Figure 2. During the data filtering stage, we adopt the MLM-Filter to generate quality scores on all 128M medium-scale data and 1.28B large-scale data. After that, an integer filtering threshold is calculated based on the closest value that retains 30% of the overall data pool, 38.4M for Medium and 384M for Large. Such threshold is set up to select all the image-text pairs, of which the quality score is larger or equal to the threshold. We report the results using each defined metric to filter data separately and we consider two MLM filters learning from different teacher models. Additionally, we also report the results of experiments with a combination of two metrics for data filtering. Finally, we select a high-quality subset from the medium or large scale image-text data pools based on different proposed quality metrics. During the evaluation stage, we adopt the selected high-quality data subset to pre-train a CLIP model and compare the performance of our CLIP model with CLIP models pre-trained on datasets filtered by other methods. \n\nBaselines. We compare the proposed MLM filter with other baseline filtering methods from Data-Comp, including applying no filtering, basic filtering, LAION filtering and CLIPScore filtering. The basic filtering method adopts three rule-based filters, filtering English only, filtering by caption length, and filtering by image size. The LAION filtering adopts both the CLIPScore filtering using ViT-B/32 CLIP model and the English filtering. The CLIPScore filtering utilizes a larger ViT-L/14 CLIP model for score generation and data filtering. \n\nTraining Details.",
            "score": 0.5037309328985271,
            "section_title": "Experiments",
            "char_start_offset": 15734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2284
                },
                {
                    "start": 2287,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2271728515625
        },
        {
            "corpus_id": "277113645",
            "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
            "text": "The success of large language models (LLMs) is closely tied to the scaling up of model size, training data, and computational resources (Brown, 2020;Hoffmann et al., 2022;Qwen Team, 2024). Among these three factors, training data serves as the foundation of LLMs' rich knowledge and capabilities, enabling them to excel in creative writing, complex reasoning, and even agentic planning. Recently, there is mounting evidence that pretraining on high-quality and diverse corpora can significantly enhance LLM performance (Penedo et al., 2024;Li et al., 2024a). For example, Microsoft's Phi series of models has been renowned for their pretraining corpus's quality and efficiency. The latest Phi-4 model (Abdin et al., 2024), trained on 10T tokens, has surpassed other models of the same size, including Qwen 2.5 (Qwen Team, 2024), which was trained on 18 trillion tokens. This has led many researchers to focus on LLM data curation methodologies, which include the selection of high-quality sub-datasets (Wettig et al., 2024), deduplication of extensive datasets (Tirumala et al., 2023), and the balancing of data from various domains (Xie et al., 2024), among other techniques. \n\nIn terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value.",
            "score": 0.5036974694388894,
            "section_title": "Introduction",
            "char_start_offset": 2053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 540,
                    "matchedPaperCorpusId": "270711474"
                },
                {
                    "start": 540,
                    "end": 557,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 1002,
                    "end": 1023,
                    "matchedPaperCorpusId": "267681974"
                },
                {
                    "start": 1133,
                    "end": 1151,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "278237416",
            "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
            "text": "Inspired by successful demonstrations for English with FineWeb [38] and Nemotron-CC [46], we created educational quality classifiers to curate high-quality German web data by training a classifier on scores given by an LLM-as-a-judge. The intuition behind this is to identify web documents that are more likely to be educational, informative, and useful for model training, as opposed to spam, low-quality, or otherwise unhelpful content. \n\nWe first use an LLM-as-a-judge to label a small subset of the data. We then leverage these labelled documents as training data to train more efficient and lightweight fastText and BERT classifiers. This two-stage approach enables us to score a large number of web documents in a computationallyefficient way, while at the same time introducing implicit regularisation to the judging process. \n\nWe again made use of Mistral-Nemo-Instruct-2407, this time to annotate a random set of 600,000 documents from German FineWeb2 according to three criteria: (1) content quality, assessing coherence, informativeness and overall quality of the content; (2) language quality, evaluating the use of language, including formality, objectivity, and the presence of errors or slang; and (3) orthography, assessing the correctness of grammar, spelling, and punctuation, including errors such as typos, incorrect verb conjugation, and incorrect declension. We show the prompt used for the LLM-as-a-judge in Appendix A, which provided scores from one to five for each criterion. \n\nFor each document, we calculated a combined educational quality score by taking the minimum over the three criteria rated by the LLM-as-a-judge. This is because we observed that the judge would often miss poor quality in one criterium, but rarely in all three. We then used these educational quality scores as the training signal for fastText and BERT quality classification models. We trained one BERT model tasked to predict the educational quality scores given the first 512 tokens of the document's text. We also trained a binary classifier based on fastText.",
            "score": 0.49791346291483873,
            "section_title": "Educational quality",
            "char_start_offset": 10707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "270711474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34716796875
        },
        {
            "corpus_id": "269005537",
            "title": "Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language Model Pre-training",
            "text": "As depicted on the left side of Figure 1, BABERT (Jiang et al., 2022) incorporates unsupervised statistical boundary information into BERT's pretraining process, which consists of three steps.\n\n1) The first step involves constructing a dictionary N to store the statistical boundary information.Each entry in the dictionary is a key-value pair, where the key represents an N-gram g = {c 1 , . . ., c m } derived from the corpus.The corresponding value comprises three statistical boundary measures: Pointwise Mutual Information (PMI), Left Entropy (LE), and Right Entropy (RE).\n\n2) Utilizing the dictionary N , a boundary-aware representation E = {e 1 , ..., e n } is constructed for each text x = {c 1 , . . ., c n } in the corpus.This representation incorporates the statistical boundary information.\n\n3) The statistical boundary information is injected into the model using the unsupervised boundary-aware learning pre-training task (UBA task).Additionally, to capture general semantic information, BABERT incorporates BERT's Masked Language Modeling (MLM) task.Therefore, the overall loss function for BABERT is defined as follows:\n\nThe above description provides a high-level overview of BABERT to facilitate understanding of our proposed method.For a more comprehensive understanding, one can refer to the original paper (Jiang et al., 2022).\u2022 LLM-based Corpus Filtering: The quality of the pre-training corpus is crucial for the performance of PLMs (Liu et al., 2019).In our work, we leverage the power of LLMs for data cleaning to ensure the quality of the corpus used for pre-training.LLMs, thanks to their well pre-training and large-scale parameters, have demonstrated remarkable success in various natural language processing tasks (Liang et al., 2022) and are known to generate high-quality text (Chen et al., 2023).Therefore, we rely on the powerful generation capabilities of LLMs for data cleaning.",
            "score": 0.49711090518689743,
            "section_title": "BABERT",
            "char_start_offset": 4487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 577
                },
                {
                    "start": 579,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 802
                },
                {
                    "start": 804,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1135
                },
                {
                    "start": 1137,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1914
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31689453125
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "Once an adequate corpus of data is collected, the subsequent step is data preprocessing. The quality of data preprocessing directly impacts the model's performance and security. The specific preprocessing steps involve filtering low-quality text, including eliminating toxic and biased content to ensure the model aligns with human ethical standards. It also includes deduplication, removing duplicates in the training set, and excluding redundant content in the test set to maintain the sample distribution balance. Privacy scrubbing is applied to ensure the model's security, preventing information leakage or other privacy-related concerns. Additionally, if fine-tuning LLMs is considered, expanding the vocabulary should also be considered. On the other hand, LLaMA 2 models [10] represent a notable exception. These models forego filtering in their pretraining corpus, as aggressive filtration might accidentally filter out some demographic groups. This approach enhances the generalizability of the base LLaMA 2 models, making them more adept across a range of downstream tasks, such as hate speech detection and privacy de-identification. Observations indicate that abstaining from additional filtering in the pretraining data enables the base model to achieve reasonable safety alignment with fewer examples [10]. While this increases both generalizability and safety alignment efficiency, the implementation of additional safety mitigations is still imperative prior to public deployment, as further discussed in Section 3.5.4. \n\nQuality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText [85] to filter out low-quality datasets. \n\nDeduplication: Language models may sometimes repetitively generate the same content during text generation, potentially due to a high degree of repetition in the training data. Extensive repetition can lead to training instability, resulting in a decline in the performance of LLMs [86].",
            "score": 0.4963321290948211,
            "section_title": "Data preprocessing",
            "char_start_offset": 29962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1536
                },
                {
                    "start": 1539,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2282
                },
                {
                    "start": 2283,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 2063,
                    "end": 2067,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "In order to extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score. With the proposed approach, we release the latest and largest Chinese dataset of 1.4 TB, each of which is associated with a quality score, facilitating the LLMs researchers to re-filter the data with desired quality thresholds. We further release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90% by human evaluation. We also release the complete tool-chain that processes the raw data into the clean texts. \n\nIn the future, we will continue to enlarge the Chinese dataset with newly incoming web data. Meanwhile, we are going to explore better algorithms and strategies for data filtering. For example, we can design quality evaluation models for each kind of data noise.",
            "score": 0.49579864991447486,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 32436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55908203125
        },
        {
            "corpus_id": "272397823",
            "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
            "text": "This filter selects high-quality The Stack v1 and StackOverflow content, significantly enhancing coding performance. FineWeb-Edu [30] employs a linear regressor built on Snowflake-arctic-embed-m [26], an advanced embedding model based on BERT [11]. This regressor, trained on 400k Llama-3 [12] annotations rating the educational value (0-5) of FineWeb dataset documents, significantly enhances STEM performance. DCLM-Baseline [18] uses a fastText [5] filter trained on positives from OpenHermes 2.5 [37], high-scoring posts from r/ExplainLikeImFive, and random negatives. It outperforms FineWeb-Edu in top-10% selection. DeepSeek-Coder-V2 [10] follows DeepSeek-Math [34] by leveraging a multistage fastText-based pipeline to recall high-quality code and math contents. Llama-3 [12] uses fastText for recognizing text referenced by Wikipedia [42] and Roberta-based [22] classifiers trained on Llama-2 [38] predictions. While prior work focuses on initial pretraining, Arctic-SnowCoder demonstrates that high-quality data from the pretraining corpus can significantly enhance model performance during continued pretraining. We are also the first to uncover the secret of data quality, revealing the importance of matching data distribution with downstream tasks. \n\nHigh-quality code data for pretraining Phi-1 [13] is one of the first to study the impact of highquality code data. It first uses a random forest classifier to filter out high-quality code data from \n\nThe Stack v1 and StackOverflow, and then creates synthetic textbook-like data and exercises using GPT-3.5 [29], showing significant coding performance with only 50B+ training tokens. DeepSeek-Coder-V2 [10], pretrained for around 14T tokens in total, achieves state-of-the-art coding performance, with a multi-stage fastText-based [5] pipeline to recall web-related code data as well as high-quality GitHub code.",
            "score": 0.49397523846117875,
            "section_title": "Related Work",
            "char_start_offset": 20921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05889892578125
        },
        {
            "corpus_id": "271693742",
            "title": "Leveraging encoder-only large language models for mobile app review feature extraction",
            "text": "Stemming from recent literature reviews in the field of LLMs (Hou et al., 2024;Naveed et al., 2024;Zhao et al., 2023), we compared different encoderonly LLMs suitable for our evaluation and comparative analysis. We focused on encoder-only architecture due to their inherent suitability for classification tasks (Hou et al., 2024). In addition, we also excluded decoder-only models (also known as generative models) due to their size and resource consumption. These models present limited applicability in large-scale contexts such as user review mining, especially in terms of memory, computational resources and time constraints. Particularly, in this study, we selected the following models: \n\n-BERT, considered the first encoder-only LLM, is renowned for its advanced contextual understanding due to its bidirectional nature (Devlin et al., 2019). It is pre-trained using the MLM objective, which enhances its ability to grasp context from both directions, making it effective for token-level tasks such as NER (Broscheit, 2019). For these reasons, we use BERT as a baseline LLM for NER tasks. -RoBERTa improves upon BERT's design and training methods through extended pre-training on a larger dataset with additional data, resulting in stronger language representations (Liu et al., 2019). It also uses MLM for pre-training but outperforms BERT in many cases (Liu et al., 2019). We include RoBERTa in our model evaluation due to its enhanced performance over BERT. -XLNet uses a unique approach by combining autoregressive and bidirectional training, considering all possible word permutations during pretraining (Yang et al., 2019). This improves its ability to understand context and model token dependencies more effectively than traditional models. Unlike BERT and RoBERTa, XLNet employs a PLM training objective. Consequently, token dependencies are modelled differently. We evaluate XLNet's performance to compare its innovative training method against the MLM objectives of BERT and RoBERTa. \n\nEncoder-only models have not significantly evolved over the past few years.",
            "score": 0.4918132850615915,
            "section_title": "Model fine-tuning",
            "char_start_offset": 25993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 117,
                    "matchedPaperCorpusId": "30075992"
                },
                {
                    "start": 828,
                    "end": 849,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1014,
                    "end": 1031,
                    "matchedPaperCorpusId": "208191596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03656005859375
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "In addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016). Compared to LLM-based classifiers (Penedo et al., 2024), the fastText-based classifier demonstrates superior inference efficiency, enabling both the filtering of higher-quality training data for LLMs and significantly accelerating the high-quality data filtration pipeline. We apply the proposed data filtering pipeline to the FineWeb (Penedo et al., 2024) and Chinese FineWeb (Yu et al., 2025) datasets (source data from Chinese FineWeb-edu-v2, which includes IndustryCorpus2 (Shi et al., 2024), MiChao (Liu et al., 2023), WuDao (BAAI, 2023), SkyPile (Wei et al., 2023), WanJuan (Qiu et al., 2024), ChineseWebText (Chen et al., 2023), TeleChat (He et al., 2024), and CCI3 (Wang et al., 2024)), resulting in the creation of higher-quality Ultra-FineWeb-en and Ultra-FineWeb-zh datasets, collectively referred to as Ultra-FineWeb. Experimental results show that LLMs trained on Ultra-FineWeb perform excellently across multiple benchmark tasks, providing empirical validation for the effectiveness of our high-quality data filtering pipeline and its efficiency in reducing computational costs. \n\nOur main contributions are as follows. The datasets and classifier will be released to facilitate the development of LLMs. \n\n\u2022 Efficient Verification Strategy: We propose a computationally efficient verification strategy that enables rapid evaluation of the impact of data on LLM training performance with minimal computational cost, significantly improving the efficiency of high-quality data filtering experiments.",
            "score": 0.48949493317549403,
            "section_title": "Introduction",
            "char_start_offset": 3468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1641
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.450927734375
        },
        {
            "corpus_id": "266977204",
            "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
            "text": "Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a). We also investigate whether the range of scores we observe with fastText generalize to other model-based measurements of English used in LLM development (Table 3). These langID systems include Compact Language Detector 2 (CLD2) (Sites, 2013), CLD3 (Salcianu et al., 2020), and langdetect (Shuyo, 2014).",
            "score": 0.48816376019462027,
            "section_title": "Pretraining Data Filters",
            "char_start_offset": 14944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1343994140625
        },
        {
            "corpus_id": "237416507",
            "title": "An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining",
            "text": "the increasing literature in this direction. While the principle seems simple, in practice, there remain many challenges.\n\nTheoretically, it appears that pre-training BERT is more susceptible to noise and size of datasets. While 'more data' may be a solution to this challenge, it remains a question that 'how much is enough', or to what extent this pretraining process benefits from increasing training data. At the same time, there is a lack of understanding or guidance on the 'quality' aspect of the data used for training such LMs. Ultimately, how to select a 'balanced' corpus, how does this matter, is there a way to focus on high quality but smaller data for pre-training LMs and if so, how should this 'quality' be measured? We argue that these could be interesting questions to explore, as they may help towards addressing the inaccessibility of the current LM training approaches that are extremely computationally demanding.\n\nHaving identified the links of our results to the literature from a theoretical point view, we next develop a series of analysis to further understand the data quality issues that may contributed to our negative results. This includes a look into the data we used for training MT models in order to understand why we were unable to replicate the success of [29]. We present them in Sections 7.2 to 7.4.",
            "score": 0.4880737946418902,
            "section_title": "Relation to the literature",
            "char_start_offset": 69704,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160400390625
        },
        {
            "corpus_id": "268680360",
            "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
            "text": "Previous research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;Dodge et al., 2021;Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English.We remove any personally identifiable information in the datasets such as phone numbers, email addresses etc.\n\nThe filtering process comprises of multiple stages, including normalization, deduplication and classification, that helps us to distinguish between high and low-quality data.This approach ensures that our model is trained on a curated dataset, enhancing its ability to generate coherent and relevant outputs.We train the released models on approximately 175 billion tokens of filtered data.",
            "score": 0.48700591881936484,
            "section_title": "Foundation Model Training",
            "char_start_offset": 3237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 487
                },
                {
                    "start": 489,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 797
                },
                {
                    "start": 797,
                    "end": 879
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 165,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "Thus, modeldriven data filtering, which employs a neural classifier to select high-quality content, has emerged as a better choice (Gunasekar et al., 2023;Shao et al., 2024). Notably, datasets such as FineWeb-edu (Penedo et al., 2024), Chinese FineWeb-edu (Yu et al., 2025), CCI3-HQ (Wang et al., 2024), and DCLM (Li et al., 2024) have demonstrated the efficacy of this paradigm by incorporating model-based classifiers following preprocessing stages, achieving not only substantial improvements in dataset quality but also measurable enhancements in downstream LLM performance across various benchmark tasks. Nevertheless, existing model-driven filtering methods still suffer from two main challenges: (1) There is a lack of efficient validation to quickly verify the filtering results, typically requiring large-scale training to observe the effect, resulting in high costs and low efficiency. (2) They heavily rely on manually-selected seed data, and the data for training classifiers often depends on human expertise, introducing significant subjectivity. \n\nTo address these challenges, we design an efficient data filtering pipeline based on an efficient verification strategy. This verification approach enables impartial seed data selection and facilitates iterative data filtering processes. Specifically, in contrast to conventional approaches that verify data quality by training LLMs from scratch using candidate corpora, our proposed efficient verification strategy leverages a nearly-trained LLM as a foundation. We incorporate candidate corpora during the final training steps and utilize the resulting performance improvement as a metric for assessing data quality. This verification strategy significantly enhances evaluation efficiency while maintaining quality assessment accuracy. Based on our efficient verification strategy, we can impartially select high-quality seed data for classifier training. Building upon the assumption that \"high-quality seed data is beneficial for LLM training\", we develop and optimize the strategy for selecting classifier training seeds and recipes, while carefully curating balanced sets of both positive and negative samples to ensure classifier quality and robustness. In addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016).",
            "score": 0.48592984976400616,
            "section_title": "Introduction",
            "char_start_offset": 1245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2352
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "267095066",
            "title": "Orion-14B: Open-source Multilingual Large Language Models",
            "text": "Data quality is essential in the training of LLMs. To assure high-quality data, we have implemented a series of measures for data filtering, detailed as follows: \n\n\u2022 Text normalization: The datasets contain a large number of texts from various sources, such as web pages and ebooks. These texts are often accompanied by HTML, special characters, or other format tags, which are not useful for LLM training. We employ a series of tools, such as regular expressions and format parsers, to effectively eliminate them. \n\n\u2022 Harmful content removal: The Internet contains harmful and spam content. Our approach to mitigate this involves a two-stage process: the initial stage utilizes keywords and regular expressions matching, followed by a deep learning-based model designed to identify and remove such content. It is important to note that entirely eliminating harmful text from the training dataset could lead to a scenario where the trained model lacks the ability to identify and appropriately response to harmful information (Touvron et al., 2023b). Therefore, we intentionally retain a minimal amount of harmful text in the dataset. This approach ensures that the model remains capable of recognizing and effectively addressing such content. \n\n\u2022 Personal information removal: Some of the text data includes personal details like names, phone numbers, and addresses. We utilize rule-based methods for detection and either substitute these with placeholders or remove them entirely. \n\n\u2022 Quality filtering: This part is crucial in data processing. We first apply a set of rules to filter the data, such as removing texts with excessive repetition. Additionally, we use N -gram perplexity models to exclude texts with anomalously high perplexity. Lastly, our proprietary data quality models are employed to select high-quality data. We emphasize that while high quality is essential for LLM training, achieving a balance between quality and quantity of training data is a non-trivial task. Our models are optimized to retain as much data as possible while maintaining high data quality, which is one of the key factors in the successful training of LLMs.",
            "score": 0.4852197697797212,
            "section_title": "Data Quality",
            "char_start_offset": 7263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48486328125
        },
        {
            "corpus_id": "278237416",
            "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
            "text": "We also trained a binary classifier based on fastText. \n\nFor the binary fastText classifier, the low-and high-quality training data subsets consisted of 185,403 documents each. The low quality subset consisted of documents with educational quality scores of one or two, whereas the high quality subset consisted of documents scoring four or five. We used 95% of the data (and the remaining 5% for validation) to train a fastText model to classify between high-and low-quality data. It reached 92% precision and 91.5% recall on the validation set. \n\nTo train the BERT classifier, we randomly selected a maximum of 75,000 documents from each class, which we had previously labelled with the LLM-as-a-judge with scores ranging from one to five. The resulting dataset consisted of 75,000 documents for each score above one, and 25,981 documents with scores of one. We used 95% of this dataset for training to predict the one to five scores. The model achieved an overall accuracy of 42% and a macro-average accuracy of 46% when evaluated on the remaining 5% of the data, which served as the validation set.",
            "score": 0.4843448421681556,
            "section_title": "Educational quality",
            "char_start_offset": 12720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 57,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2235107421875
        },
        {
            "corpus_id": "278339044",
            "title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code",
            "text": "Recent approaches leverage large language models (LLMs) to generate synthetic annotations for training quality classifiers, which are then used to filter web-scale corpora by retaining high-quality samples (e.g., FineWeb [9], Stack-Edu [3]). Instead of training a separate classifier, we directly prompt Llama-3.3-70B-Instruct [8] to evaluate each Python code snippet on a scale of 0-10, based on ten criteria, including code readability, modularity, and adherence to naming conventions, derived from the Google Python Style Guide. The detailed scoring prompt and the distribution of the quality scores are provided in Appendix C. We exclude samples scoring below 6, retaining only those deemed sufficiently high-quality, and use this filtered subset alongside multilingual data for continual pre-training in our ablation studies. As shown in Figure 3, LLM-based filtering yields modest improvements (less than 1 point) over linter-based filtering on the HumanEval and HumanEval+ benchmarks. \n\nGiven these limited gains, we compare LLM-based scoring to our LLM-driven rewriting pipeline, which refines code snippets by enhancing clarity and correctness (Section 3.3). Comparing Figure 3 and Figure 4, although the rewriting pipeline requires 1.22 times the computational resources of LLMbased scoring (a 22% increase), it achieves significantly greater performance gains on HumanEval and HumanEval+ (detailed in Appendix F). Consequently, we do not incorporate LLM-based scoring in subsequent experiments, which favors the more effective and cost-efficient rewriting approach.",
            "score": 0.48388991943580295,
            "section_title": "LLM-based scoring filtering",
            "char_start_offset": 9242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "252081257",
            "title": "BertSRC: transformer-based semantic relation classification",
            "text": "BERT is a pre-trained model that leverages the structure of the transformer encoder [19,20]. Pre-trained models learn and utilize universal text embeddings rich in grammatical and semantic features from pre-training on a vast amount of textual data, and only a simple additional layer is needed for the aiming task. The basic BERT model was trained on the Book corpus (800 M words) and Wikipedia (2.5B), achieving SOTA in most common NLP tasks [20]. Since then, its variant models, which were pre-trained on domain-specific data, such as BioBERT [31], SciBERT [32], and PubMedBERT [33] as well as advanced versions of BERT with tweaked pre-training methods or a structure of layers, such as ALBERT [34] and RoBERTa [35], have been announced. \n\nIn the pre-training stage of BERT, a masked language model (MLM) and next sentence prediction (NSP) were utilized to learn various characteristics of natural languages [20]. The MLM method is a method of randomly replacing 15% of tokens with [MASK] tokens in the input sentence, expecting the BERT model to predict the original word of the [MASK] token. The pre-training of BERT involves two different sentences divided by the [SEP] token as input. At this time, 50% of the sentence pairs are in order, with the next sentence being the actual sentence that follows the prior in the original text, and the rest are not, with the first sentence being followed by a random sentence. NSP involves models learning to determine whether these two statements are in order. To develop a BERT model trained with these methods in the pre-training stage to perform downstream NLP tasks, such as relation extraction, an additional layer for detailed tasks is appended after the transformer encoder layer which has learned weights, and further fine-tuning is performed using relevant data for the desired tasks. This is how fine-tuning can provide a model for handling detailed downstream tasks. \n\nDuring the fine-tuning stage, the simplest approach to processing input text for the relation classification task is using a single sentence containing the relationship between entities without any pre-processing treatment. However, to achieve optimal performance, introducing a slightly more complex input data processing method is necessary.",
            "score": 0.4824929109213552,
            "section_title": "Pre-training and Fine-tuning stage of BERT",
            "char_start_offset": 31749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 88,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "220919723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038116455078125
        },
        {
            "corpus_id": "271874495",
            "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
            "text": "The success of large language models (LLMs) is significantly influenced by the quality and quantity of the pre-training corpus. Researchers have developed various data curation pipelines to enhance dataset quality, focusing on raw web crawling, text extraction, repetition and toxic content removal, and, notably, quality filtering (Brown et al., 2020;Rae et al., 2021;Penedo et al., 2023). \n\nQuality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020;Gao et al., 2020;Chowdhery et al., 2023) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models. These methods mitigate the biases introduced by reference datasets but encounter challenges due to the indirect relationship between absolute perplexity and document quality. This indirect relationship inadvertently favor data with simple and repetitive content. Although such content is easier for models to predict, it contributes minimally to learning diversity and complexity (Wettig et al., 2024). \n\nTo address these issues, we introduce a simple yet effective quality filtering approach named Scal-ingFilter, which inversely leverages recent scaling laws in generative modeling to assess data quality. The core idea is to analyze the perplexity differences between two pre-trained models on the same data and assess the data quality based on these differences. We find a positive correlation between data quality and perplexity differences by inversely deriving Chinchilla scaling law (Hoffmann et al., 2022).",
            "score": 0.4796261713787634,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 352,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 369,
                    "end": 389,
                    "matchedPaperCorpusId": "259063761"
                },
                {
                    "start": 724,
                    "end": 744,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 761,
                    "end": 784,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "268533147",
            "title": "Using Large Language Models for sentiment analysis of health-related social media data: empirical evaluation and practical tips",
            "text": "Pre-trained BERT-based LLMs (BERT_base_uncased and BioClinicalBERT) were fine-tuned on a random sample of the evaluation datasets. Each evaluation dataset was split into a training, validation, and test set, following a 0.8-0.1-0.1 ratio. SiEBERT (Sentiment in English) was already fine-tuned on various texts with sentiment labels, including reviews and tweets 26 . Therefore, we directly adopted the fine-tuned check point of the model, without further tuning using our study data. Since the maximum token size allowed for BERT-based models is 512, we truncated the input text to include only 512 tokens; texts containing less than 512 tokens were padded with extra tokens so that input texts are of the same length. \n\nExperiment 2: comparing zero-shot LLMs using generic versus data-specific prompts The goal of Experiment 2 was to investigate whether and how different prompting strategies affect the performance of the zero-shot LLMs. Subsequently, we tested basic prompts against data-specific prompts. We also tested chainof-thought prompts which required LLMs to reason and have shown promising results in prior studies 29 . However, the performance from our pilot study was extremely poor when using chain-of-thought prompts and even worse than generic prompts. This is probably because chain-of-thought prompts are useful for simple reasoning tasks such as solving mathematical problems or answering questions. Therefore, we decided to exclude chain-of-thought prompts in our evaluation. \n\nWhile there are many viable prompts, to make a fair baseline for the basic and generic prompts, we only conducted moderate prompt engineering, following instructions provided in previous studies 7 . Because sentiment analysis is essentially a text classification task with pre-defined sentiment labels, we formatted the prompts as multiple-choice problems, where each option is listed after a letter, followed by a new line character. We also instructed the LLMs to only produce the letter, both to reduce the number of tokens generated and to make the evaluation easier. After rounds of testing, we finalized the generic prompt and used it across all LLMs and on all evaluation datasets.",
            "score": 0.47540091028586806,
            "section_title": "positive B. negative C. neutral D. not sure Text: [REPLACE WITH DATA].\"",
            "char_start_offset": 13051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 364,
                    "matchedPaperCorpusId": "249903898"
                },
                {
                    "start": 1128,
                    "end": 1130,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033843994140625
        },
        {
            "corpus_id": "272126224",
            "title": "The Training Process and Methods for LLMs Using an Own Knowledge Base",
            "text": "Large Language Models (LLMs) have made significant progress in the fields of Natural Language Processing (NLP) and Computer Vision (CV) in recent years and have demonstrated strong capabilities in various applications. These models, such as GPT-4 and BERT, typically undergo pretraining on massive text data to master language's complex structures and semantic relationships [1] . This pre-training process enables the models to generate high-quality text, answer questions, translate languages, summarise text, and analyse sentiments. As Llama3 and Mistral 8x22B provide capabilities to solve production-level tasks, the framework or products for LLM pre-training, finetuning, evaluation, and deployment based on their knowledge base are rapidly emerging. Personalised AI applications are on the horizon, as anyone can fine-tune models with their application data. specific domains. Firstly, by designing and implementing an efficient data collection and processing workflow, we ensure the quality and relevance of our knowledge base, enhancing the model's performance in specific tasks. Secondly, we investigate how to optimise the model training process, including hyperparameter tuning and training architecture design, to maximise model performance under limited resources. Finally, we explore the training process that is suitable for large language models. We utilise advanced inference optimisation techniques (such as model quantisation and knowledge distillation) to improve the model's inference speed and resource utilisation efficiency.",
            "score": 0.4742996765696751,
            "section_title": "Introduction 1.1 Research Background",
            "char_start_offset": 39,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1549
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06109619140625
        },
        {
            "corpus_id": "261339179",
            "title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models",
            "text": "The past few years have witnessed the booming of pre-trained language models. BERT (Devlin et al., 2019) is designed to understand the context of words in a sentence and has been used for tasks such as sentiment analysis and question answering. RoBERTa (Liu et al., 2019) is an improved version of BERT with better pre-training techniques and larger training data. T5 (Raffel et al., 2020) is designed to perform a wide range of natural language processing tasks, including language translation and summarization. XLNet (Yang et al., 2019) is designed to handle long sequences of text and has achieved state-of-the-art results on several natural language processing tasks. GPT-3 (Brown et al., 2020) is one of the most advanced LLMs with 175 billion parameters, capable of performing a wide range of natural language processing tasks. Along with the open-sourced ones like GLM (Du et al., 2021), BLOOM (Laurenc \u00b8on et al., 2022), OPT (Zhang et al., 2022) and LLaMa series (Touvron et al., 2023), LLMs have remarkably revolutionized the field of natural language processing and are being used in a wide range of applications. \n\nNevertheless, LLMs have billions of parameters and are often pre-trained on large amounts of text data, which require significant computational resources to train and deploy. There is a call for faster inference time and lower memory requirements to make LLMs more practical.",
            "score": 0.47380628556636684,
            "section_title": "LARGE LANGUAGE MODELS",
            "char_start_offset": 3544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1402
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 104,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 368,
                    "end": 389,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 520,
                    "end": 539,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 679,
                    "end": 699,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06597900390625
        },
        {
            "corpus_id": "271088966",
            "title": "A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training",
            "text": "The use of pre-trained text quality classification/regression models is a common element of low-quality content filtration methods described in the literature [6,27,35,46].For training classifier models, corpora that are considered to be of good quality are usually used, such as Wikipedia text corpora, BookCorpus2 and other curated dataset [6].\n\nStatistical Models for Automatic Language Identification.Using statistical, lightweight text language identification models such as langid.py3[18], fasttext 4 [15], langdetect5 [37] is another method of filtering qualitative data from CommonCrawl corpora [33,35] described in the literature.\n\nProbabilistic N-grams Language Models.Filtering low-quality content based on n-gram lightweight language models trained on good-quality data collections is an element of data cleaning and filtration processes described in the literature [31].Pipelines like CCNet [46] employ statistical KenLM models to assess text perplexity during the data filtering phase.\n\nThe poor quality of data collected in web corpora is also largely due to the difficulty of extracting the main content from websites.Contamination caused by text extraction from unwanted HTML elements such as menus, headers, footers, breadcrumbs, advertisements, copyright notices, spam largely affects the performance of language models [2,23,26,27,34].Analysis of the most frequently occurring n-grams conducted on OpenWebText, C4, mC4, OSCAR, The Pile and RedPajama indicates a large share of sequences of repeated punctuation characters such as dashes, question marks, dots; HTML tags, and content derived from the extraction of such page elements like headers, menus etc [13].\n\nComparative analyses of the quality of models trained on raw and filtered pre-training corpora confirm the negative impact of low-quality texts in web corpus and point to the need for methods of filtering high-quality data from web corpora in the process of LLMs pre-training [23,35].",
            "score": 0.47361093682834143,
            "section_title": "Data Quality and Noise",
            "char_start_offset": 8855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 346
                },
                {
                    "start": 348,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 639
                },
                {
                    "start": 641,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 883
                },
                {
                    "start": 883,
                    "end": 999
                },
                {
                    "start": 1001,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1682
                },
                {
                    "start": 1684,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "218974065"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "3411445"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "195505104"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1339,
                    "end": 1342,
                    "matchedPaperCorpusId": "1152099"
                },
                {
                    "start": 1348,
                    "end": 1351,
                    "matchedPaperCorpusId": "218974065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2314453125
        },
        {
            "corpus_id": "257833996",
            "title": "The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling",
            "text": "The rapid progress within the field of Natural Language Processing (NLP), has to a great extent benefited from having access to open datasets. These are often derived from Common Crawl 1 , such as C4 (Raffel et al., 2020), OSCAR (Su\u00e1rez et al., 2019;Ortiz Su\u00e1rez et al., 2020) and The Pile (Gao et al., 2021). While the majority of these datasets have included some form of quality filtering and deduplication, recent work has applied more aggressive filters and deduplication processes to their datasets prior to LLM pre-training. Wenzek et al. (2020) propose an automatic pipeline for extracting massive high-quality monolingual datasets from Common Crawl. \n\nThe pipeline includes deduplication and language identification using fastText (Joulin et al., 2016b), and is extended by removing low-quality documents, using a small Kneser-Ney language model trained on Wikipedia. The authors show that their filtering process results in better model performance. Brown et al. (2020) present GPT-3 and describes their corpus, which is a filtered and deduplicated version of Common Crawl, extended with several curated high-quality datasets. Rae et al. (2021) train Gopher on their cleaned corpus Mas-siveText, a diverse dataset with a large portion de- Our data pipeline is heavily inspired by the related literature, and parts of the above work are hence revisited and further explained in Section 4.",
            "score": 0.4734601423737756,
            "section_title": "Related Work",
            "char_start_offset": 1406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1397
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 221,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 250,
                    "end": 276,
                    "matchedPaperCorpusId": "219573244"
                },
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 960,
                    "end": 979,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346923828125
        },
        {
            "corpus_id": "273098032",
            "title": "GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to Filter Language Model Pretraining Data",
            "text": "Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities across a wide range of tasks. As the field progresses, there is growing interest in developing specialized LLMs tailored to specific domains or applications (Lee et al., 2023;Gupta et al., 2024;Li et al., 2024). A critical component in this process is the curation of high-quality, domain-specific datasets for training and fine-tuning these models. However, the task of filtering vast amounts of web-scale data to create such datasets presents significant challenges in terms of cost, time, and effectiveness. Existing approaches to data filtering and curation primarily rely on two strategies: \n\n(1) sourcing data from specific, trusted sources, and (2) employing pre-existing quality and toxicity detection models to filter harmful content. For instance, medical LLMs often utilize datasets derived from PubMed to ensure domain specificity. While these data filtering methods have their merits, they suffer from notable limitations in terms of both flexibility and comprehensiveness. Many domains lack comprehensive, exclusive sources of high-quality text data, and pre-trained detection models typically focus on general quality metrics and toxicity, limiting their applicability for domain-specific queries or customized quality rubrics. Furthermore, relying solely on established data sources ignores the vast amount of potentially valuable information available on the broader internet, leading to datasets that may be limited in scope and diversity. These constraints can significantly hinder the development of specialized language models that require rich, domain-specific training data. \n\nRecent advancements in general-purpose language models, such as GPT-4o, offer a potential solution to these challenges. These models can act as effective filtering mechanisms when provided with appropriate filtering prompts. To illustrate as an example, one could employ GPT-4o to iterate over all text snippets of the internet to determine whether each piece pertains to the 2024 presidential election. In this context, the machine learning model serves as a binary classification mechanism, evaluating one snippet at a time. However, the computational cost of applying models like GPT-4o to web-scale datasets is prohibitively expensive for most organizations. In this paper, we present Figure 1: System Overview.",
            "score": 0.4723346884338308,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2378
                },
                {
                    "start": 2379,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "257803472"
                },
                {
                    "start": 291,
                    "end": 310,
                    "matchedPaperCorpusId": "258461606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.367431640625
        },
        {
            "corpus_id": "235419945",
            "title": "Variance-Aware Machine Translation Test Sets",
            "text": "There are two main factors that may affect our proposed filtering approach: the filtering percentage \u03bb and the filtering metric M. Hence, a series of empirical experiments was conducted on the WMT20 benchmark to explore the best settings for building the most discriminative test sets, and the finalized setting would subsequently be used to validate its generality in other WMT benchmarks. Choice of Filtering Percentage \u03bb affects the amount of data to be preserved and is also an indicator that reflects the discernment of the current data sets in terms of the evaluation metrics. As shown in Figure 2, using only a partial test set can improve the evaluation correlation of automatic metrics, but the most effective percentage setting depends on the type of evaluation metric. Compared to the BLEU metric, the metrics driven by the pre-trained models achieve the local optimal correlation using a smaller proportion of the test set, i.e., \u03bb \u2265 50. The underlying reason for this may lie in the granularity of the evaluation in terms of the semantics: a metric that is better in parsing the semantics needs fewer data to distinguish the MT systems because of the larger impact of those discriminative samples in the comparison. However, the percentage setting may vary from language to language. \n\nFigure 3 shows that filtering 60% of original data still can improve the correlation performance for both to-English and from-English translation directions, confirming the robustness of this setting. For fitting most of the metrics and languages, we filter 60% of the instances out of the original test sets in the subsequent experiments. Choice of the Filtering Metric M matters because the discernment of a test instance can not be estimated without an accurate evaluation of the MT systems' performance. Figure 4 presents how the scores given by the different metrics affect the correlation of filtered test sets. Filtering the test set based on the scores given by the BERTS-R metric outperforms the test sets created by other metrics. \n\nIt is reasonable that the BERTS-R metric consistently achieves the best correlation when using it as the evaluation metric (also as shown in Figure 2), and thus is better at quantifying the differences between the hypotheses and filtering out the non-discriminative instances.",
            "score": 0.4721010224928154,
            "section_title": "Ablation Study",
            "char_start_offset": 15066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2039
                },
                {
                    "start": 2042,
                    "end": 2318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0703125
        },
        {
            "corpus_id": "204877385",
            "title": "Interactive Evidence Detection: train state-of-the-art model out-of-domain or simple model interactively?",
            "text": "We built two interactively trained models, bilstm direct and bilstm fine , and used BERT as static model trained on the out-of-domain data. \n\nWe refer to the bilstm fine after its pre-training but before additional fine tuning as bilstm pre . \n\nTable 3 shows the models and which data they are trained on, ouf-of-domain, in-domain, or both. We decided to use a BiLSTM with 100 nodes, a dense layer for classification, and no input for the topic for these experiments because the in-domain training data is small and always specific to a single topic. All interactively trained models used 100-dimensional GloVe embeddings (Pennington et al., 2014) as input features and a dropout of 0.5 after the embedding layer and before the classification layer. We addressed the class imbalance by weighting the classes similar to King and Zeng (2001) using the implementation provided by scikit-learn. 6 To reduce the effect of the random initialisation, we repeated all experiments with 10 different randomisation seeds. bilstm direct The directly trained model was trained as described above and received no additional input. We trained this model for 10 epochs in each iteration with one additional training document. \n\nbilstm pre The pre-trained model uses the same architecture than the directly trained one. We changed no hyper-parameter except the number of epochs compared to the directly trained model. That means, we trained the bilstm pre model for five epochs on the out-of-domain training data and used a learning rate of 0.001 with a dropout of 0.5. \n\nbilstm fine For fine-tuning, we replaced the classification layer of the bilstm pre model with a new one and trained this new layer for five epochs with a learning rate of 0.001. Afterwards, we unfroze the other layers and trained the complete network for five more epochs with a learning rate of 0.001. This is similar to gradual unfreezing, presented by Howard and Ruder (2018). \n\nBERT Short for Bidirectional Encoder Representations from Transformers.",
            "score": 0.4709325414331658,
            "section_title": "Models",
            "char_start_offset": 12723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 647,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "15000608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083465576171875
        },
        {
            "corpus_id": "234813543",
            "title": "Filtered BERT: Similarity Filter-Based Augmentation with Bidirectional Transfer Learning for Protected Health Information Prediction in Clinical Documents",
            "text": "We propose a filtered BERT method for effective data augmentation of clinical documents that include PHI, by applying a word-similarity-based filtering algorithm. The overall structure of the filtered BERT for the augmentation model is similar to that of the BERT-based augmentation method proposed in a previous study [30]. However, in the augmentation method that predicts the masked word through BERT, we added a filter that checked that word. The filter compares the similarity between the masked word and words in the original sentence. Thus, only words with a certain degree of similarity passed through the filter. The detailed structure of the proposed filtered BERT is shown in Figure 2. \n\nBERT-based augmentation method proposed in a previous study [30]. However, in the augmentation method that predicts the masked word through BERT, we added a filter that checked that word. The filter compares the similarity between the masked word and words in the original sentence. Thus, only words with a certain degree of similarity passed through the filter. The detailed structure of the proposed filtered BERT is shown in Figure 2. The similarity filter was designed to check whether the words in the vector masked by the BERT model were similar to the original words. Figure 3 shows the detailed processing of the filter algorithm. To apply the BERT model, the words of the original sentence were masked (Xmasked) and those predicted through context-based reasoning in the BERT (Xpredicted) were converted into word vectors based on fastText [23] word embedding. Cosine similarity was calculated using the converted word vector, and word similarity was measured through this. The calculated cosine similarity had a value from -1 to 1, where -1 meant the masked and predicted words were different in meaning, and 1 meant they were the same. If the calculated cosine similarity was within the preset range, the predicted was finally returned to replace the masked word. Figure 2 shows the detailed processing of the filter algorithm. For example, if \u2208cossim is set to 0, and the cosine similarity of Xmasked and Xpredicted is \u22120.7, it cannot pass the filter, and Xpredicted has to be predicted again.",
            "score": 0.4706809250771125,
            "section_title": "Data Augmentation 2.3.1. Filtered BERT for Augmentation Structure",
            "char_start_offset": 9636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2204
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06719970703125
        },
        {
            "corpus_id": "257806425",
            "title": "A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat",
            "text": "We did experiments with the annotated dataset in \u00a73 to show the effectiveness and the efficiency of our method. Model trainings were conducted on Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz with a single processor. RAM size is 26GB. Cache memory consists of 32KB for L1d, 32KB for L1i, 1024KB for L2, and 28,160KB for L3. The model implementation is as follows: \n\nfastText classification To get the word representations, we create a pre-trained model using the fastText module (Bojanowski et al., 2017) with all the comments in \u00a73. For pre-processing, this data is tokenized by the Japanese morphological analyzer MeCab (Kudo, 2006). Additionally, a word was lemmatized and half-width characters were converted to full-width. Hyperparameters for the pre-training are as follows: The number of dimensions for word representation is 300. We used skip-gram to train word representation. The threshold \u03b8 in \u00a74.2 was determined with development set (\u03b8 = 0.98). \n\nSVM We calibrated the prediction results using a calibrated ClassifierCV 3 provided by scikit-learn to remove the effect of bias in the unbalanced data. \n\nLightGBM This model was optimized with optuna (Akiba et al., 2019). \n\nAs baseline models, we used each component of our method and existing pre-trained BERT-based models. We used the BERT-base4 and BERT-large5 models pre-trained from Japanese Wikipedia. Additionally, it is essential to compare with light BERT models for a fair evaluation of inference speed. Over the last couple of years, variants of BERT have been proposed to make the model size light and inference speed efficient. Specifically, we tried Distil-BERT6 (Sanh et al., 2019), ALBERT7 (Lan et al., 2019), and Poor-Man's BERT (Sajjad et al., 2020). Poor-Man's BERT is the method that removes some layers from an original BERT. Their experimental results showed that dropping the top layers works consistently well across different tasks when dropping 4 and 6 layers.",
            "score": 0.47035248362961424,
            "section_title": "Settings",
            "char_start_offset": 11210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 499,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01302337646484375
        },
        {
            "corpus_id": "274165864",
            "title": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings",
            "text": "AWS) instance has an inference latency time of over 140 seconds, while our best model has an inference latency time of about 0.05 seconds (see Appendix 8.1) While the results are promising, there is is need for improvement in several directions. For starters, our investigation was for English only, and we have not yet explored fine-tuning Sentence-BERT on multilingual inputs. This restricts the effectiveness of our guardrail to English-speaking users, limiting its utility for global use. Second, our embedding model is constrained to text-based inputs, and does not accommodate other modalities, such as speech or video, which are increasingly common in interactions with LLMs. Finally, our current solution only provides generic unsafe input filtering and does not support few-shot topic-based filtering. As a result, application developers cannot define specific additional topics they wish to filter as unsafe, which limits the customization and flexibility of the guardrail. Addressing these limitations would significantly enhance the applicability and robustness of our system. Future work will explore ways in which to fine-tune an embedding model capable of both unsafe prompt filtering and few-shot topic filtering with minimal data. \n\nOther directions for future investigation are suggested by observations of results already obtained. We believe that our model's performance can be improved significantly, given that we only used a fraction of the AEGIS data. After our data preprocessing, we retained 9,674 public annotated human-LLM interaction instances, compared to around 26,000 total instances in the corpus (see Appendix 8.3, Figure 4, for the full AEGIS category distribution). When doing ablation studies varying the training dataset size, we found that there existed a monotonically increasing trend between the F1 score and the amount of training data used for embedding model (see Appendix 8.4, or Table 6). We can thus surmise that much better results could be obtained with substantially more data. This in turn suggests using techniques for data augmentation (e.g., paraphrasing) or machine-labeling (e.g., ensembling of powerful teacher models).",
            "score": 0.46939382345069125,
            "section_title": "Results",
            "char_start_offset": 18568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2177
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140625
        },
        {
            "corpus_id": "272880910",
            "title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models",
            "text": "Recently, large language models (LLMs) have significantly advanced the field of artificial intelligence (Zhao et al., 2023;Hadi et al., 2023;Minaee et al., 2024). Due to the unprecedented number of parameters (model size) and the pre-training on huge amount of training data, LLMs are generalizable a broad spectrum of downstream tasks. However, in practice, the computation resources limit both the model size and the volume of data used in pre-training. In this situation, judiciously selecting train datasets is critical for producing highly performance LLMs (Brown, 2020;Du et al., 2022;Gururangan et al., 2020;Hoffmann et al., 2022;Raffel et al., 2020). In particular, the quality 1 arXiv:2409.16986v2 [cs.AI] 5 Oct 2024 of the training datasets vary dramatically, while the LLaMA-3.1 report (Dubey et al., 2024) shows that the use of high quality data in later training stages can greatly improve model performance. \n\nTypical straightforward data selection approaches include rule-based data filtering (Raffel et al., 2020;Rae et al., 2021), querying high-performance models (e.g., GPT-4) (Wettig et al., 2024;Sachdeva et al., 2024), surrogate models (Lin et al., 2024;Shao et al., 2024), etc. Although these methods have achieved success on some datasets and models, they rely on simple heuristics to select training data. Without explicitly measuring the impact of the selected data on the model, these methods tend to produce sub-optimal pretraining results. To address this issue, some researchers (Xia et al., 2024;Yu et al., 2024) start evaluating each data instance by assigning it a score that reflects its impact on the model. Frequently used scoring methods include the influence function (Xia et al., 2024), early loss (Albalak et al., 2023), and perplexity (Chen et al., 2024).",
            "score": 0.4690250228813317,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1795
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 161,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 575,
                    "end": 591,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 615,
                    "end": 637,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 637,
                    "end": 657,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1095,
                    "end": 1116,
                    "matchedPaperCorpusId": "267681974"
                },
                {
                    "start": 1508,
                    "end": 1526,
                    "matchedPaperCorpusId": "267522839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "273323858",
            "title": "Data Processing for the OpenGPT-X Model Family",
            "text": "Training data for LLMs is primarily based on web-based data sources like Common Crawl 6 . In this section, we present an overview of similar initiatives to train LLMs to provide context for the data processing pipeline described in this paper. By outlining related datasets and their corresponding pipelines, we aim to highlight the key features and challenges of building such datasets, offering a comparative perspective to the approach taken in our work. We provide an overview in Table 3 (see Appendix). \n\nTrove 10 , which was released alongside the dataset. The pipeline builds upon the methodology used in RefinedWeb, beginning with text extraction from WARC files using the trafilatura tool. Language filtering is performed with fastText, applying a cutoff score of 0.65 to ensure the inclusion of high-quality English documents. After this, the dataset undergoes both document-wise and linewise heuristic filtering to further enhance quality by removing noise and non-linguistic content. The final deduplication step employs the MinHash algorithm, ensuring a clean and diverse dataset that is well-suited for large-scale language model training.",
            "score": 0.46855578825401945,
            "section_title": "Related Work",
            "char_start_offset": 2212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1822509765625
        },
        {
            "corpus_id": "274638463",
            "title": "Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning",
            "text": "It is crucial to understand the distinction between encoder models, like BERT, and decoder LLMs in the context of text classification tasks. Prior research has shown that BERT-like encoder models generally outperform LSTM and traditional machine learning methods on classification tasks [86,47]. The performance of BERT-like encoder models also surpasses decoder LLMs without fine-tuning [17]. However, our results demonstrate that fully fine-tuned decoder LLMs, such as Llama3-70B, are able to outperform encoder models like RoBERTa-large on text classification tasks (Table 3). This is likely due to the larger parameter size of the fine-tuned decoder LLMs (billion-level) compared to the encoder models (million-level). Larger models with more parameters are generally expected to achieve better performance, especially when coupled with the larger training data (trillion-level tokens) used for the decoder LLMs, compared to the encoder models (billion-level tokens). \n\nWithin the Llama family of LLMs, we observed that the Llama3 models consistently outperformed the Llama2 models of the same scale, both in fine-tuned and pre-trained, instruction-tuned settings. This can be attributed to the improved quantity (15T tokens vs. 1.8T tokens) and quality of the training data used for Llama3 [74]. Additionally, our results show that the larger Llama models (70B) generally performed better than their smaller counterparts (8B), for fine-tuned and pre-trained instruction-tuned LLMs. This aligns with previous studies [49,74] and suggests that the increased size and complexity of the larger models enable them to learn more intricate patterns in the data, leading to better performance on classification tasks. \n\nTable 3 also showed that the close-source LLMs such as GPT4 and GPT4o have achieved the best performance among the pre-trained, instruction-tuned models, outperforming the openly-available Llama2 and Llama3 models.",
            "score": 0.46823245815099457,
            "section_title": "LLMs in Classification Task",
            "char_start_offset": 25383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1931
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033905029296875
        },
        {
            "corpus_id": "214407947",
            "title": "Joint Entity and Relation Extraction with a Hybrid Transformer and Reinforcement Learning Based Model",
            "text": "Dataset We evaluate our proposed model on a widely used dataset which is developed by the distant supervised method and the test set is manually annotated (Hoffmann et al. 2011). Similar to Takanobu et al. (2019), we also filtered the dataset by removing the sentences which have no relation, i.e. the None relation type, and relations in training set but not in the test set. There are 62, 648 sentences, 74, 312 triplets in the training set and 370 sentences in the test set, denoted as TestSet1. The size of the relation set is 12. Because TestSet1 is manually annotated with almost no noisy data, to better evaluate our model on both joint extraction and noisy filtration tasks, we also randomly selected 5 percent of the training data as the additional test set, denoted as TestSet2, which contains 3, 763 sentences. We use another 5 percent of the remaining training data as the validation set to tune the hyperparameters referring to the previous studies. \n\nParameter Settings Pre-training a transformer based model is computationally expensive, and our main goal is to show the effectiveness of combining pre-trained model and downstream models, so we use the pre-trained language model BERT (Devlin et al. 2019) as our pre-trained encoder. We use the BERT-base version with 12 encoder layers, 12 heads attention, 768 hidden size and about 110M parameters. The number of LSTM units in the decoder layer is 768. The learning rate is 5e \u2212 5 for the encoder and entity detection module, and is set to 0.003 in the reinforcement learning module, and the batch size is 16. \n\nBaselines We choose both the pipelined methods (FCM) and the joint learning methods as the baselines. For joint learning methods, we also consider the feature-based model (CoType) and neural network based models. Besides, we also do some ablation studies on different variants of our method to prove the effectiveness of different modules.",
            "score": 0.467597718689825,
            "section_title": "Experiments Experimental Setting",
            "char_start_offset": 17113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 177,
                    "matchedPaperCorpusId": "16483125"
                },
                {
                    "start": 190,
                    "end": 212,
                    "matchedPaperCorpusId": "53250562"
                },
                {
                    "start": 1200,
                    "end": 1220,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0223236083984375
        },
        {
            "corpus_id": "268723886",
            "title": "BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models",
            "text": "Recently, the emergence of LLMs has attracted substantial attention and research efforts across various fields, primarily owing to their superior linguistic capability.The foundation for modern LLMs is the Transformer architecture [10,48].which is fundamental in major language models like BERT [13] and GPT [42].BERT [13] achieves promising results by pre-training on large corpus and fine-tuning on downstream tasks [31,32,52].Similarly, GPT [42] introduces auto-regressive language modeling, generating text sequentially based on previously produced tokens.Both BERT and GPT series models require extensive pre-training on a massive corpus to acquire general linguistic knowledge, leveraging tasks such as masked language modeling [13], next token prediction [42], etc.However, infusing domain-specific knowledge during the pretraining phase of LLMs poses challenges due to the substantial data requirements.In specific domains, the availability of sufficient data for effective pre-training is often limited, thus constraining LLMs' applicability.This limitation is particularly acute in fields with stringent data privacy regulations, such as legal and medical domains.Another crucial training stage for LLMs is Supervised Fine-Tuning (SFT), which involves training LLMs using task-specific datasets with labeled examples.This stage adapts the general linguistic knowledge acquired during pre-training to specific tasks, such as sentiment analysis [17], text classification [16,18], and dialogue systems [14,41].Despite the effectiveness of SFT, high-quality taskspecific annotation is usually costly.This cost is further amplified by the scale of parameters in LLMs, making the adaptation of LLMs to specific domains during this stage financially prohibitive.In this paper, we investigate how to leverage existing data to adapt LLMs to specific domains effectively, without the need of additional LLM pre-training or extra annotations.",
            "score": 0.46713045664123937,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 4931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 772
                },
                {
                    "start": 772,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "258041275"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "235792270"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "259076101"
                },
                {
                    "start": 1513,
                    "end": 1516,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1895751953125
        },
        {
            "corpus_id": "225344069",
            "title": "Modern approaches to multiclass intent classification based on pre-trained transformers",
            "text": "the multiclass classi\ufb01cation results. Method. This study uses a comparative model analysis technique. In turn, each model is inlined into a common pipeline for data preparing and clearing, and the model training but with regard to its speci\ufb01c requirements. The following models applied in real projects have been selected for comparison: Logistic Regression + TF-IDF, Logistic Regression + FastText, LSTM + FastText, Conv1D + FastText, BERT, and XLM. The sequence of models corresponds to their historical origin, but in practice these models are used without regard to the time period of their creation but depending on the effectiveness of the problem being solved. Main Results. The effectiveness of the multiclass classi\ufb01cation models on real data is studied. Comparison results of modern practical approaches are described. In particular, XLM con\ufb01rms the superiority of transformers over other approaches. An assumption is made considering the reason why the transformers show such a gap. The advantages and disadvantages of modern approaches are described. Practical Relevance. From a practical point of view, the results of this study can be used for projects that require automatic classi\ufb01cation of intentions, as part of a complex system (voice assistant, chatbot or other system), as well as an independent system. The pipeline designed during the study can be applied for comparison and selection of the most effective model for speci\ufb01c data sets, both in scienti\ufb01c research and production.",
            "score": 0.46544302861366205,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240936279296875
        },
        {
            "corpus_id": "258557362",
            "title": "Large Language Models Meet NL2Code: A Survey",
            "text": "As the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.\n\nEarly models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al. In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.",
            "score": 0.46524331053210843,
            "section_title": "Large and Premium Data",
            "char_start_offset": 12190,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 782,
                    "end": 804,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 851,
                    "end": 870,
                    "matchedPaperCorpusId": "249642442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1644287109375
        },
        {
            "corpus_id": "272524632",
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "text": "In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
            "score": 0.4648686348308666,
            "section_title": "INITIAL PRETRAINING EXPERIMENTS",
            "char_start_offset": 26288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "265445936",
            "title": "A Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs",
            "text": "For instance, LR compared to BERT has an average loss in recall of \u224858 pp, entailing the risk of missing a large proportion of the satisfied provisions in \u03f5. On the one hand, the 11.8 pp in precision (the advantage of LR over BERT) entails a fraction of falsely satisfied provisions introduced by BERT which can be filtered out by an expert with relative ease. The low recall of LR, on the othe hand, entails the risk of missing a lot of satisfied provision, compromising thereby the completeness of the DPA. Therefore, for binary classification, we select BERT as the best performing solution. \n\nWith regard to multi-class classification, RoBERTa yields the best results in terms of accuracy, recall, and F 2 . Compared to RoBERTa, legal-BERT and BERT achieve comparable performance in terms of F 2 with a loss of 2.6 pp and 3.5 pp, respectively. As shown in the table, ML-based solutions demonstrate similar behavior to the one observed for binary classification. That is, they achieve high precision values at the cost of low recall. BiLSTM is still an exception that achieves comparable results to LLMs. For multi-class classification, we select RoBERTa as the best performing solution. \n\nThe remarkably lower recall in the case of traditional ML-based solutions confirm that the text classification problem in the context of completeness checking of DPA is complex. Not having any ground knowledge about the language (in contrast with LLM-based solutions) makes the prediction task for ML-based solutions more challenging. The reason could be due to the commonalities and textual overlaps between the different text segments in a given DPA which satisfy different provisions. BiLSTM, which was widely applied in the NLP literature prior to LLMs, performs exceptionally well. In addition to the fact that BiLSTM is built using recurrent neural networks, it is also effective since it learns the text in a bidirectional manner (similar to BERT). Bidirectional is essential in learning about the context and words co-occurrences in any language. LLM-based solutions obtained language capabilities during the pre-training phase and hence have an advantage over ML-based solutions (including BiLSTM) to better distinguish the DPA text.",
            "score": 0.4641905656307992,
            "section_title": "Answers to the RQs",
            "char_start_offset": 54484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0285491943359375
        },
        {
            "corpus_id": "265525791",
            "title": "Performance Analysis of Federated Learning Algorithms for Multilingual Protest News Detection Using Pre-Trained DistilBERT and BERT",
            "text": "However, for clients with highly Non-IID texts (see Figure 4 on page 6), the general recommendation based on the study results is that FedYogi should be used for the federated training of LMs and LLMs, as it shows the most stable model performance among the compared FL algorithms for both data distribution settings. \n\nThus, the comparative performance analysis shows that there are two main factors in an FL system that influence primarily the global LM model inference quality: the underlying distribution of the training data between the clients and the proper choice of an FL algorithm. However, based on the results of this study, the data distribution has a bigger impact than the FL algorithm. Another reason for the relatively small macro F1 score differences can be attributed to the pre-training of DistilBERT and BERT [35]. \n\nThis work also demonstrates that FedYogi is the most advanced FL algorithm so far and provides the highest utility on Non-IID texts in federated NLP applications. However, all FL algorithms have in common that an average function is used as an aggregation strategy. We think that a different aggregation function, such as a min-max or rotation approach between local models, could lead to comparable or even improved model performance. \n\nRegarding the convergence behavior, which was not the main focus of this study, it was noticeable that the global model in the test FL system reached the state of equilibrium quite fast after a few federated training rounds. Here we also assume it can be attributed to the large pre-training task of BERT and DistilBERT, which is also confirmed in other works [32], [33]. However, further research on the effects and use of pre-trained LMs in FL systems is required.",
            "score": 0.4630547003356852,
            "section_title": "C. WHICH FL ALGORITHMS PERFORM BEST AND ARE THE MOST STABLE IN DETECTING PROTEST NEWS IN ALL FOUR LANGUAGES? (RQ 3)",
            "char_start_offset": 46209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "246531797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.115966796875
        },
        {
            "corpus_id": "271855012",
            "title": "A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution",
            "text": "Language models (LMs) are computational models capable of understanding and generating human languages. These models possess the transformative ability to predict the likelihood of word sequences or generate new text given a specific input [2]. LMs can be categorised into four main stages of development: Statistical Language Models (SLM), Neural Language Models (NLM), Pre-trained Language Models (PLM), and Large Language Models (LLM) [26]. SLMs construct word prediction models based on the Markov assumption, while NLMs employ neural networks like multi-layer perceptrons (MLP) and recurrent neural networks (RNNs) to estimate the probability of word sequences. PLMs are LMs based on the transformer architecture, incorporating a selfattention mechanism. Bidirectional Encoder Representations from Transformers (BERT) [4] was introduced by pre-training bidirectional language models on large, unlabelled corpora with a specific pre-training task [26]. BERT utilises the masked language model (MLM), where some tokens in the input are randomly masked, and the model predicts the original vocabulary ID of the masked word based on contextual information alone [4]. Additionally, BERT incorporates a next-sentence prediction task to jointly pre-train representations of text pairs. This design allows BERT to be fine-tuned for various NLP tasks, achieving state-of-the-art performance. Scaling up the model and training data improves its effectiveness in downstream tasks like question-answering and language inference. These larger PLMs are often referred to as LLMs [18]. An innovative application of LLMs is ChatGPT, known for its human-level conversational capabilities [26]. This utilizes the close source LLMs such as GPT-3.5, GPT-4 and the latest model GPT-4o developed by OpenAI. Open-source LLM include models such as Llama and Mistral by Mistral AI3 .",
            "score": 0.4625635997441282,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 4907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 823,
                    "end": 826,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1163,
                    "end": 1166,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "254366666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023101806640625
        },
        {
            "corpus_id": "274655852",
            "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
            "text": "To scale our classification approach to the full dataset of 68.8 million entries, we trained a FastText model (Joulin et al., 2017) on the LLM-labeled data. FastText was selected for its efficiency with large text corpora and its ability to handle out-of-vocabulary words through n-gram representations. \n\nWe constructed a training dataset from the LLM-labeled data, which contains 90,000 samples. Using a 95:5 train-test split, we trained the FastText model which achieved 89.5% F1 score on the validation set. The trained FastText classifier processed the entire pre-filtered dataset, identifying approximately 18.8 million deduplicated entries as likely tutorials. This represents a 72.7% reduction from the pre-filtered set while maintaining high recall of genuine tutorial content.",
            "score": 0.462415206274423,
            "section_title": "FASTTEXT FILTER",
            "char_start_offset": 7850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 786
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 131,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08990478515625
        },
        {
            "corpus_id": "271745061",
            "title": "Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning",
            "text": "To determine the significance of various components in our pipeline, we perform an ablation study. We follow the same approach as \u00a7 4.3 where each condition is run with different seeds 8-times and we report the average F1-score and the standard deviation of the results in Table 5. Our baseline approach involves generating counterexamples with a fine-tuned GPT-3.5 model and applying all three filters defined in \u00a7 3.2 before using the data for active learning. In this study, we vary the generator model between the fine-tuned GPT-3.5 and the off-the-shelf GPT-4o model, and we also evaluate the impact of including or excluding each filter when using the GPT-3.5 \n\nmodel. The ablation study is conducted using one of the three datasets, YELP, with the BERT model. The results show that the counterfactual examples with all filters have 2X better performance in downstream active learning tasks compared to methods that do not employ the filters (p<0.0001). This shows the value of carefully filtering through LLM generated counterfactuls to make usable data for model training.",
            "score": 0.4612872349469642,
            "section_title": "A.3 Counterfactual Filtering Methods",
            "char_start_offset": 37614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1080
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "273993773",
            "title": "Knowledge Bases in Support of Large Language Models for Processing Web News",
            "text": "With the introduction of Transformer [50] in 2017, the NLP field has seen a rising number of efficient language models. Pre-trained Large Language Models (LLMs), like ChatGPT [46], Bard [32], Chat-Sonic [9], BERT [20], XLNet [58], RoBERTa [30], etc., have been used extensively for NLP and other tasks. Since pre-trained models sll contain huge amounts of textual information, they can handle a variety of tasks easily and often effectively by down-streaming the models for tasks at hand, especially after model fine-tuning via task-specific datasets. \n\nBidirectional Encoder Representation from Transformers (BERT) [20] has been fine-tuned for NLP tasks, such as sentiment analysis [16,57], text classification [25,43,49], etc. Upon training, BERT introduces masks to input sentences and predicts masked tokens. Due to its reliance on masks, BERT neglects dependency between the masked tokens positions [58]. XLNet [58] overcomes the limitation of BERT using its autoregressive formulation. BERT is trained modestly to achieve sound results [30]. RoBERTa ehnances BERT by training with bigger batches of more data, dynamicaly changing the masking pattern, training on longer word sequences, and removing the next sentence prediction. Although pre-trained models are capable for NLP tasks, they are not always efficient for knowledgedriven tasks due to the discrepancy of fine-tuning's specific domain and pre-training's wide domains [29].",
            "score": 0.46087848786788,
            "section_title": "Large Language Models",
            "char_start_offset": 6877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1439
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "204848225"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "239049545"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "247579720"
                },
                {
                    "start": 719,
                    "end": 722,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1434,
                    "end": 1438,
                    "matchedPaperCorpusId": "202583325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03759765625
        },
        {
            "corpus_id": "271328477",
            "title": "SETTP: Style Extraction and Tunable Inference via Dual-level Transferable Prompt Learning",
            "text": "We first conduct a comparative experiment using the classifier method.We examine the performance differences between smallscale Pretrained Language Models like BERT, RoBERTa, MacBERT, and LLMs.For small-scale PLMs, we extensively fine-tune them using datasets such as GYAFC, Yelp, Shakespeare, and Genshin.In contrast, LLMs rely on the knowledge embedded within their pretraining corpora.The experimental results are presented in Table 8.\n\nThis study presents a comparative analysis of BERT and Chat-GPT models, focusing on their classification accuracy across highresource and low-resource datasets.The results indicate that both models exhibit high accuracy levels when dealing with high-resource datasets.However, there is a notable decrease in accuracy for more low-resource styles for small-scale pre-trained language models.In contrast, the evaluation performance of LLMs like ChatGPT remains consistently high, demonstrating their robustness in handling lowresource datasets.We have designed several evaluation strategies based on ChatGPT or similar LLM, and different strategies use different prompts.The prompts below can be found in Table 14.\n\nNo-Reference Explicit Rating(NER) The core of this method relies on LLM's huge training corpus and uses prior knowledge to make judgments.By using given input texts, we guide ChatGPT to directly generate ratings, independently assessing the absolute quality of each text in specific aspects or overall.It is worth noting that if the model gives detailed reasons, the evaluation results will be more consistent with humans, so we require the model to give a specific analysis.\n\nReference-Based Explicit Rating(RER) Using given input texts, we conversationally guide ChatGPT in generating ratings directly.Specifically, we pre-design the answers of the LLM and use them as input texts.As known, LLMs provide responses based on previous conversation history.By providing preset prior answers, we can effectively guide the model in evaluation.\n\nPaired Comparison Rating(PCR) This method focuses on assessing the relative quality of texts.We employed a cross-blind casting strategy to overcome biases such as order and self-generation (where models tend to prefer their own generated answers).",
            "score": 0.45973498850788647,
            "section_title": "B.1 Comparison of Classifier Methods",
            "char_start_offset": 33575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 438
                },
                {
                    "start": 440,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1109
                },
                {
                    "start": 1109,
                    "end": 1152
                },
                {
                    "start": 1154,
                    "end": 1292
                },
                {
                    "start": 1292,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1629
                },
                {
                    "start": 1631,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 1837
                },
                {
                    "start": 1837,
                    "end": 1909
                },
                {
                    "start": 1909,
                    "end": 1993
                },
                {
                    "start": 1995,
                    "end": 2088
                },
                {
                    "start": 2088,
                    "end": 2242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053985595703125
        },
        {
            "corpus_id": "260091356",
            "title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models",
            "text": "The substantial size and inconsistent quality of many large datasets necessitates a rigorous filtering pipeline. We apply the following steps to filter training data: \n\nDe-Duplication: Duplicated entries within training data can negatively impact model performance [Hernandez et al., 2022], and potentially lead to overfitting. Consequently, we remove duplicate entries from our dataset. Considering the dataset's volume, we employ hash functions to identify and eliminate text pairs that map to duplicate hash values. We normalize whitespace and capitalization before checking for duplicates. Empty pairs and pairs with identical elements are also removed. \n\nLanguage Filtering: Since we design our embedding models for English, we use the fasttext-language-identification model 2 based on the fasttext text classification method [Joulin et al., 2017] to remove non-English training items from the dataset. \n\nConsistency Filtering: Consistency filtering means excluding training pairs with low semantic similarity. Previous studies suggest that eliminating low-similarity pairs using an auxiliary, albeit less precise, model boosts performance [Dai et al., 2023, Wang et al., 2022]. We employ the all-MiniLM-L6-v2 model 3 for consistency filter-2 fasttext-language-identification (https: //huggingface.co/facebook/fasttext-language-identification) 3 all-MiniLM-L6-v2 model (https://huggingface.co/ sentence-transformers/all-MiniLM-L6-v2) ing in this manner: We generate embeddings for 1M pairs (q i , p i ) i randomly sampled from D pairs . For every pair (q, p) \u2208 D pairs in the dataset, we verify whether p is among the top two passages most similar to q based on the cosine similarity of their embeddings compared to all passages p i , i = 1, ..., 1M. \n\nThe application of these preprocessing steps reduces the size of the dataset from over 1.5 billion mixed-quality pairs to 385 million high-quality pairs. This reduction permits us to train our model with significantly less data than typical embedding models without sacrificing embedding quality. 4",
            "score": 0.4593830737742354,
            "section_title": "Pairwise Data Preparation",
            "char_start_offset": 3487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 831,
                    "end": 852,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1145,
                    "end": 1162,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2374267578125
        },
        {
            "corpus_id": "267365346",
            "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
            "text": "In this section, we present the evaluation results of three different evaluation settings as described in the previous section as shown in Table 5. The Pair-Wise Winning Score indicates the result directly comparing with the corresponding model trained with full data. These values that are greater than 1.0 represent better responses generated by our Superfiltering models than full data models. The detailed win-tie-lose numbers are presented in Figure 2. Moreover, the performance of our models and baseline models on the Huggingface Open LLM Leaderboard and the AlpacaEval Leaderboard are also presented in Table 5 where we can see our models using 5%, 10%, 15% data outperform the models trained with full data on both benchmarks on both LLaMA2-7B and LLaMA-13B settings. These results further showcase the effectiveness of our Superfiltering. Moreover, the usefulness of Superfiltering on the high-quality Alpaca-GPT4 dataset further shows the potential of our method, which is astonishing that a weak language model like GPT-2 is able to filter the data responses generated by GPT-4. \n\nFor the human evaluation, we compare the performances between models trained with 5% Superfiltring data and full data based on LLaMA2-7B models, on Alpaca and Alpaca-GPT4 Datasets. In the comparison (1) Alpaca 5% VS. Alpaca 100%, our model wins on 50 out of 100 instruction, ties on 18, and losses on 32 instructions. In the comparison (2) Alpaca-GPT4 5% VS. Alpaca-GPT4 100%, our model wins on 49 out of 100 instruction, ties on 5, and losses on 46 instructions. This human evaluation of Superfiltering further validates our method.",
            "score": 0.45892621628347285,
            "section_title": "Main results",
            "char_start_offset": 19421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1626
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045867919921875
        },
        {
            "corpus_id": "257359737",
            "title": "A question\u2013answer generation system for an asynchronous distance learning platform",
            "text": "In this section, we will discuss the findings of all the experiments conducted in the preceding section. In experiment 1, we compared the SBERT approach to the BERT and TF-IDF approaches, which have been used in prior research to efficiently select sentences that correspond to the topics in a course. The speech filtering module uses multisource teaching materials and the sentence similarity filtering methods of SBERT, BERT, and TF-IDF to choose sentences that address the course's important concepts. In a prior study by Westermann et al. (2020), BERT was used to score the similarity of paragraphs. In this paper, the BERT model was evaluated using validation data extracted from existing datasets. This is identical to the study conducted by Lahitani et al. (2016), who examined the similarity between essay evaluations. In this study, the TF-IDF approach was used for the research model. In research employing the BERT and TF-IDF approaches, the concept of semantic relationships was not examined, providing the foundation for research undertaken to apply semantic relationships in the conducted trials. To evaluate the results of each technique, we Fig. 11 The Results of Manual Evaluation of Question Relevance employed the manual evaluation method, which consists of surveying the course's enrolled students. In this study, we employed two criteria: informativeness and questionability. The findings of the experiment indicate that the content of the sentences selected using SBERT is superior to that of the sentences selected using the previous similarity filtering method. The highest manual evaluation score for informative indicators, with a value of 5.29, is obtained using the SBERT method, while a value of 4.70 is obtained using the BERT method, and a value of 3.40 is obtained using the TF-IDF method. The highest score for the askability or questionability indicator, with a score of 5.03, is obtained using the SBERT approach, while scores of 4.42 and 3.32, are obtained for the BERT and TF-IDF approaches, respectively.\n\nMoreover, to capture a more accurate similarity value setting, the similarity setting for SBERT was also evaluated. The evaluation findings indicate that sentences can be filtered effectively when the value is set to 0.5. The highest manual evaluation scores, with values of 5.29 and 4.91, are obtained",
            "score": 0.45865027240609474,
            "section_title": "Discussion",
            "char_start_offset": 45812,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 549,
                    "matchedPaperCorpusId": "235814324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00586700439453125
        },
        {
            "corpus_id": "271769615",
            "title": "Enhanced Financial Sentiment Analysis and Trading Strategy Development Using Large Language Models",
            "text": "This study begins with the fine-tuning of pretrained language models, specifically OPT, BERT, LLAMA 3, and RoBERTa, sourced from Hugging Face, to tailor their capabilities for specialized financial analysis (Hugging Face, 2023). LLMs, originally designed for broad linguistic comprehension, require significant adaptation to perform niche tasks, such as forecasting stock returns through textual analysis. This necessity enforces the adaptation phase, where the models are recalibrated post their original training on extensive data, preparing them for specific analytical functions (Radford et al., 2018). \n\nBesides OPT, BERT, LLAMA 3 and RoBERTa, our analysis incorporates FinBERT, a variant of BERT pre-trained specifically for financial texts, and the Loughran and McDonald dictionary. Fin-BERT and the Loughran and McDonald dictionary do not necessitate the fine-tuning process because they are already tailored for financial text analysis. FinBERT leverages BERT's architecture but is fine-tuned on financial texts, providing nuanced understanding in this domain (Huang et al., 2023). The Loughran and McDonald dictionary, a specialized lexicon for financial texts, aids in traditional textual analysis without the complexity of machinelearning models (Loughran and McDonald, 2022). \n\nWe present a unique approach that integrates fine-tuning pre-trained LLMs with financial text data. This section outlines our process of adapting LLMs for the financial domain, including the steps of fine-tuning and the specific features used in our sentiment analysis. Our methodology involves the systematic adaptation of models such as OPT, BERT, FinBERT, LLAMA 3 and RoBERTa, focusing on domain-specific nuances by fine-tuning them on a comprehensive dataset of financial news. This process not only improves the models' understanding of financial sentiment but also enhances their predictive accuracy regarding stock market movements. By leveraging the advanced capabilities of LLMs and tailoring them specifically for financial text, our approach presents a robust framework for sentiment-based financial forecasting. \n\nThe use of LLMs such as OPT, BERT, FinBERT, LLAMA 3 and RoBERTa in financial sentiment analysis offers distinct advantages over traditional methods, particularly in handling the complexity and unstructured nature of financial text data.",
            "score": 0.4583926483853072,
            "section_title": "Methods",
            "char_start_offset": 4180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2353
                }
            ],
            "ref_mentions": [
                {
                    "start": 1069,
                    "end": 1089,
                    "matchedPaperCorpusId": "252666016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.032470703125
        },
        {
            "corpus_id": "271328899",
            "title": "Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction",
            "text": "The results of our method and baselines are shown in Table 2. Our results are obtained by taking the average of five different random seeds.Although our method did not achieve state-of-the-art results, we saw significant improvements over the baseline BERT model across all datasets by employing it as data augmentation.We believe achieving the performance by an extremely simple BERT-based CSC model is impressive, highlighting the effectiveness of the data filtering mechanism.Since the CSC task does not involve adding and deleting characters, most previous methods adopt non-autoregressive methods.However, we are interested in how large language models (LLMs) perform in the CSC task due to their powerful learning and generalization abilities.So we further conduct experiments on a proprietary LLM (GPT-3.5)and an open-source LLM (ChatGLM).The reason for unsatisfactory CSC performance for LLMs can be two-fold.On the one hand, they will likely give outputs of different lengths.On the other hand, they may replace some correct words according to their understanding, leading to higher recall and lower precision.\n\nOur data filtering strategy is incorporated into a BERT-based model, so we check its effects by comparing the base model.In our subsequent experiments, we use the official evaluation from SIGHAN.BERT* denotes the results from this re-evaluation.Table 3 illustrates that our filtering method achieves an all-around improvement on BERT, including lower FPR, and lower ECE.We can conclude that training on the refined corpus delivers a performant and well-calibrated CSC model, successfully mitigating over-correction.Therefore, we empirically verify the overall effectiveness of our data filtering strategy.",
            "score": 0.45794213259428956,
            "section_title": "Main Results",
            "char_start_offset": 16371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1119
                },
                {
                    "start": 1121,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.110107421875
        },
        {
            "corpus_id": "273233982",
            "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
            "text": "We collect and carefully filter a diverse range of mathematical data to ensure relevance and quality for continued pretraining of LLMs. The data includes math-related web content, synthetic data, code utilizing mathematical packages, and mathematical textbooks. \n\nMath-related Web Data. Web data offers a broad range of real-world mathematical examples. \n\nWe start with the OpenWebMath (Paster et al., 2023) dataset, which contains mathematical web pages sourced from Common Crawl. Observing that a significant portion of these documents are unrelated to mathematics, we instruct the Mixtral-8x7B-Instruct model with a carefully designed prompt (detailed in Appendix A) to filter out irrelevant texts. Examples of irrelevant texts are shown in Appendix D. This reduces the dataset from 13.7B tokens to 4.8B tokens (measured using the Llama-3 tokenizer). We call this filtered version filtered-OpenWebMath. \n\nTo further expand the dataset, we train a fastText classifier (Joulin et al., 2016) using filtered-OpenWebMath as positive samples and random Common Crawl data as negative samples (training details are explained in Appendix. B). This model helps identify additional math-related documents within the Common Crawl data from Matrix (Zhang et al., 2024), a general pretraining dataset. A second round of filtering is performed, where Mixtral-8x7B-Instruct annotates a portion of these documents, and a new fastText classifier trained based on these annotations further refines the data. This produces 6.4B tokens, which we label as filtered-CC-En-math. Finally, we combine filtered-OpenWebMath and filtered-CC-En-math, resulting in a comprehensive 11.2B-token math-related web dataset. \n\nPrompt: You will be presented with a text related to math. I need you to identify all the complex computations in it. For each complex computation, find out the conditions needed for the computation, the LaTeX expression that conducts the computation, and the result of the computation. et al., 2024).",
            "score": 0.4562171707920253,
            "section_title": "BASIC DATA",
            "char_start_offset": 6464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255859375
        },
        {
            "corpus_id": "247043089",
            "title": "A reproducible experimental survey on biomedical sentence similarity: A string-based method sets the state of the art",
            "text": "following constraints: (a) the pre-processing methods and tools used by state-of-the-art methods; and (b) the availability of resources and software tools. Figure 2 details all the possible combinations of pre-processing configurations that are evaluated in this work. String, word and sentence embedding, and ontology-based methods, are evaluated using all the available configurations except the WordPieceTokenizer [90], which is specific to BERT-based methods. Thus, BERT-based methods are evaluated CBOW WE model trained on PubMed using word2vec program M24 Newman-Griffis et al. [79] GloVe WE model trained on PubMed M25 BioConceptVec GloV e [78] GloVe We model trained on PubMed M26 BioWordVec int [80] FastText [56] WE model trained on PubMed + MeSH M27 BioWordVec ext [80] FastText [56] trained on PubMed + MeSH M28 BioNLP2016 win2 [81] FastText [56]  using different char filtering, lower casing normalization, and stop words removal configurations. We use the Pearson and Spearman correlation metrics together with their harmonic score values to determine the impact of the different pre-processing configurations on the performance of the methods evaluated herein. However, we set the best overall performing pre-processing configuration using the harmonic average scores, as well as answering the remaining research questions. \n\nMost methods receive as input the sequences of words making up the sentences to be compared. The process of splitting sentences into words can be carried out by BERT [85] trained on PubMed abstracts M44 ClinicalBERT [87] BERT [85] trained on PubMed abstracts M45 PubMedBERT [88] (abstracts) BERT [85] trained on PubMed abstracts",
            "score": 0.4555035409843299,
            "section_title": "Pre-processing methods evaluated in this work",
            "char_start_offset": 24624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "3603249"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "6219679"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "209444420"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "17458010"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1567,
                    "end": 1571,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01751708984375
        },
        {
            "corpus_id": "248426721",
            "title": "What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification",
            "text": "We pick the best performing training scenario (cross-annotation filter, 2.3) and compare fastText with contextualized embeddings: BERT and the domain-specific SciBERT. The central columns of  specific trained language models (improvement of 4.5 F1 points and almost stochastic dominance). \n\nCompared to the CNN, full transformer fine-tuning results in the best model (rightmost columns). We tested different setups to feed the input to the transformer (see appendix E), finding two entity spans and the full sentence as best setup. The full finetuned transformer model confirms the dominance of training setup (2.3) over (2.2).",
            "score": 0.45512993406068225,
            "section_title": "Contextualized Word Embeddings",
            "char_start_offset": 22680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 627
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007965087890625
        },
        {
            "corpus_id": "264451826",
            "title": "Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning",
            "text": "1. LLMs: BERT [44], RoBERTa [45] and LLaMA [46], LLaMA-2, OPT, GPT-3.5, GPT-4. \n\n2. The prompt tuning (Prompt-T) for LLMs [47,48] does not fine-tune a model on a labeled dataset for a specific task. Instead, they instead use the LLMs' existing knowledge and adjust the input prompts to get the desired output. 3. Regularization fine-tuned (RegFT) LLMs [48] employ a regularization technique to retain the pre-training weights. This approach has been proven effective in few-shot fine-tuning, as it prevents the erasure of valuable knowledge from pre-training. 4. Zero-shot prompting GPT-3, GPT-3.5 and GPT-4 via OpenAI's official API [49,7] . 5. The debiasing methods: BERT-F [50] fine-tunes the BERT with forgotten examples at each training epoch. \n\nThe forgotten examples in a model are correctly classified at some point and are misclassified in the following training. ReWeighting [51] involves training a naive model to predict based on dataset biases and another, robust model in ensemble with it, encouraging the latter to focus on more generalizable data patterns. The confidence regularization method (Reg-conf) [52] is designed to provide sufficient incentives for models to learn invariant representations from all training data. Z-filtering is a data augmentation that trains data generators to produce high-quality, label-consistent samples while removing spurious correlations. The product-of-experts (POE) [53] combines the probability distributions of the bias-only and the foundation model to make predictions based on different input characteristics. Next, the base model is trained using the cross-entropy loss of the combined classifier. The Learned-Mixin (Lmin) [51] method is a two-stage approach that involves: (1) training a naive model with prior knowledge of dataset biases and ( 2) training a robust model with the naive model in an ensemble to focus on patterns that are more likely to generalize.",
            "score": 0.4543509045162581,
            "section_title": "Compared Baselines",
            "char_start_offset": 14640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "237452662"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "237452662"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "231775891"
                },
                {
                    "start": 885,
                    "end": 889,
                    "matchedPaperCorpusId": "202539031"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "218470144"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "215191351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061981201171875
        },
        {
            "corpus_id": "270878713",
            "title": "How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise on Machine Translation",
            "text": "Data filtering is a straightforward way to remove the noise from the translation training corpus to mitigate its negative impact.There are two methods to ensure the semantic alignments between the source and target in a sentence pair: (1) surface-level filters, e.g., removing sentence pairs that differ a lot in source and target length; (2) semantic-level filters, relying on quality estimation models to score each sentence pair (Kepler et al., 2019;Rei et al., 2020;Peter et al., 2023).Other studies consider the denoising as outlier detection (Taghipour et al., 2011) or ranking problem (Cui et al., 2013).\n\nIn this paper, we mainly consider semantic-level filters: (1) Laser (Artetxe and Schwenk, 2018), a sentence alignment pre-filter tool for web-mined corpora, i.e., CCAligned and WikiMatrix; (2) Bicleaner (Zaragoza-Bernabeu et al., 2022), a classifier that predicts if a sentence is a translation of another.It is also a part of the pre-processing pipeline for generating the ParaCrawl dataset; (3) Comet (Rei et al., 2020), a widely-used quality estimation model for machine translation.\n\nAll these approaches rely on external models to select high-quality data before training.However, our approach does not require such pre-selection but focuses on model training dynamics, which can be applied more broadly.",
            "score": 0.454217841715444,
            "section_title": "Pre-Filter",
            "char_start_offset": 5643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1099
                },
                {
                    "start": 1101,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1322
                }
            ],
            "ref_mentions": [
                {
                    "start": 453,
                    "end": 470,
                    "matchedPaperCorpusId": "221819581"
                },
                {
                    "start": 470,
                    "end": 489,
                    "matchedPaperCorpusId": "265067098"
                },
                {
                    "start": 548,
                    "end": 572,
                    "matchedPaperCorpusId": "41382625"
                },
                {
                    "start": 592,
                    "end": 610,
                    "matchedPaperCorpusId": "14544096"
                },
                {
                    "start": 681,
                    "end": 708,
                    "matchedPaperCorpusId": "56895585"
                },
                {
                    "start": 1016,
                    "end": 1034,
                    "matchedPaperCorpusId": "221819581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "268296906",
            "title": "An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment",
            "text": "across models). BERTScore generally performs better across all models, though its effectiveness is not reliable enough (0.34 in 'All-DS', and 0.20-0.60 across models). Both metrics exhibit slightly higher correlation when evaluating Control-T5 (LENS: 0.34, BERT f1: 0.60) and Llama-3.2-3B (LENS: 0.15, BERT f1: 0.39) compared to GPT-4 (LENS: 0.01, BERT f1: 0.20) and Qwen2.5-72B (LENS: 0.03, BERT f1: 0.33). However, their performance declines notably after removing Newsela-derived simplification outputs. \n\nMore Challenges in Evaluating High-quality Simplification Models. To further compare the metrics' evaluations of high-and low-quality outputs across models, we incorporate visualizations (see Figure 6) after downsampling. For GPT-4 and Qwen2.5-72B, regardless of the evaluation metric used, the scores of high and low-quality simplification outputs appear to blend, revealing a lack of discriminative capability. This indicates that these metrics struggle to differentiate when the overall quality is high, making them less suitable for evaluating advanced LLMs like GPT-4 and Qwen2.5-72B. In contrast, for Llama-3.2-3B and Control-T5, high-quality sentence pairs rated by humans tend to receive higher scores from both metrics, with LENS showing a clearer alignment. However, low-quality sentence pairs rated by humans exhibit a wider range of scores, showing the metrics' limitations in capturing quality variations.",
            "score": 0.45303329895697436,
            "section_title": "Automatic Metrics",
            "char_start_offset": 44638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06658935546875
        },
        {
            "corpus_id": "270226640",
            "title": "Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark",
            "text": "\u2022 Loss function: the sum of the mean MLM likelihood and the mean NSP likelihood [28].Following the success of BERT, several BERT-based models have been introduced, such as Robustly Optimized BERT Pretraining Approach (RoBERTa) [54], Distilled version of BERT (DistilBERT) [55], and A Lite BERT (ALBERT) [56], each offering unique enhancements and optimizations while maintaining the core bidirectional approach of the original BERT model.One of the limitations of the BERT network structure is that no independent sentence embeddings are computed, which makes it difficult to use for various pair regression tasks due to large number of combinations.To allow for more efficient sentence-level embeddings, Sentence-BERT (SBERT) introduces the siamese and triplet network structures to generate highly effective semantically meaningful sentence embeddings that can be compared with cosine similarity, which has served as a cornerstone for further research [3], [44].Another cornerstone work is Simple Contrastive Learning of Sentence Embeddings (SimCSE) [57] using unsupervised and supervised contrastive learning, which is widely adopted by recent state of the art text embeddings.\n\nb) Large Language Models: The widespread use of ChatGPT has showcased the impressive abilities of Large Language Models (LLMs) in following instructions, in-context learning with minimal few-shot examples and amazing conversation abilities with humans.While some of the best performing LLMs like GPT-4 [58] are proprietary with limited technical information available, some open-source LLM models like LLaMA-2 [59], LLaMA-3 [60] and Mistral [61] have made some notable efforts to catch up [16].One advantage of using LLMs for text embedding is that they are extensively pre-trained on web-scale data already, which does not need the contrastive pre-training step used in existing state of the art text embedding models.At present, the foundation for the majority of LLMs is the Transformer architecture, which employs layers of multi-head attention in a very deep neural network.",
            "score": 0.45214208630595165,
            "section_title": "B. Background",
            "char_start_offset": 11580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1180
                },
                {
                    "start": 1182,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1901
                },
                {
                    "start": 1901,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "258832362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0360107421875
        },
        {
            "corpus_id": "277955900",
            "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
            "text": "As the scale of training corpora continues to grow and data-centric AI evolves, there is an increasing need for systematic approaches to select highquality pre-training data. This need has spurred the development of comprehensive pre-training data processing pipelines (Penedo et al., 2023;He et al., 2023He et al., , 2024a)), and data selection methods. Existing pre-training data selection methods can be categorized into three primary approaches: natural language quality-based methods, diversity-based methods, and classifier-based methods. \n\nFor natural language quality-based methods, Gopher (Rae et al., 2021) and RedPajama (Weber et al., 2024) propose empirical rules like controlling the ratio of word and number token in texts to improve language modeling. Additionally, previous works (Muennighoff et al., 2024;Wenzek et al., 2020) have shown that selecting data with perplexity (PPL) scores on validation datasets can lead to superior performance on downstream tasks compared to using the entire dataset. Another notable method is DSIR (Xie et al., 2023b), which streamlines the selection process by employing hashed N-gram features (named as data importance scores) to efficiently identify high-quality data within large datasets. Meanwhile, another line of works utilize clustering (Zhang et al., 2025) or deduplication (Abbas et al., 2023;He et al., 2024b) to enhance diversity of pre-training datasets. \n\nMore recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data.",
            "score": 0.4517848867445222,
            "section_title": "Related Work",
            "char_start_offset": 4197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 822,
                    "end": 842,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1048,
                    "end": 1067,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 1334,
                    "end": 1354,
                    "matchedPaperCorpusId": "257557221"
                },
                {
                    "start": 1696,
                    "end": 1717,
                    "matchedPaperCorpusId": "267681974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "261530723",
            "title": "Data-Juicer: A One-Stop Data Processing System for Large Language Models",
            "text": "After tokenization and pre-training, LLMs have succeeded in stimulating a wide range of advanced capabilities. The LLM pre-training data generally includes various types derived from the web crawlers [26,71], dialogues or social media [107], book-length formal texts [36,110], rigorous encyclopedias and academic texts [31,100], structured coding texts [18,57], and more texts from financial, medical and legal domains [58,91,104]. A challenge is nonetheless posed in the careful processing and formulation of pre-training data to filter noise, redundancy, irrelevance, and potentially toxic [33,62]. \n\nFine-tuning Data. Numerous studies have underscored that fine-tuning -the process of refining pre-trained LLMs using a smaller, task-specific dataset -can further enhance or unlock additional capabilities of LLMs [40,53,97,98]. Crucially, this process also paves the way for better aligning the behavior of these advanced models with human values and preferences [60,68]. \n\nIn this phase, though the data volume decreases exponentially compared to the pre-training phase, the format of fine-tuning data is quite different [73]. Typically, given a textual dataset {( 1 ,  1 ,  1 ), ..., (  ,   ,   ), ..., (  ,   ,   )}, the goal of fine-tuning is to adjust the pre-trained LLM  0 to find  * that maximizes the likelihood of the task-oriented response   for the user query   : \n\nHere   stands for task-specific instructions, such as \"summarize the following text: \", optionally accompanied by a few demonstrative samples for in-context learning [9]. The fine-tuning data can be broadly categorized into two types: Instruct Fine-Tuning (IFT) datasets to enhance the instruction-following abilities of LLMs and are usually adapted from existing NLP benchmarks [4,61]; and Chat Fine-Tuning (CFT) datasets for enhanced dialog ability and human value alignment [70,92]. There are preliminary explorations emphasizing the importance of data diversity over volume for fine-tuning data [20,95].",
            "score": 0.45150439958966176,
            "section_title": "BACKGROUND AND RELATED WORKS 2.1 Large Language Model (LLM) Data",
            "char_start_offset": 9174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 596,
                    "end": 599,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1760,
                    "end": 1763,
                    "matchedPaperCorpusId": "246485605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.398681640625
        },
        {
            "corpus_id": "269187631",
            "title": "Fewer Truncations Improve Language Modeling",
            "text": "Pre-training Data Pre-training data is pivotal to the quality of language models.There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2020b), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2022;Marion et al., 2023;Chen et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023a;Raffel et al., 2020a)) propose various filtering strategies to improve data quality.Our work broadly applies on top of these pre-training datasets.\n\nData grouping in language model training Recent transformer language models have adopted different strategies to group training data into batched sequences in order to tackle the variable document length problem.For encoder-only models, the choice of data formatting was first studied in RoBERTa (Liu et al., 2019), which shows that concatenating sentences from more than one documents in the same training sequence results in very little performance degradation.Krell et al. (2021) proposed an approximation-based combinatorial packing method to accelerate BERT training (Devlin et al., 2019), yet without improving downstream performance.It is worth mentioning that document truncation is less of a concern for encoder models for two reasons: first, they are usually trained on relatively short text spans of 128-512 tokens that only respect sentence boundary.In such case, document-wise truncation is inevitable given the limited context size.Second, they are not intended for openended generation, and thus, hallucination is not an issue.",
            "score": 0.45035597502185076,
            "section_title": "Related Work",
            "char_start_offset": 23240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 354
                },
                {
                    "start": 354,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 642
                },
                {
                    "start": 644,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 206,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 394,
                    "end": 412,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 450,
                    "end": 473,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 495,
                    "end": 516,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1216,
                    "end": 1236,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "272708263",
            "title": "Multi-Document Grounded Multi-Turn Synthetic Dialog Generation",
            "text": "We evaluate the model output qualities with 4 automatic metrics: (1) F1, standard metric used in Choi et al. (2018), Reddy et al. (2019), Feng et al. (2021), (2) RougeL, (3) Bert-Recall, (4) Recall, the metric reported to achieve the highest correlation with human evaluation for answer correctness of instruction following models in question answering (Adlakha et al., 2023). We study the model performance in four settings: (i) zero-shot, (ii) human-generated benchmark training data (human benchmark), (iii) synthetic data with unanswerable queries and without the LLM-as-a-Judge Table 4: Testset performances on answerable queries. Comparisons are among the models fine-tuned on human annotated benchmark training data vs. synthetic (without LLM-as-a-Judge) vs. synthetic-filtered (with LLM-asa-Judge) data. The highest scores in each condition are bold-faced. Models fine-tuned only on the synthetic data out-perform those fine-tuned on the human benchmark data. The only exception is the CoQA data set scored with short answer as reference, for which the models trained on the synthetic data under-perform those trained on the human benchmark data except for Recall. However, we show in \u00a73.2 that humans overwhelmingly prefer outputs generated by the models fine-tuned on the synthetic data to those by the models trained on the human benchmark data. Synthetic-filtered is particularly effective for OR-QuAC, indicating that synthetic dialogs generated from retrieved passages are noisier than those generated from human generated single document and therefore, post-filtering improves the model accuracy by a significant margin. \n\nfilter (synthetic), (iv) synthetic data without unanswerable queries and with the LLM-as-a-Judge filter (synthetic-filtered). We report the performance of the teacher model under zero-shot settings. \n\nTable 4 shows the results on the subset of the test sets with answerable queries.",
            "score": 0.45022724535236686,
            "section_title": "Automatic Evaluations",
            "char_start_offset": 14252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "52057510"
                },
                {
                    "start": 117,
                    "end": 135,
                    "matchedPaperCorpusId": "52055325"
                },
                {
                    "start": 135,
                    "end": 156,
                    "matchedPaperCorpusId": "237940558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0460205078125
        },
        {
            "corpus_id": "218630023",
            "title": "APPCorp: a corpus for Android privacy policy document structure analysis",
            "text": "To evaluate whether the BiLSTM structure improves the classification performance on the same sentence encoder, we plot the F1-score (of BERT + P-Attn and BERT + P-BiLSTM) against number of sentences, and the results are shown in Figure 5. We can observe that the F-score of BERT + P-BiLSTM and BERT + P-Attn shows similar trend with the increase of sentence number and in general BERT + P-BiLSTM has advantage on paragraphs with multiple sentences compared to BERT + P-Attn. Both models show best F-score with sentence number equals to 4, and worst performance with sentence number equals to 6. The testing dataset for sentence number equals to 6 is around 10 per fold. Therefore, the results are relatively sensitive to the number of wrongly classified data. The wrongly classified paragraphs may have some entailment of the mis-classified label. For paragraphs with sentence number to be 4, the categories are among those that have high F-score, e.g., First Party Collection and Use, Specific Audiences and Policy Changes. BERT + P-Attn performs slightly better (0.50%) than BERT + P-BiLSTM for paragraphs with 2 sentences. By zooming into the details of the wrongly classified data, we find that most of them may contain descriptions that is related to the other labels. For instance, some of the descriptions on First Party Collection and Use may contain a few words on Third Party Share and Collection and the \"party\" is not clearly stated. BERT + P-BiLSTM may learn rich representation of the paragraph and capture those features. However, the softmax value for the true label is vary close to the (wrongly) predicted label by BERT + P-BiLSTM.",
            "score": 0.4491124694638523,
            "section_title": "Analysis",
            "char_start_offset": 24272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1649
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029468536376953125
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Up-sampling is often employed when pre-training data is partly limited, such as data from a high-quality domain like Wikipedia or text in a rare language for training multilingual LLMs [60,82]. Hernandez et al. [40] study up-sampling of data subsets and find that repeating only 0.1% of training data 100 times significantly degrades performance. In contrast, our work focuses on repeating the entire pre-training corpus for multiple epochs rather than up-sampling parts of it. \n\nAlternative data strategies Large pre-training datasets are commonly filtered to remove undesired samples or reduce noise [101]. Perplexity-based filtering, whereby a trained model is used to filter out samples with high perplexity, has been found beneficial to reduce noise in web-crawled datasets [121]. Mixing of data is employed for the pre-training data of multilingual LLMs, where text data from different languages is combined [23,126,100,74]. However, both for code and natural language models, mixing different (programming) languages has been reported to under-perform monolingual models [80,113]. Some work has investigated mixing code and natural language data for prediction tasks, such as summarizing code snippets [44] or predicting function names [4]. Several pre-training datasets for LLMs include low amounts of code data [31,89,95]. However, these past works generally do not provide any ablation on the drawbacks of including code or the benefits for natural language task performance. We perform a detailed benchmarking of mixing Python and natural language in LLM pre-training at 10 different mixing rates.",
            "score": 0.44848372678403,
            "section_title": "Related Work",
            "char_start_offset": 22872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1608
                }
            ],
            "ref_mentions": [
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "8820379"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "matchedPaperCorpusId": "9279336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33251953125
        },
        {
            "corpus_id": "265658802",
            "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
            "text": "Auditing LLM Outputs. Prior work on detecting anomalies such as stereotypes, toxicity, or hallucinations in LLM models has concentrated on analyzing the model's generated content such as the percentage of anomalous options preferred or chosen [29,41]. Other work has explored the propagation of bias to downstream tasks, including coreference resolution [58], sentiment analysis [42], topic modeling [20], and prediction models [18]. However, the effectiveness of these approaches is heavily reliant on the quality of pre-trained downstream models. A different line of work has examined bias in the activations of LLMs, using principal component analysis [4,32,58], clustering [4], or training detection classifiers on the latent space [3,4,8,22]. Other work has studied distance metrics between word pair representations [4,8]. However, this approach has shown inconsistency detection results within contextual scenarios [22,29,35]. Furthermore, these approaches assume the availability of fully labeled training data and require predefined anomalous patterns. Few prior work has addressed the identification of unknown biases in LLMs, particularly in the context of unbiased sentence classification [52]. In this work, our goal is to detect whether an LLM encodes anomalies (e.g., hallucinations) within its hidden states. We work under the assumption that only \"normal\" (e.g., true) data is available, while the presence of anomalous (e.g., false) data remains undisclosed.",
            "score": 0.4470266649369051,
            "section_title": "Related Work",
            "char_start_offset": 3464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1476
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "260704383"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 655,
                    "end": 658,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 658,
                    "end": 661,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 677,
                    "end": 680,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 739,
                    "end": 741,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 741,
                    "end": 743,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 743,
                    "end": 746,
                    "matchedPaperCorpusId": "248118886"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 825,
                    "end": 827,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "248118886"
                },
                {
                    "start": 926,
                    "end": 929,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "221949175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "268536921",
            "title": "Harnessing Large Language Models for Text-Rich Sequential Recommendation",
            "text": "Large Language Models are advanced linguistic models consisting of neural networks ranging from tens of millions to trillions of parameters, trained substantially on vast volumes of untagged texts using methods like self-supervised or semi-supervised learning approaches [20,42].The foundation for these LLMs is the Transformer [32] structure, which stands as a cornerstone in the field of deep learning for Natural Language Processing (NLP).Typically, LLMs can be classified into two different types, respectively discriminative LLMs and generative LLMs.For discriminative LLMs, BERT [15] introduces a bidirectional transformer architecture and establishes the concept of the Masked Language Model (MLM) for model pre-training.XLNet [40] incorporates sequence order permutations, facilitating comprehension of word contexts within their surrounding lexical environment.For the generative LLMs, GPT [26] first proposes to pre-train the model by predicting the next word in a sentence.InstructGPT [21] further proposes Reinforcement Learning from Human Feedback (RLHF) for fine-tuning.Llama and Llama-2 [30,31] are two famous collections of LLMs ranging in scale from 7 billion to 70 billion parameters.In this paper, we select the Llama models as the summarizer and recommender.\n\nRecently, several studies have focused on how to extend the input length limitations of existing LLMs [39].However, the challenges of increased computational overhead and performance degradation remain unresolved.Through the user preference summarization method proposed in this paper, we can handle theoretically infinite user behavior sequences and significantly reduce the training overhead of downstream recommendation models.",
            "score": 0.44688707862827315,
            "section_title": "Large Language Models",
            "char_start_offset": 6869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1278
                },
                {
                    "start": 1280,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 734,
                    "end": 738,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04168701171875
        },
        {
            "corpus_id": "268697055",
            "title": "Finetuning LLMs for Automatic Concept to TTI Prompt Generation (Student Abstract)",
            "text": "The task of processing text is done by a variety of current models which we considered such as BERT (Devlin et al. 2019) and GPT2 (Radford et al. 2019). \n\nAlthough LLMs are not trained for any specific task, they can be specialized through finetuning. \n\nBy comparing existing text generation LLMs on the bases of complexity, accessibility for finetuning, and computational cost, we decided that GPT2 would be most beneficial. We used the 400m LAION dataset (Schuhmann et al. 2021) to finetune GPT2. This dataset is sufficiently large for finetuning, at 400 million caption-image pairs, and was used to train Stable Diffusion. The LAION dataset was filtered to remove explicit images, however it was not filtered to remove explicit text. We performed explicit text filtering using a blacklist of 835 words in addition to basic preprocessing steps, e.g., removing empty strings. \n\nWe chose the largest model of GPT2, gpt2-xl, as it produced the highest quality prompts in our initial experiments. We froze the first 42 layers and trained the last 6. We found that freezing fewer layers would lead to incoherent generations and freezing more layers would not. \n\nAnalysis of the initial output revealed that the output resembled the middle of a caption rather than one complete caption. Thus, we finetuned the model on a modified version of the dataset that added starting and ending tags to each caption to signify that these captions are complete. Both models were trained on the first 450,000 captions in the dataset with 10% of the input being used for evaluation.",
            "score": 0.4468192253306602,
            "section_title": "Method",
            "char_start_offset": 2488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1564
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 119,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 130,
                    "end": 151,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 457,
                    "end": 479,
                    "matchedPaperCorpusId": "241033103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01971435546875
        },
        {
            "corpus_id": "27849960",
            "title": "Low-Resource Neural Headline Generation",
            "text": "Pre-training the decoder as a LM seems natural, since it is essentially a conditional LM. During NHG model training the decoder is fed only headline words, which is relatively little data compared to the document contents. To improve the quality of the headlines it is essential to have high quality embeddings that are a good semantic representation of the input words and to have a well trained recurrent and output layer to predict sensible words that make up coherent sentences. When it comes to statistical models, the simplest way to improve the quality of the parameters is to train the model on more data, but it also has to be the right kind of data (Moore and Lewis, 2010). \n\nTo increase the amount of suitable training data for the decoder we use LM pre-training on filtered sentences of the training set documents. For filtering we use the XenC tool by Rousseau (2013) with the cross-entropy difference filtering (Moore and Lewis, 2010). In our case the indomain data is training set headlines, out-domain data is the sentences from training set documents, and the best cut-off point is evaluated on validation set headlines. The careful selection of sentences is mostly motivated by preventing the pre-trained decoder from deviating too much from Headlinese, but it also reduces training time. \n\nBefore pre-training we initialize the input and output embeddings of the LM for words that are common in both encoder and decoder vocabulary with the corresponding pre-trained encoder embeddings. We train the LM on the selected sentences until perplexity on the validation set headlines stops improving and then use it to initialize the decoder parameters of the NHG model (highlighted with dotted background in Figure 1). \n\nA similar approach, without data selection and embedding initialization, has also been used by Alifimoff (2015).",
            "score": 0.4460529019910757,
            "section_title": "Decoder Pre-Training",
            "char_start_offset": 5768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1846
                }
            ],
            "ref_mentions": [
                {
                    "start": 670,
                    "end": 682,
                    "matchedPaperCorpusId": "8170227"
                },
                {
                    "start": 936,
                    "end": 948,
                    "matchedPaperCorpusId": "8170227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0129241943359375
        },
        {
            "corpus_id": "271039028",
            "title": "A Survey of Data Synthesis Approaches",
            "text": "When utilizing the LLMs' in-context learning capability to generate synthetic data, there is often a risk of encountering copy-paste behavior or generating data that closely resembles the in-context examples.Maintaining the distribution consistency or focusing on domain shift also benefits from some postprocessing.Focusing on data distribution when filtering ensures the effectiveness of data augmentation.Wang et al. (2023) employed Rouge-L to compare the generated instruction-following data with the instruction-following data in the task pool.They filtered out data with excessively high similarity to ensure the diversity of synthetic data within the task pool.Yu and Zhang (2024); Gao et al. (2020) filter and evaluate synthetic data by NLP metrics, utilizing BLEU scores to assess semantic relevance and diversity scores to evaluate surface form variation.The filtering method Yang et al. (2020) proposed, named G-DAUGc-Influence, removes detrimental synthetic data by analyzing their influence on validation loss.Shakeri et al. (2020) address the task of QA and critique the efficiency of previous filtering methods, which predominantly relied on pre-trained QA models for selection.To enhance efficiency, they propose a novel filtering strategy that utilizes a Language Model score, based on the relevance between the answer, context, and question, as a metric to filter and select generated data.Suhaeni and Yong (2023) directly addresses issues of class imbalance by enriching the dataset with diverse and novel synthetic reviews.To ensure the dataset to be diverse, they filtered out data by similarity score.\n\nIn Liu et al. (2022b) work, they filtered the generated examples to keep the most ambiguous ones based on the model.Wang et al. (2023) employed Rouge-L to compare the generated instructionfollowing data with the instruction-following data in the task pool.They filtered out data with excessively high similarity to ensure the diversity of synthetic data within the task pool.Lee et al. (2022a) utilized CLIP to compute the similarity between dialogue turns and images when generating synthetic visual dialogue, ensuring text-image alignment.The study by Thakur et al. (2021) introduces a sampling strategy that encompasses multiple methods, with BM25 Sampling being identified as the most efficient.",
            "score": 0.4456228711102051,
            "section_title": "Data Distribution",
            "char_start_offset": 27236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1623
                },
                {
                    "start": 1625,
                    "end": 1741
                },
                {
                    "start": 1741,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2000
                },
                {
                    "start": 2000,
                    "end": 2166
                },
                {
                    "start": 2166,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 426,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 668,
                    "end": 687,
                    "matchedPaperCorpusId": "266272567"
                },
                {
                    "start": 886,
                    "end": 904,
                    "matchedPaperCorpusId": "250390686"
                },
                {
                    "start": 1408,
                    "end": 1431,
                    "matchedPaperCorpusId": "261388283"
                },
                {
                    "start": 1741,
                    "end": 1759,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23095703125
        },
        {
            "corpus_id": "269502145",
            "title": "Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities",
            "text": "As mentioned in previous sections, deficiencies in training datasets can lead to the generation of hallucinations by the model.Consequently, enhancing the quality of datasets to mitigate hallucinations is a viable approach.More specifically, this can involve the refinement of pre-training datasets as well as SFT datasets.\n\nPre-training Data.Zhou et al. [306] proposed that the majority of knowledge possessed by LLMs is acquired during the pre-training phase.It is evident that a high-quality pre-training process is inextricable from high-quality pre-training datasets.Given the ever-increasing size of pre-training datasets currently used by LLMs (for example, the pre-training datasets for Llama2 amount to 2 trillion tokens), the challenge and importance of enhancing dataset quality are escalating.The training team for GPT-3, Brown et al. [17], improved the quality of the training dataset through a three-step process: (1) filtering the initial training dataset by referencing a compendium of high-quality corpora and employing similarity-based methods, (2) implementing fuzzy deduplication at the document level to prevent redundancy, and (3) augmenting the pre-training dataset with known high-quality training data corpora.The training team for Llama2, Touvron et al. [237], proposed the use of upsampling from higher-quality data sources (such as Wikipedia) to construct pre-training corpora.The training team for Falcon, Penedo et al. [182], extracted data from the internet by establishing a set of heuristic rules.The training team for Phi-1.5, Li et al. [130], compiled approximately 20 billion tokens of synthetic, textbook-like data to enable the model to learn common sense reasoning and general knowledge.Experimental results suggest that this model, with only 1.3 billion parameters, outperforms larger models on many benchmarks, underscoring the significance of high-quality pre-training datasets for the capabilities of LLMs.\n\nSFT Data.",
            "score": 0.4455791588514397,
            "section_title": "Data.",
            "char_start_offset": 52498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 323
                },
                {
                    "start": 325,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1949
                },
                {
                    "start": 1951,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 360,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39794921875
        },
        {
            "corpus_id": "260704205",
            "title": "Improving Requirements Completeness: Automated Assistance through Large Language Models",
            "text": "The predictions from Step 3 are noisy (i.e., have many false positives). To reduce the noise, we subject the predictions to a pre-trained ML-based filter. The most accurate ML algorithm for this purpose is selected empirically (see RQ3 in Section 5). The selected algorithm is trained on the development and training portion of our dataset (P 1 in Table 2, as we discuss in Section 5). Due to our features in Table 1 being generic and normalized, the resulting ML model can be used as-is over unseen documents without re-training (see RQ4 in Section 5 for evaluation of effectiveness). The output of this step is the list of BERT predictions that are classified as \"relevant\" by our filter; duplicates are excluded from the final results.",
            "score": 0.4451295154428053,
            "section_title": "Filtering Noise from Predictions",
            "char_start_offset": 35288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05340576171875
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "In this paper, we construct a higher-quality Ultra-FineWeb dataset (including English data Ultra-FineWeben, approximately 1T tokens, and Chinese data Ultra-FineWeb-zh, approximately 120B tokens, totaling approximately 1.1T tokens). This dataset is based on the FineWeb and Chinese FineWeb datasets, utilizing our proposed efficient data filtering pipeline. Through rigorous experimental evaluations, we demonstrate that Ultra-FineWeb-en and Ultra-FineWeb-zh outperform FineWeb-edu and Chinese FineWeb-edu-v2 when used for small-scale model training from scratch. Additionally, we show the effectiveness of the high-quality data filtered by our classifier on the DCLM-Pool and MAP-CC datasets, further confirming the reliability and effectiveness of our proposed pipeline. These results indicate that classifiers based on our efficient data filtering pipeline can select higher-quality data with reduced computational cost, thereby improving model training performance. We provide a detailed description of the implementation of our efficient data filtering pipeline, especially the efficient verification strategy driven by classifiers in the pipeline. This strategy enables reliable assessment of training data impact on LLM performance while maintaining minimal computational requirements. Furthermore, we present detailed methodologies for classifier seed data selection, training recipes, and FastText model training configuration, ensuring experimental reproducibility and result transparency. This study aims to provide novel insights and methodologies for high-quality data filtering, offering valuable references for data quality optimization in future LLM training processes, and contributing to the further development of LLMs.",
            "score": 0.4450163698234618,
            "section_title": "Conclusion",
            "char_start_offset": 28827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1737
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46337890625
        },
        {
            "corpus_id": "273949938",
            "title": "EUREKHA: Enhancing User Representation for Key Hackers Identification in Underground Forums",
            "text": "In [13], it has been observed that when models are trained on smaller datasets, they become more sensitive to hyperparameter choices, which increases the risk of overfitting. To address this issue, we conducted a grid search over a set of pre-defined hyperparameters recommended by the BERT authors, including batch size [16,24], learning rate [1e-5, 5e-5], and epochs [1,5], to find the best settings. Each experiment was conducted five times with shuffled data, and the results were averaged for consistency. We evaluated four RoBERTa [32], ALBERT [22], and XLNet [66] using the R3 user sequence representation, which was divided into training (60%), validation (20%), and testing (20%) sets. Fig. 5 illustrates the optimal hyperparameter combinations for each LLM model, evaluated based on the F1-score, which was prioritized due to the imbalanced nature of the dataset. Moreover, Table V illustrates that LLM hyperparameter tuning achieved the best performance results, outperforming the current state-of-the-art methods, as shown in Fig. 6. \n\nFurthermore, Table VI presents a detailed comparison of these LLMs. Fine-tuned LLMs consistently outperform stateof-the-art GNN-based methods. BERT, in particular, leads the group with an accuracy of 95% and an F1-score of 84%, highlighting the effectiveness of the EUREKHA framework. \n\nXLNet required the longest training time (170 minutes), significantly more than RoBERTa and BERT, which were completed in 29-30 minutes. This extended training time is primarily due to the number of epochs needed for XLNet to converge effectively. XLNet, despite its advanced capabilities and strong performance in text-rich tasks, relies on a complex permutation-based architecture. \n\nThe strong performance of these models in text-heavy tasks, without relying on graph structures, underscores their efficiency in identifying key hackers in underground forums.",
            "score": 0.4439155589692265,
            "section_title": "E. LLMs Hyperparameter Tuning",
            "char_start_offset": 30983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "256791847"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "236580625"
                },
                {
                    "start": 372,
                    "end": 374,
                    "matchedPaperCorpusId": "201848274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0292510986328125
        },
        {
            "corpus_id": "270040522",
            "title": "Next-Generation Spam Filtering: Comparative Fine-Tuning of LLMs, NLPs, and CNN Models for Email Spam Classification",
            "text": "Q1: Which of the fine-tuned models-GPT-4, BERT, RoBERTa, or CNN-exhibits the most effective predictive abilities in email spam detection? \u2022 Q2: Why is fine-tuning LLMs and NLP models essential for tasks specific to particular domains? \u2022 Q3: Which of the fine-tuned models exhibits superior generalization abilities following cross-dataset evaluation? \u2022 Q4: What is the significance of prior-training data analysis, and how can plots provide valuable insights into the effectiveness of models' predictions? \u2022 Q5: Are LLMs, specifically the GPT-4 model, efficient tools for spam detection, and how can they be used to enhance existing spam filter technologies? \n\nTo achieve our study's objectives, we conduct a comprehensive literature review on spam filtering in Section 2. Section 3 outlines the materials and methodology employed in this study, while Section 4 presents the prediction results for the models and their efficacy during cross-dataset evaluation. Section 5 delves into the discussion of the results, extracting valuable insights based on the research findings.",
            "score": 0.4437285207716028,
            "section_title": "\u2022",
            "char_start_offset": 3211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036163330078125
        },
        {
            "corpus_id": "273022742",
            "title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction",
            "text": "Motivations and Research questions Pretrained LLMs (et al, 2023a;Jiang et al., 2023) are known for containing rich knowledge with billions of parameters, which have achieved impressive results in auto-regressive text generation tasks. These models have also been extensively examined in classification-based problems (Zhao et al., 2021;Wei et al., 2023). However, these models often do not outperform discriminative encoder models such as BERT because their original generationfocused mechanism, which generates answers over a large vocabulary, may not capture task-specific patterns as efficiently as label-supervised BERT models. To address this drawback, recent work (Li et al., 2023) proposed directly extracting latent representations from the final LLaMA decoder layer and mapping them into the label space through feed-forward layers. Specifically, the LLM heads, which have been found ineffective, are removed and replaced by a classification head trained from scratch using CrossEntropy loss. This approach has shown promising results. However, exploration in the area of Continual Learning, specifically Fewshot Continual Relation Extraction (FCRE), has not yet been thoroughly investigated. Therefore, in this work, we conduct extensive experiments to answer the following research questions (RQs): \n\n\u2022 RQ1: How the performance would LLMs yield in FCRE tasks? Will it yield significantly better results compared to conventional BERT-based models? How will the limited data in the FCRE scenario impact the generalization of this model class? It would be interesting to examine the behavior of an LLM, which contains rich prior knowledge in the context of the FCRE problem, where each task only has very little data, and the model will usually be forgotten and severely overfit. \n\n\u2022 RQ2: Our study also aims to assess the effectiveness of employing our MIM strategy for LLMs, particularly in addressing the challenges of forgetting prevention and overfitting reduction. Does using LLM heads according to our strategy eliminate the prejudice about the unsuitability of LLMs in classificationbased problems, specifically FCRE? \n\nHow to adapt BERT-based FCRE methods to LLMs? Because current FCRE methods are used for BERT-based backbones, which are \"encoderonly\" language models.",
            "score": 0.44251623824901765,
            "section_title": "Exploiting LLMs for FCRE",
            "char_start_offset": 10690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2133
                },
                {
                    "start": 2136,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 336,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "270824909",
            "title": "The impact of large language models on higher education: exploring the connection between AI and Education 4.0",
            "text": "Large language models represent a crucial advancement in GAI (Kaur et al., 2022).The effectiveness of LLMs is based on their training with extensive textual datasets, enabling them to understand and generate human language in a coherent and contextualized manner (Wang et al., 2023;Whitehouse et al., 2023).Capable of performing a wide variety of linguistic tasks, LLMs stand out as versatile and powerful tools for applications requiring a deep understanding of language (Gan et al., 2023).\n\nThese models began to gain prominence in 2017 with the emergence of a range of transformer-based machine-learning models that allow for better performance in language-based tasks compared to previous machine-learning techniques (Perkins, 2023).Google introduced BERT (Bidirectional Encoder Representations from Transformers) in 2018, which used bidirectional training in natural language processing to improve Google searches' relevance of search results and suggestions (Devlin et al., 2019).This LLM can be finetuned with additional task-specific data to perform a wide range of Natural Language Processing tasks, learning complex relationships between words, and better understanding the structure of language (Martin et al., 2023).\n\nIn 2019, OpenAI released the Generative Pre-trained Transformer 2 (GPT-2), and in 2020, the Generative Pre-trained Transformer 3 (GPT-3) (Dale and Viethen, 2021).This LLM is described as an AI-powered writing assistant and is one of the most developed and widely known LLMs, noted for its ability to generate coherent and relevant content (Neumann et al., 2023).In the medical field, various LLMs such as BioBERT, PubMedBERT, and ClinicalBERT were developed from BERT, or Med-PaLM, fine-tuned from PaLM.Similarly, ChatDoctor or Hua Tuo are based on Meta's LLaMa (Huang et al., 2023b).",
            "score": 0.44194861009370795,
            "section_title": "Large language models",
            "char_start_offset": 8900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 491
                },
                {
                    "start": 493,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1228
                },
                {
                    "start": 1230,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 80,
                    "matchedPaperCorpusId": "266926128"
                },
                {
                    "start": 472,
                    "end": 490,
                    "matchedPaperCorpusId": "265352038"
                },
                {
                    "start": 964,
                    "end": 985,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1206,
                    "end": 1227,
                    "matchedPaperCorpusId": "262096275"
                },
                {
                    "start": 1569,
                    "end": 1591,
                    "matchedPaperCorpusId": "257166266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01490020751953125
        },
        {
            "corpus_id": "271244753",
            "title": "ClaimCompare: A Data Pipeline for Evaluation of Novelty Destroying Patent Pairs",
            "text": "In order to assess the effectiveness of our dataset in training LLMs to assist with novelty determination, we test whether fine-tuning these models outperforms a baseline pre-trained BERT-based transformer model.",
            "score": 0.4415654502234896,
            "section_title": "Experimental Setup and Results",
            "char_start_offset": 11299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03802490234375
        },
        {
            "corpus_id": "276776523",
            "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution",
            "text": "A.1 Quality Control Due to extensive pre-training on massive corpora, LLMs encapsulate substantial world knowledge within their parameters (Yu et al., 2023;Wang et al., 2023). As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control. Given an input sequence x, we denote the (length-normalized) negative log-likelihood (NLL) loss in generating the output sequence y with m tokens as follows: \n\nFor each candidate training instance {(q, A \u227aa , a, a \u2032 )}, we compare the NLL loss reduction ratio in generating a candidate with and without input passages to select valid negative sentences: \n\nwhere [ * ] denotes the concatenation. The motivation is to ensure that the selected negative samples a \u2032 remain less faithful to the input question than the positive answer a, even when evidence passages are provided. This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4. For each training item, we use model \u03b8 0 to sample six negative sentence candidates, as described in \u00a72.2.1. After filtering via Eq. 4, we retain at most two of the most negative sentences, ranked by the NLL loss reduction ratio, for filtered setting (some items may contain only one valid negative sentence after filtering). In contrast, the non-filtered setting uses two randomly selected sentences from the six candidates. As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics.",
            "score": 0.441288060021433,
            "section_title": "A Method Details",
            "char_start_offset": 30354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "273022916",
            "title": "End-to-End Speech Recognition with Pre-trained Masked Language Model",
            "text": "In Table VI, we compare the BECTRA results obtained from using various pre-trained masked LMs other than BERT BASE . RoBERTa BASE 5 is an extension of BERT that is constructed with an improved pre-training procedure [80]. However, there was little improvement over the results using vanilla BERT BASE . BECTRA greatly benefited from increasing the capacity of a pre-trained LM, with BERT LARGE 6 achieving the best overall performance. \n\nBECTRA incurs a high computational cost, especially during inference, primarily due to the multiple forward passes in BERT (i.e., K = 10 times) with the O(N 2 ) computational and memory complexities in self-attention layers. To mitigate this drawback, we explored lightweight variants, including DistilBERT BASE 7 and ALBERT BASE 8 . DistilBERT distills BERT's knowledge into a more compact model [81], while ALBERT reduces model size by sharing common parameters across layers [82]. Both lightweight models achieved superior results compared to the baseline models, with only minor performance degradation compared to BERT BASE . \n\nIn alignment with the observation in Section VI-E, the BERT variants gave more importance to considering punctuation than casing.",
            "score": 0.4408676322360636,
            "section_title": "F. Application of BERT Variants",
            "char_start_offset": 41972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022247314453125
        },
        {
            "corpus_id": "222177254",
            "title": "A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction",
            "text": "As argued in Section 4, we hypothesized that the filtering-based denoising approaches are not wellsuited for GEC. To verify this hypothesis, we employed the following three filtering-based denoising baseline methods in addition to a base model trained in noisy parallel data D (henceforth, no denoising).\n\nCross-entropy filtering (CE filtering) The dual conditional cross-entropy filtering method was proposed by Junczys-Dowmunt (2018) and achieved the highest performance on the noisy parallel corpus filtering task at WMT2018 .\n\nIn this study, we prepared forward and reverse pretrain models using the BEA-train dataset to adapt the filtering method to GEC. We obtained the filtered data by removing 20% of the sentence pairs 10 with higher scores from the training data and used them for training.\n\nSentence-level error detection filtering (SED filtering) Asano et al. (2019) demonstrated the effectiveness of the sentence-level error detection (SED) model as a filtering tool to preprocess GEC input. Considering these findings, we adopted SED as a filtering-based denoising method for training data. More specifically, we discarded the sourcetarget sentence pairs in the noisy parallel data D if the SED model predicted the target sentence as an incorrect one. Following Asano et al. (2019), we obtained binary-labeled data using the BEA-train dataset to prepare a training set for the SED model, and then fine-tuned BERT (Devlin et al., 2019) on the prepared data.\n\nLanguage model filtering (LM filtering) Language model-based filtering is a method based on the hypothesis that if the perplexity of a target sentence is larger than that of the source sentence, the target sentence is more likely to contain noise. LM filtering has the same motivation as the one underlying the fail-safe mechanism. We used GPT-2 as the pre-trained language model.    methods, such as SED and LM filtering, generally achieved better results compared to the baseline models; however, they resulted in lower performance in smaller datasets such as BEA. This could be caused by the fact that these filtering methods have filtered out the training instances containing not only noise but also many correct corrections that may still be partially useful for training. As shown",
            "score": 0.44083978709999694,
            "section_title": "Baselines",
            "char_start_offset": 14608,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 859,
                    "end": 878,
                    "matchedPaperCorpusId": "198996284"
                },
                {
                    "start": 1427,
                    "end": 1448,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1611328125
        },
        {
            "corpus_id": "271582354",
            "title": "AN AUTOMATED NEW APPROACH IN FAST TEXT CLASSIFICATION: A CASE STUDY FOR KURDISH TEXT",
            "text": "The findings indicated that Bert achieved an accuracy of 92%, surpassing the performance of traditional machine-learning algorithms by two percentage points (Badawi, 2023). In another study, researchers addressed the issue of high dimensionality in feature space and the limitations of conventional machine learning algorithms such as SVM, NB, and KNN by introducing FastText as a novel classification model. The study revealed that the FastText model attained an impressive F1-score of 0.9286, outperforming the other models (Yao et al., 2020). Birol Kuyucu used the FastText classifier to analyze the TTC-3600 Turkish dataset. Remarkably, no preprocessing steps such as tokenization, stemming, lemmatization, stop word removal, lowercase conversion, or dimensionality reduction were applied. Following this, the performance of FastText was compared with K-NN, decision tree J48, and Multinomial Na\u00efve Bayes (NV). The results demonstrated that FastText surpassed the other models, achieving an impressive accuracy score of 93.52%, despite the absence of preprocessing steps (Kuyumcu et al., 2019). Additionally, Amalia conducted a comparison between the FastText model and TF-IDF as one of the BOW models for 500 new articles in a low-resource Bahasa Indonesia. The study revealed that TF-IDF requires more preprocessing steps and is time-consuming for model prediction. Moreover, it was observed that FastText classification exhibited superior performance with a 0.97 F1-score compared to TF-IDF (Amalia et al., 2020).",
            "score": 0.4401443448742017,
            "section_title": "RELATED WORK",
            "char_start_offset": 5197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1520
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 544,
                    "matchedPaperCorpusId": "221719237"
                },
                {
                    "start": 1075,
                    "end": 1097,
                    "matchedPaperCorpusId": "201103512"
                },
                {
                    "start": 1498,
                    "end": 1519,
                    "matchedPaperCorpusId": "221715127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005512237548828125
        },
        {
            "corpus_id": "266977204",
            "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
            "text": "Appendix B includes additional details on the reproduction of each filter. Descriptions of pretraining data curation are sometimes too vague or non-existent to allow for exact replication (OpenAI, 2023), but multiple recent and prominent LLMs still allude to the use of model-and heuristic-based data filters (Touvron et al., 2023a;Gemini Team et al., 2023;Chowdhery et al., 2023). \n\nModel-based quality. We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2020;Lauren\u00e7on et al., 2022;Muennighoff et al., 2023;Marion et al., 2023). \n\nHeuristic-based quality. Another quality filtering approach for web text applies rule-based heuristics (Raffel et al., 2023;Rae et al., 2021). We examine 19 document-level heuristics and thresholds from Gopher (Raffel et al., 2023). These heuristics remove documents that do not meet thresholds pertaining to document and word length, textual repetition, and frequencies of symbols and common English words (Appendix B). \n\nEnglish langID. Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a).",
            "score": 0.43983257103779994,
            "section_title": "Pretraining Data Filters",
            "char_start_offset": 13304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1749
                }
            ],
            "ref_mentions": [
                {
                    "start": 1108,
                    "end": 1129,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1129,
                    "end": 1152,
                    "matchedPaperCorpusId": "257378329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "269188338",
            "title": "Sampling-based Pseudo-Likelihood for Membership Inference Attacks",
            "text": "Large Language Models (LLMs) bring about a game-changing transformation in various services used on a daily basis (Brown et al., 2020;Touvron et al., 2023).The pre-training of LLMs relies on massive-scale web data of mixed quality (Zhao et al., 2023).While pre-processing such as filtering is applied to construct as clean datasets as possible, it is unrealistic to remove everything undesired (Almazrouei et al., 2023).There is a risk of unintentionally leaking benchmark data, copyrighted * Masahiro Kaneko and Youmi Ma contributed equally to this work. 1Our code is available at: https://github.com/nlp-titech/samia texts, or personal information into the pre-training data (Kaneko and Baldwin, 2024).The leakage of benchmark data can lead to an overestimation of LLMs' capabilities, preventing an appropriate assessment of their true performance (Yu et al., 2023;Zhou et al., 2023).Additionally, LLM generation based on copyrighted texts or personal information can result in serious violations of the law (Yeom et al., 2017;Eldan and Russinovich, 2023).\n\nMembership Inference Attacks (MIA) consider the task of determining whether a given target text is included in the training data of a model (Shokri et al., 2016).Generally, because models are trained to fit the data, a text included in the training data tends to exhibit a higher likelihood compared to ones unseen in the training data (Yeom et al., 2017).Existing MIA studies rely on this idea and thus require the likelihood of a text computed by the model (Carlini et al., 2021;Ye et al., 2022;Mattern et al., 2023;Shi et al., 2023).It is impossible to apply the existing studies to the models that do not provide a likelihood.",
            "score": 0.43976179390080966,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 557
                },
                {
                    "start": 557,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 886
                },
                {
                    "start": 886,
                    "end": 1058
                },
                {
                    "start": 1060,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1416
                },
                {
                    "start": 1416,
                    "end": 1596
                },
                {
                    "start": 1596,
                    "end": 1690
                }
            ],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1029,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1200,
                    "end": 1221,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1396,
                    "end": 1415,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1519,
                    "end": 1541,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1541,
                    "end": 1557,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 1578,
                    "end": 1595,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11981201171875
        },
        {
            "corpus_id": "254974513",
            "title": "Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis",
            "text": "To fully comprehend the significance of our findings, it is necessary to consider the limitations of this study. Firstly, we evaluate only two Transformer-based models on a small number of text classification tasks. Although we used the models with different pre-training paradigms, it is possible that the findings do not generalize across models within the same family. In addition, we used the base variants of BERT and ELECTRA, which both feature 12 layers. Since our early stopping criterion is influenced by the number of layers whose smoothness we approximate, there is a possibility that smoothness would distribute differently for models with more or fewer layers. Another limitation is that we did not investigate these models' performance on tasks other than text classification, and the results may not be generalizable to different types of NLP tasks. Since there are many different ways to measure the quality of an AL stopping criterion and we only wanted to illustrate the usefulness of smoothness patterns, we only compared the proposed ALSBI method against an average baseline. However, a more comprehensive comparison with other approaches from the literature would provide a better understanding of the merit of our method. \n\nLastly, we only scratched the surface of different training regimes for PLMs in the context of AL. Many new training regimes are emerging in the field, especially the ones focused on efficiency and modularity. We leave the exploration of these methods for future work.",
            "score": 0.43912761555270524,
            "section_title": "Limitations",
            "char_start_offset": 29113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1514
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06512451171875
        },
        {
            "corpus_id": "277787386",
            "title": "Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content",
            "text": "Another deep learning-based keyword extraction improvement is EmbedRank, in which sentence embeddings are utilized to rank key phrases concerning the document context [12], [13]. Web scraping is currently a compulsory technique for automatic online data collection, particularly in the case of customer feedback analysis. \n\nIt facilitates large-scale harvesting of data from diverse sources such as social media sites, forums, and review sites, enabling convenient sentiment analysis and trend identification [15]. Traditional scraping approaches produce high levels of irrelevant or duplicate content that need postprocessing tasks to improve data relevance and verify the quality of information retrieved [14]. Even though numerous research works have been carried out on rule-based filtering methods, these kinds of methods frequently are not able to deal with irrelevant content associated with specific domains since they can't cater to subtle, context-based sophistication in text [16]. To prevent this drawback, recent research emphasis has been on utilizing deep learning models for automatic filtering and categorization of content. Research demonstrates that BERT fine-tuning on domain-specific corpora significantly improves classification accuracy, particularly for technical domains such as banking and finance [17]. However, while BERT-based models effectively capture semantic meaning, they are often hindered by real-world user-generated text, which is comprised of informal text, abbreviations, and contextual ambiguities. As such, alternative evaluation strategies, such as model ensembling and transfer learning, are being explored to boost classification robustness [18], [19]. The advent of large language models (LLMs) like GPT [21] has added more interest in classification methods using APIs. These models, developed with massive datasets comprising multiple languages and intended domains, provide better contextual comprehension and flexibility. In research, it has already been seen that LLM APIs perform better than conventional deep learning models in various text classification tasks, specifically where the setting includes informal or highly contextual content [20]. Research furthermore shows that combining pre-trained transformer-based models and LLM APIs can guarantee further generalization, proposing a hybrid approach to filtering out unwanted content and improving aspect classification in complex datasets [20].",
            "score": 0.4390265640893515,
            "section_title": "II. LITERATURE REVIEW",
            "char_start_offset": 5389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "249224515"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "266679174"
                },
                {
                    "start": 1692,
                    "end": 1696,
                    "matchedPaperCorpusId": "225011359"
                },
                {
                    "start": 1750,
                    "end": 1754,
                    "matchedPaperCorpusId": "258157875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0943603515625
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Language models pre-trained on large amounts of text corpora form the foundation of today's NLP (Gururangan et al., 2020;Rogers et al., 2020). They have proved to provide state-of-the-art performance against most standard NLP benchmarks (Wang et al., 2019a;Wang et al., 2019b). However, these models require large computational resources that are not always available and have important environment implications (Strubell et al., 2019). Moreover, there is limited research in the applicability of pre-trained models in classification tasks with small amount of labelled data. Some related studies (Lee et al., 2020;Nguyen et al., 2020;Huang et al., 2019;Alsentzer et al., 2019) investigate whether it is helpful to tailor a pre-trained model to the domain while others (Sun et al., 2019;Chronopoulou et al., 2019;Radford et al., 2018) analyse methods for fine-tuning BERT to a given task. However, these studies perform evaluation on a limited range of datasets and classification models and do not consider scenarios with limited amounts of training data. \n\nIn particular, this paper aims to estimate the role of labeled and unlabeled data for supervised text classification. Our study is similar to Gururangan et al. (2020) where they investigate whether it is still helpful to tailor a pre-trained model to the domain of a target task. In this paper, however, we focus our evaluation on text classification and compare different types of classifiers on different domains (social media, news and reviews). Unlike other tasks such as natural language inference or question answering that may require a subtle understanding, feature-based linear models are still considered to be competitive in text classification (Kowsari et al., 2019). However, to the best of our knowledge there has not been an extensive comparison between such methods and newer pre-trained language models.",
            "score": 0.4389701273625768,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 121,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 237,
                    "end": 257,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 257,
                    "end": 276,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 412,
                    "end": 435,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 654,
                    "end": 677,
                    "matchedPaperCorpusId": "102352093"
                },
                {
                    "start": 769,
                    "end": 787,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 787,
                    "end": 813,
                    "matchedPaperCorpusId": "67855637"
                },
                {
                    "start": 1201,
                    "end": 1225,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1715,
                    "end": 1737,
                    "matchedPaperCorpusId": "119297355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02752685546875
        },
        {
            "corpus_id": "268297267",
            "title": "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey",
            "text": "LLMs are super-large deep learning models pre-trained on vast amounts of data, containing tens of billions to trillions of parameters. They construct extensive unsupervised training based on these parameters, enabling them to more accurately learn patterns and structures of natural language, thereby understanding and generating natural language texts. Compared to traditional NLP models, LLMs demonstrate better proficiency in understanding and generating natural texts, and also exhibit certain logical thinking and reasoning abilities, which is widely in programming [33], vulnerability detection [34], and medical text analysis [35]. In 2017, Vaswani et al. [36] introduced the Transformer architecture, which uses parallel processing and attention mechanisms to provide an effective method for processing sequential data (especially text). This significantly enhances the efficiency of dealing with sequential data and supports more efficient training on large datasets, fostering the rapid development of LLMs such as the GPT series, BERT, and Transformer models. The training of LLMs primarily includes two key stages: pre-training and fine-tuning. \n\n\u2022 Pre-training: At this stage, the model is typically trained on a very large and diverse dataset. These datasets may include texts from a variety of sources such as the Internet, books and news, or large text datasets published by many organizations and research institutions for academic research. E.g. general text corpora, social media data, user-generated content, and dialogue data). For example, GPT-3, developed by OpenAI, was pre-trained using CommonCrawl, constituting 45TB of compressed plaintext before filtering [37]. Regarding multimodal LLMs, CLIP's training dataset encompasses 400 million pairs of images and text, while Stable Diffusion was trained on a dataset consisting of two billion examples sourced from LAION-2B [38]. The purpose of pre-training is to enable the model to learn a wide range of language patterns, structures, and knowledge. Through this process, the model acquires a broad ability to understand language, including understanding vocabulary, grammar, and even some common sense. This stage does not focus on any specific task but rather provides a general foundation for language understanding.",
            "score": 0.4386925345614245,
            "section_title": "III. BACKGROUND ON LARGE LANGUAGE MODELS (LLMS)",
            "char_start_offset": 10492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 633,
                    "end": 637,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1896,
                    "end": 1900,
                    "matchedPaperCorpusId": "258352812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07305908203125
        },
        {
            "corpus_id": "271097500",
            "title": "Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients",
            "text": "Network Architecture.For the pretraining task, we adopt the LLaMA-based architecture with sizes ranging from 60 million to 7 billion, following the setups from [1,36].During downstream experiments, we select various pre-trained models to evaluate the general effectiveness of Q-GaLore, including RoBERTa [54] base, LLaMA-3-8B [55], Gemma-7B [56], and Mistral-7B [57].\n\nPre-Training.We pre-train the LLaMA models on C4 dataset [58].The C4 dataset is a massive collection of Common Crawl's web crawl corpus, meticulously filtered and cleaned to ensure highquality language modeling and training.It is widely used for pre-training large language models due to its diverse and extensive textual content.We train the models on this sufficiently large dataset without data repetition and scales the model size up to 7 billion parameters, a crucial experiment for demonstrating the effectiveness of the proposed methods for practical pre-training.\n\nFine-Tuning.The downstream tasks cover two categories: (i) GLUE benchmarks [59], a series of widely used tasks for evaluating the downstream performance of natural language understanding; (ii) MMLU [60] that evaluates the natural language understanding ability of LLMs, this task covers various domains, including STEM, social sciences, humanities and others.\n\nBaselines.We consider five baseline methods for comparison: (i) Full: Models are trained with the original Adam [61] optimizer.Both weights, gradients, and optimization states are maintained with full rank and full precision (BF16 format).(ii) Low-Rank: The original weights are factorized into low-rank components: W = U V , and U and V are optimized via Adam [62].(iii) LoRA: LoRA [22] introduces low-rank adaptors for training the models, W = W 0 + U V , where W 0 is the pretrained weights, which are frozen during training.We use the initialized weight as W 0 during pretraining and only optimize U and V .And we default to 32 for LoRA alpha and 0.05 for LoRA dropout.",
            "score": 0.43821751206122816,
            "section_title": "Implementation Details",
            "char_start_offset": 16455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 367
                },
                {
                    "start": 369,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 940
                },
                {
                    "start": 942,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1301
                },
                {
                    "start": 1303,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "259836974"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "276937920",
            "title": "LREF: A Novel LLM-based Relevance Framework for E-commerce Search",
            "text": "3.4.1 Overall Performance. As shown in Table 1, our proposed LREF method achieves significant improvements compared to the baselines. Furthermore, taking LLM Base as a reference point, we observe that both BERT and DeBERTa outperform it. This indicates that the straightforward application of LLMs to relevant tasks, specifically by fine-tuning LLMs with all annotated data (SFT), does not inherently surpass the performance of classical BERTlike discriminative models. Only after we address the need for high-quality data in SFT, optimize the internal reasoning steps, and mitigate optimistic bias, our proposed LREF, which also leverages LLMs, achieve optimal performance.",
            "score": 0.43760540418800137,
            "section_title": "Offline Evaluation",
            "char_start_offset": 21014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06463623046875
        },
        {
            "corpus_id": "246738423",
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "text": "In 2017, Mikolov et al. [12] proposed the FastText model; the network architecture of FastText is very similar to CBOW (Continuous Bag-of-Words) [13], and FastText is mainly used for text classification and displays positive computational efficiency. Peters et al. [14] generalizes traditional word embedding research ELMo (Embedding from Language Model) along a different dimension in 2018, and they suggested extracting context sensitive features and applying pre-training tasks to text classification tasks. \n\nFine-tuning based on pre-training for a language model has been developed for text generation tasks (e.g., GPT (Radford et al., 2018) [15]; Google proposed the BERT (Bidirectional Encoder Representations from Transformers) model in 2018 [16]. The BERT model is based on a self-attention model architecture. The successful application of BERT has put the natural language text classification task into a new level. \n\nThe BERT model mainly focuses on words or more fine-grained cloze learning and does not make full use of the lexical structure in the training data. Li [17] used the application of the BERT model in text classification at the sub-word level. Baidu proposed the ERNIE (Enhanced Representation through Knowledge Integration) model in 2018 [18]. Compared with BERT, ERNIE has improved two masking strategies; the first is the phrase-based masking strategy, and the second is the entity (name, location, organization, product, etc.) based masking strategy. Although the BERT and ERNIE models can achieve positive results in various tasks in the field of natural language processing, it is difficult for scientific research teams in universities as well as in small-and medium-sized enterprises to pre-train BERT from the base model due to the large size of the model, the large amount of data, and the large number of resources required for training. In the field of computer vision, Hinton et al. [19] proposed a training method based on the \"teacherstudent network idea\" in 2015. The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability.",
            "score": 0.4373335199391355,
            "section_title": "Introduction",
            "char_start_offset": 3875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "222020002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0235137939453125
        },
        {
            "corpus_id": "246608158",
            "title": "Extracting Software Requirements from Unstructured Documents",
            "text": "The main purpose of this study was about assessing three major classification metrics: Precision, Recall, and F1-score. Those metrics are perceived as the golden standard in Machine Learning and Deep Learning areas. \n\nResult of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements.",
            "score": 0.43695225188765463,
            "section_title": "Experiments with PURE documents",
            "char_start_offset": 15016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 860
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00933837890625
        },
        {
            "corpus_id": "265658802",
            "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
            "text": "This section provides additional details on the large language models (LLMs) audited in our experiments. \n\nBERT base We use the BERT base model (uncased) [16] provided by HuggingFace8 , which is a pre-trained transformer model designed for the English language, and the corresponding Tokenizer. BERT is trained via self-supervised training on unlabeled raw texts, utilizing two objectives: masked language modeling and next-sentence prediction. Its training data includes a large corpus of English texts, and it is important to note that the model can have biased predictions. The model has been primarily designed for fine-tuning downstream tasks such as sequence classification and token classification. \n\nBERT is constructed as a transformer-based model, which employs an encoder architecture with a self-attention mechanism that allows it to capture contextual information from both preceding and following words in a sentence, creating a bidirectional understanding. The BERT base model encoder has 12 layers, and 12 attention heads per layer. The embedded space of the model has a dimension of 768. In BERT, the [CLS] token is a special token added to the beginning of each input sequence. The final hidden state of the [CLS] token is often used as a summary representation of the entire input sequence for downstream classification tasks. Our analysis focuses on the hidden representations of the [CLS] token, as it is commonly used for downstream classification tasks [28,47]. \n\nOPT We use the Open Pre-trained Transformer (OPT) language model [56] (6.7B) provided by HuggingFace 9 . It has been specifically trained to align with the performance and sizes of the GPT-3 models employing causal language modeling (CLM) objectives. The training data for OPT comprises a fusion of filtered datasets sourced from various origins, such as BookCorpus, CC-Stories, The Pile, Reddit, and CCNewsV2, which is comprised mainly of English text, with a small quantity of non-English data. OPT serves multiple purposes, including text generation and evaluation via prompts, and can be fine-tuned for specific tasks. The training of the pre-trained model spanned approximately 33 days and harnessed the power of multiple GPUs.",
            "score": 0.43670337793604097,
            "section_title": "B LLM Models",
            "char_start_offset": 26402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "235422673"
                },
                {
                    "start": 1480,
                    "end": 1483,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0251312255859375
        },
        {
            "corpus_id": "272689526",
            "title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora",
            "text": "To demonstrate the effectiveness of our ensemble approach, we present examples that are not filtered by Good KenLM but are successfully removed by our ensemble approach. As illustrated in Figure 2, our approach effectively filters advertising and communication-style content, which are generally unsuitable for LLM training. Since advertising content is usually written politely, Good KenLM, trained only on high-quality datasets, struggles to detect it. Conversely, Bad KenLM, trained on spam mail and SNS data, successfully identifies such content as well as communicationstyle content. Therefore, our ensemble approach more effectively filters these types of content.",
            "score": 0.4365808066441789,
            "section_title": "Main Results",
            "char_start_offset": 11555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 670
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "275336918",
            "title": "Swift Cross-Dataset Pruning: Enhancing Fine-Tuning Efficiency in Natural Language Understanding",
            "text": "Deep learning progress has been fueled by massive datasets (Tan et al., 2024;Gadre et al., 2024), but managing and training on such data poses computational and storage challenges (Yang et al., 2023). Dataset pruning, or coreset selection, aims to identify a subset that achieves comparable model performance to the full dataset (Mirzasoleiman et al., 2020;Killamsetty et al., 2021), reducing training and storage costs while maintaining model effectiveness (Huang et al., 2021;Xia et al., 2022) This challenge is particularly evident in language model (LM) training, which involves two distinct scenarios: pre-training and fine-tuning, each requiring different data handling approaches. Data for pre-training, like those used for BERT, comprise large-scale, unlabeled, and diverse corpora like BookCorpus and English Wikipedia, collectively containing over 3,300 million words (Devlin et al., 2019). These corpora aim to facilitate the learning of broad language representations. In contrast, datasets for fine-tuning for downstream tasks, such as SWAG (113,000 examples) (Zellers et al., 2018), are smaller, labeled, and task-specific, designed to evaluate targeted abilities such as commonsense reasoning. Although dataset efficiency techniques such as language filtering, quality assessment, and deduplication (Albalak et al., 2024;Longpre et al., 2024) are proposed for large-scale pre-training corpora, they are not suitable for finetuning. \n\nWhile single-dataset fine-tuning benefits from a narrow target distribution (Albalak et al., 2024), establishing general dataset pruning rules for cross-  dataset fine-tuning remains challenging due to the diversity of natural language processing (NLP) tasks. Common benchmarks reveal significant variations in task types, dataset sizes, and domains.",
            "score": 0.4360045995991606,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1799
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 77,
                    "matchedPaperCorpusId": "264426070"
                },
                {
                    "start": 77,
                    "end": 96,
                    "matchedPaperCorpusId": "258352812"
                },
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "248887235"
                },
                {
                    "start": 329,
                    "end": 357,
                    "matchedPaperCorpusId": "211259075"
                },
                {
                    "start": 357,
                    "end": 382,
                    "matchedPaperCorpusId": "229339854"
                },
                {
                    "start": 458,
                    "end": 478,
                    "matchedPaperCorpusId": "235825933"
                },
                {
                    "start": 478,
                    "end": 495,
                    "matchedPaperCorpusId": "259298636"
                },
                {
                    "start": 878,
                    "end": 899,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1073,
                    "end": 1095,
                    "matchedPaperCorpusId": "52019251"
                },
                {
                    "start": 1314,
                    "end": 1336,
                    "matchedPaperCorpusId": "268032975"
                },
                {
                    "start": 1336,
                    "end": 1357,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 1525,
                    "end": 1547,
                    "matchedPaperCorpusId": "268032975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "241583607",
            "title": "Effectively Leveraging BERT for Legal Document Classification",
            "text": "In particular, when fine-tuning BERT, we normally add a classification layer (either Soft-Max or Sigmoid) on the C (or CLS) representation in BERT output layer, in order to compute the prediction probabilities as in Figure 1. In the legal domain, Zheng et al. (2021) found that pre-training BERT on legal documents before fine-tuning on particular tasks lead to a better performance than pre-training BERT on general documents. However, Chalkidis et al. (2019) found that BERT did not perform well on the violation prediction task due to the length of the documents that are mostly longer than 512 tokens. They dealt with the long legal documents by using a hierarchical BERT technique (Chalkidis et al., 2019). Difference from the previous work, we investigate the effectiveness of variances of pre-trained BERT-based models and compare several methods to handle the long legal documents in legal text classification. \n\nSeveral attempts (Beltagy et al., 2020;Zaheer et al., 2020;Pappagari et al., 2019) have been made to enable BERT-like models to work on documents with more than 512 tokens. For example, Beltagy et al. (2020) and Zaheer et al. (2020) used several different attention-mechanism techniques, such as global attentions and sliding window attentions to enable learning on a longer number of tokens. Pappagari et al. (2019) investigated different approaches to apply BERT on sequence chunks of texts in a document before aggregating the features using techniques, such as max pooling and mean pooling. In this work, we adapt these techniques to learn how to effectively use BERT on long legal documents.",
            "score": 0.4359249855640206,
            "section_title": "Related Work",
            "char_start_offset": 4637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 266,
                    "matchedPaperCorpusId": "233296302"
                },
                {
                    "start": 980,
                    "end": 1003,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1314,
                    "end": 1337,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06304931640625
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "During testing procedure, we will classify the samples of testset with Regression, Perplexity, BERTEval and FastText models repectively, and then compute the precisions of them on positive data. As in Table 4, it shows the precisions of different evaluaiton models on the testset. TP represents the number of \"True Positive\" samples, while FP represents the number of \"False Positive\" samples. From this table, we can see that our BERTEval evaluaiton model gets a much better performance than the regression and perplexity approaches. Besides, benefiting from the good classified results of our BERTEval model, the FastText-based model could further improve the classification precision. This result indicates that using BERTEval to guide the construction of the FastText-based evaluation model is effective. And with this FastText-based evaluation model, our EvalWeb tool-chain could achieve a better performance while effectively improving processing efficiency and resource utilization. In section 3.3, after filtering data with a desired quality threshold, we can obtain a Chinese text dataset. In order to ensure the quality of this dataset, We will hire some human evaluators to evaluate its quality. In this method, we will randomly sample 1000 examples from the dataset for three times. After that, three human evaluators will be hired to assess the quality of these data respectively, and the quality of these data is required to be evaluated from the following four aspects: \n\n\u2022 Informativeness: Whether the text contains enough knowledge and information, or is just meaningless crap. \n\n\u2022 Fluency: Whether the text has formatting issues, capitalization mistakes, or evident grammatical errors that impair readability. \n\n\u2022 Coherence: Whether the text progressively forms a coherent body of information on a topic through its successive sentences. \n\n\u2022 Toxicity: Texts used for pre-training should endeavor to exclude offensive remarks, sexually explicit content, and politically sensitive statements to mitigate potential generative risks. \n\n\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\" <url> http://www.juyesh.com/SonList-1094890.html <source_domain> www.juyesh.com During evaluation procedure, each text is assigned a label of either \"True\" or \"False.\"",
            "score": 0.4358463976800375,
            "section_title": "Comparison Results",
            "char_start_offset": 23296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01332855224609375
        },
        {
            "corpus_id": "248085462",
            "title": "A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition",
            "text": "On CoNLL-2003, BERT achieves a micro-F1 score of 71.4 at K = 10, XLM-R a competitive score of 71.2, while GottBERT only achieves F 1 = 64.8. Similar performance differences between the three encoders can be observed for the other two datasets at K = 5 and K = 10. At K = 1, XLM-R consistently outperforms BERT and GottBert, with GottBERT showing the worst performance. The results show that BERT, a model trained with less, but likely quality training data (Wikipedia, OpenLegalData, News) produces representations that are more suited for low-resource NER in most of the evaluated settings, compared to GottBERT (145GB of unfiltered web text), and XLM-RoBERTa (\u2248100GB filtered CommonCrawl data for German).",
            "score": 0.4357317941105316,
            "section_title": "German results",
            "char_start_offset": 15797,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03326416015625
        },
        {
            "corpus_id": "270040522",
            "title": "Next-Generation Spam Filtering: Comparative Fine-Tuning of LLMs, NLPs, and CNN Models for Email Spam Classification",
            "text": "In the literature review section, previous research on spam filtering was thoroughly examined, highlighting the consensus regarding the efficacy of advanced machine learning algorithms in spam detection. These studies have delved into a spectrum of algorithms, from simple to advanced, as well as hybrid approaches to enhance spam detection and filtering. \n\nThis study will initially focus on utilizing the groundbreaking GPT-4 LLM, BERT, and RoBERTa NLPs, alongside a parallel comparison with CNNs. Both LLM and NLPs have demonstrated proficiency in understanding human text. In the proposed approach, the models will undergo fine-tuning on two different datasets using few-shot learning, with their spam detection capabilities assessed through direct pre-and post-fine-tuning comparisons. \n\nIn the initial stages of our research, we rigorously assess GPT-4's proficiency in accurately identifying spam emails. This involves conducting an exhaustive performance evaluation comparing the base GPT-4 model with its fine-tuned version. Subsequently, we proceed to fine-tune the BERT and RoBERTa models for spam detection. Both the fine-tuned LLM and NLP models will be compared, demonstrating how the fine-tuning process bolsters their ability to adeptly address the multifaceted challenges posed by spam emails with enhanced precision. Additionally, their effectiveness will be directly compared with a trained CNN on the same datasets. \n\nOur research findings affirm that both LLM and NLPs achieve nearly 100% accuracy in their predictions, with the fine-tuned GPT demonstrating a slightly higher performance compared to its base version and the RoBERTa model. It was interesting, though, to find that the BERT model surpassed the fine-tuned GPT-4 model slightly, with the CNN-trained model achieving far less accurate results than LLM and NLPs. The entirety of the code utilized in this research, including methods for data cleansing, fine-tuning methodologies, and the datasets themselves, is accessible in a GitHub repository licensed under the opensource MIT license [41]. \n\nTo achieve the objectives outlined in this paper, it was crucial to adopt a specific research methodology due to the extensive and intricate processes involved. This methodology has been carefully selected to support not only the implementation of data cleaning techniques and prompting engineering but also the following deployment and refinement of the models.",
            "score": 0.4357219978751333,
            "section_title": "Materials and Methods",
            "char_start_offset": 25931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2076
                },
                {
                    "start": 2079,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.062225341796875
        },
        {
            "corpus_id": "273022742",
            "title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction",
            "text": "RQ1: How the performance would LLMs yield in FCRE tasks? Table 4 depicts the increase in final accuracy after learning 8 FCRE tasks when the BERT-based backbone is replaced by the LLM backbone. Specifically, improvements can be as much as 3.75% in the case of LLAMA-2-7B, and 8.75% for Mistral-7B across both datasets. In addition, Table 6 shows the full results of FCRE models on both datasets. Mostly, during the training of eight tasks, the LLMs tend to provide higher accuracy than the BERT-based models. For some immediate tasks, LLAMA2-7B can achieve up to 16% higher accuracy than BERT-based models in TACRED, although their accuracy can be slightly lower in other cases. Besides, the differences in performance after training the first task and the last task (Accuracy drop -column \u2206 \u2193) in LLMs are smaller than in BERT-based models, from 2 to 5% in the case of LLAMA2-7B and as much as 8% for Mistral-CPL. These experimental results confirm the general superiority of LLM in solving FCRE compared to the class of conventional BERT-based models. \n\nOn the other hand, pre-trained LLMs are known to be knowledge-rich models with high generalization capabilities. However, for the first task, LLMs achieve accuracies of around 96% on FewRel and around 86% on TACRED, having no clear advantage over BERT-based models. Besides, the results in Table 6 clearly demonstrate the degradation of prior knowledge when applying pre-trained LLM in FCRE. In particular, the model's accuracy can drop by 30 -32% for LLAMA2-7B and by 20 -25% for Mistral-7B, after training 8 tasks. \n\nThanks to thorough training on large datasets, LLMs with billions of parameters contain a wealth of knowledge and have great potential in downstream tasks. However, in some cases, with the current operating mechanism of an autoregressive decoder, employing such a model with billions of parameters, as opposed to one with hundreds of millions (BERT), proves exceedingly expensive for only marginal improvements in accuracy.",
            "score": 0.4355168917627289,
            "section_title": "Using LLM for FCRE",
            "char_start_offset": 17718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1998
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03521728515625
        },
        {
            "corpus_id": "226262191",
            "title": "Active Learning for BERT: An Empirical Study",
            "text": "However, the use of AL with deep pre-trained models for text classification -and BERT in particular -has so far received surprisingly little consideration. Thus, while recent papers have demonstrated the value of AL for various deep-learning text classification schemes (Shen et al., 2017;Zhang et al., 2017;Siddhant and Lipton, 2018;Prabhu et al., 2019), the potential of AL combined with BERT is yet to be explored. First, given the unique properties of pre-trained models, and the expectation that such models will yield adequate performance even with small amounts of training data, it is unclear a priori whether -and to what extent -established AL paradigms can further enhance their classification performance. Moreover, more recent Deep AL strategies, such as Core-Set (Sener and Savarese, 2017) and Dropout (Gal and Ghahramani, 2016), were developed in the vision domain for CNNs. The value of these strategies on top of the BERT transformer architecture remains unclear. \n\nOur goal in this work is threefold. We study the potential of (i) various AL strategies; (ii) in conjunction with BERT, an arguably outstanding text classification scheme; (iii) within a highly challenging -yet common -real-world scenario of class imbalance and scarce labeled data. To address this goal, we conduct a systematic study, considering traditional and advanced AL strategies coupled with BERT for a wide range of datasets. We focus on three scenarios: A balanced setting, serving as a reference, where the prior of the class of interest is not too small; the more challenging imbalanced setting, where the class prior is \u2264 15% but we assume a way to obtain an unbiased set of positive samples to be used for initial training; and finally, the imbalanced-practical setting, which is similar to the imbalanced one, but takes a step further towards a truly practical setup, in which there is no access to an unbiased positive sample. Instead, we assume the user has access to a biased sample, hopefully enriched with positive examples, obtained by issuing simple queries of keywords associated with the positive class.",
            "score": 0.43535664991360373,
            "section_title": "Introduction",
            "char_start_offset": 2159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "2754649"
                },
                {
                    "start": 816,
                    "end": 842,
                    "matchedPaperCorpusId": "160705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017303466796875
        },
        {
            "corpus_id": "273162873",
            "title": "LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for Denial-of-Service Attacks",
            "text": "Large Language Models (LLMs). Large language models (LLMs) are advanced AI models designed to understand and generate human-like text by training on vast text data. These models generally use the Transformer architecture [Vaswani, 2017]. These LLMs scale up to billions of model parameters and present promising performance on various tasks. Representative examples include GPT [Achiam et al., 2023], BERT [Kenton and Toutanova, 2019], Llama [Touvron et al., 2023], etc. \n\nSafety alignment or guardrails of LLMs. Safety alignment refers to the process of guiding LLMs to produce outputs that adhere to ethical norms. Stiennon et al. [2020] introduced Reinforcement Learning from Human Feedback (RLHF), which utilizes human feedback and preferences to enhance the capabilities of LLMs, becoming a standard approach to LLM training. Supervised fine-tuning [Achiam et al., 2023] or instruction-tuning [Touvron et al., 2023] can further improve LLMs on specialized tasks using additional data of prompt-response (input-output) demonstrations, e.g., using safety datasets to enhance the LLM's safety assessment capability. As data quality is paramount for train-time safety alignment, recent studies aim to effectively create safety-related training data involving diverse malicious scenarios [Bianchi et al., 2023, Bhardwaj andPoria, 2023]. \n\nOn the other hand, guardrails are external protective mechanisms that prevent unsafe or harmful content in LLM services at the inference time. The rule-based filter is a conventional and straightforward approach to directly match and remove certain patterns in the language, such as toxic words and hate speech. Model-based filters leverage pretrained, fine-tuned, or prompted models to determine the safety of a conversation. Llama Guard [Inan et al., 2023], for instance, is a model fine-tuned from LLAMA to identify unsafe content and the category of safety violations. OpenAI [2024] also released moderation API for a similar purpose.",
            "score": 0.434990768452291,
            "section_title": "Background and Related Work",
            "char_start_offset": 4943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 236,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 617,
                    "end": 639,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1763916015625
        },
        {
            "corpus_id": "271903621",
            "title": "Importance Weighting Can Help Large Language Models Self-Improve",
            "text": "Fundamentally improving LLMs' reasoning ability essentially requires fine-tuning on a large amount of highquality supervised data. However, this methodology faces the threat that the stock of high-quality language data will be exhausted in some day (Villalobos et al. 2022). Selfimprovement emerges as a promising approach to utilize the inherent knowledge to make supervision for self-training LLMs. While LLMs can easily generate extensive data, the data quality is not always guaranteed (Huang et al. 2023b) and training on unfiltered data may even cause performance degradation (Shumailov et al. 2023b). Therefore, an essential requirement in LLM self-improvement is data filtering. \n\nPioneering works (Wang et al. 2023b;Bai et al. 2022;Xu et al. 2023) use language models to generate diverse types of data such as feedback, instructions, and questions. They filter data by heuristic rules as well as manual inspection, which is challenging and costly. LMSI (Huang et al. 2023a) proposed a framework including generating data for a question-only dataset and using the majority voting (selfconsistency) (Wang et al. 2023a) to select the most consistent answers, which is empirically proven to be effective among various tasks. LMSI also demonstrates that the answer correctness is positively relevant to self-consistency. Along with this work, MoT (Li and Qiu 2023) proposes further filtering the consistent answers by entropy, which measures the answer uncertainty. Self-Alignment (Li et al. 2024) shows it is feasible to prompt the LLM self-filtering the generated data. To comprehensively evaluate the generated data, some works use external pre-trained LMs as the reward model to score the generated data, such as GENIE (Yehudai et al. 2024) and ReST (G\u00fclc \u00b8ehre et al. 2023). With external supervision from the reward model, their filtering strategies are typically more considered.",
            "score": 0.4342409538312278,
            "section_title": "Related Work LLM Self-Improvement",
            "char_start_offset": 5377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 510,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 582,
                    "end": 606,
                    "matchedPaperCorpusId": "36790775"
                },
                {
                    "start": 1485,
                    "end": 1501,
                    "matchedPaperCorpusId": "260866107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "272636816",
            "title": "Comparative analysis of BERT and FastText representations on crowdfunding campaign success prediction",
            "text": "In this study, we explored the effectiveness of advanced AI techniques in predicting the success of Kickstarter crowdfunding campaigns by analyzing the sentiments conveyed in campaign blurbs. Our comparative analysis of BERT and FastText representations, combined with the LSTM and GBM models, revealed that BERT representations significantly outperform FastText. The highest accuracy rate of 0.745 was achieved by combining all BERT layer representations with an LSTM model. Fine-tuning the pretrained BERT model with our dataset resulted in a 3% performance improvement, underscoring the importance of model customization to the specific dataset. \n\n. Performance of BERT vs. FastText: BERT representations consistently outperformed FastText across all metrics, demonstrating the advantage of deep contextual embeddings. \n\n. Fine-tuning benefits: Fine-tuning the BERT model with our specific dataset improved performance, highlighting the value of adapting pre-trained models to domain-specific data. \n\n. Model comparisons: LSTM models, which handle sequential data effectively, showed slightly better performance than GBM models, particularly when combined with BERT representations.",
            "score": 0.43414689620429125,
            "section_title": "Key findings",
            "char_start_offset": 30673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1185
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.010986328125
        },
        {
            "corpus_id": "269614136",
            "title": "FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering",
            "text": "The emergence of large language models (LLMs) has revolutionized the field of natural language processing (NLP) and opened up new opportunities for AI applications in various domains.LLMs, such as GPT-4 [3], BERT [20], are deep neural networks pre-trained on massive text corpora, which can learn rich linguistic knowledge and perform a wide range of language tasks.For example, GPT-4 has shown impressive capabilities in language generation, translation, summarization, and question answering, surpassing human performance in some cases [3].BERT has achieved state-ofthe-art results on various NLP benchmarks, such as GLUE [21] and SQuAD [22], demonstrating its effectiveness in language understanding and reasoning.\n\nThe success of LLMs can be attributed to their ability to capture the intricate patterns and semantics of natural language from large-scale unsupervised learning.By training on diverse text data, LLMs can acquire a broad range of knowledge and skills, which can be transferred to downstream tasks with minimal fine-tuning.This transfer learning paradigm has greatly reduced the need for task-specific labeled data and enabled the development of generalizable and scalable AI systems.Moreover, the pre-training objectives of LLMs, such as masked language modeling [23] and permutation language modeling [24], have been designed to encourage the models to learn contextual representations and long-range dependencies, which are essential for understanding and generating coherent and informative text.\n\nHowever, the application of LLMs in education is still an emerging area that requires further exploration.Existing work has mainly focused on using LLMs for automated essay scoring [25], question answering [26].For example, Xue.J et al. [27] fine-tuned BERT on a large dataset of student essays and their corresponding scores, and achieved state-of-the-art performance on automated essay scoring.Abdelghani et al. [28] employed GPT-3 to generate diverse and informative answers to student questions, and demonstrated its potential for intelligent tutoring systems.While these studies have shown promising results, they have not fully exploited the potential of LLMs for interactive and explainable education, which is the focus of our FOKE framework.",
            "score": 0.4334725712739643,
            "section_title": "Large Language Models",
            "char_start_offset": 11836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 717
                },
                {
                    "start": 719,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1518
                },
                {
                    "start": 1520,
                    "end": 1626
                },
                {
                    "start": 1626,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1916
                },
                {
                    "start": 1916,
                    "end": 2084
                },
                {
                    "start": 2084,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 1757,
                    "end": 1761,
                    "matchedPaperCorpusId": "237519736"
                },
                {
                    "start": 1934,
                    "end": 1938,
                    "matchedPaperCorpusId": "254018349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037750244140625
        },
        {
            "corpus_id": "270562306",
            "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
            "text": "In this section, we introduce both the pre-and post-training techniques we adopted and developed in ChatGLM, including the model architecture, pre-training data, alignment, and All Tools. We have detailed technical reports introducing each of the major techniques we used to reach GLM-4. \n\nPre-Training Data. Our pre-training corpus consists of multilingual (mostly English and Chinese) documents from a mixture of different sources, including webpages, Wikipedia, books, code, and research papers. The data processing pipeline mainly includes three stages: deduplication, filtering, and tokenization. The deduplication stage improves data diversity by removing duplicated or similar documents, with both exact and fuzzy deduplication. The filtering stage for webpages improves data quality by removing noisy documents that contain offensive language, placeholder text, source code, etc. The tokenization stage converts text into a sequence of tokens for further processing. The number of tokens in the pre-training data directly affects model training speed. To optimize this aspect, we employ the byte-level byte pair encoding (BPE) algorithm [34] to separately learn the Chinese and multilingual tokens and merge them with the tokens of the cl100k_base tokenizer in tiktoken [27] into a unified vocabulary with a size of 150,000. In the final training set, we re-weight different sources to increase the importance of high-quality and educational sources like books and Wikipedia. To this end, the pre-training corpus consists of around ten trillion tokens. \n\nThroughout the four generations of ChatGLM development, our findings align with existing studies [60]: data quality and diversity are crucial for building effective LLMs. Despite the empirical lessons and insights gained, we have to date yet to identify a fundamental principle that could guide the processes of data collection, cleaning, and selection, which might inspire future research directions. \n\nArchitecture. The GLM family of LLMs is built on Transformer [43]. In GLM-130B [53], we explored various options to stabilize its pre-training by taking into account the hardware constraints we faced at the time.",
            "score": 0.4331750282271254,
            "section_title": "ChatGLM Techniques",
            "char_start_offset": 8058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 1145,
                    "end": 1149,
                    "matchedPaperCorpusId": "1114678"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13623046875
        },
        {
            "corpus_id": "259837230",
            "title": "PolyLM: An Open Source Polyglot Large Language Model",
            "text": "2) Rule-based filtering. Following Rae et al. (2021);Scao et al. (2022), we eliminate irrelevant or low-quality content using various rules and heuristics, including repetition removal (the document with the excessive line, paragraph, or n-gram repetitions is removed), document-wise filtering (removing outlier documents by overall length, symbol-to-word ratio, the ratio of ellipsis, invisible characters, numbers, and dates, etc.), and line-wise corrections (such as URL filtering, long words removal, and whitespace standardization). \n\n3) ML-based quality filtering. We further filter low-quality multilingual documents using several small n-gram-based language models (e.g., KenLM (Heafield, 2011)) for different languages trained on their gold-standard corpora. In addition, similar to Raffel et al. (2020); Smith et al. (2022), we also train a 2-gram fastText (Joulin et al., 2016) classifier to filter the low-quality English documents. This classifier uses Wikipedia, and Books from The Pile (Gao et al., 2020) as the positive samples Based on the POLYLM multilingual pre-training dataset, we derived a vocabulary with 256K token entries using Byte-Pair Encoding (BPE) (Sennrich et al., 2015) with the implementation from Sen-tencePiece (Kudo & Richardson, 2018). To enhance the mathematical capabilities of our model, we follow Touvron et al. (2023) to split all numbers into individual digits. The unknown characters are fallback to byte encoding of UTF-8 to guarantee the coverage of rare words (e.g., emoji, and special symbols). For tokenizer training, we sample multilingual documents with a similar distribution as Conneau et al. (2019) used to increase the number of vocabulary tokens associated with low-resource languages and alleviate the bias towards high-resource languages. We compare the compression rate on different language corpora of different tokenizers.",
            "score": 0.4330462881850923,
            "section_title": "DATASET",
            "char_start_offset": 12377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 702,
                    "matchedPaperCorpusId": "8313873"
                },
                {
                    "start": 792,
                    "end": 812,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1631,
                    "end": 1652,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "273321421",
            "title": "Optimizing Customer Satisfaction Through Sentiment Analysis: A BERT-Based Machine Learning Approach to Extract Insights",
            "text": "Despite its advantages, sentiment analysis faces challenges such as dealing with complex language and sentiment variation. Using models such as BERT helps mitigate these issues, although further development in preprocessing techniques and model training is required [16], [17]. Table 10 shows the error rates before and after applying the various preprocessing techniques [4], [7], [24]. \n\nThe graph in Figure 5 illustrates the impact of different preprocessing techniques on error rates. \n\n(1) X-axis: Different preprocessing techniques (Basic Preprocessing, Advanced Preprocessing, Contextual Training Data, and Error Correction) \n\n(2) Y-axis: Error rates (percentage of incorrect classifications). \n\nEach bar represents a preprocessing technique with two segments: \n\n(1) Before Mitigation: Error rate before applying the technique. \n\n(2) After Mitigation: Error rate after applying the technique. \n\nThis study demonstrates the effectiveness of BERT in sentiment analysis, with significant improvements over traditional models. Implementing advanced models, such as BERT, can lead to better customer insights and service 151486 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. improvements. Future studies should explore aspect-based sentiment analysis and predictive analytics for further enhancement [3], [4], [16], [20], [21], [27]. \n\nThe results obtained from the technical implementation of the BERT model and comparative models are consistent with expectations based on prior research. The use of advanced preprocessing techniques and rigorous model evaluation contributed to the robustness of the findings. The consistent performance improvements observed in BERT, as compared to traditional models, underscore the technical soundness of the methodologies employed. \n\nIn this section, we present the performance metrics of the BERT model alongside other comparative models, such as Na\u00efve Bayes, SVM, and Random Forest. We evaluate these models based on accuracy, precision, recall, and F1 scores to determine their effectiveness in sentiment analysis.",
            "score": 0.4325787752644936,
            "section_title": "F. ERROR ANALYSIS",
            "char_start_offset": 39903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "261187792"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "261484925"
                },
                {
                    "start": 372,
                    "end": 375,
                    "matchedPaperCorpusId": "269479455"
                },
                {
                    "start": 377,
                    "end": 380,
                    "matchedPaperCorpusId": "270219173"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "215860543"
                },
                {
                    "start": 1383,
                    "end": 1386,
                    "matchedPaperCorpusId": "264596133"
                },
                {
                    "start": 1388,
                    "end": 1391,
                    "matchedPaperCorpusId": "269479455"
                },
                {
                    "start": 1393,
                    "end": 1397,
                    "matchedPaperCorpusId": "261187792"
                },
                {
                    "start": 1411,
                    "end": 1415,
                    "matchedPaperCorpusId": "159118993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01214599609375
        },
        {
            "corpus_id": "274166121",
            "title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization",
            "text": "While our method shows clear advantages in PPL, a gap remains between PPL and downstream performance. The lack of domain-specific test sets also limits further analysis. To address this, we create downstream tasks from unlabeled datasets for detailed domain-specific evaluation. \n\nAutomatic construction of sentence continuation tasks across domains. To assess base model performance, we use sentence continuation tasks where the model selects the better continuation between two options. As shown in Figure 6, we use the SlimPajama test set as the correct sentence and have the model generate the incorrect alterna- tives. First, \"LLM Filtering\" selects consecutive sentences with a causal relationship, then \"Split\" each sentence into two parts: the first as the question, and the second as the right option. For this large-scale filtering, we use GPT-4o-mini. \n\nNext, \"LLM Rewrite\" generates incorrect options. Since LLMs struggle to create incorrect but related content, we follow Deng et al. (2024) to first generate a reasonable continuation, then modify it to create an incorrect version. Finally, \"LLMs Verification\" scores both options and filters out cases where the scores don't match the true answers. We use GPT-4o in these procedures. We further select questions with the largest score differences. In total, we select 400 questions per domain. A case is shown in Table 5. \n\nQuestion If the latter described their efforts to adapt to European conditions, Right Option the former insisted that Muslims adhere to proper canons of learning and textual interpretation. \n\nWrong Option it also highlighted the resilience and ingenuity that had brought them this far despite challenges. \n\nTable 5: Case For the sentence continuation task. \n\nOur method outperforms the baselines consistently across the domains. As shown in Table 4, our method consistently outperforms the Constant and Sheared Llama scheduling strategies, achieving better downstream performance in most domains. This demonstrates that our approach not only improves learning in hard domains but also preserves or enhances performance in others. Additionally, our benchmark results align well with the average performance across 15 tasks, performing slightly below the open-source Sheared Llama 2.7B model. This consistency confirms the quality and reliability of our benchmark and results.",
            "score": 0.43246391386643196,
            "section_title": "Robustness across Different Domains",
            "char_start_offset": 18322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2279
                },
                {
                    "start": 2280,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 985,
                    "end": 1003,
                    "matchedPaperCorpusId": "273655004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06719970703125
        },
        {
            "corpus_id": "269982223",
            "title": "Large language models for sentiment analysis of newspaper articles during COVID-19: The Guardian",
            "text": "Sentiment analysis has extensive applications across various domains, including social media analysis, public sentiment monitoring, product review analysis, and customer feedback analysis [19]. It has been demonstrated to be used not only for social media and advertising industry [19,21,22], but can also be used to study public behaviour in pandemic such as the COVID-19 [10] and compare texts to evaluate the quality of translations from lowresource languages [23]. \n\nRecurrent Neural Networks (RNNs) [24,25] are deep learning models for processing sequential data and are prominent for processing text data and performing tasks such as sentiment analysis. The Long Short-Term Memory (LSTM) network [26,27] is an advanced RNN for modelling data that have longterm dependencies. In the last decade, variants of the LSTM network have emerged, and the encoder-decoder LSTM [28] became a prominent variant for processing and modelling text data. The Transformer model is an advanced encoder-decoder LSTM with attention mechanism [29] that was designed for NLP tasks, particularly for tasks involving sequential data. It is currently the mainstream framework for building LLMs. The Transformer model lays the foundation of the BERT (Bidirectional Encoder Representations from Transformers) [30] model that is a pre-trained LLM used for various NLP tasks, including sentiment analysis [31]. RoBERTa (Robustly optimised BERT approach) [32] is a variant of BERT that utilises larger-scale datasets for pre-training, longer sequence lengths, and dynamic masking. These improvements enhanced the RoBERTa model performance and generalisation capability for sentiment analysis. BERT-based LLMs have been widely applied in sentiment analysis of content on social media. Wahyudi and Sibaroni [33] used BERT for sentiment analysis of TikTok based on the reviews obtained from the Google Play Store and the Apple App Store. Kikkisetti et al. [34] utilised LLMs for identifying emerging coded antisemitic hate speech on extremist social media platforms.",
            "score": 0.4322844497623408,
            "section_title": "Introduction",
            "char_start_offset": 2075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "16768404"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "16768404"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "246663281"
                },
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "220886688"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "233324490"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "257496742"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "160013244"
                },
                {
                    "start": 1382,
                    "end": 1386,
                    "matchedPaperCorpusId": "233940858"
                },
                {
                    "start": 1781,
                    "end": 1785,
                    "matchedPaperCorpusId": "255586845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05877685546875
        },
        {
            "corpus_id": "257766452",
            "title": "Koala: An Index for Quantifying Overlaps with Pre-training Corpora",
            "text": "Large Language Models (LLMs) have achieved state-of-the-art results in NLP and on many benchmarks have reached the performance ceiling (Chowdhery et al., 2022). This evergrowing success has been facilitated by the algorithmic and computational progress in scaling up model sizes (Wei et al., 2022a;Chowdhery et al., 2022;Zhang et al., 2022;Brown et al., 2020), integrating human feedback (Ouyang et al., 2022), adopting modes of instructional inference at both zero-or few-shot settings (Chen et al., 2022;Kojima et al., 2022;Wei et al., 2022b;Nye et al., 2021), as well as the ability of feeding them massive volumes of free text during pre-training. \n\nRecent works exhibit various cases which highlight the sensitivity of downstream behaviour of LLMs (and their smaller variants) to the frequency of observed overlap between pre-training corpora and test set (Carlini et al., 2022;T\u00e4nzer et al., 2022;Razeghi et al., 2022;Magar and Schwartz, 2022;Lewis et al., 2020). In the generative setting, several issues such as hallucination (Dziri et al., 2022), undesired biases (Feng et al., 2023;Kirk et al., 2021), or toxicity (Gehman et al., 2020) have been attributed partly or fully to the characteristics of the pre-training data, while a parallel line of works have emphasised on the positive role of filtering the pre-training data for safety and factual grounding (Thoppilan et al., 2022). \n\nThe above observations are not a comprehensive list but echo the undeniable role of pre-training data in how these models would function in practice. Understanding the limitations imposed by pre-training data would also lead to more informed algorithmic and computational innovations (Collier et al., 2022). However, these forensic studies are done either at a small scale or by using surrogate sources such as web search hit counts.",
            "score": 0.4318781763041288,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 359,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "247450508"
                },
                {
                    "start": 1034,
                    "end": 1054,
                    "matchedPaperCorpusId": "248227301"
                },
                {
                    "start": 1073,
                    "end": 1092,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 1092,
                    "end": 1110,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "221878771"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3017578125
        },
        {
            "corpus_id": "270562468",
            "title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models",
            "text": "In this section, we will outline the process of building Aquila-Med-cpt from a massive general pretraining database, including how to collect medicalrelated corpora from pre-training databases, rulebased quality filtering methods, and LLM-based data quality selection methods.It is worth noting that this method is also applicable to any domain.Data Classification Since Aquila's general pretraining (Aquila-pt) corpus comes from multiple data sources, it already contains domain information.However, since there is no clear domain label, we first need to classify the data to make full use of the medical domain data in Aquila-pt.Specifically, we first randomly sample 20k data from Aquila-pt and use the upsampling method to ensure that the ratio of Chinese and English being 1:1.Based on the sampled data, GPT-4 is used to perform two rounds of domain label annotation to improve label accuracy.The data with different labels twice are removed, and finally 17k seed data is retained.Then we design a classifier using the Bert-based multilingual pre-training model.The parameter settings are as follows: batch-size is 64, learning rate is 2e-5, training epoch is 10, and the optimal checkpoint is selected according to the accuracy.The medical domain F1 of the classifier can reach 84%.Rule-based Data Quality Filtering Since Aquila-pt mostly comes from web pages, the overall quality is not high.In order to remove the noise data, we design a rule-based data filtering solution, including rules for removing data with insufficient tokens, excessive special characters, toxic content, and private information.LLM-based Data Quality Filtering By sampling and checking the data after rule filtering, we found that there exists the following problems: (1) the data contains advertising and marketing information, which will greatly affect the output preference of the trained model; (2) the data contains grammatical errors, semantic incoherence, splicing of multiple unrelated content, image and video editing information, etc.We believe that such data is not beneficial for model training because the model cannot obtain much valuable information through autoregressive learning.",
            "score": 0.43169357192719754,
            "section_title": "Data Collection and Decontamination",
            "char_start_offset": 5216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 492
                },
                {
                    "start": 492,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3447265625
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Table 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.",
            "score": 0.4308158478055716,
            "section_title": "Effect of training size",
            "char_start_offset": 9068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019683837890625
        },
        {
            "corpus_id": "261049411",
            "title": "How Good Are LLMs at Out-of-Distribution Detection?",
            "text": "However, these studies have mainly focused on evaluating the OOD detection performance of small-scale encoder-based Transformers, such as RoBERTa and BERT. \n\nRecently, large language models (LLMs) have made significant strides in various cognitive tasks, yet their capabilities on OOD detection remain largely unexplored. Unlike relatively small-scale PLMs used by prior studies, LLMs often display notable differences. In particular, the majority of previously prominent PLMs utilized for OOD detection adopt the encoder-based architecture, such as BERT and RoBERTa. These models are predominantly designed with a pre-training objective that focuses on sentence classification. However, recent LLMs (Touvron et al., 2023a;Zeng et al., 2022;Du et al., 2022;Chowdhery et al., 2022;Chung et al., 2022) exclusively adopt an autoregressive training objective during pre-training. Consequently, the hidden states of LLMs are specialized for next token prediction, which could influence their performance in OOD detection. Moreover, previous works test changes in OOD detection when adapting PLMs to downstream tasks through discriminative finetuning, even for decoder-based models (Cho et al., 2023). However, a more intuitive approach is to probe the pre-training knowledge of LLMs through generative fine-tuning, which better aligns LLMs' pre-training objective with downstream tasks. Thus, it is imperative to extensively investigate the OOD detection capabilities of LLMs to gain deeper insights into their potential and limitations. \n\nThis paper aims to fill this gap by offering a comprehensive and structured assessment of OOD detection with LLMs across varying scales (ranging from 7B to 65B). Notably, our evaluation process is specifically designed to consider the scaling laws of LLMs with commonly utilized OOD detection detectors, ensuring broader and more generalized findings. In summary, our analysis has revealed the following new insights: \n\n1. Discriminative vs. generative fine-tuning. We have observed that generative fine-tuning demonstrates greater resilience to the issue of ID overfitting when compared to discriminative finetuning.",
            "score": 0.43069439415349575,
            "section_title": "Introduction",
            "char_start_offset": 1876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "266418196",
            "title": "Automating the Identification of Feedback Quality Criteria and the CanMEDS Roles in Written Feedback Comments Using Natural Language Processing",
            "text": "This study focused on the use of an LLM to train the four classification models. In view of this, a transformer language model was selected. Transformer language Van Ostaeyen et al. Perspectives on Medical Education DOI: 10.5334/pme.1056 models are neural networks that are currently becoming the most common architecture to develop and train language models. They quickly replaced earlier machine learning architectures for completing various NLP tasks [38]. \n\nThe state-of-the-art transformer language model BERT (Bidirectional Encoder Representations From Transformers) [39] has been revolutionary for NLP research and applications. It has been proven that transformer models with an encoder architecture*, such as BERT, are particularly suitable for sentence classification [40]. These neural architectures are first initialised using large-scale unlabelled corpora (pre-training) and subsequently trained on labelled data for a specific task (fine-tuning). To pretrain BERT, Devlin and colleagues [39] used the BookCorpus (800 M words) and English Wikipedia (2.500 M words). Since BERT was trained on English data, it is hardly directly applicable to other languages. A multilingual BERT model exists, but language-specific models are expected to result in superior performance [41]. Therefore, monolingual models with the BERT architecture were developed for different languages (e.g., Italian: AlBERTo [42]). \n\nAs our research data were written in Dutch, we used two Dutch BERT models: BERTje [41] and RobBERT [43], released in 2019 and 2020 respectively. The primary distinction between these two models is the size of the corpus utilised for pre-training. The corpus used to train BERTje was about 12 GB, compared to a corpus size of 39 GB to train RobBERT [43]. Additionally, the BERTje model was pre-trained with both Masked Language Modelling* (MLM) and Next Sentence Prediction* (NSP) tasks, while pre-training for RobBERT remained restricted to the MLM task.",
            "score": 0.4305840476485372,
            "section_title": "MODEL SELECTION",
            "char_start_offset": 10022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "238744157"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "204914950"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "210714061"
                },
                {
                    "start": 1766,
                    "end": 1770,
                    "matchedPaperCorpusId": "210714061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043365478515625
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "The optimal data strategy is dependent on the dataset at hand and we cannot give universally applicable filtering recommendations. By looking into C4 and OSCAR, we have covered two of the most commonly used English text datasets. Our findings on both datasets were overall in agreement with each other. We have highlighted some of the differences, such as deduplication being more effective on OSCAR due to it being more noisy than C4. Further, we have focused on large-scale pre-training datasets. There is a lot of research on the optimal fine-tuning dataset and methodology for LLMs [94,62,128,85,117,68,116,134,115,36,125,72,63]. More investigations of resolving data-constraints when fine-tuning LLMs may be of interest for future work. \n\nOther modalities or architectures Our work focuses on text datasets and uses the GPT transformer architecture [88]. Prior work has experimented with many variations to the GPT or transformer architecture [27,104,96], as well as scaling laws for non-text datasets [1]. Overall, variations of the GPT or transformer architecture have proven very robust and generalizable to other domains [43,18,70,66,71,104,26]. Nonetheless, it may be of interest for future work to test the applicability of our findings in this work to different data modalities or model architectures. \n\nOther strategies There are numerous strategies to solve data constraints not covered in this work that are worth exploring. Like we have shown for Python, future research may consider to what extent augmenting with a natural language (e.g. Chinese) improves performance in another language (e.g. English) and what is the best language to choose [61,124]. Similarly, while we have looked at deduplication and perplexity filtering, other filtering strategies, such as popularity-based filters [3,133] and toxicity filters [33,38,64,87,86] are worth exploring.",
            "score": 0.4305681945139532,
            "section_title": "Other datasets",
            "char_start_offset": 56255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1134,
                    "end": 1137,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 1840,
                    "end": 1843,
                    "matchedPaperCorpusId": "250243785"
                },
                {
                    "start": 1843,
                    "end": 1846,
                    "matchedPaperCorpusId": "258832491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.268310546875
        },
        {
            "corpus_id": "204008158",
            "title": "Investigating the Effectiveness of Representations Based on Word-Embeddings in Active Learning for Labelling Text Datasets",
            "text": "We evaluate the performance of active learning using Linear-SVM models 3 which have been shown empirically to perform well with high dimensional data [32]. We tune the hyper-parameters of the SVM models every 10 iterations (i.e. 100 labels requested). We preprocess text data by converting to lowercase, removing stop words, and removing rare terms (for the whole dataset, word count less than 10 or document frequency less than 5). 4 We set the number of topics to be used by LDA 5 to 300 following [8]. For FastText, we adopt two versions of FastText: 1) we use the pre-trained subword FastText (FT) model trained with Wikipedia (300 dimensions). 6 2) continually training original FastText model with local corpus (without label information) which is referred to as FastText trained (FT T). For BERT, we use bert-large-uncased model (1,024 dimensions). 7 Though the original paper [20] suggests using the vector of \"[CLS]\" token added in the head of a document as a document-level representation, in practice, researchers find that averaging the word embeddings of the document is an equivalent, sometimes, greater option8 . Since BERT is configured to take as input a maximum of 512 tokens, we divided the long sequence with L length into k = L/511 fractions, which is then fed to BERT to infer the representation of each fraction (each fraction has \"[CLS]\" token in front of 511 tokens, namely, 512 tokens in total). The vector of each fraction is the average embeddings of words in that fraction and the representation of the whole text sequence is the mean of all k fraction vectors. It should be noted that we do not use any label information for finetuning any model to ensure fair comparisons. A summary of the dimensionality of each representation is given in Table 1. \n\nIn uncertainty sampling, the most uncertain examples are equivalent to those closest to the class separating hyper-plane in the context of an SVM [33]. In the information density selection strategy, we use entropy to measure the \"informativeness\" and all parameters are set following [29].",
            "score": 0.4302936859928581,
            "section_title": "Model Configuration",
            "char_start_offset": 13606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 1928,
                    "end": 1932,
                    "matchedPaperCorpusId": "7806109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018768310546875
        },
        {
            "corpus_id": "272968899",
            "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
            "text": "An LLM as a Quality Evaluator. As language models like BERT and BART have advanced, evaluation methods have evolved. Traditional metrics have been supplemented by approaches such as BERTScore [46], which uses contextual embeddings to assess semantic similarity effectively, and BLEURT [39], which leverages human-annotated data for more accurate text quality predictions. The advent of GPT-4 introduced more sophisticated methods like GPTScore [19], utilizing its zero-shot capabilities for versatile text evaluation, and LLM-Eval [28], which employs a single LLM prompt to robustly evaluate conversational quality, correlating strongly with human judgments. Additionally, the LLM-as-a-judge approach [47] finds GPT-4 approximating human evaluations with high agreement rates, offering a scalable alternative where traditional methods are impractical. These advancements highlight the increasing use of LLMs not only in generating text but also in evaluating it, a direction that aligns with this work and enhances the scalability, versatility, and precision of assessment methods. Human Preferences. The rule-based method was manually developed based on observations of the training set and incorporating human preferences via the   () reward model described in section 4.2.1. Our training set comprised 810 samples, with 296 of these identified as having paper structures. To detect these structures seen in 296 syntheses, we implemented a rule-based binary classifier using a set of predefined rules. This classifier utilized a paper structure vocabulary comprising 17 terms and 9 reference identifier regular expressions. The vocabulary-based identifier achieved an F1-score of 67.80%, while the reference identifier detection reached an F1-score of 90.87%. When both identifiers were combined, the final model achieved an overall F1-score of 98.67%, with only 10 misclassified cases. Further analysis involved annotating outputs from the SFT and Vanilla models and comparing them to our model's results. Human annotations indicated that the Vanilla model had 31.62% of its syntheses with paper structure, while the SFT model had 2.99%.",
            "score": 0.4301167663125612,
            "section_title": "Discussion",
            "char_start_offset": 47040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "127986044"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "215548699"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06622314453125
        },
        {
            "corpus_id": "258668089",
            "title": "A Review on Deep-Learning-Based Cyberbullying Detection",
            "text": "The Facebook research team created FastText [36] as a library. It has two uses. The first is efficient word representation learning, and the second is sentence categorization. The method supports both supervised and unsupervised representations of words and sentences. On Facebook, if anyone puts a status update on their Facebook timeline about purchasing a bike, after a few moments, they may see an ad related to bikes. Facebook uses the text data to serve you better ads by using FastText. Figure 6 shows the word embedding for 3-gram in FastText. 4.1.7. BERT Bidirectional Encoder Representation from Transformers (BERT) [37] is based on the transformer architecture. Wikipedia (2500 million words) and Book Corpus (800 million words) are part of a vast corpus of unlabeled text that has been pre-trained. The success of BERT mainly lies in the pre-trained step, which has been trained with a large number of texts. The BERT model gathers information from both the left and right sides of a sentence context. Figure 7 shows an example of bi-directionality. If we forecast the nature of a word by choosing other words to its left or right sides, by selecting both sides of this term, BERT precisely predicts the exact meaning. The transformer is the foundation of the BERT architecture. BERT has two variants: BERT base and BERT large. BERT base has 12 layers of transformer blocks, 12 attention heads, and 110 million of parameters. BERT large, on the other hand, has 24 transformer layers, 16 attention heads, and 340 million parameters. Figure 8 shows the architecture of BERT base and BERT large. Figure 9 and 10 shows the input representation of BERT model and output as the embedding of BERT base respectively. BERT has been pre-trained on two natural language challenges. The first is Masked Language Modeling (MLM), which studies word relationships. The second is Next Sentence Prediction (NSP), which is necessary to comprehend how sentences relate to one another. There are some variations of BERT that are also used for the cyberbullying detection problem. (3) train on longer sequences; and (4) dynamically change the masking pattern over the training data.",
            "score": 0.42999440307837344,
            "section_title": "FastText",
            "char_start_offset": 23151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043609619140625
        },
        {
            "corpus_id": "270765610",
            "title": "L3i++ at SemEval-2024 Task 8: Can Fine-tuned Large Language Model Detect Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text?",
            "text": "LMs Two Transformer-based models have been fine-tuned as sequence classifiers, namely RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020).RoBERTa is a Transformers model pretrained on a large corpus of English data in a self-supervised fashion using a masked language modeling (MLM) objective.Meanwhile, XLM-R is a multilingual version of RoBERTa that was pretrained on 2.5TB of filtered CommonCrawl data containing 100 languages.These models are also suggested as the baseline methods from SemEval-2024 Task 8 organizers.\n\nLLMs Given the recent success of the LLMs architectures for solving downstream NLP tasks, we decided to follow the same vein to build our classifier.As such, we start with LLaMA-2 (Touvron et al., 2023), an LLM model pre-trained for the sequence classification task, using its corresponding tokenizer to preprocess data.We then fine-tune the model on the training subset of collected data.Consequently, the fine-tuned model is used for inference on the testing subset.Finally, the obtained classification scores are evaluated against the ground truth.LLaMA-2 To make the comparison comparable, we fine-tuned LS-LLaMA6 (version: LLaMA-2-7bhf ) using the HuggingFace Transformers PyTorch Trainer class with the same configuration: batch size = 16, learning rate = 1e-5, and the number of epochs = 10 with max length = 256 and Lora = 12.",
            "score": 0.4298312696660797,
            "section_title": "Model-based models",
            "char_start_offset": 9191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 145,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03302001953125
        },
        {
            "corpus_id": "259274808",
            "title": "Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models",
            "text": "Large Language Models. Several language models have been proposed recently. One of the earliest pre-trained language models is ELMo which was proposed to model the word context (Peters et al., 2018). ELMo learns the word context by pre-training a two-layer bidirectional LSTM network on large data and fine-tuning it on downstream tasks. BERT followed this learning strategy with a Transformer model pre-trained on large datasets (Devlin et al., 2019). The performance of BERT outperformed other models on several downstream tasks. This learning paradigm motivated researchers to propose either new architectures (e.g., BART (Lewis et al., 2019) and GPT-2 (Radford et al., 2019)) or enhanced pre-training techniques (Liu et al., 2019;Sanh et al., 2021;Wang et al., 2022). Scaling language models in terms of model size or data used for model pre-training has shown its effectiveness in several downstream tasks (Zhao et al., 2023). This led to the introduction of the \"large language models (LLM)\" term. These models are trained on large datasets and usually have billions of parameters. Such LLMs showed a better performance compared with the smaller models with similar architectures and pre-training tasks (e.g., GPT-3 (Brown et al., 2020) vs GPT-2). Recently, a significant number of LLMs have been introduced, such as GPT-3, LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), BLOOM (Muennighoff et al., 2022), andChinchilla (Hoffmann et al., 2022). ChatGPT2 is one of these LLMs that was developed based on the GPT model series (GPT-3.5 and GPT-4) and showed a powerful performance with dialogue tasks.",
            "score": 0.42908709662767364,
            "section_title": "Related Work",
            "char_start_offset": 3841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 198,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 656,
                    "end": 678,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 752,
                    "end": 770,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054290771484375
        },
        {
            "corpus_id": "273186794",
            "title": "SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness",
            "text": "The performance on SkillMatch for each method is shown in Table 2. Our contrastive learning objective for the Sentence-BERT model significantly outperforms all other models. This shows the effectiveness of modeling skill relatedness based on skill co-occurrence in job ads. The domain-specific static vectors outperform their generic versions, despite being trained on fewer data. Notably, the generic Word2Vec model performs much worse compared to the fastText model. Further analysis reveals that this is due to words not being modeled in the generic Word2Vec model. The number of skill phrases in SkillMatch without a Word2Vec representation drops from 193 to 36 when using the domain-specific model. fastText does not suffer from this out-of-vocabulary issue by design. Table 2. Comparison of model performance for the skill relatedness task. For the static vector models, the domain-specific versions refer to the models trained from scratch on job ads. For Sentence-BERT, the domain-specific model refers to its fine-tuned variant based on the proposed self-supervised skill co-occurrence objective. \n\nDespite its popularity, the pretrained Sentence-BERT model is outperformed by generic fastText vectors. We assume this is partly because Sentence-BERT models are trained to represent full sentences, which might harm their ability to meaningfully represent shorter phrases. As such, our fine-tuning strategy for Sentence-BERT effectively serves both purposes of learning domain-specific skill relations, as well as learning to effectively represent shorter phrases instead of full sentences. \n\nFinally, qualitative analysis of the skill relatedness scores allows inspecting the effect of our proposed Sentence-BERT fine-tuning approach. Fig. 1 displays one such example: a heatmap of similarity scores between two clusters of IT skills, being three web development related (HTML, CSS, JavaScript) and three machine learning related skills (ML, Deep Learning, NLP ).",
            "score": 0.42907989415453895,
            "section_title": "Results",
            "char_start_offset": 10096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014556884765625
        },
        {
            "corpus_id": "269614115",
            "title": "LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection",
            "text": "Neither LingM nor LLM utilizes the merits of both linguistics and ML well at the same time.For the former, basic ML algorithms learn from the linguistic data only.The latter explores the most advanced ML systems to process text data but without specific linguistic knowledge usage.We study the integration LingL here.\n\n1) Experimental Setup: We perform an experimental study for our proposed LingL which integrates linguistic knowledge and LLM.We consider 11 transformer-based LLM, of which CT-BERT [5], BERTweet [16], and Twitter-RoBERTa [17] are specialised for working with the informal language used on social media, while the others (e.g., ALBERT [18], BERT [12], DistilBERT [19], Longformer [20], RoBERTa [13], XLM [21], XLM-RoBERTa [22], and XLNet [23]) are trained on more formal data sources.For each tested LLM, we train the model for 10 epochs.We configure early-stopping for training to avoid over-fitting and validate the detection performance every 600 training steps.We also conduct five independent runs for each experiment and report the statistics.\n\n2) LLM Performance: We aim to analyze LingL's performance and we start with the baseline LLM without linguistic knowledge usage.We report the results in Table III and we have the following observations.LLM is effective for our application.All the tested models can push the detection error rate below 4%.The best-performed LLM is CT-BERT [5], which is reasonable as the LLM is specialized for COVID-related content.Note that our LingM introduced above achieves about 20% error rate in most runs and LLM's performance improvement is significant.The implication here is that linguistic knowledge's effectiveness in detecting fake news is not on par with advanced ML models like LLM.However, we would like to mention that ML relies substantially on training data.When the real-world data to be analyzed does not share much similarity with the training data, the ML performance may drop significantly.Linguistic knowledge offers scientific support to LingML and helps ML models to be less sensitive to specific data distributions and to be more generalizable in practice.",
            "score": 0.42902189614549235,
            "section_title": "C. LingL Performance Analysis",
            "char_start_offset": 20712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 91,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 317
                },
                {
                    "start": 319,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1066
                },
                {
                    "start": 1068,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1307
                },
                {
                    "start": 1307,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1612
                },
                {
                    "start": 1612,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 502,
                    "matchedPaperCorpusId": "218665481"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 539,
                    "end": 543,
                    "matchedPaperCorpusId": "225062026"
                },
                {
                    "start": 652,
                    "end": 656,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 721,
                    "end": 725,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1406,
                    "end": 1409,
                    "matchedPaperCorpusId": "218665481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0305023193359375
        },
        {
            "corpus_id": "271855741",
            "title": "LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification",
            "text": "The overall performance of all the compared baselines and LaiDA is compared on the test set and reported in Table 2. LaiDA achieves the highest accuracy compared to several baselines, demonstrating its effectiveness. Interestingly, the application of pre-trained BERT [2] for Euclidean distance similarity retrieval did not outperform random retrieval. The emphasis on similarity and diversity in selecting in-context examples might explain LLM-BERT's underwhelming performance versus LLM-Random, potentially stemming from Euclidean distance's inadequacy for similarity assessment and the broader diversity inherent in LLM-Random. Notably, LLM-Mix-tuning, leveraging multi-task learning, exhibits commendable results, partially attributed to the metaphor analysis prior to component selection, which augments the model. Nevertheless, despite doubling the training data, the enhancement is modest.",
            "score": 0.4288495674068459,
            "section_title": "Main Results",
            "char_start_offset": 14485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 896
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12127685546875
        },
        {
            "corpus_id": "216914042",
            "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
            "text": "Table 4 shows the overall results for the three tasks. As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true. This also reinforces the challenging nature of the benchmark, as even BERT is far from the human annotator performance (estimated on 85.3% for accuracy). Clearly, the definition information is more helpful than the hypernyms for BERT, while the combination of both attains the best overall results. \n\nIt is also remarkable the high recall of BERT, in contrast to the precision. This is mainly attributed to the domain-specific datasets as we are going to analyze below. Domain-Specific Results. Table 5 presents the results split by domain. Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes. In fact, perhaps surprisingly, the results on the domain-specific domains do not drop substantially with respect to the WNT/WKT test set, even though the training and development instances came from the same source (i.e. WordNet and Wiktionary). This can be attributed to the fact that specific domains highly constrain the set of possible senses for a word, resulting in an easier WSD classification task (Magnini et al., 2002). On the other hand, WordNet is known to be quite fine-grained (e.g., the noun run has 16 different senses in WordNet, plus many other senses including run as a verb). \n\nIn general, the BERT model can attain a very high recall in the domain-specific datasets, while the precision is still not too low. This model can be helpful in a retrieval setting where the recall may be relevant -for example, when the data is going to pass to a human than can filter.",
            "score": 0.42861345801608935,
            "section_title": "Results",
            "char_start_offset": 24967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1413,
                    "matchedPaperCorpusId": "7857920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.016021728515625
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "Recent years have witnessed the rapid progress of large language models (LLMs). The models, such as GPT-3 [5], BLOOM [6], LLaMA [7], Falcon [3], PaLM [8] and GPT-4 [9], become more and more powerful, even performing better than humans in some natural language understanding and generation tasks. During the development, it is evident that the scale and quality of pre-training data play a crucial role on LLM's capability. A large-scale and high-quality dataset is the foundation of LLMs and is the source of all the LLM's amazing capabilities. \n\nIn order to expedite the research on LLMs, several large-scale datasets have been made publicly available in recent years, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4]. Previous studies usually collect the raw texts at first from various sources, such as Wikipedia, GitHub, ArXiv, Stack Exchange, and CommonCrawl, in which CommonCrawl data often accounts for the vast majority. Then, handcrafted rules are designed to filter out the raw data in three steps: extracting the data in the language of interested, filtering out the noisy texts with language-specific rules and data deduplication. It should be noted that, most of the previous studies mainly focus on the collection of English-centered texts, and there is lack of a complete tool-chain for extracting clean data centered in other languages, e.g. Chinese. Furthermore, previous work usually directly release the final data, without giving the fine-grained information of the text, such as the quality of each text, limiting the potential that assists LLM researchers to re-filter the data according to their desired quality threshold. \n\nTo address these problems, in this paper we introduce a new complete tool-chain EvalWeb, which could extract high-quality Chinese texts from raw web data. The whole process can be divided into two parts. The first part is similar to previous studies and mainly utilizes manually designed rules to filter out explicit noisy data, generating the initial Chinese clean data. This part processes the web texts with two modules: preparation module and processing module.",
            "score": 0.4285314130309516,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28125
        },
        {
            "corpus_id": "269804455",
            "title": "Take Its Essence, Discard Its Dross! Debiasing for Toxic Language Detection via Counterfactual Causal Effect",
            "text": "We conducted various baselines to mitigate lexical bias in TLD models, including both weakening lexical prior with original data and unbiased training with data filtering.For methods of weakening lexical prior, we selected Masking (Ramponi and Tonelli, 2022), LMixin (Swayamdipta et al., 2020) and InvRat (Chuang et al., 2021).We evaluated the methods on three commonly used PLMs, including BERT-base (Devlin et al., 2019), RoBERTabase and RoBERTa-large (Liu et al., 2019).For methods of data filtering, two data filtering methods were introduced and applied on RoBERTa-large, including AFLite (Bras et al., 2020) and DataMaps (Swayamdipta et al., 2020).The filtered training data size is 33% of the original training set.\n\nIn the experimental stage, we trained TLD models on the training set of (Founta et al., 2018) and saved their best parameters on the validation set.Then we respectively evaluated the performance of models on the test set and adversarial dataset.To further prove the generalization of our CCDF, we also evaluated its performance on balanced training data filtered by AFLite and DataMaps, respectively.We did not perform any pre-processing of the datasets, or any hyperparameter search, but followed all the settings in Zhou et al. (2021b).We use one NVIDIA GeForce RTX 3090 to perform the experiments.AdamW is used as the optimizer for model training.",
            "score": 0.42851705491131675,
            "section_title": "Baselines and Experimental Settings",
            "char_start_offset": 17096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 722
                },
                {
                    "start": 724,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1374
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 258,
                    "matchedPaperCorpusId": "250390692"
                },
                {
                    "start": 267,
                    "end": 293,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 305,
                    "end": 326,
                    "matchedPaperCorpusId": "235422377"
                },
                {
                    "start": 401,
                    "end": 422,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 594,
                    "end": 613,
                    "matchedPaperCorpusId": "211076210"
                },
                {
                    "start": 627,
                    "end": 653,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 796,
                    "end": 817,
                    "matchedPaperCorpusId": "3640499"
                },
                {
                    "start": 1242,
                    "end": 1261,
                    "matchedPaperCorpusId": "231741340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.082275390625
        },
        {
            "corpus_id": "277620832",
            "title": "Concept-based Rubrics Improve LLM Formative Assessment and Data Synthesis",
            "text": "Currently, two main approaches are widely used in research on automated answer assessment: supervised training of end-to-end neural networks, often involving fine-tuning of PLMs, and in-context learning with large language models. Among supervised approaches, SFRN [23] employs relational networks [34] to learn the relationship between an input question, student answer, and reference answers; BERT [10] was used as the initial encoder. AsRRN [22] is a more advanced relation network using recurrence [28] along with a contrastive loss function [19]. Fine-tuned PLMs like RoBERTa [26] have also been shown to achieve strong performance [33,35]. Ega\u00f1a et al. [13]   model fine-tuned on the MNLI dataset [39], to explore few-shot strategies on the SemEval dataset [12]. \n\nWith the emergence of large language models (LLMs), evaluating their effectiveness in student answer assessment tasks has become a popular research area. Several studies have explored this topic. For example, one study [8] compared the performance of fine-tuned BERT models with in-context learning using few-shot examples from LLMs, including GPT [6] and LLaMA [37], on shortanswer datasets. Another study [33] evaluated the performance of LLMs against RoBERTa-MNLI on the RiceChem dataset [33]. Additionally, a recent study [18] demonstrated that incorporating XML [5] with answer samples as input to GPT-4 can achieve strong performance on the SemEval dataset. \n\nPrevious research has explored using synthetic data generated by LLMs to support text classifier training. For example, one study demonstrated that LLMs can generate synthetic data from diverse domains to improve the generalization ability of NLI models [16]. Another work [21] investigated the effectiveness of mixing LLM-generated synthetic data with a few real-world samples in many common text classification tasks. Some research highlighted that highly subjective text classification tasks, such as humor detection, gain little benefit from LLM-generated data [24].",
            "score": 0.42846627239226115,
            "section_title": "Related Work",
            "char_start_offset": 2551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "243865267"
                },
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "4412459"
                },
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 444,
                    "end": 448,
                    "matchedPaperCorpusId": "266177185"
                },
                {
                    "start": 502,
                    "end": 506,
                    "matchedPaperCorpusId": "46208513"
                },
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "222291214"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "269293382"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "259263861"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 763,
                    "end": 767,
                    "matchedPaperCorpusId": "4897445"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "270765547"
                },
                {
                    "start": 1119,
                    "end": 1122,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "269293382"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "269293382"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "262045158"
                },
                {
                    "start": 1338,
                    "end": 1341,
                    "matchedPaperCorpusId": "6735362"
                },
                {
                    "start": 1691,
                    "end": 1695,
                    "matchedPaperCorpusId": "267750265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "251432469",
            "title": "An End-to-End Named Entity Recognition Platform for Vietnamese Real Estate Advertisement Posts and Analytical Applications",
            "text": "As discussed in Section III-B, ensuring data quality before feeding to the NER module is one of our top priorities. Fig. 3 indicates that the noisy records make up a significant amount of the sample in the working dataset. The ratio of noisy records on the three subsets, including training: development: test, is 35.97%, 35.91%, and 26.46%, respectively. Through experiments, we found that in addition to pre-processing the data, removing low-quality (noise) posts also plays an essential role in improving the performance of our proposed system (as mentioned in Section V-C). \n\nWe investigate several algorithms, including transformersbased pre-trained language models and deep neural network models to classify noisy records. In this study, state-ofthe-art models such as Text-CNN [21], Bi-LSTM [22], BERT cased [23], BERT uncased [23], XLM-R base [24], XLM-R-Vietnamese base [25], PhoBERT base [16], and PhoBERT large [16] are deployed and fine-tuned to choose the most appropriate model for our noise filtering task. \n\nWe strive to improve the performance of the noise filtering module as it will directly impact our primary NER task. Therefore, we also evaluated the performance of the models 15 https://airflow.apache.org/ 16 https://www.postgresql.org/ T i -> Flagged as not-noisy record end end end procedure we conducted using the precision, recall, and F1-macro metrics. Table 2 presents the experimental results obtained with the noise filtering module. Since the given dataset has a significantly imbalanced noise ratio, the average macro F1-score, the harmonic mean of precision and recall, is the most suitable measure for this task. The results show that PhoBERT large is the best performing model with a 0.8697 F1 score. Furthermore, PhoBERT large could execute parallel processing on words, minimizing vanishing gradients and assisting the model in learning more effectively. \n\nBy employing PhoBERT large , the best-performing model we found in this study, we construct a simple and efficient procedure for the noise filtering task, presented in Procedure 1.",
            "score": 0.4284503359300057,
            "section_title": "B. NOISE FILTERING",
            "char_start_offset": 18149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "18375389"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "211677475"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "211677475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034027099609375
        },
        {
            "corpus_id": "277621873",
            "title": "DynClean: Training Dynamics-based Label Cleaning for Distantly-Supervised Named Entity Recognition",
            "text": "The comparison against the baselines is summarized in Table 2. As our method, we present the results of BERT and RoBERTa trained on the best-cleaned version of each dataset, i.e., the CoNLL03 (D \u2032 4 ), Wikigold (D \u2032 4 ), WNUT16 (D \u2032 4 ), and BC5CDR (D \u2032 3 ). We first observe that LLMs still face certain challenges in the NER task, with a significant performance gap compared to fully supervised RoBERTa. This result is similar to previous study (Qin et al., 2023), indicating that LLMs still face significant challenges in NER task. Our approach also outperforms all LLMs across four datasets. We further found that LLMs outperform KB-Matching, indicating the feasibility of using LLMs for distant annotation. We have provided a related discussion in Appendix E. Despite using less data for model training, we outperform all previous approaches with relatively balanced precision and recall scores. When comparing with Conf-MPU BERT , BERT trained on the cleaned CoNLL03 and BC5CDR achieves an F1 score improvement of 4.54% and 4.28%, respectively. Our method also outperforms Conf-MPU LBiLSTM , which is enhanced with lexicon feature engineering, with the F1 score improvement of 3.68% and 1.43%, respectively. Compared to MProto BERT , we have 4.12% F1 improvement on the CoNLL03 dataset. As for the CReDEL which also focuses on enhancing the data quality of distantly supervised datasets, our approach outperforms it by 2.9% F1 score on the BC5CDR dataset. Our approach also outperforms multiple baselines that utilize negative sampling techniques. The main reason our method achieves significant improvement is that it is able to remove a large number of false negatives and positives from the distantly annotated data. We further test the effectiveness of using our cleaned datasets with the self-training method, and the results can be found in Appendix F.",
            "score": 0.4273923933985552,
            "section_title": "Baselines Comparison",
            "char_start_offset": 19865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1229248046875
        },
        {
            "corpus_id": "276249986",
            "title": "Do we really have to filter out random noise in pre-training data for language models?",
            "text": "Large language models (LLMs) have fundamentally transformed the research landscape in natural language processing. The remarkable performance of these autoregressive models is largely attributed to pre-training on extensive datasets, which are gathered by crawling text from the whole Internet. Given the sheer volume of these datasets, they inevitably encompass a wide variety of noise [1,2]. Consequently, it is imperative to understand its impact, as the quality of pre-training data plays a decisive role in the effectiveness of LLMs [3]. Previous studies [4,5] highlight that low quality data can significantly decrease the knowledge capacity and performance of a model, and recursive training of LLMs with synthetic data can lead to model collapse [6,7]. However, little attention has been paid to the impact of random noise within datasets. Due to anti-crawling mechanisms, decoding errors, and huge amounts of unmaintained websites, the raw data obtained through web crawling inevitably contains a substantial amount of random noise [8][9][10]. Although theoretically it may not be challenging to remove such noise, practical limitations Figure 1: Overview of the study and methodology. (a) The common scenario in which a GPT model, pre-trained on filtered data P c , demonstrates robust performance. (b) When the pretraining dataset is contaminated with random noise P n , the resultant language model may exhibit unpredictable behavior. (c) Our approach follows [17] and focuses on the effective fine-tuning of black-box noisy models for downstream tasks P d . \n\nOn the other hand, further experiments reveal that a noisy model that exhibits a lower NTP loss experiences a 1.5% decrease in accuracy on downstream tasks. This indicates that performance on downstream tasks is not solely rely upon the NTP loss. Given the common practice of fine-tuning a pre-trained foundation model rather than undergoing a full pre-training process from scratch, we opt to follow the work of [17] by exploring how to efficiently fine-tune language models only using extracted features for downstream tasks when the pre-training data and model weights are not accessible, which reflects real-world application scenarios for LLMs. To mitigate the potential adverse effects of noise, we propose a novel plug-and-play Local Gradient Matching (LGM) loss.",
            "score": 0.42730367164977623,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 390,
                    "end": 392,
                    "matchedPaperCorpusId": "264803575"
                },
                {
                    "start": 560,
                    "end": 563,
                    "matchedPaperCorpusId": "262825178"
                },
                {
                    "start": 563,
                    "end": 565,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 757,
                    "end": 759,
                    "matchedPaperCorpusId": "269005923"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "239885872"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "255186162"
                },
                {
                    "start": 1472,
                    "end": 1476,
                    "matchedPaperCorpusId": "263310780"
                },
                {
                    "start": 1986,
                    "end": 1990,
                    "matchedPaperCorpusId": "263310780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "235624271",
            "title": "Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature",
            "text": "To evaluate the accuracy of the process, we used SemRepDS to process 300 randomly selected abstracts followed by human evaluation to identify any potential systematics errors. Since the relations from the 300 abstracts were used to train the BERT filter, we also sampled 50 abstracts containing 224 relations before filtering. The same informatics students (AB and YZ) assessed the correctness of these relations based on the source sentences. We then report the precision of semantic relations before and after filtering.",
            "score": 0.4268795056642635,
            "section_title": "Evaluating semantic relations generated by SemRepDS",
            "char_start_offset": 10926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0108184814453125
        },
        {
            "corpus_id": "266167098",
            "title": "Frequency Balanced Datasets Lead to Better Language Models",
            "text": "A number of works have studied the impact of word frequency on different aspects of LLM and, in particular, on the quality of the delivered representations. Kassner et al. (2020) have studied BERT models and possible memorization based on token frequency, demonstrating that if a token appears fewer than 15 times, the model will disregard it, while a token that appears 100 times or more will be predicted more accurately. Zhou et al. (2022) demonstrated that high frequency words and low frequency words are represented differently by transformer LLM, in particular by BERT. Wei et al. (2021) found evidence that BERT models struggle to correctly predict a verb when it belongs to a set of word pairs (S-V), or bigrams, that appears less than 10 times or more than 100 times in the training corpus. Razeghi et al. (2022) also examined the strong impact of the frequency of terms in pre-training data, although in a GPT model, and found a strong correlation of token frequencies with the resulting model performance. Puccetti et al. (2022) provided evidence directly linking the presence of outlier dimensions (Kovaleva et al., 2021) with the frequency of tokens in the pre-training data. Outlier dimensions are those dimensions at which parameters with an unusually high magnitude -in terms of standard deviation--are found consistently across the model layers. Puccetti et al. (2022) results suggest that outliers are due to the highly skewed token frequency distribution in the textual pre-training data. Moreover, Fuster-Baggetto and Fresno (2022) demonstrated that the non-fine-tuned BERT models contain token frequency biases that distort the embedding representation space. The distortion leads to the already observed poor performance on semantic tasks because of what Gao et al. (2019) already diagnosed as anisotropy in transformer LMs or the representation degeneration problem: the embeddings concentrate in a hypercone instead of occupying the whole space. Consistently, Ethayarajh (2019) found that randomly sampled words tend to be highly similar when measured by cosine similarity.",
            "score": 0.4264073939773747,
            "section_title": "Related Work",
            "char_start_offset": 4310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 442,
                    "matchedPaperCorpusId": "248693662"
                },
                {
                    "start": 577,
                    "end": 594,
                    "matchedPaperCorpusId": "237513713"
                },
                {
                    "start": 801,
                    "end": 822,
                    "matchedPaperCorpusId": "256631096"
                },
                {
                    "start": 1018,
                    "end": 1040,
                    "matchedPaperCorpusId": "248986509"
                },
                {
                    "start": 1111,
                    "end": 1134,
                    "matchedPaperCorpusId": "235313996"
                },
                {
                    "start": 1364,
                    "end": 1386,
                    "matchedPaperCorpusId": "248986509"
                },
                {
                    "start": 1519,
                    "end": 1552,
                    "matchedPaperCorpusId": "256631003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14306640625
        },
        {
            "corpus_id": "273654923",
            "title": "Improving Model Evaluation using SMART Filtering of Benchmark Datasets",
            "text": "In this section, we present the results of applying SMART filtering on popular datasets. Our methodology is generic and can be applied to various dataset types. We demonstrate its efficacy using three well-established multiple-choice question answering datasets: ARC (Clark et al., 2018), MMLU (Hendrycks et al., 2021a) and CommonsenseQA (Talmor et al., 2019). We release our code and provide SMART filtering version of each dataset. 1 o identify easy and data contaminated examples, we evaluate 7 open source models from the top of the Open LLM leaderboard (Hugging- Moreover, as these models came from different organizations, they were likely trained on different datasets. Ideally, this minimizes the risk of bias towards any particular model when identifying easy and contaminated examples (see Section 5.1 for more discussion on this). We used open-source models, as we aim to develop a methodology that is accessible to any AI practitioner with sufficient computation resources, thereby avoiding the substantial closed-source API costs. \n\nThe effectiveness of SMART filtering is shown in Table 1 as the percentage of examples filtered for ARC,2 MMLU, and CommonsenseQA. 3 The change in model performance after SMART filtering is shown in Table 2. Notably, the relative ranking of models remains almost unchanged across both the original and filtered datasets. However, our methodology significantly reduces the dataset sizes, achieving up to a 68.9% reduction for ARC, which directly correlates to a commensurate reduction in computational costs for model evaluation. \n\nThe impact of SMART filtering varies significantly across datasets. In ARC, 64.4% of the examples were classified to be 'easy', compared to only 27.6% in CommonsenseQA. In MMLU, a significant 4.37% of the dataset was found to be data contaminated, while CommonsenseQA had none. Additionally, 12.57% of ARC and 4.56% of MMLU examples were identified as similar. These findings highlight that different datasets might suffer from different types of low-quality examples. For MMLU, a widely accepted dataset for measuring LLM performance, we were able to filter out 43% of the dataset while maintaining almost the same model ranking.",
            "score": 0.4264048847443388,
            "section_title": "Results: Efficiency & Informativeness",
            "char_start_offset": 15197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 359,
                    "matchedPaperCorpusId": "53296520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381591796875
        },
        {
            "corpus_id": "261452986",
            "title": "Examining the Potential of Generative Language Models for Aviation Safety Analysis: Case Study and Insights Using the Aviation Safety Reporting System (ASRS)",
            "text": "This section provides an overview of LLMs and their pre-training and fine-tuning processes and highlights their significance as foundational models. Furthermore, it explores recent advancements in the field, with a particular focus on generative language models and their relevance to the present work on aviation safety. \n\nLLMs, such as Bidirectional Encoder Representations from Transformers (BERT) [9] and the Generative Pre-trained Transformer (GPT) family [10-13], LLaMA [14], Llama 2 [15], LaMDA [16], and PaLM [17], are advanced NLP systems that have shown remarkable capabilities in understanding and generating human-like text. These models are built upon Transformer neural networks with attention mechanisms [18]. Neural networks, inspired by the functioning of the human brain, consist of interconnected nodes organized in layers that process and transform input data. The attention mechanism enables the model to focus on relevant parts of the input during processing, effectively capturing dependencies between different words and improving contextual understanding. Transformers' neural architectures have been particularly successful in NLP tasks, providing an efficient and effective way to process text. \n\nThe training process of LLMs involves two main stages: pre-training and fine-tuning. During pre-training, the model is exposed to vast amounts of text data from the Internet or other sources, which helps it learn patterns, grammar, and semantic relationships. This unsupervised learning phase utilizes large corpora of text to predict masked words, allowing the model to capture the linguistic nuances of the language. The pre-training stage often involves a variant of unsupervised learning called self-supervised learning [19], where the model generates its own training labels using methods such as Masked Language Modeling (MLM), Next-Sentence Prediction (NSP), generative pre-training, etc. This enables the model to learn without relying on human-annotated data, making it highly scalable. Unsupervised pre-training typically uses general-purpose texts, including data scraped from the Internet, novels, and other sources. This helps overcome the non-trivial cost and limits on scaling associated with data annotation. \n\nAfter pre-training, LLMs are often fine-tuned on specific downstream tasks.",
            "score": 0.4263963867178604,
            "section_title": "Large Language Models (LLMs) as Foundation Models",
            "char_start_offset": 11575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2248
                },
                {
                    "start": 2251,
                    "end": 2326
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1748,
                    "end": 1752,
                    "matchedPaperCorpusId": "2309950"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08038330078125
        },
        {
            "corpus_id": "252081257",
            "title": "BertSRC: transformer-based semantic relation classification",
            "text": "While applying an effective masking methodology for input sentences and downstream layers to the model is crucial, it is also important to select a pre-trained language model that best fits our tasks and data as the base model. Therefore, we compared the performance of various existing pre-trained BERT models for our dataset with the same hyperparameters specified in Table 4, setting two masked sentence input and two-token layer for all models. The models were evaluated using fivefold cross-validation with all of the train, validation, and test sets combined (Table 6). \n\nComparisons have shown that PubMedBERT models pre-trained on abstracts from PubMed and full-text articles from PubMedCentral performed better than others. This confirms that when building a downstream model using a pre-trained language model, the data used for pre-training should be homogeneous to those used for fine-tuning. Therefore, PubMedBERT, which performed the best on our PubMed datasets, will be used as the base model in later experiments.",
            "score": 0.42615063639116585,
            "section_title": "Semantic relation classification model Performance comparison between pre-trained models",
            "char_start_offset": 44274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240936279296875
        },
        {
            "corpus_id": "268889543",
            "title": "Sailor: Open Language Models for South-East Asia",
            "text": "The data quality is crucial during continual pre-training.We employ aggressive cleaning parameters, extended filtering list for each language, and multi-round deduplication.Consequently, even though we started with well-curated open datasets, e.g., MADLAD-400 clean set (Kudugunta et al., 2023), we still further removed 31.11% in data cleaning and 11.16% in data deduplication.By extensively filtering out noisy, harmful, and duplicated content, we are able to significantly improve the efficiency of the pre-training process and the stability of the optimization procedure.Furthermore, LLMs are less prone to memorization issues when training data has undergone thorough deduplication (Lee et al., 2022).Figure 2: Initially Sailor models were trained on 200B tokens using a greedy tokenization strategy.Subsequently, they were fine-tuned using BPE dropout for an additional 2B tokens, with the dropout rate as 0.1.As observed, BPE dropout improves the robustness.Figure 3: We initially pre-train a 120M model using a corpus of 20B tokens focusing on English.Subsequently, we continually pre-train the model using a mixed corpus comprising both English and SEA languages.Each data point here corresponds to a different configuration of data mixture and learning rate.As indicated, under a fixed total tokens, there is a trade-off between the model's performance on English and SEA languages.",
            "score": 0.42589955239968696,
            "section_title": "Aggressive Data Cleaning and Deduplication",
            "char_start_offset": 5454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 58,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1172
                },
                {
                    "start": 1172,
                    "end": 1268
                },
                {
                    "start": 1268,
                    "end": 1392
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 294,
                    "matchedPaperCorpusId": "261682406"
                },
                {
                    "start": 687,
                    "end": 705,
                    "matchedPaperCorpusId": "235829052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264404296875
        },
        {
            "corpus_id": "278339044",
            "title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code",
            "text": "Large Language Models (LLMs) have demonstrated remarkable zero-shot and few-shot capabilities across diverse tasks, yet their proficiency in mathematical reasoning and program synthesis remains constrained by the quality of pre-training corpora. Existing public datasets for specialized domains, such as The-Stack-v1 and v2 for code [1,2] and Finemath-4+ for mathematics [3], rely primarily on rule-based extraction from web crawls (e.g., CommonCrawl) [4] or model-based scoring to filter lowquality samples. However, these approaches often retain noisy, redundant, or stylistically inconsistent data, limiting their effectiveness-particularly in the growing trend of multi-stage pretraining (or mid-training) aimed at enhancing mathematical reasoning and program synthesis (e.g., OLMo 2 [5], Nemotron-H [6], and Phi-4 [7]). For instance, as shown in Figure 1, continual pre-training of Llama-3.1-8B [8] on The-Stack-v1/v2 typically maintains baseline performance on benchmarks like HumanEval and HumanEval+ but struggles to achieve significant gains due to unaddressed data quality issues. Unlike prior methods that focus on filtering or preserving original samples, we propose rewriting pre-training corpora to eliminate noise and redundancy, delivering high-quality, self-contained data that enables efficient model learning. \n\nThis paper introduces two openly licensed datasets under the Llama 3.3 Community License, designed to advance LLM capabilities in code generation and mathematical reasoning. SwallowCode (\u224816.1 billion tokens) refines Python snippets from The-Stack-v2 through a novel four-stage pipeline: sequential syntax validation, pylint-based style filtering, and a two-stage LLM rewriting process that enforces style conformity and transforms snippets into algorithmically efficient, self-contained examples. By rewriting, rather than merely filtering, we eliminate noise and redundancy that persist in datasets like Stack-Edu, providing high-quality data that drives rapid accuracy improvements (Figure 1).",
            "score": 0.4257106392534541,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 2027
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437744140625
        },
        {
            "corpus_id": "225094545",
            "title": "Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports",
            "text": "We evaluate the performance of the machine learning algorithms by using precision, recall, and F1-measure metrics. The summary results are calculated using microand macro-based measures. Early experiments using Word2Vec embeddings [9] and SVM classifier showed unsatisfactory performance compared to fastText embeddings and GNB classifier. Thus, these results are omitted from Table 2. \n\nThe results in Table 2 show that a simple terminology-based pre-processing step leads to slight improvements over the baseline with micro F1 of 0.59 in comparison to baseline micro-F1 of 0.57. Despite the small amount of data, we found that corpus trained embedding provide a notable advantage over pre-trained embeddings in the classifiers performance. fastText classifier outperformed GNB model, especially when domain-based embeddings were used. A non-verbatim example of a sentence where fastText model, based on corpus-trained embeddings performs better than pre-trained embedding models is: 'The police received information that the subject was selling crack'. A potential reason for fastText to classify correctly this sentence versus the classifiers using pre-trained embeddings is that the word 'crack' has the meaning of a 'drug' in the reports. However, this is not the widely accepted meaning for this word and thus it cannot be interpreted correctly by pre-trained models. The GNB based on pre-trained BERT model outperforms the classifiers based on pre-trained embeddings, however it does not lead to improvements over the domain-based models. Fine-tuning BERT is the best performing classifier with micro-F1 of 0.64 and macro-F1 of 0.59 which gives 0.5 improvement over the baseline. The improvement in the results achieved by fine-tuning BERT indicate the importance of adapting even the more context-aware pre-trained language models to the specific domain, especially when the domain contains highly specialised language. Further, the poor performance of classifiers based on pre-trained word models shows the lack of transferability of pre-trained embeddings for a highly specialised domain such as the safeguarding reports.",
            "score": 0.42567402178099994,
            "section_title": "Results",
            "char_start_offset": 10054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "16447573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0120086669921875
        },
        {
            "corpus_id": "248967058",
            "title": "Discovering trends and hotspots of biosafety and biosecurity research via machine learning",
            "text": "To demonstrate the effectiveness of our method, we selected Random, LDA and PLSA as the baseline models to compare with LDAPR. The results of perplexity(\u00d710  We find that LDAPR shows significant and consistent improvement over the other methods. \n\nTo verify the effectiveness of each component of LDAPR, we conduct ablation experiments on the language models (BERT [33], BioBERT [36] and pretrained FastText [8]) and clustering algorithms (K-means [34,67]; we choose the point closest to the center as the center point) . The results of perplexity (\u00d710 3 ) of -Kmeans, -Pretrained FastText, -BERT, -BioBERT and LDAPR are 0.386, 0.242, 0.324, 0.207 and 0.125. Although BERT is a great language model, it does not perform well due to the constraints of the task and corpus. And compared with other methods, our model achieved the best performance. The experimental results demonstrate that the selection of each component is reasonable and effective.",
            "score": 0.425478304554777,
            "section_title": "Performance comparison",
            "char_start_offset": 29313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 948
                }
            ],
            "ref_mentions": [
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 408,
                    "end": 411,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "11291770"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "212637907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017242431640625
        },
        {
            "corpus_id": "269149352",
            "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
            "text": "Additionally, researchers have examined the usefulness of newer models like GPT-3.5 and evaluated the effectiveness of CoT in improving annotation quality (He et al., 2024). LLMbased annotation has also been extended to lowresource languages where hiring human annotators is challenging (Choi et al., 2024). \n\nIn this work, we introduce a novel approach to filtering noisy documents from multi-document summarization dataset by extending cost-efficient LLM-based annotation beyond traditional data annotation tasks. By leveraging the capabilities of LLMs, our study facilitates real-world dataset cleansing, enhancing the quality of existing datasets. This attempt is noteworthy as it broadens the scope of LLM applications, offering effective solutions for improving dataset quality and streamlining its cleansing process, minimizing reliance on human annotations.",
            "score": 0.42511068756547754,
            "section_title": "Related Work",
            "char_start_offset": 6760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 865
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 172,
                    "matchedPaperCorpusId": "257805087"
                },
                {
                    "start": 287,
                    "end": 306,
                    "matchedPaperCorpusId": "267547969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251708984375
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "Current high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency. Notably, most of our large-scale experiments are conducted in a distributed manner using a Spark4 cluster. For data preprocessing, we implement several key steps, including removing redundant empty lines and extra spaces, stripping diacritics, and converting all English text to lowercase. Additionally, we adopt the DeepSeek-V2 tokenizer (Liu et al., 2024), which outperforms traditional tokenization methods (such as space-based tokenization for English and Jieba5 for Chinese). Meanwhile, we preserve structural information such as \\n, \\t, and \\r. To ensure dataset integrity and balance, the final training set comprised 600K samples, evenly split between positive and negative examples. \n\nFor training details, we trained a fastText classifier with a vector dimension of 256, a learning rate of 0.1, a maximum word n-gram length of 3, a minimum word occurrence threshold of 5, and a total of 3 training epochs. Additionally, during inference, we maintain the default threshold of 0.5 to simplify operations and ensure experimental consistency, avoiding the need for additional tuning steps.",
            "score": 0.4245897601978594,
            "section_title": "FastText-based Quality Filtering",
            "char_start_offset": 14544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1932
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3515625
        },
        {
            "corpus_id": "261100951",
            "title": "Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature",
            "text": "Recent advancements in NLP, including data acquisition [15,77], model architecture development [97,105], and large-scale pretraining [31,46,58,72,76,104], have significantly propelled the field forward. For example, we considered Large Language Models (LLMs) for NER tasks, inspired by the effectiveness of models like ChatGPT and GPT4 in zero-shot and few-shot learning scenarios [47,57]. However, despite their capabilities, [47,57] noted that domain-specific NER tasks often perform better with supervised learning models than with current LLMs. Therefore, we adopted a supervised learning approach, fine-tuning transformer-based pre-trained language models such as BERT [31], Albert [56], Distil-BERT [83], and SciBERT [12]. Our selection criterion for the final model was based on its performance in our NER task and efficiency at inference. \n\nThen, we applied a percentile-based filtering process to the corpus of publications based on the ESG and DLT classified named entities within the corpus. This method selects publications with substantial DLT and ESG content, using a threshold percentile to exclude marginally relevant papers. Seed papers were included to maintain foundational references. The filtering is represented as: \n\nWhere  is the final set of papers,   is an individual paper,  (  ) is the ESG and DLT content density of a paper,  is the threshold percentile, and  is the set of seed papers. Content density  (  ) for each paper is calculated by the ratio of the number of DLT and  ESG relevant named entities (  and   ) to the total number of tokens  (  ) (Equation 2): \n\nOur filtering methodology involved: \n\n(1) Excluding papers below the 10 th percentile (see Figure 4a) in the total token count to avoid distortions due to PDFto-text conversion issues or unusually short papers (e.g., below 100 tokens). ( 2) Computing DLT content density and retaining papers above the 90 th percentile (see Figure 4b), ensuring a strong focus on DLT topics. (3) Filtering for at least the 70 th percentile (see Figure 4c) in ESG content density to confirm relevance to ESG.",
            "score": 0.4243438625520351,
            "section_title": "Mapping taxonomies using NLP",
            "char_start_offset": 10358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 95,
                    "end": 99,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "203626972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053497314453125
        },
        {
            "corpus_id": "202889310",
            "title": "Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs",
            "text": "Translation quality is very vulnerable to out-of-domain data and noisy data (Chen et al., 2016;Niehues & Waibel, 2010;Koehn & Schroeder, 2007;Eidelman et al., 2012). Since dataset we use comes from a different domain from WMT2017, it would be less favorable if we directly apply a trained model on this large but noisy dataset to the WMT17 test set. One option to handle this issue is to do data selection and filtering before training (Belinkov & Bisk, 2017;Hassan et al., 2018). Hassan et al. (2018) proposes to first learn sentence representations from the provided training data in WMT2017 (target domain), and then reject training instances if their similarity with the target sentences is below a prespecified threshold. We did not choose this method for two reasons: (1) Hassan et al. (2018) select different training instances for different target domains. This means every time we encounter a new domain, we have to retrain the model; (2) the value of data filtering threshold is crucial but hard to decide: it is unrealistic to tune its value since each threshold value corresponds to a different filtered training set, on which a brand new model has to be trained. \n\nInspired by large-scale pretraining strategies such as BERT (Devlin et al., 2018) and Elmo (Peters et al., 2018), we used a pipelined approach: we first pretrain an NMT model on the massive dataset, and then fine-tune the model on the training set of the target domain. This strategy naturally addresses the aforementioned two caveats of data pre-filtering approach: the pretrained model can be easily adapted to a set of training data of an arbitrary domain, and it is no longer needed to find the optimal data selection threshold. Moreover, since the model will be fine-tuned at later stage, it is more immune to noise in the data at the first stage. We combine the WMT 20M data with our new 40B data to do the pretraining, and then fine-tune the model on the WMT 20M data.",
            "score": 0.42390002123016934,
            "section_title": "Pipelines: Pretraining and Fine-tuning",
            "char_start_offset": 9040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 118,
                    "matchedPaperCorpusId": "9973121"
                },
                {
                    "start": 118,
                    "end": 142,
                    "matchedPaperCorpusId": "9536363"
                },
                {
                    "start": 142,
                    "end": 164,
                    "matchedPaperCorpusId": "218515250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0699462890625
        },
        {
            "corpus_id": "271270519",
            "title": "Dynamic Sentiment Analysis with Local Large Language Models using Majority Voting: A Study on Factors Affecting Restaurant Evaluation",
            "text": "Although the model using DNN achieved high accuracy as expected, it is noteworthy that most of them did not surpass that of Llama 2. Additionally, among the DNN models, the highest accuracy was shown when BERT was used for acquiring word embeddings (Model 6).Although Model 7 employs more complex architectures of LSTM and Multihead Attention, Model 6 with FFNN was superior in prediction.This confirms the significant improvement of BERT over word2vec and FastText.Second, all rule-based models (Models 9-11) showed better performance than the machine-learning model (Model 12).This result can be attributed to the relatively short length of the review texts and low text complexity.Alternatively, this could be due to the poor generalization performance of the machinelearning methods to imbalanced data, as restaurant ratings tend to gather at extreme values, such as 1 or 5.However, all the models scored above the chance level.\n\nFinally, a comparison of the processing times among the LLMs showed that only Model 1, with 70B parameters, required a significantly longer processing time.Compared with Model 2, Even when analyzing only 1,000 samples, a total durations differ in more than 16 hours for a 2.4% improvement in the prediction error.From a practical standpoint, this increase in the processing time for improvement of the accuracy cannot be considered a reasonable trade-off.It is true that Model 2 has a lower accuracy than Model 1; however, its RMSE of 0.551 indicates that the model still effectively predicts sentiment.It accurately estimates higher values as high and lower values as low, with the fastest processing speed among the proposed LLMs.Therefore, this study employs Llama 3 with 8B parameters and quantized in 3-bit (Model 2) for Study 2 and 3.",
            "score": 0.42352466028282165,
            "section_title": "Study 1: Effects of Model Scale and Precision on",
            "char_start_offset": 29172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 932
                },
                {
                    "start": 934,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1537
                },
                {
                    "start": 1537,
                    "end": 1666
                },
                {
                    "start": 1666,
                    "end": 1774
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0063629150390625
        },
        {
            "corpus_id": "266167098",
            "title": "Frequency Balanced Datasets Lead to Better Language Models",
            "text": "Historically, using more training data has been considered a key to improving the performance of natural language processing (NLP) tools. The same seems to be true for transformer-based large language models (LLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) that exhibit their best performance when being trained with really massive quantities of texts. However, the question arises whether more training data is the only factor for improving results or whether specific characteristics of the provided texts have a particular impact on the learning efficiency of these powerful models. Recent research (Wei et al., 2021;Razeghi et al., 2022) has found evidence about the impact of the absolute frequency of pretraining data tokens in the prediction capacities of the model. Besides, high-frequency terms have proved to be behind some phenomena related to the geometry of the representations causing problems for so called semantic tasks that rely on similarity assessments (Ethayarajh, 2019;Fuster-Baggetto and Fresno, 2022). In this paper, we report on our experiments to better understand the impact of token frequency in the pre-training data on language model (LM) quality, in terms of perplexity, and on the quality of the representations that these models provide for fine-tuning. Our ultimate goal is to study to what extent the quantity of pre-training data could be lessened, since this could be of great importance for training LM for low-resource languages (LRLs). \n\nTexts are known to follow Zipf's law with some tokens occurring very frequently, some occurring with medium frequency, and many tokens that rarely occur forming a long tail. Different disciplines have addressed the problems of using long-tailed training data where some elements appear on most of the data but most of the elements are under-represented. Under-sampling and oversampling techniques to directly adjust the number of examples by removing and adding data respectively have been proposed (Cui et al., 2019;Raunak et al., 2020). In this context, we propose an under-sampling algorithm whose objective is to balance token frequency by removing the sentences in which high-frequency tokens (and bigrams) occur, thus improving the estimation of model probabilities for low-frequency tokens.",
            "score": 0.4232786877588424,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 249,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 626,
                    "end": 644,
                    "matchedPaperCorpusId": "237513713"
                },
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "256631096"
                },
                {
                    "start": 997,
                    "end": 1015,
                    "matchedPaperCorpusId": "202120592"
                },
                {
                    "start": 1015,
                    "end": 1048,
                    "matchedPaperCorpusId": "256631003"
                },
                {
                    "start": 2019,
                    "end": 2039,
                    "matchedPaperCorpusId": "222290834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2420654296875
        },
        {
            "corpus_id": "265456109",
            "title": "Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat Corpus Generation and Evaluation",
            "text": "In this section, we elaborate on our methodology for assessing the quality of the generated corpus through the utilization of BERT-based language models (LLMs). Our approach seeks to evaluate the linguistic coherence and contextual understanding of the corpus sentences, ultimately contributing to the enhancement of the performance of the fine-tuned LLM on downstream tasks. To initiate the quality assessment process, each sentence in the corpus is subjected to tokenization. Subsequently, each token within the sentence is individually masked, one at a time. This meticulous token-wise masking enables a granular examination of the language model's predictive capabilities when faced with the absence of specific tokens. The probability distribution over the masked tokens is computed using an Italian BERT model. For every masked token T i , the model generates a probability distribution conditioned on the entire sentence, excluding the masked token p(T i |T j ) \u2200j \u0338 = i \n\nThis step aims to capture the model's prediction of the masked token given the contextual information provided by the rest of the tokens in the sentence. The Non-Negative Log-Likelihood (NLL) is employed as a metric to quantify the quality of the sentence. For each masked token in the sentence, the NLL is computed based on the actual token observed in the corpus. Mathematically, the NLL is defined as: \n\nHere, the negative logarithm of the predicted probability is taken, and the mean NLL across all masked tokens within the sentence is calculated. This process is repeated for each sentence in the corpus. The rationale behind utilizing NLL as a quality assessment metric lies in its sensitivity to the predictability of the masked tokens. Higher NLL scores indicate greater uncertainty and randomness in the language model's predictions, suggesting potential issues such as sentence malformation or lack of coherence. Conversely, lower NLL scores signify more predictable and contextually aligned predictions, indicative of well-structured and semantically sound sentences. The evaluation process is depicted in Figure 3 The computed NLL scores are utilized to evaluate and compare the quality of the original corpus, the filtered corpus, and the generated one. This comparative analysis serves as a crucial step in gauging the efficacy of our corpus generation methodology and its impact on language model performance.",
            "score": 0.4231403358082564,
            "section_title": "Quality Assesment",
            "char_start_offset": 14488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2404
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1561279296875
        },
        {
            "corpus_id": "270711474",
            "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale",
            "text": "We fine-tuned this linear regressor on 410,000 of our Llama 3 synthetic annotations for 20 epochs with a learning rate of 3e-4 (while keeping the embedding and encoder layers frozen). We selected the checkpoint with the highest F1 score on the held-out validation set containing the remaining 50,000 samples, treating Llama 3 annotations as ground-truth. After training, we rounded the model's output scores to integers from 0 to 5. We then used fixed thresholds to classify whether a given document from FineWeb was educational. We investigated the impact of using different thresholds for the filtering and ultimately chose a minimum threshold of 3 for FineWeb-Edu, which ultimately gave the best trade-off between performance on knowledge and reasoning intensive benchmarks and the performance on other benchmarks like HellaSwag [66]. With a threshold of 3, the model achieved an F1 score of 82% on the validation set, indicating strong performance in distinguishing high-quality educational content. \n\nApplying the classifier to the 15 trillion tokens of FineWeb required 6,000 H100 GPU hours. \n\nTo confirm the effectiveness of education filtering at a larger scale, we conducted a larger ablation training a 1.71B model on 350 billion tokens, similar to the FineWeb filtering ablations mentioned above. As shown in Fig. 10 and Fig. 11 On MMLU, FineWeb-Edu can match the final performance of Matrix with almost 10x fewer tokens, demonstrating the effectiveness of classifiers trained on LLM annotations for large-scale data filtering. Additional evaluation plots can be found in Appendix F.2.",
            "score": 0.4230979656845969,
            "section_title": "FineWeb-Edu",
            "char_start_offset": 26465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07611083984375
        },
        {
            "corpus_id": "265308678",
            "title": "Oasis: Data Curation and Assessment System for Pretraining of Large Language Models",
            "text": "The training dataset utilizes well-known high-quality sources like Wikipedia, WebText (Radford et al., 2019), and Books as positive samples, meanwhile extensive various web pages as negative samples. A neural model like fastText or BERT trained on this dataset is responsible for scoring documents in quality (Touvron et al., 2023;Brown et al., 2020;Gao et al., 2020). (Wu et al., 2021) also consider utilizing a model to classify advertisements. However, the neural filter could bias the filtered corpus due to the positive source of the training set (Dodge et al., 2021;Welbl et al., 2021). Some works (Du et al., 2022;Wei et al., 2023) organize the positive sample in a mixture of various sources of high-quality texts to decrease the bias from the positive source. (Penedo et al., 2023) abandoned the neural filter on account of worrying about undesirable biases. \n\nDeduplication: Repetition contents in pretraining corpus are proven to hurt the LLM's performance (Lee et al., 2021). Corpus cultivation pipelines focus on fuzzy deduplication at the document (Zhang et al., 2022;Biderman et al., 2023;Rae et al., 2021) or line (Touvron et al., 2023) levels. These large-scale deduplication processes are mainly based on the locally sensitive hash algo-rithm (Rajaraman and Ullman, 2011) by means of collision to calculate similarity. (Sun et al., 2021) calculate the MD5 of the three longest sentences to match the redundancy documents. (Penedo et al., 2023) further construct a huge prefix array to drop duplicate substrings. These methods significantly improve the efficiency of the deduplication process, but the memory requirements become a barrier to deployment on a larger scale.",
            "score": 0.4228704798079592,
            "section_title": "Data Cultivation",
            "char_start_offset": 5459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 108,
                    "matchedPaperCorpusId": "259064270"
                },
                {
                    "start": 331,
                    "end": 350,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 604,
                    "end": 621,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1082,
                    "end": 1104,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.151123046875
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "Public datasets like Common Crawl2 and multilingual datasets (Kreutzer et al., 2022) usually contain low-quality data that hampers the training of LLMs. Hence, existing works usually perform quality filtering using hand-crafted heuristics (Yang et al., 2019;Raffel et al., 2020;Nijkamp et al., 2022), a trained classifier (Brown et al., 2020;Gao et al., 2020;Du et al., 2022;Touvron et al., 2023a;Wettig et al., 2024), metric thresholding (Wenzek et al., 2020;Muennighoff et al., 2023) or combinations of these techniques. Besides instance-level filtering, embedding clustering is also adopted to filter one cluster at a time (Kaddour, 2023). \n\nDespite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023b). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration.",
            "score": 0.4227106823263234,
            "section_title": "Quality Filtering",
            "char_start_offset": 10941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1685
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 84,
                    "matchedPaperCorpusId": "232307434"
                },
                {
                    "start": 239,
                    "end": 258,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 278,
                    "end": 299,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 322,
                    "end": 342,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 359,
                    "end": 375,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 439,
                    "end": 460,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 771,
                    "end": 794,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 983,
                    "end": 1003,
                    "matchedPaperCorpusId": "268096300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "248780176",
            "title": "CML: A Contrastive Meta Learning Method to Estimate Human Label Confidence Scores and Reduce Data Collection Cost",
            "text": "The experiment is conducted with the following steps: 1) In a classification task, we train CML to learn the confidence score from the training set, i.e. to predict the correct goal category. 2) We set a threshold to filter good quality data based on the learned confidence score (the threshold is treated as a hyperparameter). 3) Using the filtered training data set, we train a separate BERT-based error detection model. 4) We train the same error detection model using only biased training data. 5) We run the above two models on the same test set and compare their performance. For model evaluation, commonly used metrics such as accuracy, precision, recall, and F1 score are used. Table 2 shows the model performance. The result obtained after CML filtered data outperforms the result obtained using biased training data for all metrics. In addition, based on the last two columns, with the data filtering setup, we select most correct labeled instances and very few incorrect instances. Figure 3 indicates the sensitivity experiment result of threshold for filtering. Figure 3 illustrates that threshold is sensitive for all evaluation metrics.",
            "score": 0.4227036012882279,
            "section_title": "Experiment setup",
            "char_start_offset": 18971,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205810546875
        },
        {
            "corpus_id": "219417426",
            "title": "Comparing the Quality and Speed of Sentence Classification with Modern Language Models",
            "text": "The LMs selected for testing are presented in Table 1. We chose classic LMs that provide static word-level embeddings, including Glove [1], FastText [21], a size optimized LM called BPEmb [22] that operates on a sub-word level, and a variety of recently developed deep learning LMs that create high-quality contextualized embeddings. \n\nThe complex architecture of recent models, such as BERT, Robustly Optimized BERT Pretraining Approach (RoBERTa), and XLNet, allows for choosing from many layers (heads) of the LM model that output usable token embeddings. The quality of these embeddings varies across the LM layers and even across language tasks [23]. For these LMs, we compared embeddings from the default output layer and mixing embeddings from all available output layers, as proposed in a technique called scalar mix [23].",
            "score": 0.4222164922523632,
            "section_title": "Selection of the LMs",
            "char_start_offset": 7878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "1957433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038330078125
        },
        {
            "corpus_id": "266999123",
            "title": "Exploring the Potential of Large Language Models in Self-Adaptive Systems",
            "text": "To lay the foundation for the subsequent discussions, we briefly introduce LLM focusing on core aspects. \n\nArchitectures of LLMs and Pre-training. LLMs refer to transformer-based, large-scale language models that contain billions of parameters and are pre-trained on massive text data [26]. For instance, GPT-3 has 175 billion parameters and uses preprocessed 570GB of text data for training. The architecture of LLMs can be categorized into three main types: (i) encoder-only, in which the encoder encodes the input text into a hidden representation to capture the relationships between words and the overall text context; (ii) encoder-decoder, in which the encoder processes the input into a hidden space, and the decoder translates the abstract representation from hidden space into relevant text expression; and (iii) decoder-only, which is used in models like GPT, gradually generates the output text by sequentially predicting the subsequent tokens. LLMs are pre-trained using datasets that include web pages, books, conversational text, and program code. The data undergoes pre-processing, such as quality filtering, de-duplication, to improve data quality, and privacy data reduction to enhance privacy. \n\nFine-tuning of LLMs. After pre-training, fine-tuning is a technique that uses application-specific customized datasets to provide additional training of the model, thereby improving its performance on the specific task [23]. A representative example is OpenAI's Codex [5] that is based on GPT-3 and specifically fine-tuned for code generation. In addition to traditional fine-tuning, LLMs require two new types of tuning: (i) Instruction tuning to enhance LLM ability to accurately comprehend and execute tasks as directed by (user-given) natural language instructions; and (ii) Alignment tuning to align LLM more closely with human values such as helpfulness and honesty. \n\nUtilizing LLMs. After pre-training and optional fine-tuning, LLMs can be utilized to solve various tasks when given suitable prompts.",
            "score": 0.4222029528198241,
            "section_title": "BACKGROUND: LARGE LANGUAGE MODEL",
            "char_start_offset": 3285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "259601654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1474609375
        },
        {
            "corpus_id": "276647515",
            "title": "NeoBERT: A Next-Generation BERT",
            "text": "Data has emerged as one of the most critical aspects of pre-training, and datasets with increasing quantity and diversity are frequently released. NeoBERT takes advantage of the latest datasets that have proven to be effective. \n\nDataset BERT and NomicBERT were pre-trained on two carefully curated and high-quality datasets: Wikipedia and BookCorpus (Zhu et al., 2015). As Baevski et al. (2019) demonstrated that increasing data size can improve downstream performance, Liu et al. (2019) pre-trained RoBERTa on 10 times more data from BookCorpus, CC-News, OpenWebText, and Stories. However, RoBERTa's pre-training corpus has become small in comparison to modern web-scraped datasets built by filtering and deduplicating Common Crawl dumps. Following the same trend, we pre-trained NeoBERT on RefinedWeb (Penedo et al., 2023), a massive dataset containing 600B tokens, nearly 18 times larger than RoBERTa's. Although RefinedWeb does not have strict high-quality constraints, we believe that exposing the model to such a large and diverse dataset will improve its real-world utility. \n\nSequence Length BERT and RoBERTa were pre-trained on sequences up to 512 tokens, which limits their downstream utility, especially without RoPE and YaRN. NomicBERT increased the maximum length to 2, 048 and employed Dynamic NTK interpolation at inference to scale to 8192. To further broaden NeoBERT's utility, we seek to increase the context length. However, due to the computational cost associated with pre-training, we adopt a two-stage pre-training procedure similar to LLMs like LLaMA 3. In the first stage, we train the model for 1M steps (2T tokens) using sequences truncated to a maximum length of 1, 024 tokens, referring to this version as NeoBERT 1024 . In the second stage, we extend the training for an additional 50k steps (100B tokens), increasing the maximum sequence length to 4, 096 tokens. We refer to this final model as NeoBERT 4096 .",
            "score": 0.4221191317019557,
            "section_title": "Data",
            "char_start_offset": 8873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "83458715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07904052734375
        },
        {
            "corpus_id": "272731041",
            "title": "The Applicability of LLMs in Generating Textual Samples for Analysis of Imbalanced Datasets",
            "text": "The first three datasets are balanced, thus the author constructed them with some imbalanced labels. As an encoder model, the author employed the CNN and BERT pre-training models. Encoder (CNN or BERT)+LIC model studies indicate that producing positive samples for the minority class can increase classification task accuracy and Macro-F1 by up to 1%. However, the author's works require further improvement in the quality of constructed samples. Also, the data set used was not imbalanced initially and needs exploration with real data with imbalanced data. \n\nIn financial few-shot text classification tasks, Loukas et al. [32] present that fine-tuned LLMs can beat fine-tuned Masked Language Models (MLMs), even when given fewer instances. The author did a thorough investigation on the trade-offs between stand performance and LLMs and MLMs in a few-shot text classification in Banking77, a financial intent detection dataset. The dataset is divided into two subsets: train (10,003 examples) and test (3,080 samples), containing a total of 13,083 annotated customer service questions labeled with 77 intents. In the training subset, the label distribution is significantly imbalanced. The research demonstrated the effectiveness and efficiency of in-context learning using conversational LLMs.",
            "score": 0.4221057148948838,
            "section_title": "B. DEEP LEARNING APPROACHES",
            "char_start_offset": 14826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "265128933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048858642578125
        },
        {
            "corpus_id": "271957618",
            "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data",
            "text": "Current trends in pre-training Large Language Models (LLMs) tend to concentrate on model and dataset size scaling Chowdhery et al. (2022); Nostalgebraist (2022); OpenAI (2023); Google (2023). Therefore, vast amounts of effort have been invested in understanding neural scaling laws-the power-law relationship between the loss of artificial deep networks and the size of the pre-training dataset for a fixed compute budget (Hestness et al., 2017;Rosenfeld et al., 2019;Henighan et al., 2020;Kaplan et al., 2020;Gordon et al., 2021;Hernandez et al., 2021;Jones, 2021;Zhai et al., 2022;Hoffmann et al., 2022;Clark et al., 2022;Neumann and Gros, 2022). In addition, recent work focuses on training a fixed-size model but using very large, trillion-token datasets (Touvron et al., 2023a,b). However, the effectiveness of these systems also fundamentally relies on the quality Longpre et al. (2023), variability and coverage of the pre-training data Hashimoto (2021); David et al. (2010) and not only the size. In particular, the richness and variety of data, otherwise known as data diversity, is a key aspect of data quality that plays an important role in researchers' and practitioners' choice of pre-training corpora for general capabilities (Gao et al., 2020;Brown et al., 2020;Touvron et al., 2023a;Eldan and Li, 2023;Gunasekar et al., 2023). In other words, diverse data is high quality data when your goal is to instill general capabilities in a model. \n\nIn addition, experimental settings demonstrate that the level of variety and diversity in pre-training data is likely a strong causal factor in the development of in-context learning (ICL) in LLMs, an essential component of the versatility and generality of LLMs (Xie et al., 2022;Shin et al., 2022;Chan et al., 2022).",
            "score": 0.4215233487615928,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1776
                }
            ],
            "ref_mentions": [
                {
                    "start": 510,
                    "end": 530,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 565,
                    "end": 583,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 605,
                    "end": 624,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 944,
                    "end": 960,
                    "matchedPaperCorpusId": "235826265"
                },
                {
                    "start": 962,
                    "end": 981,
                    "matchedPaperCorpusId": "7505943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1190185546875
        },
        {
            "corpus_id": "272718025",
            "title": "Metadata and Review-Based Hybrid Apparel Recommendation System Using Cascaded Large Language Models",
            "text": "With the need of making interaction with technology more intuitive and natural, larger models have been developed. Advanced language models with a significantly higher number of parameters came to be known as large language models, or LLMs. More parameters enable the models to perform better on a variety of language tasks by enabling it to identify more complex patterns and relationships within the text. LLMs are based on the complex architectures of transformers built upon the core mechanisms of attention and self-attention. In order to learn the statistical properties of a language, large amount of text data is utilized to train a LLM, enabling them to perform a wide array of NLP tasks with high accuracy. \n\nFor our cascaded approach that we will be looking in the subsequent section, our sentence transformers are based on the BERT and T5 LLMs. A basic operation workflow in a sentence transformer can be seen in Figure 2. BERT is chosen for its superior ability to understand context through its bidirectional training, capturing relationships between words in both directions making BERT highly effective for tasks requiring deep contextual understanding, such as semantic similarity and question answering. T5 frames all NLP tasks as text-to-text problems and this allows T5 to use transfer learning effectively by pre-training on a massive and diverse corpus, enabling it to generalize well across various NLP tasks.",
            "score": 0.4214621773222415,
            "section_title": "B. CASCADED TRANSFORMER ARCHITECTURE 1) TRANSFORMERS",
            "char_start_offset": 27201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0391845703125
        },
        {
            "corpus_id": "248228141",
            "title": "WikiOmnia: filtration and evaluation of the generated QA corpus on the whole Russian Wikipedia",
            "text": "The mean result for each pair was counted, and QA pairs where values were less than the corresponding thresholds 60%, 50%, 40%, were removed. 2) Matching persons and locations separately instead of the third step. 3) Checking if the 'gold' BERT model score is over 0.99, filtering out complicated examples. 4) Calculating if word mover's distance between generated answer and 'gold' answer is between 1.1 and 1.5, using the fastText model for Russian9 . \n\nThe overall pipeline is presented in Figure 1.",
            "score": 0.4212822414416093,
            "section_title": "Filtration of Generated Data",
            "char_start_offset": 10884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 502
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01129150390625
        },
        {
            "corpus_id": "257060193",
            "title": "How to detect propaganda from social media? Exploitation of semantic and fine-tuned language models",
            "text": "Similar is the case with FastText and other feature models. Thus, the wrapper method selected an influential set of features and the results (Table 10) demonstrated the effectiveness of the results. \n\nResults on the various combinations of features are demonstrated in the lower part of Table 10 on both the dev and test parts of the dataset. It is evident from the results that the wrapper method demonstrated robust performance as compared to the filter method. All three evaluation metrics show significant improvement in performance for all the combinations. The best performance is obtained by char tri-gram + word2vec with 97.64% recall, 97.76% f1-score, and 97.82% AUC on the dev dataset and with 97.78% recall, 97.93% f1-score, and 97.93% AUC on the test dataset. More than 97% performance against the three evaluation metrics is very promising. In addition, three combinations (char tri-gram + word2vec, char tri-gram + BERT, and char tri-gram + LSA) presented more than 97.50% performance by the wrapper method. But the filter method again did not perform well. The performance improvement demonstrates and proves the effectiveness of the wrapper method as compared to the filter method.",
            "score": 0.42119299822032336,
            "section_title": "Experiment 3: Impact of feature selection on classification performance",
            "char_start_offset": 34914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.010650634765625
        },
        {
            "corpus_id": "268532324",
            "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
            "text": "Table 2 presents the results of our main experiments.Amongst the non fine-tuning approaches, GPT-4 stands out as the superior LLM, demonstrating the strongest performance across three datasets.For the pretrained GPT-4 model, few-shot learning outperforms zero-shot learning.But for the pre-trained Llama-7B-chat model, few-shot learning produces much worse results compared to zeroshot learning, attributed to the smaller model's heightened sensitivity to the selection of few-shot demonstrations (Chen et al., 2023;Wang et al., 2024).\n\nFor the fine-tuned models, we observe that training on the entire noisy dataset without curation can even degrade model performance.Fine-tuning with only half of the data, refined through automatic filtering, yields better results than utilizing the complete, uncurated dataset.Moreover, training data curated via our Auto-Correct strategy further enhances model performance.Figures 3,4,5 depict for each dataset: a wrong response automatically identified in the Auto-Filter stage that was subsequently corrected in the Auto-Correct stage.\n\nOur fine-tuned models can outperform even the most advanced model, GPT-4 with three-shot prompting.This highlights how even the most powerful LLMs may lack the capability to adequately address specific domain challenges.Unlike some other fine-tuning research, GPT-4 was not involved in any part of the data curation or training process underpinning our fine-tuned LLMs here.(Li et al., 2023b), and confidence-based evaluator.For each of the three resulting filtered dataset versions, we fine-tune the GPT-3.5 Turbo model and report its resulting performance.This experiment is repeated across SQuAD-N, Email-N, and DROP-N datasets.We fine-tune a GPT-3.5 Turbo model on two datasets curated via Auto-Correct applied with candidate responses y \u2032 generated from either: the pretrained GPT-3.5 Turbo base Foundation model, or the fine-tuned version of this LLM trained on our Auto-Filtered dataset.",
            "score": 0.42102251096141075,
            "section_title": "Results",
            "char_start_offset": 20375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 53,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 535
                },
                {
                    "start": 537,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1076
                },
                {
                    "start": 1078,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06793212890625
        },
        {
            "corpus_id": "272881471",
            "title": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs",
            "text": "We demonstrated the importance of evaluating the quality of training data for fine-tuning tool-using LLMs. We introduce two data-evaluation approaches. The first is a rigorously devised intrinsic quality assessment, for which we implement automated metrics. The second uses in-context evaluation, that measures the educational value of training examples. While the former method is more explainable and dependable, the latter is computationally cheaper. We apply both approaches to filter data instances from two large datasets of differing qualities. The resulting subsets of training data demonstrate comparable or superior quality in terms of model performance, despite their smaller size compared to the original datasets. Overall, we observe that it is worthwhile to more carefully choose the training data for tool-using LLMs. If investing in better methods of data generation is costly, automatic post-hoc filtration can be a great alternative.",
            "score": 0.4209923110292291,
            "section_title": "Conclusion",
            "char_start_offset": 24403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 951
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40234375
        },
        {
            "corpus_id": "265043784",
            "title": "Ziya2: Data-centric Learning is All LLMs Need",
            "text": "The second one is that open-source LLMs often do not come with opensource data. The effectiveness of data processing methods directly impacts the performance of LLMs. Currently, there is no standardized methodology or criteria for cleaning pre-training data. The third one is that many studies on LLMs prioritize expanding the capacity of model parameters and pre-training data to enhance model performance, often overlooking the impact of the quality of pre- training data on model performance. Specifically, to our best knowledge, there is no study investigating which attributes of pre-training data exert the most significant influence on the LLMs. Under constraints of computational budget and a certain number of parameters, it is also worth exploring which type of data should be prioritized. \n\nIn this work, we focus on the continual pretraining strategies and understanding the intricate relationship between data and model performance. First, we propose a new data processing pipeline consisting of five steps to derive high-quality pretraining data from a vast corpus. We also involve human evaluation to ensure the quality of the processed data. Then, we use LLaMA2-13B as the base model and propose a three-stage continual pretraining strategy. The first training stage utilizes lots of unsupervised data in English and Chinese. The second stage uses a relatively small amount of data but contains many supervised datasets. The third stage employs little augmented data that focuses on improving math abilities. Finally, we obtain Ziya2, and the whole data-centric training process is shown in Figure 1. \n\nThe evaluation of Ziya2 is performed on several representative benchmarks, and the results \n\nshow that Ziya2 has a significant improvement over LLaMA2. Particularly, Ziya2 improves LLaMA2 by 10% on MMLU, 61% on CMMLU, and 68% on C-Eval, respectively. Especially in terms of mathematical benchmarks, Ziya2 improves LLaMA2 by 138% on GSM8K and 120% on MATH, and for programming benchmarks, Ziya2 improves it by 89% on HumanEval. Compared to other open-source models of the same size, Ziya2 also demonstrates superior performance.",
            "score": 0.42066748850023583,
            "section_title": "Introduction",
            "char_start_offset": 1953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2146
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3154296875
        },
        {
            "corpus_id": "252111036",
            "title": "Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-Aware Neural Machine Translation",
            "text": "In the initial stage of system preparation, the data was cleaned and filtered using the OpusFilter (Aulamo et al., 2020) toolkit. With the use of the toolkit, language detection filtering based on fastText (Joulin et al., 2016) was performed, duplicates were removed, and simple heuristics based on sentence length were applied. Then, using Moses (Koehn et al., 2007) pre-processing scripts, punctuation was normalized and nonprinting characters removed. Finally, the text was tokenized into subword units using Senten-cePiece (Kudo and Richardson, 2018) with the unigram language model algorithm (Kudo, 2018). For Ukrainian\u2192Czech and Czech\u2192Ukrainian models trained from scratch, we used separate vocabularies for the source and the target language. Each vocabulary consisted of 32,000 units. \n\nWe used concatenated data from the Flores-101 (Goyal et al., 2022) benchmark (flores101-dev, flores101-devtest) for our development set, as provided by the task organizers. corpora in the constrained track and the corpora used for system training after filtering.",
            "score": 0.42035284643034787,
            "section_title": "Data",
            "char_start_offset": 1291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1058
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 120,
                    "matchedPaperCorpusId": "220066978"
                },
                {
                    "start": 347,
                    "end": 367,
                    "matchedPaperCorpusId": "794019"
                },
                {
                    "start": 527,
                    "end": 554,
                    "matchedPaperCorpusId": "52051958"
                },
                {
                    "start": 597,
                    "end": 609,
                    "matchedPaperCorpusId": "13753208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023193359375
        },
        {
            "corpus_id": "268819482",
            "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
            "text": "However, the specific contribution and effectiveness of LLMs in the context of data augmentation for hate speech detection remain under-explored.This calls for further research to fill in these gaps.The current paper proposes a new optimized use of the BERT model using contextual cosine similarity in an iterative loop at the end of each data augmentation pipeline to filter out the closest augmented sentences compared to the original sentence.On the other hand, considering the potential LLMs, this study investigates the impact of integrating LLMs, such as GPT, into the data augmentation pipeline for enhancing the performance of supervised machine learning models in hate speech detection.For this purpose, five benchmark datasets from different domains have been selected and a set of baseline models have been employed to evaluate the performance of the developed model (s).Two datasets were related to cyberbullying and hate speech.Two others were related to sentiment analysis, and the fifth one to question categorization with six different question types; see, Section 4.1 for the details of this process.We tested four popular DA methods: synonym replacement with FastText and wordnet, BT, and BERT MLM as a baseline models.We compared the result of each method after applying BERT-cosine similarity as a label filtration condition.Our method offers a wider range of substitute words and easy implementation.Furthermore, it reveals some limitations of previous methods and prevents word replacement approaches that were incompatible with the annotated labels of the original sentences.Through the experiment, we demonstrate the proposed contextual filtration improves classifiers' performance and reduces significant label alteration.This research answers the following research questions:\n\nQ1. Are all previous augmentation methods capable of sentence augmentation without changing the original labeling?Q2.Does BERT-cosine similarity measure improve the database augmentation quality in terms of less label alteration and machine learning performance?Q3.Does the combination of different DA methods helps enrichment of dataset quality, or does it pose an adverse effect by over-fitting the classifier of having too many augmented sentences?",
            "score": 0.4195884525339918,
            "section_title": "Introduction",
            "char_start_offset": 6254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1345
                },
                {
                    "start": 1345,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1747
                },
                {
                    "start": 1747,
                    "end": 1802
                },
                {
                    "start": 1804,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 2066
                },
                {
                    "start": 2066,
                    "end": 2069
                },
                {
                    "start": 2069,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "250390551",
            "title": "ANTS: A Framework for Retrieval of Text Segments in Unstructured Documents",
            "text": "Lo et al. (2021) developed a two-level transformer framework incorporating languagespecific or domain-specific pre-trained BERT transformers as sentence encoders, which outperformed state-of-the-art text segmentation models on a semantic coherence measure. \n\nTo develop domain-specific models, often with limited labeled training data, researchers have tested how transformer-based language models pretrained on large amounts of general-domain data can be leveraged and adapted for a specific domain. To extract content elements from regulatory filings and property lease agreements, Zhang et al. (2020) segmented documents into paragraphs and trained BERT at the paragraph level, which achieved reasonable accuracy. They also found that training with fewer than 100 documents was sufficient to achieve an F1 score similar to that of the same model trained with the entire set of documents. Araci (2019) introduced FinBERT, a fine-tuned BERT model for the financial domain, by conducting additional pre-training and fine-tuning of BERT using text from financial news articles. FinBERT outperformed other pre-trained models with as few as 250 training examples in a sentiment analysis task involving financial phrases. \n\nWe go beyond the works mentioned that only provided information retrieval, topic segmentation, or document summarization to extract any targeted section that a social science researcher needs through a quick and manual-labor saving framework. Building on the above related works, we focus on refining a generic transformer model Training -8g:",
            "score": 0.41913913982256346,
            "section_title": "Related Work",
            "char_start_offset": 5403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1562
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0169830322265625
        },
        {
            "corpus_id": "237513875",
            "title": "Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?",
            "text": "The BERT and the WordConv supervised entity detectors have an average accuracy of 63.55\u00b10.1%, and 58.81 \u00b1 0.3% over 5 runs respectively. The 'EM' column in Table 2 shows the proportion of Quoref questions (the validation split) that can be answered by the Model-Agnostic Filter algorithm. 'Overlap' and 'Encoder' are the two strategies for the 'Sentence Selection' step, and 'Supervised' and 'Unsupervised' are the same for the 'Entity Type Matching'. \n\nThe final Model-Agnostic Filter algorithm uses the token overlap approach to select a sentence from the context and uses the best fine-tuned BERT model to find the entity type for the answer. With this, we can filter out 6.3%(155/2418) questions from the dev set. While this number is not very high, a similar exercise on SQuAD determines that at least 21% questions can be answered by this shortcut, which is consistent with prior findings (Jia and Liang, 2017). \n\nIn the Model-Dependent Filter algorithm, individually, the fine-tuned model filters out 55% dev instances, and the frozen encoder model filters out 6% of them, and in total, they filter out 56% (there is a large overlap).",
            "score": 0.41883564933621203,
            "section_title": "Experiments & Results",
            "char_start_offset": 20528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1141
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 916,
                    "matchedPaperCorpusId": "7228830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01036834716796875
        },
        {
            "corpus_id": "271319004",
            "title": "AI Language Models: An Opportunity to Enhance Language Learning",
            "text": "High-proficiency L2 learners may have been tasked \n\nwith proportionally more open-ended prompts, encouraging individual expression and contributing to variability in texts, hence lower similarity scores. \n\nWhy are there differences among the three LLMs in these L2 writing assessment tasks? The current work speculates the following: the differences among BERT, Llama2, and T5 arise from variations in their architectures, training datasets, and training objectives. BERT, a bidirectional transformer, is pre-trained on BooksCorpus and English Wikipedia, using a masked language model objective [2]. T5, based on a text-to-text framework, is trained on the Colossal Clean Crawled Corpus (C4) and handles various NLP tasks as text generation problems [3]. Llama2, a fine-tuned transformer, utilizes diverse internet-based data, including recent content, and balances understanding and the generation of tasks through specific training adjustments [23]. These distinctions result in unique performance variations across L2 writing assessment tasks. It is likely that the text-to-text framework is particularly suitable for L2 text detection at the sentence level, enabling T5 to have a more sensitive LLM in L1-and L2-writing differentiation. On the other hand, masked language modeling seems more sensitive for word-level measurements, enabling BERT to have a more appropriate LLM in writing assessment within a sentence (c.f. Table 2).",
            "score": 0.418064294267734,
            "section_title": "Interpreting LLM Similarity Scores in an L2 Setting",
            "char_start_offset": 43565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 52,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1435
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018829345703125
        },
        {
            "corpus_id": "270870219",
            "title": "Iterative Data Generation with Large Language Models for Aspect-based Sentiment Analysis",
            "text": "Recently, we have witnessed the great success of large language models (LLMs) [57], [15], [58], [14], [59] in many downstream language generation and understanding tasks. Owing to the instruction-tuning approach [16], LLMs can generate fluent and high-quality contexts following the human's instruction. Unfortunately, in the context of ABSA, directly using LLMs is not an optimal choice. Prior empirical studies [60], [61] show that LLMs might under-perform the traditional BERT [3] models in some fine-grained language understanding tasks, e.g., ABSA. Thus, employing BERT-style PLMs is still a viable option for ABSA. Alternatively, in this paper, we attempt to take advantage of LLMs' abilities and enforce them to generate more high-quality data for boosting the performance of existing ABSA models. Notably, with the advancement of LLMs, some prior studies also attempt to prompt LLMs for data generation [17], [18]. However, in the field of ABSA, directly prompting LLMs struggles to generate the desired pseudo-label data, as LLMs are prone to hallucinations, leading to undesired data generation. To this end, we design a novel iterative data generation mechanism and a self-reflection data filtering module to better guide the data generation of LLMs.",
            "score": 0.41795303771955306,
            "section_title": "C. Large Language Models",
            "char_start_offset": 8727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1261
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "271926161"
                },
                {
                    "start": 917,
                    "end": 921,
                    "matchedPaperCorpusId": "232379879"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054290771484375
        },
        {
            "corpus_id": "277634004",
            "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety",
            "text": "The development of LLMs has seen significant advancements since the introduction of BERT (Bidirectional Encoder Representations from Transformers) in 2018, which marked a turning point in natural language processing (NLP). BERT (Devlin et al., 2019), developed by Google, leveraged a transformer architecture that allowed for bidirectional comprehension, thus providing a more nuanced understanding of context compared to prior unidirectional models. Its key innovations (e.g. masked language modeling and next sentence prediction) enabled the BERT to excel in a wide range of NLP tasks. Following BERT, numerous variations of encoders and encoder-decoders based LLMs were developed in relatively quick succession, each improving upon BERT's foundation by optimizing training techniques or reducing model size while maintaining performance or expanding to multi-billion parameter models. These models have significantly advanced applications such as sentiment analysis, topic modeling, text classifications and summarization, and human speech-emulative generative text interactions. More recently, the NLP field has shifted toward even larger and more generalized models, such as GPT-4, LLaMA, Gemini, and many others which utilize the transformer architecture at an unprecedented scale (Radford et al., 2019). These models are trained on vast datasets and demonstrate remarkable versatility across diverse tasks with minimal fine-tuning. As LLMs continue to evolve, their increasing complexity and capability reflect the broader shift towards models that can perform a wide range of tasks with greater accuracy and contextual understanding. This trajectory illustrates how the field has rapidly transitioned from task-specific NLP models to highly flexible, generalized language models, thereby expanding their applicability across different domains, including healthcare, social media analysis, and public policy development Nowadays, several options are available for users to utilize generative pre-trained transformers models (GPT) that have been trained on vast corpora of text (i.e., the Internet). Free access is available to users limited to lower-tiered performance and less flexibility for fine-tuning with custom data (e.g., Google's Bard/Gemini, OpenAI's ChatGPT, Meta's LLaMA).",
            "score": 0.41781939193127493,
            "section_title": "BACKGROUND",
            "char_start_offset": 3320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 1287,
                    "end": 1309,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0263519287109375
        },
        {
            "corpus_id": "267312030",
            "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
            "text": "Large language model (LLM) pre-training has been largely democratized and open-sourced, allowing various academic labs, and industries to pre-train custom LLMs. Yet, a key differentiator between these models is the composition and size of the data used to train them. Data curation strategies are required to filter out scrapes of the web that are unstructured and/or poorly phrased (Eisenstein, 2013). While some of these strategies have been made public (Brown et al., 2020;Wenzek et al., 2020;Penedo et al., 2023), most state-of-the-art data curation techniques are unknown to the research community, and only anecdotal evidence remains. Research on data curation requires multiple rounds of re-training, making it an expensive endeavour to document techniques that lead to practical improvements. On the other hand, scaling laws for language models (such as Chinchilla scaling laws (Hoffmann et al., 2022)) show that with increasing model sizes, we should also increase both the training compute and data size linearly. This is infeasible because (a) high-quality data is limited (Villalobos et al., 2022), and repeating for even a small number of epochs (4 or more) results in diminishing returns or overfitting (Muennighoff et al., 2023;Touvron et al., 2023;Xue et al., 2023); and (b) pre-training for such long durations is prohibitively expensive. \n\nMeanwhile, the use of synthetic data has gained prominence in the paradigm of aligning pre-trained LLMs via instruction fine-tuning, RLHF (Ouyang et al., 2022), and instruction backtranslation (Li et al., 2023b). Recently, in the context of pre-training, synthetic data was used to generate datasets such as Tiny Stories (Eldan & Li, 2023) and Textbook quality synthetic data (Gunasekar et al., 2023;Li et al., 2023c). These were used to train smaller language models (like the Phi model family) that were as performant as larger language models on certain tasks.",
            "score": 0.41720981551534664,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 401,
                    "matchedPaperCorpusId": "13163299"
                },
                {
                    "start": 456,
                    "end": 476,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 476,
                    "end": 496,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1264,
                    "end": 1281,
                    "matchedPaperCorpusId": "265881860"
                },
                {
                    "start": 1496,
                    "end": 1517,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "196203342",
            "title": "Context-specific Language Modeling for Human Trafficking Detection from Online Advertisements",
            "text": "In our study, we employ the feature models described above and compare the results of the binary classification corresponding to them. We use logistic regression and compute the precision and recall curve (PRC) to evaluate the performance of different models. Moreover, in this application, it is important to have a model with good recall while keeping high precision, i.e., a high positive predictive value (PPV) to avoid unnecessary actions. To do so, we investigate the sensitivity of models in different high PPVs. \n\nPre-processing. We choose to not remove stop words or not use any stemming or lemmatization techniques as we are faced with different writing structures which could be informative for our model. We test the impact of emojis and punctuation by training and testing our model by creating two text sets. In the first text set, we keep the emojis and punctuation and remove them in the second set. In the second set, we convert the emojis to words. Numbers in the texts are removed, because: 1) we have made the labels based on phone numbers and 2), the ads are likely to have the same age or same price throughout the texts. We then divide the data into an 80/20% training/testing set. In the following sections, we describe how each set of features is processed while using logistic regression as our fixed classification model. \n\nLDA Features. We begin with features coming from LDA topic modeling scores where we assign it to 12 topics. Gensim LDA is implemented by making a bag of words dictionary of our training set. We find this optimal topic number where we examined the explained LDA feature set via crossvalidation on January 2017 alone. \n\nAWV Features. Our FastText model is trained on a set including a minimum count of 2 words and a window size of 3 to give us a vector of dimension 100. After training the FastText model, the average word vector of the training set is computed. Using this saved language model from the training set, we compute the feature test vectors. \n\nBERT Features. For encoding our texts using BERT, we make a list of all documents and use the BERT service client.",
            "score": 0.4171634936485354,
            "section_title": "Experiments",
            "char_start_offset": 8512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012237548828125
        },
        {
            "corpus_id": "265294814",
            "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
            "text": "After the data cleaning, we fine-tune two widely recognized pre-trained language models, BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019), and evaluate their performance on different datasets. We use the F-1 score as the evaluation metric when assessing the Jigsaw Civil Comments dataset due to its class imbalance (#class 0/#class 1 \u2248 10). For the other conversation datasets where the labels are relatively evenly distributed, we use test accuracy as the evaluation metric. \n\nIn Tables 5 and 6, the BERT and GPT2 are fine-tuned on the raw training data (Raw) and cleaned training data (Docta), respectively. This comparison aims to evaluate the effectiveness of data cleaning for the downstream tasks. There are two versions of test data used for evaluation: (1) i.i.d. as \n\nPublished as a conference paper at ICLR 2024  (2) Consensus, meaning that only the test instances where the raw labels agree with the cleaned labels (Docta labels) are selected. Compared to simply trusting raw labels or Docta labels to test, the consensus labels should be arguably better since a) even though the ground-truth labels are unknown, the consensus labels are likely to have higher data credibility than the raw labels, and b) they rule out the potential bias introduced by the label cleaning algorithm. From both tables, we can observe that utilizing a cleaned training set consistently yields superior performance compared to employing the raw training set, a trend observed in both versions of test sets. This underscores the critical significance of filtering out noisy samples. To further validate the faithfulness of the labels cleaned by Docta, we ask ChatGPT and in-house human annotators to judge the correctness of the Docta labels when they disagree with the raw labels. Table 7 shows that the model trained on the data examples cleaned by Docta will have significantly better performance on the dataset calibrated by ChatGPT or human annotators.",
            "score": 0.41702960254159455,
            "section_title": "EVALUATION OF PREDICTIVE PERFORMANCE",
            "char_start_offset": 18548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "207870323",
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
            "text": "We assess the quality of the resulting dataset by learning unsupervised word and sentence representations through fastText and BERT models. For fastText, we train 300dimensional word embeddings on the head, middle and tail subsets of the English and Polish CommonCrawl corpora, sorted by document perplexity. We evaluate these on standard semantic and syntactic analogy datasets (Mikolov et al., 2013). We observe in Table 1 a steady increase in performance as we go from the tail to the head of the dataset, confirming the positive impact of our filtering method based on document perplexity. We also train BERT models on the English (en), Russian (ru), Chinese (zh) and Urdu (ur) languages, using either the Wikipedia corpora or our new CommonCrawl datasets. For these languages, we use respectively 16G, 5G, 1.1G and 106M of raw Wikipedia data (full datasets), and we cap the head CommonCrawl data to 21G, 21G, 17G, 2.2G for English, Russian, Chinese and Urdu. That is, we consider roughly the same amount of data for English, but increase the amount of data for Russian, Chinese and Urdu. We train a BERT-BASE architecture (Devlin et al., 2018) on each of these corpora, without next sentence prediction (NSP) as in (Lample and Conneau, 2019). For better comparison, we early-stop all our models after two days of training on 16 Volta32 GPUs, and use the exact same number of steps for each model. We evaluate each model on the XNLI (Conneau et al., 2018) corpus by using the training data in each language. Results presented in",
            "score": 0.4168054594730525,
            "section_title": "Training models on this dataset",
            "char_start_offset": 16864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 1220,
                    "end": 1246,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0165252685546875
        },
        {
            "corpus_id": "264994068",
            "title": "DECENTRALISED AUTONOMOUS SOCIETY THROUGH LARGE LANGUAGE MODELS\u2019 BASED AGENTS: A PATHWAY TO EMPOWER SMALL COMMUNITIES",
            "text": "LLMs serve as potent knowledge bases, capable of answering a wide range of queries and providing insights across numerous topics. In application-specific contexts, they can fill knowledge gaps in the absence of subject-matter specialists, making them a powerful tool for various projects. However, pre-training these models poses significant challenges due to enormous computational requirements and sensitivity to data quality and training tricks. Hence, the development of more systematic, economical pre-training approaches, considering factors such as model effectiveness, efficiency optimization, and training stability, is of paramount importance.",
            "score": 0.41650328839260453,
            "section_title": "Utilization and Training Considerations",
            "char_start_offset": 9906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 653
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04638671875
        },
        {
            "corpus_id": "269149352",
            "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
            "text": "Dataset quality has been an interest to researchers because of its importance in ensuring the quality of the model trained with the dataset (Budach et al., 2022). Previous studies found that large amounts of data automatically crawled from the web may contain noisy documents, and proper filtering procedures can be an efficient solution against them (Xu and Koehn, 2017;Khayrallah and Koehn, 2018;Kry\u015bci\u0144ski et al., 2019;Luccioni and Viviano, 2021;Kreutzer et al., 2022). Accordingly, several studies in text summarization investigated various strategies to filter out noisy data (Matsumaru et al., 2020;Nan et al., 2021;Guo et al., 2022) and released new datasets with better quality (Grusky et al., 2018;Urlana et al., 2022). However, their strategies are primarily composed of coarse rule-based methods and less interpretable model output, or costly human investigation has been applied for constructing new datasets. Furthermore, such strategies have not been applied to multi-document summarization datasets. \n\nIn the meantime, with the advancement of LLMs (Zhao et al., 2023), researchers have explored the usage of LLMs for data annotation, a task that traditionally relied on human annotators. Initial attempts have revealed the potential capabilities of models like GPT-3 for data annotation (Wang  et al., 2021). These studies indicate that GPT-3 can annotate datasets more efficiently and costeffectively than human annotators. This results in enhanced downstream task performance, with the model trained on the GPT-3 annotated dataset outperforming the one trained on the human-annotated dataset. Subsequent studies have further demonstrated the capabilities of GPT-3, showing its ability to generate labeled data using external knowledge or instructions about desired labels and domains (Ding et al., 2023). Additionally, researchers have examined the usefulness of newer models like GPT-3.5 and evaluated the effectiveness of CoT in improving annotation quality (He et al., 2024).",
            "score": 0.4163806626790893,
            "section_title": "Related Work",
            "char_start_offset": 4938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 371,
                    "matchedPaperCorpusId": "216812284"
                },
                {
                    "start": 371,
                    "end": 398,
                    "matchedPaperCorpusId": "44090489"
                },
                {
                    "start": 398,
                    "end": 422,
                    "matchedPaperCorpusId": "201666437"
                },
                {
                    "start": 422,
                    "end": 449,
                    "matchedPaperCorpusId": "233864521"
                },
                {
                    "start": 581,
                    "end": 605,
                    "matchedPaperCorpusId": "218487340"
                },
                {
                    "start": 622,
                    "end": 639,
                    "matchedPaperCorpusId": "253238030"
                },
                {
                    "start": 686,
                    "end": 707,
                    "matchedPaperCorpusId": "13752552"
                },
                {
                    "start": 707,
                    "end": 727,
                    "matchedPaperCorpusId": "251395063"
                },
                {
                    "start": 1801,
                    "end": 1820,
                    "matchedPaperCorpusId": "254877171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12890625
        },
        {
            "corpus_id": "276617859",
            "title": "Kanana: Compute-efficient Bilingual Language Models",
            "text": "After the cascaded filtering, we further apply language-specific model-based filtering on high quality documents (Su et al., 2024;Shao et al., 2024b;Li et al., 2024;Penedo et al., 2024a) separately on English and Korean. For English web documents, we utilize a DCLM (Li et al., 2024) classifier. \n\nFor Korean web documents, due to the lack of publicly available high quality classifiers, we iteratively train edu filter as high quality classifier using FastText (Joulin et al., 2017) based on the FineWeb-Edu pipeline (Penedo et al., 2024a). When applying the FineWeb-Edu pipeline, we observe that most of the documents are classified as uneducational, leading to a distribution imbalance. To address this issue, we iteratively retrain the classifier by augmenting educational documents from the previous iteration. \n\nTo assess the quality of our edu filter and Korean web corpus, we perform experiments by continual pre-training Llama 3 8B with 25B tokens. As shown in Table 2, the quality of our Korean web corpus is comparable to that of FineWeb 2 (Penedo et al., 2024b), which is the largest open-source Korean corpus. Furthermore, when using our edu filter to extract high quality data from Korean web corpus, we observe a significant performance improvement in the experimental results through training. Interestingly, we observe that using high quality English data, regardless of the quality of Korean data, can improve the scores on Korean benchmarks such as KMMLU and HAE-RAE, as well as the English benchmark MMLU. The results from this experiment make a foundation of our intuition for data mixture strategy in the staged pre-training in the following section. In summary, we share two insights to consider when building bilingual corpora with underrepresented language for enhanced computational efficiency. (1) Prioritize quality over quantity. For languages that do not have vast tokens available, such as Korean, prioritizing quality over quantity is an effective solution.",
            "score": 0.416261610786659,
            "section_title": "Data",
            "char_start_offset": 7679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 130,
                    "matchedPaperCorpusId": "274446256"
                },
                {
                    "start": 149,
                    "end": 165,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 462,
                    "end": 483,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040771484375
        },
        {
            "corpus_id": "273098691",
            "title": "How Much Can RAG Help the Reasoning of LLM?",
            "text": "This shows that RAG can help to improve the reasoning capability of LLM. \n\nHowever, in real-world RAG scenarios, the information retrieved from documents is not always directly usable and often requires further processing because documents may contain noise information (Jiang et al., 2023b;a), and some documents may even be completely distracting, containing incorrect answers (Shi et al., 2023a;Wu et al., 2024). Such noise and distracting documents can negatively impact performance. While some works attempt to fine-tune the model to filter out noise (Xu et al., 2024) and distracting documents (Yan et al., 2024), these filtering processes may require additional reasoning depth and could ultimately harm overall performance. Also some works train another filter model (Yan et al., 2024;Asai et al., 2023), but this approach results in additional reasoning cost and makes it difficult to eliminate the inherent noise in the documents. \n\nTherefore, a critical question arises: How difficult is the filtering problem, and can we effectively solve it within a limited number of layers? If the efforts to filter out noise is even more than the help RAG brings, RAG fail to improve the reasoning capability. First, we show that fine-tuning methods like LoRA also struggle to filter out noise without compromising reasoning capability, since filtering irrelevant tokens while maintaining the original attention patterns for relevant tokens proves challenging. This indicates that the filtering process cannot be effectively incorporated into the original reasoning steps of LLMs, necessitating additional reasoning depth for filtering, which can ultimately degrade the reasoning capability of the model. \n\nThen we show that judging the relevance of a token can hardly be implemented by limited number of transformer layers. Considering the query 'Alice is exhausted but Bob is still excited; how does Bob feel?' Here, 'exhausted' is noise and should be excluded from the inference. However, assessing relevance necessitates considering the token 'Bob' in the query alongside 'Alice' the subject of 'exhausted'. Thus, evaluating relevance requires the information from three or more tokens. Yet, the attention mechanism typically computes only pair-wise relationships, making it challenging to resolve this issue within a limited number of transformer layers (Sanford et al., 2024).",
            "score": 0.4160213462484328,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 75,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07574462890625
        },
        {
            "corpus_id": "253735385",
            "title": "L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi",
            "text": "In this work, we present a simple approach to training sentence BERT models for low-resource language using the synthetic corpus. We have evaluated these models using a KNN-based classification setup and embedding similarity method on different Hindi and Marathi datasets. Various FastText and pre-trained multilingual and monolingual BERT models have also been evaluated. FastText models are found to perform competitively with monolingual BERT models while the monolingual BERT models outperform multilingual ones. Without any taskspecific finetuning, the LaBSE model is found to perform the best for both Hindi and Marathi languages. We highlight the lack of Hindi and Marathi sentence-BERT models in the public domain and hence release MahaSBERT and HindS-BERT, the sentence-BERT models created using synthetic datasets. Through a comparative analysis of their performance, we show that these Sentence-BERT models have an upper hand in the quality of embeddings as compared to all BERT as well as FastText models. They are highly advantageous for the task of semantic sentence similarity. We conclude that the method of two-step training proves to be efficient for developing MahaSBERT-STS and HindSBERT-STS. Finally, we hope that our work facilitates further study and trials in the Hindi and Marathi NLP domains.",
            "score": 0.4158997696561937,
            "section_title": "Conclusion",
            "char_start_offset": 17915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007404327392578125
        },
        {
            "corpus_id": "263909512",
            "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
            "text": "Today, machine-learning-powered text classification models have been widely applied in diverse applications such as detecting biased or toxic language on online platforms (Wiegand et al., 2019) and filtering spam emails (Jindal and Liu, 2007). However, the performance of these models largely depends on the quality of the training data. This poses a substantial challenge in practice, especially when models need to be built for a novel task domain or to incorporate new classification categories, as the training data collection and curation process is often costly, time-consuming, and complex. \n\nMeanwhile, with the recent advancements in large language models (LLMs), researchers have started to explore the potential of utilizing LLMs for generating synthetic data tailored to specific tasks and augmenting the training data in lowresourced data settings (Kumar et al., 2020;Yoo et al., 2021;Hartvigsen et al., 2022;Sahu et al., 2022). Most recently, a few studies also investigate into the feasibility of generating a synthetic dataset from scratch using LLMs to support zeroshot learning (Ye et al., 2022;Wang et al., 2021;Tang et al., 2023;Gao et al., 2023). While LLMbased data augmentation is often found to outperform other data augmentation methods in boosting the model performance, mixed results are reported regarding whether the LLM-generated synthetic data can effectively support model training to enable a level of model performance that is comparable to models trained on the data collected in the real world and carefully annotated. This leaves uncertainty for researchers and practitioners in deciding whether to rely on LLMs for synthetic data generation or to proceed with the traditional data collection and curation pipeline when they need to construct a text classification model for a new task. Naturally, one may wonder what factors might moderate the effectiveness of LLM-generated synthetic data in facilitating successful model training. \n\nWe conjecture that one such factor could be the subjectivity of classification tasks. Indeed, language is inherently subjective and interpretive (Benveniste, 1971;Wiebe et al., 2004).",
            "score": 0.41571587891512574,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 193,
                    "matchedPaperCorpusId": "174799974"
                },
                {
                    "start": 220,
                    "end": 242,
                    "matchedPaperCorpusId": "207163195"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0699462890625
        },
        {
            "corpus_id": "226283542",
            "title": "An Empirical Exploration of Local Ordering Pre-training for Structured Learning",
            "text": "We compare various pre-training strategies and show the results in Table 2. As split in this table, we arrange the models into three groups: \n\n(1) The first group includes models without pretrained encoders. \"Random\" gets random initialization, and \"fastText\" gets its word lookup table (3) The third group only contains \"BERT\", which directly utilizes pre-trained BERT4 . \n\nIn the first group, where there are no pre-trained encoders, the performances drop drastically in lowresource cases. The pre-trained static word embeddings help in some way, but its degree of performance drop is very similar to the baseline: there are performance gaps of nearly 14 points between 10k and 1k training sizes. If we adopt pre-trained encoders, as in the second and third group, the performance clearly improves for all training sizes. Particularly, in the low-resource (1k) settings, the performance drops from the 10k settings are much smaller than those in the first group. \n\nThe more interesting comparisons are among those in the second group, where the settings are kept the same except for pre-training strategies. Firstly, BiLM performs worst in this group. The reason may be that BiLM contains unidirectional decoders, which cannot make full use of the inputs. The performance of our local ordering strategy (LBag) is very close to those of the MaskLM, with performance gaps of only 0.2 to 0.4 in LAS. Furthermore, if we combine the ordering and MaskLM objectives as in the Hybrid model, there can be further improvements. This suggests that local or-dering pre-training may capture orthogonal information from MaskLM. Overall, the model performances in the second group do not differ too much, suggesting that the effectiveness of contextualized pre-training can be realized as long as the model is capable enough. \n\nUnsurprisingly, BERT performs the best, possibly due to its larger model and training corpus. Nevertheless, if calculating the gaps between the second group and BERT, we can find that they are relatively consistent as training sizes get smaller. In contrast, the gaps between the first group and BERT obviously get larger in lower-resource settings. This again suggests the effectiveness of contextualized pre-training.",
            "score": 0.41569831439141325,
            "section_title": "Comparisons",
            "char_start_offset": 14901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 140
                },
                {
                    "start": 143,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2234
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0439453125
        },
        {
            "corpus_id": "258564612",
            "title": "Web Content Filtering Through Knowledge Distillation of Large Language Models",
            "text": "In addition to acting as a pre-filter, domain-to-category mapping lists and label propagation signatures are often used to create the training sets for machine learning models. However, machine learning algorithms tend to memorize patterns rather than understand underlying concepts [11,12], thus learning from already labeled URLs is insufficient for accurate content classification in the long tail of the URL distribution. A model whose parameters are configured to memorize the head of the distribution is undesired as signatures already cover such domains without risking false positives. Therefore, our objective is to identify models with superior generalization capabilities for out-of-distribution samples. \n\nFor unknown or new domains, the model must infer a description from the URL. It is useful to view URL classification, especially for web content filtering, as a natural language processing task, considering URLs as semi-sentences. For a fair amount of our categories, the URL will frequently have explicit words to advertise its content, specifically semantically related keywords for the given category. For example, a site selling weapons will often contain keywords such as \"armaments\" or \"glock\", or \"gun\". The current state-of-the-art in URL detection and our chosen baseline, URLTran [9], frames URL detection as a natural language processing task and fine-tunes a pre-trained BERT model [13] to detect phishing URLs. The BERT model is an early example of the transformer architecture [14] which has since been refined and scaled, giving rise to large language models. Large language models (LLMs) are state-of-the-art on natural language tasks [15]. LLMs are first pre-trained on large amounts of unlabeled textual data in a task-agnostic manner, learning a general understanding of language such as syntax and semantics [15]. Once pre-trained LLMs can effectively generalize to new tasks upon fine-tuning or few-shot prompting with much smaller amounts of data [15]. The amount of data needed for LLMs to generalize to new tasks is often several orders of magnitude less than the amount of data needed to fully train a smaller model.",
            "score": 0.4156444812570089,
            "section_title": "Introduction",
            "char_start_offset": 4191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 1308,
                    "end": 1311,
                    "matchedPaperCorpusId": "235377216"
                },
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1846,
                    "end": 1850,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1987,
                    "end": 1991,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05224609375
        },
        {
            "corpus_id": "255694119",
            "title": "A Levenshtein distance-based method for word segmentation in corpus augmentation of geoscience texts",
            "text": "BERT (Devlin et al. 2019) is a pre-training model composed of a bidirectional transformer model (Vaswani et al. 2017). For the upstream language preprocessing of Chinese word segmentation, it has always been a hot issue, and the BERT model, as the advanced language preprocessing model, can obtain high-quality embedding vectors, which is beneficial to the downstream task for word classification. Figure 4 presents the pretraining architecture of the BERT model. The input is a series of tokens, which are first embedded to form a vector and then encoded in the transformer model. The corresponding output is a vector sequence with dimension H, wherein each vector corresponds to the input token sharing the same index. \n\nCompared with that of the traditional transformer encoder, the vector representation of the input of BERT's transformer encoder has more segment embeddings, which can better represent semantic information. The BERT model adopts two pre-training tasks: the mask-LM task and next sentence prediction (NSP). When the BERT model is pre-trained, the goal is to minimize the combined loss function of the two pre-training tasks, wherein mask-LM is used to capture contextual information and NSP is used to infer the relationship between sentences. \n\nBecause of the large-scale unmarked data and deep structure used in pre-training, the model can capture rich semantic patterns and complex language phenomena from plain text, and well understand the language (Peters et al. 2018;Goldberg 2019). After fine-tuning, the performance of different natural language processing tasks can be improved using the BERT model. For the multi-label classification problem, the different probability distributions of labels in label set S are calculated, and the multi-label output results are determined by a threshold. Therefore, in our proposed framework, BERT is used to generate the word vectors of the context so as to ameliorate the performance of sequence labelling tasks (Souza, Nogueira, and Lotufo 2019).",
            "score": 0.41549970407729164,
            "section_title": "BERT embedding",
            "char_start_offset": 18817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 24,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 96,
                    "end": 117,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1475,
                    "end": 1495,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0236053466796875
        },
        {
            "corpus_id": "271977975",
            "title": "Spatial\u2013Temporal Transformer Networks for Traffic Flow Forecasting Using a Pre-Trained Language Model",
            "text": "The concept of pre-training is closely linked to transfer learning [42], which involves reusing knowledge acquired from source tasks or source domains and applying it to target tasks or target domains. Traditionally, transfer learning relies on labeled data for supervised training. However, deep transfer learning has shifted this paradigm towards pre-training followed by fine-tuning. Self-supervised learning on large volumes of unlabeled data has become the norm, which enables the application of pre-trained models to various downstream tasks through fine-tuning. \n\nPre-training in NLP usually refers to language modeling based on a corpus [43]. For example, the bidirectional encoder representations from transformers (BERT) [44] builds language models based on transformer encoders. BERT performs a masked language modeling (MLM) pre-training task similar to completing the blanks, which recovers masked tokens in the input sentence based on the context. BERT serves as the foundation for a family of LMs in various NLP tasks [45]. \n\nThe generative pre-trained Transformer (GPT) [46] focuses on generative tasks in NLP. The GPT models use the Transformer decoder as the backbone, and the model performance improves as the number of layers and parameters increase. Above a certain size of parameters, LM becomes an LLM. The GPT family of LLMs has had a profound impact in a number of areas related to artificial intelligence (AI). In addition, high-quality open-source LLMs that can be deployed privately, such as Mistral [47] and Llama [48], have also been widely adopted, contributing even more to the development of AI.",
            "score": 0.4154467108380654,
            "section_title": "Pre-Trained LM",
            "char_start_offset": 11003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1628
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "14789841"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "215842252"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "237101158"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "235752381",
            "title": "Data extraction methods for systematic review (semi)automation: Update of a living systematic review",
            "text": "This is a timely LSR update, since it has a cut-off just before a the arrival of a new generation of tools: generative 'Large Language Models' (LLMs), such as ChatGPT from OpenAI, based on the GPT-3.5 model [1]. 120 As such, it may mark the current state of the field at the end of a challenging period of investigation, where the limitations of recent machine learning approaches have been apparent, and the automation of data extraction was quite limited. \n\nThe arrival of transformer-based methods in 2018 marked the last big change in the field, as documented by this LSR. Methods of our included papers only rarely progressed beyond the original BERT architecture, 14 varying mostly just in terms of datasets used in pre-training. Few used models only marginally different to BERT, such as RoBERTa with its altered pre-training strategy. 121 However, Figure 13 (reproduced from Yang et al. (2023) 122 ) shows that there has been a vast amount of NLP research and whole families of new methods that have not yet been tested to advance our target task of data extraction. For example within the new GPT-4 technical report, OpenAI describe increased performance, predictability, and closer adherence to the expected behaviour of their model, 123 and some other (open-source) LLMs shown in Figure 13 may have similar potential. \n\nEarly evaluations of LLMs suggest that these models may produce a step-change in both the accuracy and the efficiency of automated information extraction, while in parallel reducing the need for expensive labelled training data: a pre-print by Shaib et al. 124 describes a new dataset [2] and an evaluation of GPT-3-produced RCT summaries; 124 Wadhwa, DeYoung, et al. 125 use the Evidence Inference dataset and it's annotations of RCT intervention-comparator-outcome triplets to train and evaluate BRAN, DyGIE++, ELI, BART, T5-base, and several FLAN models in a pre-print; 125 and in a separate pre-print Wadhwa, Amir, et al. 126 used the Flan-T5 and GPT-3 models to extract and predict relations between drugs and adverse events. 126",
            "score": 0.4153274252893314,
            "section_title": "Latest developments and upcoming research",
            "char_start_offset": 73604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 670,
                    "end": 672,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050323486328125
        },
        {
            "corpus_id": "272524632",
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "text": "Validation losses on samples of text corpora are commonly used as a proxy for downstream performance when comparing LLMs pretrained on the same data distribution (Kaplan et al., 2020;Hoffmann et al., 2022;Wei et al., 2022), even if they have different architectures (Poli et al., 2023;Peng et al., 2023;Gu & Dao, 2024). \n\nAccording to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering. Are we missing important information that we can efficiently extract from a diverse collection of already trained models, larger and more diverse than any single organization is likely to produce? We show evidence supporting this hypothesissimple loss-performance correlation coefficients are effective when used for data selection.",
            "score": 0.4152416981955893,
            "section_title": "RELATED WORK",
            "char_start_offset": 5786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1024
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "258557329",
            "title": "Differentially Private Attention Computation",
            "text": "The development of large language models (LLMs) has been rapid and significant in recent years, with numerous breakthroughs and advancements in the field. BERT [DCLT18] achieved state-ofthe-art performance on a wide range of language tasks by training on a massive amount of text data in 2018. Since then, the GPT (Generative Pre-trained Transformer) family of models has further advanced the field. GPT-2 [RWC + 19] and GPT-3 [BMR + 20], with billions of parameters, are able to generate highly coherent and human-like text. Other notable LLMs include XLNet [YDY + 19], which addresses some of the limitations of BERT [DCLT18], and RoBERTa [LOG + 19], which improves upon BERT [DCLT18]'s training methods to achieve better performance. The rapid development of LLMs has been fueled by advancements in hardware, software, and data availability, allowing researchers and companies to train and deploy these models at an unprecedented scale. \n\nAs a result of their development, LLMs have found a wide range of applications in various fields. In the field of natural language processing (NLP) [VSP + 17, RNS + 18, DCLT18, BMR + 20], LLMs are used for tasks such as language translation [HWL21], sentiment analysis [UAS + 20], and creative writing [Ope23]. In addition, LLMs are being used to develop chatbots and virtual assistants that can understand and respond to natural language queries [BMR + 20, Ope23]. Outside of NLP, LLMs are being used in scientific research to generate new hypotheses and discover novel patterns in large datasets. The applications of LLMs are expanding rapidly, and it is likely that they will play an increasingly important role in many fields, such as computer vision [RF18], robotics [KNK21], and autonomous vehicles [ZTL + 17, BKO18]. \n\nDespite their many benefits, large language models (LLMs) have the potential to pose several privacy and security risks [Sag18, VKB23, KGW + 23, EMM + 23].",
            "score": 0.4150132985963392,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1190,
                    "matchedPaperCorpusId": "243766712"
                },
                {
                    "start": 1714,
                    "end": 1721,
                    "matchedPaperCorpusId": "195833499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018341064453125
        },
        {
            "corpus_id": "260172359",
            "title": "Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach",
            "text": "Pre-trained language models (PLM) are deep neural networks with pre-trained weights that vectorize word sequences. Such vectorial representations yield cutting-edge outcomes on NLP tasks like text classification, text clustering, questionanswering, and information extraction. PLMs suggest an objective engineering paradigm for NLP: pre-training language models to extract contextualized characteristics from text, followed by fine-tuning with task-specific objective functions [12]. Bidirectional encoder representations based on transformers (BERT) [13] is a PLM developed by Google that has significantly advanced this area since it was first announced in 2018. BERT is pre-trained in two stages: first, a self-supervised task in which the Masked Language Model (MLM) has to discover words that have been masked in a text; and second, a supervised task in which the model has to deter-mine whether a sentence B is the continuation of a sentence A (Next-Sentence Prediction, NSP). The pre-training produces at the end 12 stacked encoders, which take a sequence of tokens as input and calculate a fix-length vector for each token. These vectors' dimensions correspond to how much consideration a token should give to the other tokens. The CamemBERT model [14], one of the French variants of BERT, is based on the same architecture as BERT and was trained using MLM exclusively on a French corpus. ChouBERT [1] further pre-trains a CamemBERT-base checkpoint using MLM over French Plant Health Bulletins and Tweets mentioning natural hazards in French. The pre-training of ChouBERT improves performance in identifying plant health issues on Twitter with relatively small labelled data.",
            "score": 0.41499953575041154,
            "section_title": "B. Pre-trained Language Models",
            "char_start_offset": 5274,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 482,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "207853304"
                },
                {
                    "start": 1407,
                    "end": 1410,
                    "matchedPaperCorpusId": "248816104"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "234813543",
            "title": "Filtered BERT: Similarity Filter-Based Augmentation with Bidirectional Transfer Learning for Protected Health Information Prediction in Clinical Documents",
            "text": "Additionally, the data augmentation method was considered for time-series data, such as signal data [15]. Likewise, data augmentation was also considered in the natural language processing (NLP) field, where it was used to replace words with synonyms or for inserting and deleting random words, and showed effective and powerful performance improvement in limited data environments [16,17]. \n\nAdditionally, pre-training [18] and transfer learning [19] have been actively considered as some of the methods to overcome limited data. In the image field, pre-learning has been used as a means to train a network on a large dataset such as ImageNet [20] and to solve other problems with pretrained weights. Furthermore, in the NLP field, pretrained embeddings such as Word2vec [21], GloVe [22], and fastText [23] demonstrate effective features. Embeddings from the language model (ELMO) [24] and bidirectional encoder representations from transformers (BERT) [25] are the most representative examples of using transfer learning in the NLP field. In particular, BERT is attracting attention because it supports fine-tuning and can be applied to various fields of NLP, which require a strong performance [26]. \n\nIn this study, we propose a filtered BERT augmentation method to overcome limited data. This is to further improve the prediction performance by adding an appropriate augmentation that combines BERT and similarity filters to transfer learning to obtain limited data. We compared the performance of the existing BERT and its PHI prediction with the addition of the filtered BERT-based augmentation proposed using a representative public dataset.",
            "score": 0.4146265805736877,
            "section_title": "Introduction",
            "char_start_offset": 4428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1649
                }
            ],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "15796526"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "16081723"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1197,
                    "end": 1201,
                    "matchedPaperCorpusId": "222130373"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "247595086",
            "title": "Personalized Filled-pause Generation with Group-wise Prediction Models",
            "text": "To investigate whether the BERT, a rich word embedding model is effective for predicting FPs, we compared the prediction scores of fastText and BERT when used as the word embedding model. In this evaluation, we used the hyper-parameters described in Section 4.1.2. Table 3 lists the results. We can see that BERT has higher F-scores than fastText, which indicates that the prediction performance is improved by using BERT as a word embedding model. On the basis of this result, BERT was used as the word embedding model in the following experiments.",
            "score": 0.41462108486754223,
            "section_title": "Rich word embedding model",
            "char_start_offset": 15906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 549
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00878143310546875
        },
        {
            "corpus_id": "236504810",
            "title": "A novel deep learning approach to extract Chinese clinical entities for lung cancer screening and staging",
            "text": "We ran our experiments five times and averaged the 5 results as the final result to reduce the possible bias from dataset partitioning. \n\nBased on the annotated 100 reports by the two annotators, the inter-annotation agreement using kappa statistics [43] is 0.937, which indicates the annotation is reliable. Table 4 shows the overall performance of the proposed and benchmark models. As shown in Table 4, the BERT-BTN with pre-training achieves the best performance with 85.96% macro-F1 score and 90.67% micro-F1 score under the exact match scheme and 94.56% macro-F1 score and 96.78% micro-F1 score under the inexact match scheme in comparison with the benchmark models. \n\nTo prove the effectiveness of BERT embedding, we selected the FastText embedding, a classical embedding that represents the word using only one fixed vector, as the baseline. By analyzing the performances of these two word embedding methods, we can notice that models using BERT embedding outperform models using Fast-Text embedding with an improvement of 4.55% macro-F1 score under exact match scheme and 3.93% macro-F1 score under inexact match scheme at most. The performance improvements indicate BERT is more powerful in contextual information encoding by taking both left and right contexts of target words into account. \n\nBERT-BTN provides 0.46% overall performance improvement under exact match scheme and 1.35% under inexact match scheme compared with BERT-BiL-STM, indicating the long-term dependencies learnt by Transformer are useful for NER. When comparing BERT-Transformer with BERT-BTN, the macro-F1 score drops by 2.91% under exact match scheme and 3.46% under inexact match scheme, indicating the position information encoded by BiLSTM has a significant influence on Transformer's performance. The reason for performance reduction may be that Transformer only relies on self-attention to draw global dependencies of input and treats every position identically, which may neglect some fixed patterns in the sentences since some information is described by several clauses in a fixed order.",
            "score": 0.4141869281653885,
            "section_title": "Experiments and results",
            "char_start_offset": 13509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 138,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "17077165"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0087127685546875
        },
        {
            "corpus_id": "234358106",
            "title": "Benchmarking down-scaled (not so large) pre-trained language models",
            "text": "Pre-training data We pre-train all models on WikiText-1031 (Merity et al., 2016), a large-scale text corpus for training and evaluating language models on long-range contexts, which has served as an evaluation data set (Radford et al., 2019;Dai et al., 2019;Shoeybi et al., 2019) as well as for pre-training (Howard and Ruder, 2018). We pretrain all models on the training set of WikiText-103, which allows for learning long-range dependencies (Rae et al., 2019). The validation set is employed to compare different architectures by their validation loss during pre-training. WikiText-103 is much smaller than most pre-training corpora of modern language models. For instance, Devlin et al. (2019) trained BERT on a 3, 300M words corpus, which is approximately 32x the size of WikiText-103. Aside from this, pre-training data sets of different models often vary considerably in size, which makes fair comparisons difficult (A\u00dfenmacher and Heumann, 2020). Pre-training on the same corpus allows us to exclude the amount and quality of pre-training data as confounding factors when evaluating the different model components.",
            "score": 0.41413206486313703,
            "section_title": "Materials and Methods",
            "char_start_offset": 3137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1122
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 241,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 677,
                    "end": 697,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 923,
                    "end": 953,
                    "matchedPaperCorpusId": "209832412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028594970703125
        },
        {
            "corpus_id": "252666305",
            "title": "Using Transformer models for gender attribution in Polish",
            "text": "1) FastText: FastText [15] is a shallow neural network library created for fast text classification model training and evaluation. We used a supervised setting with hyperparameter tuning, the word embeddings were initialized randomly. The best result was obtained with wordNgrams set to 2, word dimension set to 156, and context size window set to 5. \n\n2) LSTM: Long Short Term Memory Networks [13] were used to obtain a state-of-the-art results on most NLP tasks before the era of Transformer language models [7]. In our tasks, for bidirectional LSTM, SentencePiece [19] tokenization performs better than word-level lowercase tokenization. Vocab size 50k was used with randomly initialized embeddings of size 100. We tried embedding size 300, but resulted in slightly worse classifier quality. We used one layer of 256 units, trained with Adam [16] optimizer with learning rate 0.001. The batch size used for training was 400 and sequences were trimmed and padded to 256 tokens. \n\n3) Transformer: In the last time Transformer [26] and its modification like BERT [7], RoBERTa [20] or XLM-R [4] achieve state-of-the-art in the benchmarks such as GLUE [29] or SuperGLUE [28] benchmark. Most often used bidirectional Transformers are pre-trained on huge amounts of monolingual data in the Masked Language Model (MLM) process, where the model learns a bidirectional representation of tokens. Next, pre-trained models are finetuned to the specific task. This process reduces the time to train a new model from scratch and can be easily adapted to other tasks. In our case, the downstream task is classification, where the model uses a special token ([CLS], classification token), which represents the whole sentence and helps achieve better results. \n\nWe train self-contained classifier based on the RoBERTa model in two ways: with pre-training and without pre-training (train classifier from the scratch) stage. We only used the data that was available in the HSSS challenge to avoid any data leaks in the other data sets.",
            "score": 0.4141087382334845,
            "section_title": "C. Neural Methods (self-contained)",
            "char_start_offset": 7626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "52051958"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0245208740234375
        },
        {
            "corpus_id": "272694161",
            "title": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs",
            "text": "Our results show that the pre-annotation LLM assistant can support annotator decision-making and reduce annotation time by 12% using the fine-tuned Llama 7B model (Section 4.1). The real-time LLM assistant alleviates annotator creation fatigue and provides structures for text generation tasks. Furthermore, the LLM judge agrees with expert researchers 79.55% of the time, indicating its potential as an effective judge to expand QA coverages and optimize valuable auditing resources. These findings suggest that LLMs can streamline the annotation process and reduce human workload. \n\nQuality improvement. The LLM-assisted pre-annotations maintain, or moderately increase accuracy compared to non-LLM assisted labels, suggesting that LLM assistants can enhance annotation quality, particularly when consistency across annotators is crucial. With real-time LLM assistance, we observe a 3.09% increase in helpfulness and a 0.28% increase in response honesty, with significant improvements in certain question categories such as Text Understanding, Expression/Creativity, and Object Detection/Recognition. Additionally, the LLM judge can filter out low-quality labels, boosting the quality of annotations used in downstream model training. It is also worth noting that improvements in quality may come at the cost of reduced efficiency, as evidenced by a 23.4% increase in AHT (Section 4.2). Therefore, researchers should operate at a trade-off point that best aligns with their specific annotation needs and goals. \n\nModel performance. Model performance has a significant impact on efficiency and quality. Our results show that lower agreement rates between humans and LLM assistants lead to longer review times (Section 4.1). Building on the promising gains of a fine-tuned Llama 7B model, more advanced models such Llama 3.1 405B or GPT-4 can further improve accuracy for pre-annotations, reduce annotator disagreements, and enhance annotation efficiency and quality. \n\nConversely, poor model performance can have a negative impact, such as model hallucination affecting human annotation honesty (Section 4.2) and potential low recall in the LLM judge missing low-quality annotations (Section 4.3).",
            "score": 0.41400380140288423,
            "section_title": "Enhanced annotation efficiency.",
            "char_start_offset": 43697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "272718025",
            "title": "Metadata and Review-Based Hybrid Apparel Recommendation System Using Cascaded Large Language Models",
            "text": "Evaluation using Top Hits, MRR, and NDCG metrics showed that LLM-generated descriptions are comparable to those obtained through web scraping. This can thus enable the use of LLMs for generating sentence embeddings and utilizing these embeddings to compute similarity with user preferences. \n\nPramudya and Alamsyah [16] emphasized the effectiveness of leveraging NLP models for creating personalized hotel recommendations. They conducted a comparison between BERT and RoBERTa to analyze six hotel aspects and proposed a review classification method to identify customer preferences. BERT achieved an accuracy score of 0.8963 and a macro F1 score of 0.83. For review-based recommendations, RoBERTa outperformed BERT with a cosine similarity score of 0.99917 owing to its larger pre-training corpus and more complex architecture than BERT. \n\nOur proposed work builds upon prior research by addressing the limitations of content-based filtering through a hybrid recommendation system approach [1], [4], [5]. The limitation of paragraph embedding (Doc2Vec) [2] and LSTMs [6] lies in the fact that it is unable to capture complex relationships with text data. Further attention is needed to address the rating sparsity problem which causes collaborative filtering mechanisms to fail [1], [2]. The lack of interaction between embeddings [8] needs to be addressed. As shown in [18], [23], and [24], it may prove to be beneficial to consider the sentiment associated with each review to further enhance the quality of a RS. Moreover, works such as [26], [27], [31], [32], [38], and [41] demonstrated the progressive evolution of LLMs and sentence transformers for NLP tasks. Lastly [35] and [37] showed the effectiveness that cascaded models can bring forth. Based on the literature study we discovered the efficiency of cascading techniques and propose a hybrid recommendation approach centering around cascaded LLMs. We aim to develop a novel approach for recommending apparels using a hybrid system encompassing cascading LLM model, and using sentiments as an additional parameter for final recommendations.",
            "score": 0.4137672031654109,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 19210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "268254662"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "3270531"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "237588082"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "33197033"
                },
                {
                    "start": 1053,
                    "end": 1056,
                    "matchedPaperCorpusId": "233693852"
                },
                {
                    "start": 1067,
                    "end": 1070,
                    "matchedPaperCorpusId": "237842985"
                },
                {
                    "start": 1278,
                    "end": 1281,
                    "matchedPaperCorpusId": "3270531"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "233693852"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "matchedPaperCorpusId": "254771310"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "252797938"
                },
                {
                    "start": 1386,
                    "end": 1390,
                    "matchedPaperCorpusId": "255176135"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "261089462"
                },
                {
                    "start": 1574,
                    "end": 1578,
                    "matchedPaperCorpusId": "261823768"
                },
                {
                    "start": 1674,
                    "end": 1678,
                    "matchedPaperCorpusId": "261025486"
                },
                {
                    "start": 1683,
                    "end": 1687,
                    "matchedPaperCorpusId": "227123327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.115966796875
        },
        {
            "corpus_id": "273323367",
            "title": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference",
            "text": "We train hypothesis-only models on each of our train sets (3 LLM-generated and the filtered SNLI) and evaluate them on all evaluation sets. Table 4 reports the accuracy of the hypothesis-only models. \n\nThe hypothesis-only classifiers' high accuracy confirms the existence of hypothesis-only annotation artifacts in both human and LLM-generated NLI. Surprisingly, the SNLI-trained models perform much better on the GPT-4 generated evaluation set (0.82 for NB and 0.88 for BERT) than on the 3 We did not tune hyper-parameters as our focus is identifying artifacts. We train the neural models for 1 epoch using AdamW (Loshchilov and Hutter, 2018), a learning rate of 2e-5, a weight decay of 0.01, and a batch size of 16. SNLI evaluation set (0.64 for NB and 0.72 for BERT), indicating that GPT-4 might contain similar annotation artifacts as SNLI. We also notice that hypothesis-only models trained on LLM-generated data perform much better on other LLM-elicited datasets than on SNLI, as the accuracies in the first column are much lower than the other columns in both figures. This might indicate that the LLMs produce similar biases as each other. \n\nFew unigrams needed for high NB accuracy. The high accuracy of the NB models with unigram features indicates that the hypotheses contain giveaway words-single words that are highly indicative of a label. \n\nTo determine how many give-away words are necessary to accurately classify LLM-elicited NLI, we train NB models based on the n most informative give-away words as features. We perform a chi-squared test on the words in each train set to determine informative give-away words. Figure 2 reports the accuracy of NB hypothesis-only models using just 1 to 50 features. Compared to SNLI, the LLM-elicited datasets are far easier to classify using a sparse selection of unigram features.",
            "score": 0.41375118832306473,
            "section_title": "Accuracy of hypothesis-only classifiers",
            "char_start_offset": 6272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 643,
                    "matchedPaperCorpusId": "53592270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240478515625
        },
        {
            "corpus_id": "270391394",
            "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
            "text": "Generally, LLMs refer to transformer models with hundreds of billions (or more) of parameters, which are trained on large amounts of text data at a high cost, such as GPT-3 [2], PaLM [22], Galactica [23], and LLaMA2 [24].LLMs typically possess extensive knowledge and demonstrate strong abilities in understanding, generating natural language, and solving complex tasks in practical.LLMs exhibit some abilities that are not present in small models, which is the most prominent feature that distinguishes LLM from previous pre-trained language models(PLMs), for example, in-context learning (ICL) capacity.\n\nAssuming that the language model has been provided with natural language instructions and several task demonstrations, it can generate the expected output of the test instance by completing the word sequence of the input text without additional training or gradient updates; Instruction following.By fine-tuning the mixture of multi-task datasets formatted through natural language descriptions (known as instruction adaptation), LLM performs well on unseen tasks also described in instruction form.Through fine-tuning instructions, LLM is able to follow task instructions for new tasks without using explicit examples, thus improving generalization ability.\n\nStep-by-step reasoning.For small language models(SLMs), it is often difficult to solve complex tasks involving multiple reasoning steps, such as mathematical word problems.Instead, using the chain-of-thought (CoT) cueing strategy [25][26][27], LLMs can solve such tasks by leveraging a cueing mechanism that involves intermediate reasoning steps to derive the final answer.\n\nThere have been some preliminary attempts to evaluate LLMs for text sentiment analysis tasks.In [7], the authors observed that the zero-shot performance of LLMs can be compared with fine-tuning BERT models [105].In addition, in [8], the authors conducted preliminary research on some sentiment analysis tasks using ChatGPT, specifically studying its ability to handle polarity changes, open-domain scenarios, and emotional reasoning problems.In [9], the authors comprehensively tested the effectiveness of LLMs in text sentiment analysis datasets.In [28], the authors tested the effectiveness of commercial LLMs on a multimodal video-based sentiment analysis dataset.",
            "score": 0.41354269377281416,
            "section_title": "Large Language Models",
            "char_start_offset": 4971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 605
                },
                {
                    "start": 607,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1265
                },
                {
                    "start": 1267,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1640
                },
                {
                    "start": 1642,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2084
                },
                {
                    "start": 2084,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1848,
                    "end": 1853,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038970947265625
        },
        {
            "corpus_id": "261395559",
            "title": "SP\u00b3: Enhancing Structured Pruning via PCA Projection",
            "text": "The results of the experiment on OPT 125m are shown in Table 7. For OPT 125m , we use the same experimental setup as for BERT base . \n\nF SP 3 for Large Language Model SP 3 can also be used to prune large language model (LLM), e.g., Llama (Touvron et al., 2023)). To mitigate computational overhead of LLM, our approach restricts using SFT data exclusively for model pruning. Concurrently, to reduce pruning complexities, we only prune the hidden dimensions and the filters within the FFN block, while ignoring the pruning of attention heads and layers, because coarse-grained pruning requires more training data to recover the performance of the model. We also ignore the pruning of the intermediate dimensions of the MHA block, as it may conflict with the RoPE positional encoding (Su et al., 2024) which is commonly used for LLM. The structure used to prune the LLM is shown in Fig. 5.",
            "score": 0.4135135237756606,
            "section_title": "E.3 Experiment Results on OPT 125m",
            "char_start_offset": 27043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 887
                }
            ],
            "ref_mentions": [
                {
                    "start": 782,
                    "end": 799,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03460693359375
        },
        {
            "corpus_id": "258745552",
            "title": "BERT-PG: a two-branch associative feature gated filtering network for aspect sentiment classification",
            "text": "In this paper, we present an improved BERT model with gating for aspect-based sentiment classification. In this model, we build a two-branch structure based on the BERT pre-trained model to encode context and sentence pairs (consisting of context and aspect words), after which a gating mechanism is constructed to perform secondary filtering on text features and obtain additional interactive features as a supplement to compensate for the incomplete features extracted by the BERT sentence pair method. This treatment filters the context more accurately than traditional gating mechanisms; finally, in order to solve the overfitting issue brought on by the migration of the BERT model, we also developed a novel loss function with a threshold. \n\nWe have out extensive experiments on three benchmark datasets to verify the effectiveness of our proposed model. First, by comparing with other benchmark models, our model performs better in terms of accuracy and Macro-F1 value; secondly, through ablation experiments, the validity of each independent module in the model is verified; finally, through the study of threshold parameters and case study, it is proved that the designed loss function can effectively alleviate the over-fitting problem caused by large-scale model migration, and effectively improve the final classification performance. \n\nHowever, since this paper uses BERT as a pre-training model, and the MLM of the BERT model is a direct random mask for a single token, which will lose phrase and entity information, and its NSP task can learn the sequence level information, but it is only a binary classification, and the negative sample construction is too simple, resulting in the model cannot be fully trained. After that, ERNIE (Zhang et al., 2019), ALBERT (Lan et al., 2020) and other models have improved the above problems. If we use these improved BERT models as pre-training models and further optimize our proposed two-branch structure and gating mechanism, we may be able to further improve the model performance, which is also one of our future works, and due to the instability of neural networks, in the future, we will try to train the model using the classification theory proposed by Colbrook et al. (2022) to further improve the robustness of the model.",
            "score": 0.4129068532900376,
            "section_title": "Conclusion",
            "char_start_offset": 29838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 1748,
                    "end": 1768,
                    "matchedPaperCorpusId": "158046772"
                },
                {
                    "start": 1777,
                    "end": 1795,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061309814453125
        },
        {
            "corpus_id": "274422859",
            "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
            "text": "This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances. Similar to quality evaluation methods, domain classification approaches commonly utilize models such as logistic regression, BERT, and FastText to assign domain labels to each text. Among these, FastText is a neural network-based approach similar to CBOW [22]. Compared to other methods, FastText offers efficient and rapid training on large-scale datasets while maintaining comparable classification performance. As a result, this paper employs the FastText model to classify domain labels for each text. \n\nToxicity Evaluation. Toxic texts are generally defined as those containing offensive or adult content [23,24]. Offensive content can be classified into three levels of severity: offensive text, abusive text, and hate text [25]. Adult content typically includes themes related to pornography or violence. Due to differences in cultural values and political perspectives, the analysis of toxic texts varies across countries. During the development of LLMs, the presence of toxic content in pre-training datasets would compromise the safety of LLMs. Consequently, toxicity evaluation has become increasingly important for the construction of pre-training datasets. In recent years, Caselli et al. [25] construct a toxicity dataset, RAL-E, using data from banned Reddit communities. They then train a BERT-based toxicity evaluation model with this dataset. Hartvigsen et al. [26] leverage GPT-3 to generate a dataset containing both subtle toxic texts and benign examples through adversarial techniques. In contrast, Deng et al. [27] construct a Chinese toxicity dataset with data from Zhihu and Weibo, and annotate each text with the help of human experts and LLMs. However, previous work often suffers from small data scales and insufficient coverage, limiting its ability to effectively evaluate the toxicity of LLMs. To address the above issues, this paper aims to construct a more robust toxicity evaluation model and release a larger-scale, higher-coverage toxicity dataset, thereby enhancing toxicity evaluation capabilities and improving the safety of LLMs. \n\nDatasets for Pre-training. As the foundation of LLMs, large-scale pre-training datasets play a crucial role in enhancing the capabilities of these models.",
            "score": 0.41265639263157416,
            "section_title": "Related Work",
            "char_start_offset": 3922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2228
                },
                {
                    "start": 2231,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2385
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "214051295"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "266818259",
            "title": "Large Language Models in Plant Biology",
            "text": "Using transformers in biology has allowed some recent breakthroughs, such as AlphaFold2. However, not all LLMs use transformers (e.g., HyenaDNA 27 ), and not all models that use transformers are LLMs (e.g., AlphaFold2 4 ). BERT models are typically employed in classification, Named Entity Recognition (NER), and summarization, while GPT models are usually used for text generation and translation. However, fine-tuned versions of these two types can be used for tasks that are not originally intended, e.g., ChatGPT can be used for text classification and NER with zero-shot or few-shot prompting 28 . Since LLMs are used to analyze sequences of words, many types of sequential data can be used as training input, and in biology, both types of LLMs have recently been applied to study genomic, proteomic, and gene expression data. LLMs are typically pre-trained with self-unsupervised approaches that exploit the vast publicly available genomic data. BERT models employ Masked Language Modeling (MLM), where the aim is to predict masked tokens, while GPT models employ Causal Language Modeling (CLM), where the task is to predict the next token in a sequence. The predicted new word from the sequence can then be fed back into the same model iteratively to repetitively predict the next one -this makes a model autoregressive. Thus, pre-training aims to teach the model not to memorize the data but to learn to synthesize patterns that can be used to extract features and patterns and extrapolate to unseen data. The resulting foundation models can be repurposed for other tasks by fine-tuning them with supervised learning approaches 7,24,29 . The LLM models can be divided into three architecture types: encoder-decoder, encoder-only, and decoder-only (Box 1). \n\nEncoder-only models DNABERT is a BERT-family (Bidirectional Encoder Representations from Transformers) model trained with MLM, where the model is tasked with predicting the masked token by using the upand down-stream tokens (hence bidirectional).",
            "score": 0.4126241349472916,
            "section_title": "Examples of LLMs used in biology",
            "char_start_offset": 7571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 1636,
                    "end": 1638,
                    "matchedPaperCorpusId": "221823863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03912353515625
        },
        {
            "corpus_id": "250390532",
            "title": "R2D2 at SemEval-2022 Task 6: Are language models sarcastic enough? Finetuning pre-trained language models to identify sarcasm",
            "text": "To overcome the class imbalance, we used sklearn to compute class weights which are defined as: Let X be the vector containing counts of each class X i where i \u2208 X and N be the total number of samples. Then the weights for each class were given as: weight i = N/(length(X) * X i ) where length function computes the number of classes in vector X. There was no imbalance for Subtask C for each sarcastic sample, the corresponding non-sarcastic rephrase was given. \n\n3 System overview 3.1 Pre-trained language models (PLMs): NLP, a diverse field, contains an array of tasks, but most datasets for these tasks contain only a few hundred or thousand human labelled samples. This makes training large models for these tasks a challenging task. Transfer learning using GloVe (Pennington et al., 2014) and FastText (Bojanowski et al., 2017) is one of the popular choices for solving this problem. Most recently, researchers came up with a method called pre-training (Qiu et al., 2020), which involves training general-purpose models from unannotated text data. This allows models to learn syntactic and semantic features in the text in an unsupervised setting. Transformer architecture proposed in (Vaswani et al., 2017) is the most common choice for training PLMs. These models can be finetuned on various downstream tasks using task-specific datasets. Finetuning allows models to adapt to small task-specific datasets easily and shows promising results (Sharma et al., 2021b). Next, we provide a summary of PLMs used in our approach. (Liu et al., 2019) and used the BERT architecture with slight modifications to improve its performance. They replaced MLM with dynamic masking and removed the NSP objective during pre-training. They also found that BERT was undertrained, so they trained the model for longer durations with more data and bigger batch size. RoBERTa outperformed BERT on several down-stream tasks.",
            "score": 0.41241838568595646,
            "section_title": "Background",
            "char_start_offset": 6946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 794,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 808,
                    "end": 833,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 959,
                    "end": 977,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 1191,
                    "end": 1213,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1448,
                    "end": 1470,
                    "matchedPaperCorpusId": "236459910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01450347900390625
        },
        {
            "corpus_id": "253762077",
            "title": "Non-Parametric Word Sense Disambiguation for Historical Languages",
            "text": "While the Delpher database contains OCR'd text of varying quality, the DBNL is the result of a thorough digitization campaign and presents generally high quality transcriptions. In order to make sure that only text of sufficiently quality is used for pre-training, we developed the following filtering strategy. First, we trained statistical character-level 5-gram language models using KenLM (Heafield, 2011). Specifically, we trained a single model per century of text available from the clean DBNL data. Then, for each snippet of Delpher data, we obtain a quality estimate as the perplexity that the corresponding DBNL-based model assigns to it. Manual observation of random snippets suggested discarding texts with a perplexity of 20 or higher. Furthermore, we restricted ourselves to texts published between 1500 and 1950. \n\nIn total, the remaining data set consists of 5.8B tokens from Delpher and 1.3B tokens from DBNLwhich amounts to ca. 7.1B tokens. We used this data set in order to train a WordPiece tokenizer with a vocabulary of 30,000 tokens, and pre-trained BERT with default parameters, for 1,000,000 training steps, keeping the maximum sequence length at 128 subtokens.7",
            "score": 0.4122681360231738,
            "section_title": "GysBERT",
            "char_start_offset": 12212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1187
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 409,
                    "matchedPaperCorpusId": "8313873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07977294921875
        },
        {
            "corpus_id": "272593547",
            "title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of Large Language Models",
            "text": "Comparisons with baselines. In Table 1, we compare models trained via our methods on generated data with base model and model trained via FedAvg [10] on human-annotated data (as a reference). Experiments are conducted on three datasets and evaluated by two metrics. From the table, we see that (1) our methods consistently and significantly improves the performance of the base model across datasets and evaluation metrics, indicating the effectiveness of our proposed methods. Specifically, in HotpotQA, our method can achieve 0.1873 higher BERT Score (0.2439 v.s. 0.0566). ( 2) Our methods hugely fill the gap between base model and that tuned via FedAvg on human data, further verifying FedIT-U2S's effectiveness. However, there is still a room for improvement, calling for more future works to further enhance the performance. With the increasing generation capability of LLMs [29,28], we even believe that there is potential for surpassing this baseline (FedAvg on human data). (3) Although the data filtered using the reward model is smaller in quantity, it brings a more significant improvement to the model's performance, indicating the importance of data quality in this scenario. Comparisons of generated and human-annotated data. To better understand our method, we further analyze the characteristics of our generated data by comparing it with human-annotated data from two perspectives: embedding visualization and case studies.",
            "score": 0.41214875733040046,
            "section_title": "Experimental Results",
            "char_start_offset": 15435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1441
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "14955348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061767578125
        },
        {
            "corpus_id": "271691319",
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "score": 0.4120992852366934,
            "section_title": "\u2022",
            "char_start_offset": 30142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "268825336",
            "title": "Applications of Large Language Models in Pathology",
            "text": "It can follow instructions, explain text, summarize text, carry out meaningful conversations, write computer programs, generate reports, perform text manipulations, and is proficient at generating well-written prose in different styles. In 2023, GPT4 was released, containing over 1 trillion parameters. It was significantly better than its predecessor in multilingual capabilities, contextual understanding, and reasoning skills [8]. \n\nThe earlier generation of LLMs (e.g., BERT) had fewer parameters, and therefore were trainable on relatively smaller resources (e.g., 16 TPU chips) [3]. BERT (Bert large) has 345 million parameters, which is small compared to GPT4 and many recently released LLMs. Notwithstanding, BERT is still a very useful model, and its relatively small size makes it accessible to individuals and groups with fewer resources. Several groups have been able to train LLMs from scratch. Geiping and Goldstein showed that it was possible to train a language model with similar performance to BERT by using only a single consumer GPU in a single day [9]. Mitchell et al. trained a BERT transformer model with 275,605 pathology reports, which included 121 million words, to extract data from pathology reports using eight Nvidia Tesla V100 GPUs [10]. Modern LLMs have larger training set and compute requirements. To put things into perspective, Llama 65b took Meta approximately 21 Days to train using 2048 A100 GPUs with 80 GB of RAM [11], while Llama 2 70b, which is open sourced, took Meta a total of 1,720,320 GPU hours to train using the same type of GPUs [12]. These pre-trained models are referred to as foundation models. Foundation models may be fine-tuned for specific tasks through supervised learning (e.g., using a question-and-answer dataset) with much smaller GPU resources. A human feedback loop where a person decides whether an LLM-generated response is desirable or not can further refine model performance [13]. \n\nVarious groups have published studies in almost every medical and non-medical field on the use of ChatGPT and other LLMs in their respective specialties, with mixed success.",
            "score": 0.4117043176473667,
            "section_title": "Introduction",
            "char_start_offset": 2207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 1070,
                    "end": 1073,
                    "matchedPaperCorpusId": "255185900"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "247616675"
                },
                {
                    "start": 1946,
                    "end": 1950,
                    "matchedPaperCorpusId": "261510331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0186920166015625
        },
        {
            "corpus_id": "271769785",
            "title": "HW-TSC\u2019s Speech to Text Translation System for IWSLT 2024 in Indic track",
            "text": "Language-agnostic BERT Sentence Embedding (Feng et al., 2022) is an effective parallel corpus filtering method, which can effectively filter out high-quality bilingual data. We can use the filtered high-quality bilinguals and then finetune our model. Finally, we applied this method to this competition, which greatly improved the results in the three directions. In this experiment, we get 37 million filtered high-quality bilinguals in the en2ta direction, 55 million filtered high-quality bilinguals in the en2hi direction, and 43 million filtered high-quality bilinguals in the en2bn direction from bilingual data.",
            "score": 0.4116679441046226,
            "section_title": "labse",
            "char_start_offset": 1829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 618
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 61,
                    "matchedPaperCorpusId": "220347683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033721923828125
        },
        {
            "corpus_id": "270702914",
            "title": "Task Oriented In-Domain Data Augmentation",
            "text": "In practice, the size of general corpus is orders of magnitude larger than domain-specific corpus.For example, the ads domain corpus contains about 1B tokens in our experiments, while the general web corpus contains trillions of tokens.To alleviate such a data scarcity issue, we propose to select in-domain data from general corpus.\n\nWe train a FastText (Joulin et al., 2017) classifier to identify in-domain data from large amount of domain agnostic data.Specifically, to train the FastText classifier, we select a certain number of in-domain data as positive samples and the same amount of out-of-domain data as negative samples.The trained binary classifier is then used to select in-domain data from the general corpus (e.g., the web corpus).\n\nWe apply a filter to ensure that the in-domain data (both the original in-domain corpus and the selected data) have high educational value (Gunasekar et al., 2023).In this way, we can boost the quality of the filtered in-domain data, which in turn improves performance of the models.\n\nThe proposed data selection strategy has two benefits.First, it can significantly enrich in-domain data.In practice, the amount of selected data is magnitudes larger than the in-domain dataset.For example, the original ads domain corpus contains about 1B tokens in our experiments, and we select an additional 15B tokens from the web corpus (after selection and filtering).Second, the data selection strategy enables replay (Ibrahim et al., 2024), such that generality of LLMs is largely kept after continual pre-training (see Table 5 for experiments).In more details, for a specific LLM, replay happens when the continual pre-training data contain a certain amount of pre-training data (e.g., the web corpus).It has been observed that replay is crucial to keep LLM's generality (e.g., instruction following) after training.",
            "score": 0.4116397853201212,
            "section_title": "IN-DOMAIN DATA SELECTION",
            "char_start_offset": 5646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 333
                },
                {
                    "start": 335,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 747
                },
                {
                    "start": 749,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1032
                },
                {
                    "start": 1034,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1586
                },
                {
                    "start": 1586,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 376,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0836181640625
        },
        {
            "corpus_id": "272910877",
            "title": "Data Proportion Detection for Optimized Data Management for Large Language Models",
            "text": "After detecting the data proportions, we need to prepare an enormous volume of pre-training data, typically exceeding 30 TB of raw tokens [6]. Common data processing methods, such as those described in [26], involve filtering, deduplication, language identification, and language model-based filtering. This necessitates the development of a robust data management system capable of handling over 30 TB of data. Although Data Juicer has been a pioneering system for data preparation, it still faces challenges when processing such large-scale datasets. Therefore, both algorithmic and hardware improvements are essential to further enhance data preparation systems. \n\nIn summary, the challenges in data proportion detection for LLM pre-training are multifaceted and require significant advancements across several domains. Data Proportion detection requires both advanced model and data management techniques. Addressing these challenges will not only improve the performance of LLMs but also enhance the understanding of optimal data management practices for pre-training.",
            "score": 0.41144369267895364,
            "section_title": "Robust Data Preparing System",
            "char_start_offset": 21854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1073
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "207870323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "276885331",
            "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
            "text": "The Ling models demonstrate their competitive performance through rigorous methodologies designed to enhance the quality of large-scale pre-training datasets. The corpus utilized in the model development is a diverse collection of textual and non-textual data, encompassing sources such as web content, books, academic papers, social media, encyclopedias, mathematics, and programming code. To date, we have constructed a high-quality corpus consisting of approximately 9 trillion tokens, distributed across 1 trillion tokens in Chinese, 5.5 trillion in English, and 2.5 trillion in code. The development of such a large-scale, high-quality dataset is the result of systematic improvements in several key areas: \n\n\u2022 Data curation. The majority of raw data used in this study were obtained from publicly available sources, including Common Crawl (CC), coding platforms, and encyclopedias. However, these sources often exhibit a range of quality issues. To address this, we developed specialized data cleaning pipelines tailored to the characteristics of different data types (e.g., web pages, academic papers, books, and code). The cleaning process included tasks such as text extraction and parsing from raw HTML/PDF files, deduplication, rulebased filtering, and the removal of toxic or undesirable content. Furthermore, we established a robust quality assessment framework comprising 10 categories and over 300 quality evaluation metrics. This framework enables us to systematically categorize datasets into quality tiers, which serve as a foundation for further refinement and the informed selection of training data. \u2022 High-quality data selection. To identify high-quality data, we fine-tuned models such as fastText Bojanowski et al. [2017] and BERT Devlin et al. [2019], applying fine-grained labels and attributes to the cleaned data. These attributes include metrics such as text coherence, knowledge density, educational level, and complexity. Using this approach, we were able to extract high-quality data samples from the broader dataset. Additionally, sampling experiments were conducted across various features to identify optimal strategies for data selection. This process ensured that the selected data were well-suited for enhancing downstream model performance. \u2022 Mathematics and code data.",
            "score": 0.4113318282120196,
            "section_title": "Pre-Training Data",
            "char_start_offset": 32794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2279
                },
                {
                    "start": 2280,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2098388671875
        },
        {
            "corpus_id": "269605945",
            "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs",
            "text": "In this section, we implement GOT-D to select data for pre-fine-tuning the given LLM on 8 NLU tasks each with a pre-defined domain (Gururangan et al., 2020).We evaluate the effectiveness of data selection methods on downstream task performance given a fixed selection budget.While prior work (Brown et al., 2020) suggests notable performance improvements can be achieved from extensive continued pre-training on domain datasets, we show that performance improvements on these tasks can be established by pre-fine-tuning with a limited data budget if selected properly.\n\nExperimental Setup.This experiment involves two stages: pre-training over selected data and then fine-tuning over the downstream task.First, we select data to fine-tune a pre-trained bert-base-uncased model (from Huggingface) via Masked Language Modeling (MLM) -following the standard setting of masking 15% tokens for training over the unlabeled domain-specific data.We consider two settings: (1) We apply baselines and GOT-D with a fixed selection budget of 150K samples to select from the corpus defined in Appendix C.1, (2) We simulate a more constrained resource scenario, where we limit the selection budget to 50K and the downstream training data size to 5K labeled samples.All MLMs were trained for 1 epoch over their selected data.\n\nIn the second stage, a classification head is added to the model -to train and evaluate over the domain-specific datasets.We consider 8 labeled datasets across 4 domains for our downstream tasks: Biomedicine (RCT (Dernoncourt & Lee, 2017), ChemProt (Kringelum et al., 2016)), CS papers (ACL-ARC (Jurgens et al., 2018), Sci-ERC (Luan et al., 2018)), News (HyperPartisan (Kiesel et al., 2019), AGNews (Zhang et al., 2015)), Reviews (Helpfulness (McAuley et al., 2015), IMDB (Maas et al., 2011)), as curated in Gururangan et al. (2020).",
            "score": 0.41125020282926705,
            "section_title": "ADAPTATION TO DOMAIN-SPECIFIC TASKS",
            "char_start_offset": 21282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 568
                },
                {
                    "start": 570,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1310
                },
                {
                    "start": 1312,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 312,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05419921875
        },
        {
            "corpus_id": "267938688",
            "title": "DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem",
            "text": "The key idea is to test the performance in a zero-shot transfer setup. Here, BERT-CRF exhibits the best overall performance while BERT-NER and Linear-CRF are in the second best position. In Table 4, we report the best model performance in dark purple while light purple denotes the second best. A detailed analysis of the error cases are presented in section 12. \n\nFor evaluating overall results in terms of macro-F1 score we divide methods in two eras: the pre-LLM era and the LLM era. In the pre-LLM era, BERT-CRF consistently demonstrates superior performance across different datasets (Ubuntu (Bug), Launchpad (QA), Fedora (CQA), and Linux (CQA)), securing the highest macro-F1 scores highlighted in dark purple. Following closely, RoBERTa-CRF emerges as the second-best method in terms of performance, indicated by light purple highlights in the Ubuntu (Bug) and Linux (CQA) datasets. Transitioning to the LLM era, we notice a stark con-trast in performance. Notably, the scores drastically drop, with GPT-3.5-Turbo, GPT-4, and Google BARD exhibiting significantly lower macro-F1 scores across all datasets, suggesting that despite their advanced capabilities, these models may not be directly optimized for the specific task of NER as compared to their predecessors in the pre-LLM era. However, UniversalNER demonstrates relatively better performance in this era, albeit still not reaching the effectiveness of the pre-LLM methods. Progressive learning [1] involves incrementally training machine learning models with increasing amounts of data. In our experiment, we divide the data into 25%, 50%, and 75% segments among equally distributed entities. Figure 4 illustrates the Recall rate distribution across different percentages of training data. We focus on the top three performing models and observe that BERT-CRF consistently improves with additional data, ultimately achieving the best performance when trained with 100% of the data.",
            "score": 0.41109163503107704,
            "section_title": "Sample Bugs Sample Tag Heuristics Conversion",
            "char_start_offset": 26234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1946
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0220794677734375
        },
        {
            "corpus_id": "270703043",
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "text": "Comparative studies investigating language models are crucial to advance our understanding of them, shed light on their functionalities and pinpoint their constraints. Despite previous research, a notable gap persists in the literature due to, among other cause, current pace of development in NLP. This gap is particularly significant in fields that require heightened sensitivity, such as medicine, where a thorough understanding of models is imperative [45]. Existing research in medicine is mainly focused on specific tasks or datasets or models [5,8,11,14,39]. Moreover, most of the discursive and practical assessments focus on LLMs, as can be seen below. To the best of our knowledge, there is no practical assessment in the clinical context that includes a wide number of pre-trained models, covering all Transformer-based model families, targeting settings where only consumer grade computing resources are available. \n\nThe work by He et al. [13] stands out among exiting descriptive studies, comprehensively addressing the capabilities, limitations, development and integration of language models in healthcare. The language models in scope are pre-trained and large language models. The development process is explained in detail, covering aspects such as training data, methodologies, and optimization strategies. Concerns related to the integration of LLMs into healthcare are also investigated, as fairness, accountability, transparency, and ethics. \n\nZhou et al. [84] also provide a comprehensive overview of the development and deployment of LLMs in medicine, together with the challenges and opportunities these models face. Their study is both discursive and practical, being one of its highlights. The authors detail the principles of existing medical LLMs, comprising basic model structures, number of parameters, and data sources and scales used for model development. A comparison of the performance of different LLMs across various medical tasks, also against state-of-the-art lightweight models, is also included. \n\nContinuing with practical reviews, Soni et al. [85] assessed the cost-effectiveness of pre-training and fine-tuning in BERT, BioBERT, Clinical BERT, and XLNet for medical question answering tasks. Their results indicate that BERT-based models exhibit superior performance when fine-tuned with mixed datasets (i.e., general and clinical domain data), highlighting a gap in well-generalizable medical QA datasets.",
            "score": 0.4110344423696354,
            "section_title": "Related Work",
            "char_start_offset": 16538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2037
                },
                {
                    "start": 2040,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2451
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "263829186"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "250627547"
                },
                {
                    "start": 553,
                    "end": 555,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "264425949"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "258299717"
                },
                {
                    "start": 561,
                    "end": 564,
                    "matchedPaperCorpusId": "249062918"
                },
                {
                    "start": 2087,
                    "end": 2091,
                    "matchedPaperCorpusId": "218974465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08831787109375
        },
        {
            "corpus_id": "216553717",
            "title": "Quantifying the Contextualization of Word Representations with Semantic Class Probing",
            "text": "The very limited contextualization improvement brought by the top two layers may explain why representations from the top layers of BERT can deliver suboptimal performance on NLP tasks (Liu et al., 2019): the top layers are optimized for the pretraining objective, i.e., predicting masked words (Voita et al., 2019), not for the contextualization of words that is helpful for NLP tasks. \n\nBERT layer 0 performs slightly worse than P-BERT, which may be due to the fact that some attention heads in lower layers of BERT attend broadly in the sentence, producing \"bag-of-vector-like\" representations (Clark et al., 2019), which is in fact close to the setup of P-BERT. However, starting from layer 1, BERT gradually improves and surpasses P-BERT, achieving a maximum delta of .161 in F 1 (.831-.670, layer 11 on test). Thus, BERT knows how to better interpret the word in context, i.e., contextualize the word, when progressively going to deeper (higher) layers. P-Rand performs strongly, but is noticeably worse than P-fastText and P-BERT. P-fastText outperforms P-BERT and BERT layers 0 and 1. We conjecture that this is due to the fact that fastText learns embeddings directly for words; P-BERT and BERT have to compose subwords to understand the meaning of a word, which is more challenging. However, starting from layer 2, BERT outperforms P-fastText and P-BERT, illustrating the effectiveness of self attention in better integrating the information from the context into contextualized embeddings than simple averaging in bag-of-word models. \n\nTable 3 and Table 4 jointly illustrate the high quality of word representations computed by BERT. The BERT-derived uncontextualized AVG-BERTrepresentations -modeled on Schuster et al. ( 2019)'s anchor words -show superior capability in inferring s-classes of a word, performing best among all uncontextualized embeddings.",
            "score": 0.41082783760961916,
            "section_title": "S-class inference results",
            "char_start_offset": 13736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 203,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 295,
                    "end": 315,
                    "matchedPaperCorpusId": "202541078"
                },
                {
                    "start": 597,
                    "end": 617,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0101318359375
        },
        {
            "corpus_id": "276394897",
            "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
            "text": "Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.",
            "score": 0.4107176685152487,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "269448976",
            "title": "Building a Large Japanese Web Corpus for Large Language Models",
            "text": "In  and 70B).We also observed that the improvement on Llama 2 13B brought from the presented corpus was the largest among those from other existing corpora.\n\nFuture directions include efforts towards the safety of LLMs, such as reducing harmful generations (e.g., discrimination, exclusion, toxicity, hallucination).Currently, we only use the lists of NG expressions and hostnames, but it is desirable to establish more robust filtering methods to remove harmful text for pre-training Japanese LLMs.In addition, although our study focused on the continual pre-training setting, we want to evaluate the presented corpus by training Japanese LLMs from scratch.Although we evaluated the LLMs in downstream tasks such as question-answering and summarization, it is questionable whether this can measure the \"general intelligence\" of an LLM.At the same time, training an LLM on a pre-training corpus requires huge computations.Therefore, we want to explore a lightweight method for assessing the effectiveness of pre-training corpora without building LLMs.",
            "score": 0.41043372015253865,
            "section_title": "Conclusion",
            "char_start_offset": 22454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 13,
                    "end": 156
                },
                {
                    "start": 158,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18994140625
        },
        {
            "corpus_id": "273654175",
            "title": "Foundation Models in Electrocardiogram: A Review",
            "text": "FMs are large-scale deep-learning models trained on diverse and extensive datasets in a self-supervised or semi-supervised manner [12]. FMs generally follow the pre-training and fine-tuning paradigm, which enables the model to learn a broad range of knowledge and patterns and to leverage such prior information in various downstream tasks and applications. FMs only require considerably smallscale custom data in the fine-tuning phase, showcasing remarkable adaptability and efficiency [125]. FMs have been adopted successfully across domains such as CV, NLP, and healthcare. As the data or model size massively scales up and the available data modality dramatically increases, FMs have evolved from the initial pre-trained models like BERT to contemporary large language models (LLMs), vision foundation models (VFMs), vision-language models (VLMs), and time-series foundation models (TSFMs). \n\nLarge Language Models. In the sphere of NLP, LLMs have fundamentally reshaped our understanding of syntactic and semantic information extracted from input text. Autoregressive-based LLMs, exemplified by powerful models like GPT-3 [81], PaLM [15], and ChatGPT [14,110], have redefined text generation and question-answering capabilities by predicting subsequent words in a sequence and producing contextually coherent outputs. Additionally, the emergence of the groundbreaking LLM, LLaMA [117], represents a significant advancement, striking a balance between high performance, efficiency, and accessibility through innovative encoding tokenization and memory-efficient attention techniques. On the other hand, autoencoder-based LLMs, such as BERT [19] and its remarkable variants like T5 [99], RoBERTa [76], DistilBERT [102], and ELECTRA [17], have redefined the analysis of contextual relationships within text by leveraging bidirectional context consideration. This makes them an indispensable asset for a wide range of NLP tasks, from sentiment analysis to machine translation. \n\nVision Foundation Models. Inspired by the success of FMs in NLP, practitioners have delved into the realm of vision FMs, encompassing VFMs and VLMs.",
            "score": 0.410218851288371,
            "section_title": "Background of Foundation Models",
            "char_start_offset": 5801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "232320522"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1685,
                    "end": 1689,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02001953125
        },
        {
            "corpus_id": "270215793",
            "title": "RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis",
            "text": "In some studies, researchers have explored techniques utilizing pre-trained word embeddings such as Doc2vec, Word2vec, fastText, and GloVe [26], while word embedding is crucial for sentiment analysis. Many ML and DL models have been proposed for sentiment analysis, with sequential models particularly adept at encoding long-distance dependencies in text. However, sequential models are computationally less efficient due to their serialized processing capability. In contrast, Transformer-based LLMs take comparatively less computational time due to their parallelized processing capability. \n\nIn recent years, advancements in LLMs, particularly Transformer-based architectures such as Generative Pretrained Transformer (GPT) [27], Bidirectional Encoder Representations from Transformers (BERT) [28], and Robustly Optimized BERT Pretraining Approach (RoBERTa) [29], have offered even greater potential for improving sentiment analysis tasks [30]- [35]. The Transformer-based model leverages the attention mechanism [36] which makes it more effective in NLP tasks. Particularly, attention mechanism calculates a weighted sum of the input embeddings, where the weights are determined by a learned compatibility function between the query and key embeddings [26]. This capability empowers the model to adeptly capture long-range dependencies within the input sequence, resulting in the creation of more informative representations. Younas et al. [37] proposed two LLMs, Multilingual BERT (mBERT) and XML-RoBERTa (XML-R), for the analysis of code-mixed language comments on a Twitter dataset. Experimental results demonstrated that mBERT and XML-R achieved accuracy (A) scores of 69% and 71%, respectively. In a study [38], the BERT model exhibited superior performance (with an A of 85.4%) compared to ML models for comment analysis. Moreover, comprehensive surveys on text analysis with Transformer-based LLMs are presented in studies [39], [40]. Poria et al. [41] discussed existing challenges and explored new research directions in sentiment analysis. \n\nIn this paper, we propose a hybrid model, RoBERTa-BiLSTM, designed for sentiment analysis.",
            "score": 0.40997518755723106,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "257659479"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "236481688"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "228902981"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "257659479"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "231915077"
                },
                {
                    "start": 1715,
                    "end": 1719,
                    "matchedPaperCorpusId": "232315963"
                },
                {
                    "start": 1934,
                    "end": 1938,
                    "matchedPaperCorpusId": "247218352"
                },
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "266824505"
                },
                {
                    "start": 1959,
                    "end": 1963,
                    "matchedPaperCorpusId": "218470466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06817626953125
        },
        {
            "corpus_id": "220281077",
            "title": "Towards User Friendly Medication Mapping Using Entity-Boosted Two-Tower Neural Network",
            "text": "For the CNN-based model, we test multiple word embedding models including 200-dimensional BioWordVec (Zhang et al., 2019;Chen et al., 2018) and 300-dimensional FastText word embeddings (Bojanowski et al., 2017) trained with 3,466 articles from the Mayo Clinic. The two dimensional CNN layer consisted of 200 filters with window size 2, strip as 1 and no regularization. Batch size is set to 150 and we observed model convergence after six epochs. For the pre-trained language model, we leverage Clinical BERT (Alsentzer et al., 2019), BioBERT (Lee et al., 2019), and original BERT models (Devlin et al., 2018). We used the default settings for all BERT models as provided by Devlin et al. (2018). Batch size is set to 32, learning rate is set to 5 \u00d7 10 \u22125 and dropout rate is set to 0.2. We observed the model converged after 10 epochs. We trained and evaluated all the models using a Tesla V100 GPU.",
            "score": 0.4097901560628632,
            "section_title": "Experimental Details",
            "char_start_offset": 10755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 121,
                    "matchedPaperCorpusId": "149445302"
                },
                {
                    "start": 509,
                    "end": 533,
                    "matchedPaperCorpusId": "102352093"
                },
                {
                    "start": 543,
                    "end": 561,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01073455810546875
        },
        {
            "corpus_id": "267682083",
            "title": "How to Train Data-Efficient LLMs",
            "text": "\u2022 (Figure 19) T5-Small, ASK-LLM: Average data-efficiency of pre-training T5-Small on data sampled by ASK-LLM using the {Flan-T5-Small, Flan-T5-Base, Flan-T5-Large, Flan-T5-XL, Flan-T5-XXL} scoring models, stratified by the maxmimum number of allowed repetitions over the sampled dataset. \n\n\u2022 (Figure 20) T5-Large, ASK-LLM: Average data-efficiency of pre-training T5-Large on data sampled by ASK-LLM using the {Flan-T5-Small, Flan-T5-Base, Flan-T5-Large, Flan-T5-XL, Flan-T5-XXL} scoring models, stratified by the maxmimum number of allowed repetitions over the sampled dataset. \n\n\u2022 (Figure 21) T5-Small, Perplexity filtering: Average data-efficiency of pre-training T5-Small on data sampled by Perplexity filtering using the {T5-Small, T5-Base, T5-Large, T5-XL, T5-XXL} scoring models, stratified by the maxmimum number of allowed repetitions over the sampled dataset. \n\n\u2022 (Figure 22) T5-Large, Perplexity filtering: Average data-efficiency of pre-training T5-Large on data sampled by Perplexity filtering using the {T5-Small, T5-Base, T5-Large, T5-XL, T5-XXL} scoring models, stratified by the maxmimum number of allowed repetitions over the sampled dataset. \n\n\u2022 (Figure 23) T5-Large, Perplexity filtering: Average data-efficiency of pre-training T5-Large on data sampled by Perplexity filtering using the {20k, 100k, 300k, 500k, 700k} intermediate checkpoints of T5-Large as data quality scoring models, stratified by the maxmimum number of allowed repetitions over the sampled dataset.",
            "score": 0.40969166492630293,
            "section_title": "D.2. (Figures 10 to 16) Data-quantity vs. Model-quality for Different Samplers",
            "char_start_offset": 41941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1488
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0521240234375
        },
        {
            "corpus_id": "273963288",
            "title": "Zyda-2: a 5 Trillion Token High-Quality Dataset",
            "text": "For instance, filtering for some proxy of 'educational content' has been used to great effect in the Phi (Abdin et al., 2024) series of models which-although most details are not published-appear to utilize both heavy filtering and a large amount of synthetic data generation. Open datasets utilizing this model-based filtering approach include FineWeb-* https://commoncrawl.org/ Edu (Penedo et al., 2024) and DCLM (Li et al., 2024), which perform extremely well compared to prior datasets. \n\nAt Zyphra, one of our key priorities is producing the highest-quality and most efficient models for a given parameter budget, with a special focus on small, highly powerful models which can automate many tasks cheaply and be utilized on consumer and edge devices. This has driven our innovations in model architecture (Glorioso et al., 2024;Anthony et al., 2024) and also necessitates a focus on constructing strong pretraining and annealing datasets in order to maximize the performance per FLOP and per parameter during training. High-quality datasets appear especially important for smaller models, since they have less total capacity and hence are more affected by significant quantities of noise or low-quality tokens in their training datasets. \n\nOur general approach to dataset creation is to collect all openly available and highly-performing open-source datasets and improve their quality further by removing duplicates and adding additional filtering steps. We then weight the resulting dataset mixture to obtain highest-quality subset that meets our training budget. Our previous dataset constructed with this approach was Zyda-1 (Tokpanov et al., 2024), which was used to train Zamba1-7B (Glorioso et al., 2024). Zyda-1 outperformed all major language modeling datasets at the time, such as Dolma-1.6 Soldaini et al. (2024), FineWeb (Penedo et al., 2024), and RefinedWeb (Penedo et al., 2023).",
            "score": 0.4096285115000869,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1898
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05126953125
        },
        {
            "corpus_id": "278327940",
            "title": "Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models",
            "text": "Recent advancements in NLP have significantly enhanced the capabilities of spam detection systems. LLMs like GPT-4, BERT, and FLAN-T5 have demonstrated exceptional performance in understanding textual data by leveraging extensive pretraining on diverse tasks [13,4]. These models excel at capturing semantic relationships and contextual details, addressing critical gaps in traditional ZSL methods. Studies in anomaly detection have shown that using models like BERT and GPT-4 for preprocessing tasks, such as summarization and feature extraction, improves the understanding and effectiveness of Zero-Shot Learning [12]. \n\n\u2022 BERT: As a bidirectional transformer, BERT processes text holistically, understanding the context of each word in a sentence. Its ability to generate meaningful embeddings makes it highly effective for tasks such as summarization and noise reduction in spam detection pipelines. \n\n\u2022 GPT-4: As one of the most advanced transformer-based models, GPT-4 exhibits exceptional abilities in contextual understanding and text generation. Its use in preprocessing tasks, such as content summarization and feature enrichment, has proven to support better downstream Zero-Shot Learning performance [10]. \n\nBy integrating ZSL with these advanced NLP models, spam detection systems can effectively address challenges such as concept drift, adversarial tactics, and the reliance on extensive labeled datasets.",
            "score": 0.4095063886198169,
            "section_title": "Advancements in NLP and Their Impact on Spam Detection",
            "char_start_offset": 6651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1420
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "202539891"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "268698901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055511474609375
        },
        {
            "corpus_id": "273901594",
            "title": "RRADistill: Distilling LLMs\u2019 Passage Ranking Ability for Long-Tail Queries Document Re-Ranking on a Search Engine",
            "text": "However, the slow inference speed challenges the direct use of LLMs in search engine. To address this, we trained a much smaller Language Model (SLM) to retain HCX-L's ranking ability, following a trend known as LLM distillation, similar to RankGPT and TWOLAR (Baldelli et al., 2024). This involves two stages: 1) Generating ranking label using LLMs, and 2) Training the SLM ranker. When generating ranking label with LLMs, previous studies utilize the list-wise permutation generation method, which inputs a query and a set of documents to LLM and receives an ordered list of document identifiers. However, these approach require sliding windows, which infer multiple times on partial lists due to the prompt length constraints of LLMs, causing a burden. Moreover, previous studies viewed the missing phenomenon as a problem, where LLMs fail to include all input documents in the output. However, we observed that in most cases, excluded documents due to missing are significantly irrelevant to the query, making it a valuable signal. Consequently, we reframed the missing and highlighted its impact. We developed our own label generation pipeline to address these issues, including two key techniques: 1) Pre-rank to filter documents, retaining only those effective to train SLM rankers and bypass the sliding window, 2) Consider missing as useful signals, and utilize excluded documents as hard negatives, to train SLM rankers. Our pipeline speeds up labeling and provides compact yet effective training data. \n\nIn training SLM rankers, we explored both BERT (Devlin et al., 2019) and GPT (Radford et al., 2019) styles, incorporating our training techniques. For BERT ranker, we integrate a term control layer into the training process to utilize specific term matching signals. For GPT ranker, we developed techniques to effectively utilize classification (whether relevant or irrelevant) and reasoning (rationale for relevant or irrelevant) during training, with a light-weight ranking layer. Both rankers incorporate additional training layers, but only specific parts of the model architecture (Encoder plus a classification head for BERT, Decoder plus a dense layer for GPT) are utilized during inference, reducing the burden for service applications.",
            "score": 0.40945852306066666,
            "section_title": "Introduction",
            "char_start_offset": 1606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 283,
                    "matchedPaperCorpusId": "268691914"
                },
                {
                    "start": 1562,
                    "end": 1583,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1592,
                    "end": 1614,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1190185546875
        },
        {
            "corpus_id": "276408138",
            "title": "InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning",
            "text": "These fasttext models are then used to recall relevant content from the remaining corpus. \n\nDeduplication An excessive amount of homogeneous data can be detrimental to small language models. therefore, it is crucial to ensure data diversity. To optimize data efficiency while maintaining semantic diversity, we implemented a global Min-Hash algorithm to efficiently detect and eliminate near-duplicate documents. \n\nQuality assessment To ensure comprehensive data quality across diverse domains, we establish a two-step evaluation framework. First, we employ a domain-specific quality assessment tool: \n\n(1) For web content, we leverage FineWeb-eduscorer (Penedo et al., 2024) to evaluate document quality through multiple dimensions; \n\n(2) For mathematical content, we utilize a model-based classifier following (Lozhkov et al., 2024a) that scores reasoning and deduction capabilities on a 1-5 scale, filtering to retain only high-quality samples; \n\n(3) For code data quality validation, we utilize a static analysis tool to validate syntactic correctness and identify potential structural issues. We also perform comprehensive ablation experiments through continued pre-training on LLaMA 3.2-1B, comparing the performance between models trained on raw and filtered datasets. The model trained on filtered data exhibits better performance across multiple code-related benchmarks compared to its counterpart trained on the raw dataset. Moreover, the filtering process facilitates accelerated convergence, enabling the model to achieve desired performance metrics with fewer training steps. \n\nDecontamination To ensure fairness of comparison, we implemented a token-level 10-gram decontamination algorithm to remove potentially contaminated content in standard benchmarks.",
            "score": 0.40939643345327725,
            "section_title": "Heuristic filtering",
            "char_start_offset": 5981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1770
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "277787386",
            "title": "Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content",
            "text": "Research furthermore shows that combining pre-trained transformer-based models and LLM APIs can guarantee further generalization, proposing a hybrid approach to filtering out unwanted content and improving aspect classification in complex datasets [20]. Despite the above advances, an investigation into the relative effectiveness of LLMs and fine-tuned transformer models for bank-related dialogue, particularly in low-resource languages such as Sinhala, remains a gap in current research [22]. This paper fills these gaps by creating a hybrid keyword extraction system that consists of a fine-tuned SpaCy NER model, FinBERT-enhanced KeyBERT, YAKE, and EmbedRank for English, and a fine-tuned XLM-RoBERTa model with a Sinhala financial dictionary for Sinhala and code-mixed data. \n\nSimilarly, by incorporating BERT-base-uncased content filtering and LLM API tests, that is, GPT, to identify the best strategy for the removal of irrelevant comments and aspect classification. Through a comparative performance comparison between English and Sinhala banking forums, this study seeks to bridge the gap between conventional deep learning methodologies and contemporary LLM-based solutions.",
            "score": 0.40937612085169517,
            "section_title": "II. LITERATURE REVIEW",
            "char_start_offset": 7589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15966796875
        },
        {
            "corpus_id": "273901408",
            "title": "Pretraining Language Models Using Translationese",
            "text": "Fortunately synthetic data is an option and previous works such as, but not limited to, backtranslation (Sennrich et al., 2016a), sequence distillation (Kim and Rush, 2016), also known as forward translation, etc. have shown that synthetic data obtained using machine translation (MT) can supplement resource scarcity and can significantly enhance model performance (Popovi\u0107 et al., 2020;Gala et al., 2023). However, to the best of our knowledge, there has been no work on showing the effectiveness of synthetic data for pre-training LMs. Furthermore, the quality of synthetic data is also important, which many works take for granted. While round-trip-translation (Moon et al., 2020) or referenceless neural quality estimation (QE) (Rei et al., 2021) are viable, they either involve twice the compute or a reasonably large model not available for most languages, and this might not be optimal to determine the quality of synthetic documents efficiently. We thus consider TinyLMs (Eldan and Li, 2023) as an efficient alternative, which have been shown to model documents by their fluent paragraph generation capabilities. \n\nIn this paper, we focus on Indic languages such as Hindi, Gujarati, and Marathi, and present a comprehensive study of the utility of synthetic monolingual data, also called translationese (Gellerstam, 1986), obtained using machine translation (MT) for pre-training LMs. We propose a simple framework that involves training tiny language models, henceforth TinyLMs, on original web-crawled data (clean) and then using them to filter synthetic data. We then compare LMs of different scales pretrained on clean and synthetic data followed by finetuning on natural language understanding (NLP) and generation (NLG) downstream tasks, where we observe that, while unfiltered synthetic data based LMs are inferior compared to LMs trained on clean data, filtering leads to performance comparable to the latter. We further show that tuning these synthetic data LMs on small clean data leads to further improvements. We also show that these trends hold when continually pre-training LLMs such as Gemma-2B and Llama-3-8B. Our contributions are: a.",
            "score": 0.4093140691185081,
            "section_title": "Introduction",
            "char_start_offset": 1496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 128,
                    "matchedPaperCorpusId": "15600925"
                },
                {
                    "start": 366,
                    "end": 388,
                    "matchedPaperCorpusId": "227230276"
                },
                {
                    "start": 733,
                    "end": 751,
                    "matchedPaperCorpusId": "245856036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2215576171875
        },
        {
            "corpus_id": "272524632",
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "text": "We first pretrain 160M-parameter LLMs from scratch to study our primary goal of selecting pretraining data to improve downstream performance, and then we present a brief overview of results on experiments which we preregistered in an earlier version of this document (most experiments show a trend of even greater improvements at larger scales -up to 1.4B parameters -on new benchmarks and data pools; more detail is in Appendices N and M). Finally, we present an analysis on the ability of losses to predict downstream performance. Throughout our experiments, we use single-index models trained using Algorithm 1. As shown in the algorithm, we train the fastText classifier on selected vs. unselected domains and use the classifier to filter the pretraining data at the page-level. \n\nInput data matrix X. To build the input data matrix, X, we collected byte normalized loss values from a sample of 90 Open LLM Leaderboard (Beeching et al., 2023) LLMs that we could run without errors. Concretely, these values are defined as bits-per-byte L T \u2113 L B ln(2) where L T is the token count, L B is the number of UTF-8 bytes, and \u2113 is the per-token cross-entropy (Gao et al., 2020). For our initial experiments, we collected these values on the \"sample\" subset 2 of the RedPajama V2 (RPJv2) dataset (Together Computer, 2023) for all domains with \u2265 25 pages in the sample. There are 9,841 domains/features. Specifics are in Appendix F. The data for the additional preregistered experiments is discussed in Appendix N. A detailed principal components analysis of X, which reveals a variety of salient embedded information in the losses, is in Appendix L. \n\nTarget benchmark performance y. For our initial experiments, we constructed a target vector, y, for LAMBADA (Paperno et al., 2016), ARC Easy (Clark et al., 2018), PIQA (Bisk et al., 2020), and SciQ (Welbl et al., 2017).",
            "score": 0.4086021186946972,
            "section_title": "RESULTS",
            "char_start_offset": 21920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2073974609375
        },
        {
            "corpus_id": "233240947",
            "title": "Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events",
            "text": "To evaluate the importance of each module one at a time, we report the performance of the TSLM by removing the noun-phrase filtering at inference, the consistency rules, timestamp embedding, SQuAD (Rajpurkar et al., 2016) pre-training, and by replacing RoBERTa (Liu et al., 2019) with BERT (Devlin et al., 2019). These variations are evaluated on the development set of the Propara dataset and reported in Table 6. As stated before and shown in Table 6, it is impossible to remove the timestamp embedding as that is the only part of the model enabling changes in the answer at each step. Hence, by removing that, the model cannot converge and yields a 25% decrease on the F1 score. The simple consistency and span filtering rules are relatively easy to be learned by the model based on the available data, therefore adding those does not affect the final performance of the model. TSLM BERT experiment is designed to ensure a fair comparison with previous research (Amini et al., 2020) which has used BERT as their base language model. The comparison of TSLM BERT to -SQuAD Pre-training and -Timestamp Embedding in Table 6 indicates that using RoBERTa instead of BERT is not as much important as our main proposal (using Time-stamp encoding) in TSLM model. Also, TSLM BERT achieves 66.7% F1 score on the Propara test set, which is 1.2% better than the current SOTA performance.\n\nBy removing the SQuAD pre-training phase, the model performance drops with a 10.6% in the F1 score. This indicates that despite the difference between the procedural text understanding and the general MRC tasks, it is quite beneficial to design methods that can transfer knowledge from other QA data sources to help with procedural reasoning. This is crucial as annotating procedural texts is relatively more expensive and time-consuming.  Table 6: Ablation study results on the development set of the Propara document-level task. \"-constraints\", \"-Span filtering\", and \"-Timestamp Encoding\" shows our model performance while removing those modules.\n\n-SQuAD Pre-training is when we do not pre-train our base language model on SQuAD. TSLM BERT is when we use BERT as the base language model.",
            "score": 0.40839349772722333,
            "section_title": "Ablation Study",
            "char_start_offset": 21128,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 221,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 290,
                    "end": 311,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 965,
                    "end": 985,
                    "matchedPaperCorpusId": "214727633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "269009627",
            "title": "Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform",
            "text": "The training set and the language model architecture tend to accumulate substantial noisy information, which is manifested in the values of the embeddings vectors.Consequently, these vectors require some form of filtering.Intuitively, it is evident that this filtering or smoothing process can be effectively accomplished by eliminating the high-frequency components in the Fourier Transform.\n\nIn this article the object of our study is Large Language Models (LLM) and specifically Sentence Transformers [4].Sentence Transformers are a type of model designed to convert sentences into high-dimensional vector representations, making them highly useful for a variety of natural language processing (NLP) tasks.These embeddings, essentially dense vectors, capture the semantic meaning of sentences in a way that allows for efficient computation of sentence similarities, information retrieval, and clustering, among other applications.Essential part here is played by BERT (Bidirectional Encoder Representations from Transformers) [5].BERT is a deep learning model designed for natural language processing.It works by pre-training on a large corpus of text, learning contextual relations between words in a sentence.In sentence transformers, BERT and its variations developed later, such as RoBERTa [6], DistilBERT [7], ALBERT [8], ColBERT [9], is adapted to produce sentence embeddings by processing entire sentences, leveraging its pre-trained contextual insights.The model captures the essence of each sentence in high-dimensional vector space, enabling various NLP tasks like semantic similarity, classification, and more.BERT's bidirectional nature allows it to understand the context of each word based on its surroundings, significantly enhancing the quality of the generated embeddings.\n\nFigure 1 illustrates, at a very high level, how two sentences are processed through the transformer and pooling layers, resulting in the conversion into vectors u and v.These vectors can then be tested for similarity.Let u be the original embeddings vector of size N , so u \u2208 R N .Applying the Fourier Transform (FT) to u, we obtain the corresponding complex vector c in the FT domain, where c is of the same size N .We define vector a as a vector of FT amplitudes, which can be derived from Fourier coefficients:\n\nThe amplitude of each component in a is given by:",
            "score": 0.40835742432055677,
            "section_title": "3.",
            "char_start_offset": 5574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1792
                },
                {
                    "start": 1794,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2011
                },
                {
                    "start": 2011,
                    "end": 2075
                },
                {
                    "start": 2075,
                    "end": 2211
                },
                {
                    "start": 2211,
                    "end": 2307
                },
                {
                    "start": 2309,
                    "end": 2358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06500244140625
        },
        {
            "corpus_id": "258378300",
            "title": "SIDLR: Slot and Intent Detection Models for Low-Resource Language Varieties",
            "text": "In this study, we evaluate several popular multilingual Transformer-based language models, including mBERT, XLM-R, SBERT, LaBSE, LASER, and mT0. These models are capable of effectively capturing cross-lingual embeddings, enabling transfer learning across multiple languages. Below we provide a description of each model used in our experiments on the training dataset. mBERT. is the multilingual version of BERT (Devlin et al., 2019), which is an encoder model with bidirectional representations from Transformers trained with a denoising objective. mBERT is trained on Wikipedia for 104 languages including German and Italian. XLM-R. (Conneau et al., 2020) is a transformerbased multilingual masked language model pretrained on more than 2TB of filtered Common-Crawl data in 100 languages, including languages including German and Italian. XLM-R uses a Transformer model (Vaswani et al., 2017) trained with a multilingual masked language model XLM (Conneau and Lample, 2019). sBERT. Sentence-BERT (SBERT) (Reimers and Gurevych, 2019), is a modification of the pretrained BERT (Devlin et al., 2019) model that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. As we work under a multilingual context, we use the multilingual versions from previously monolingual SBERT models (Reimers and Gurevych, 2020) which is trained for sentence embedding in 50+ languages from various language families. LaBSE. Language-agnostic BERT Sentence Encoder (LaBSE) (Feng et al., 2020a) is a BERTbased model trained to generate sentence embeddings in 109 different languages. The model's pre-training approach involves a combination of masked language modeling and translation language modeling. The pre-training process combines masked language modeling with translation language modeling. LaBSE is useful for producing sentence embeddings in multiple languages and performing bi-text retrieval. LASER.",
            "score": 0.40807724575160825,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 9922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 433,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 635,
                    "end": 656,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 872,
                    "end": 894,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 949,
                    "end": 975,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1077,
                    "end": 1098,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01751708984375
        },
        {
            "corpus_id": "236460100",
            "title": "A Knowledge-Guided Framework for Frame Identification",
            "text": "The overall testing results, as shown in Table 3, demonstrate that Bert-cls and Bert-onehot are two strong baselines, outperforming all of the prior work that does not incorporate pre-training modules into their systems. Bert-onehot slightly outperforms Bert-cls in all of the testing settings, indicating joint learning target word embedding and frame embedding is helpful for FI task. Our best KGFI models, including KGFI (2layers) for FrameNet1.7 and KEFI (1-layer) for FrameNet1.5, outperform all the baseline models of FI in terms of accuracy. Compared with the stronger Bert-onehot model, our model achieves absolute 1.83% and 0.67% improvements on two datasets respectively in All test setting. With the help of lexicon filtering with LUs in FrameNet, the model predicts the exact frame evoked by the target word among a small set of candidate frames. Clearly, the improvements are credited to the model's performance improvement in predicting frames for ambiguous target words, since the model achieves absolute 3.75% and 1.56% improvements in Amb test setting on two datasets respectively. \n\nTo the best of our knowledge, few previous work focus on frame prediction without lexicon filtering  except for SimpleFrameId model, so we choose SimpleFrameId and the stronger Bert-onehot model as our baseline to compare our best model's performance under no-lexicon filter setting. As shown in Table 4, in comparison with the stronger Bert-onehot model, our model achieves absolute 5.72% and 3.63% improvements on two datasets respectively in all setting (without using LUs and compared with more than 1000 frames), signifying the generalization performance of our model achieves significant improvement, considering that the model predicts the exact frame evoked by the target word among all the frames without knowing the possible candidate frames of the target word in no-lexicon filtering setting. \n\nTo further test the performance of our best KGFI model, we use the top-K accuracy to measure the model performance without lexicon filtering. The higher top-K accuracy indicates that the model has learned better frame representations.",
            "score": 0.40781901956078465,
            "section_title": "Overall Results",
            "char_start_offset": 18506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0182342529296875
        },
        {
            "corpus_id": "233423520",
            "title": "Evaluating document representations for content-based legal literature recommendations",
            "text": "Regarding the word vector-based methods, we see that the methods which are trained on the legal corpus (Paragraph Vectors, fastText Legal , GloVe Legal ) perform similarly well with a minor advantage by fastText Legal . Moreover, there is a margin between the generic and legal word vectors even though the legal word vectors are trained on a small corpus compared to ones from the generic vectors. The advantage of Paragraph Vectors over TF-IDF is consistent with the results from Mandal et al. [28]. Limiting the document length to 512 or 4096 decreases the effectiveness of fastText Legal . A limit of 512 tokens decreases the MAP score to 59% compared to all tokens on Open Case Book. With 4096 tokens, the performance decline is only minor (90% compared to all tokens). The token limitation effect is also larger on Open Case Book than Wikisource. The 4096 tokens version of fastText Legal even outperforms all Transformer methods.\n\nLongformer-large is the best Transformer for Open Case Book with 0.031 MAP. For Wikisource, Legal-AUEB-BERT achieves the highest MAP of 0.022, closely followed by Legal-JHU-BERT. The Longformer's theoretical advantage of processing 4096 instead of 512 tokens does not lead to better results for Wikisource, for which even BERT scores the same MAP of 0.018. We generally observe that large models outperform their base counterparts 7 . Likewise, RoBERTa has higher scores than BERT as Liu et al. [27] suggested. From the Transformers category, Sentence Transformers yield the worst results. We assume that fine-tuning on the similarity datasets like NLI or STSB does not increase the performance since the models do not generalize well to other domains. However, the language model fine-tuning from Legal-JHU-BERT and Legal-AUEB-BERT  In the category of hybrid methods, the combination of text and citations improves the performance. For Open Case Book, the score summation Poincar\u00e9 + fastText Legal has the same MAP of 0.05 as fastText Legal but a higher MRR of 0.746. The MRR of Poincar\u00e9 + fastText Legal is even higher than the MRR of its sub-",
            "score": 0.40757245359191685,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 18331,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "44070470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02532958984375
        },
        {
            "corpus_id": "273662413",
            "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges",
            "text": "Chalkidis et al. [20] present an in-depth analysis of applying BERT, a pre-trained LM, in the legal domain, showcasing the need for domain-specific adaptation to enhance performance on legal NLP tasks. They explore three strategies: using standard BERT directly, further pre-training on legal corpora, and pre-training from scratch with legal-specific data. Their study found that both further pre-training and pre-training from scratch generally outperform the use of BERT directly. They introduce legal-bert, a specialised family of models optimised for legal text, which includes versions for varied computational capacities and demonstrates competitive performance with a lower environmental impact. \n\nXiao et al. [139] introduce Lawformer, a Longformer-based [10] LM adapted for Chinese legal texts, designed to handle extensive document lengths common in legal data. Recognising the limitation of standard PLMs with shorter token capacities, Lawformer employs a unique combination of sliding window, dilated sliding window, and global attention mechanisms to process long texts, making it suitable for legal AI tasks, such as LJP and LQA. Pre-trained on a vast corpus of Chinese legal documents segmented into criminal and civil cases, Lawformer integrates complex sequential dependencies across tokens using these attention techniques, enhancing model performance for legal-specific tasks. \n\nIn the development of specialised NLP tools for Arabic legal texts, a model specifically tailored to the unique linguistic features of Arabic jurisprudence was designed, introducing AraLegal-BERT [2] midway through this innovation. This model enhances NLP applications within the legal field by adapting BERT technology to Arabic's specific content needs, involving pre-training BERT from scratch using a broad range of legal documents, including legislative materials and contracts. \n\nColombo et al. [27] introduce SaulLM-7B, a novel LLM specifically designed for legal text comprehension and generation, built on the 7 billion parameter Mistral [60] architecture. This model is trained on an extensive English legal corpus, designed to meet the unique challenges of legal syntax and terms.",
            "score": 0.4075306141775148,
            "section_title": "Language Models",
            "char_start_offset": 87840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1882
                },
                {
                    "start": 1885,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "234342706"
                },
                {
                    "start": 1595,
                    "end": 1598,
                    "matchedPaperCorpusId": "252918047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1513671875
        },
        {
            "corpus_id": "204823809",
            "title": "Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection",
            "text": "Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases using FastText embeddings (Bojanowski et al., 2017) and pre-trained BERT models. Beside these representations, multiple features of readability, sentiment and emotions were considered. For the fragment-level task, they used a multi-task neural sequence tagger, based on LSTM-CRF (Huang et al., 2015), in conjunction with linguistic features. Finally, they applied sentence-and fragment-level models jointly. Team CUNLP (Alhindi et al., 2019) considered two approaches for the sentence-level task. The first approach was based on fine-tuning BERT. The second approach complemented the fine-tuned BERT approach by feeding its decision into a logistic regressor, together with features from the Linguistic Inquiry and Word Count (LIWC)2 lexicon and punctuation-derived features. Similarly to Gupta et al. (2019), for the fragment-level problem they used a Bi-LSTM-CRF architecture, combining both character-and word-level embeddings. \n\nTeam ProperGander (Madabushi et al., 2019) also used BERT, but they paid special attention to the imbalance of the data, as well as to the differences between training and testing. They showed that augmenting the training data by oversampling yielded improvements when testing on data that is temporally far from the training (by increasing recall). In order to deal with the imbalance, they performed cost-sensitive classification, i.e., the errors on the smaller positive class were more costly. \n\nFor the fragment-level classification, inspired by named entity recognition, they used a model based on BERT using Continuous Random Field stacked on top of an LSTM.",
            "score": 0.40738959738497516,
            "section_title": "Teams Participating in Both Tasks",
            "char_start_offset": 15050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 265,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0164031982421875
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "Our approach has multiple limitations. First, while we limit our analysis to a single model family to reduce variance in architecture, it limits the ecological validity of our results. Our results therefore cannot be generalized to all modern LLM architectures, and instead provide a detailed look into BERT specifically. Second, compared to state-of-theart compute intensive training procedures, our pre-training process is quite rudimentary. We limit the training process to only 8000 training steps as a heuristic to upper bound the amount of compute that each model uses; this is a simplification and does not lead to a model as powerful as those available through model hubs like HuggingFace. Third, while we make sure to evaluate bias holistically by examining both upstream and downstream differences, our bias metricssuch as log probability gaps and sentiment-are not definitive ways of measuring bias. Bias is a complex, sociotechnical, and sometimes ill-defined notion whose meaning can vary across domains and tasks. While we rely on metrics from prior work, our measures are prone to the same pitfalls and limitations in validity that most bias measurement work in NLP suffers from (Goldfarb-Tarrant et al. 2023).",
            "score": 0.4073029977504161,
            "section_title": "Limitations",
            "char_start_offset": 31638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1225
                }
            ],
            "ref_mentions": [
                {
                    "start": 1194,
                    "end": 1224,
                    "matchedPaperCorpusId": "221878771"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "265220901",
            "title": "On the Impact of Calibration Data in Post-training Quantization and Pruning",
            "text": "We explore a diverse variety of data sources to create our calibration sets. Following previous work (see Section 2), we include random web text and curated model pre-training datasets. To maintain the integrity of the zero-shot evaluations, we avoid using evaluation data as a source of calibration data, following Frantar et al. (2023). We therefore consider the following five data sources: \n\n\u2022 C4 (Raffel et al., 2020): We use the Colossal Clean Crawled Corpus as our baseline, following Frantar et al. (2023). This consists of web text from Common Crawl, filtered with multiple heuristics to form a subset of clean English text. \n\n\u2022 CNN-DM (Hermann et al., 2015;See et al., 2017): The CNN/Daily Mail corpus consists of news articles from both publishers, covering a broad range of topics. We include this corpus since it provides a focused yet distinct genre of high-quality long-form text. \n\n\u2022 RedPajama (Together Computer, 2023): Since the pre-training data for LLaMA is not publicly available, we instead use an open-source reproduction. This mainly consists of web text (Common Crawl and C4), in addition to selected highquality sources such as arXiv, GitHub, Stack Exchange, and Books3 (Gao et al., 2020). \n\n\u2022 RefinedWeb (Penedo et al., 2023): Assembled through stringent filtering and deduplication of Common Crawl, RefinedWeb is a curated model pre-training dataset. Penedo et al. (2023) find that models trained with this dataset exhibit superior zero-shot generalization abilities compared to alternatives such as The Pile (Gao et al., 2020). \n\n\u2022 Wikipedia: We select English Wikipedia as a source of high-quality encyclopedic text. Specifically, we use a preprocessed and cleaned version of the dump from 2022-03-01, prior to the \"knowledge cutoff\" of our selected models. , 2022). This includes base models (LLaMA and OPT) as well as instruction-tuned models (Vicuna). Additionally, we select these three LLM families since they offer models of comparable sizes.",
            "score": 0.40720567539221353,
            "section_title": "Calibration Data Sources",
            "char_start_offset": 9840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 422,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 667,
                    "end": 684,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 1231,
                    "end": 1252,
                    "matchedPaperCorpusId": "268096300"
                },
                {
                    "start": 1379,
                    "end": 1399,
                    "matchedPaperCorpusId": "268096300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "267122910",
            "title": "SPCM: A Machine Learning Approach for Sentiment-Based Stock Recommendation System",
            "text": "Subsequent to the categorization of labeled data, we partition the dataset into training and test sets in a 7:3 ratio. Following the methodology proposed by Sousa et al. [53], we employ the BERT model as a pre-trained model, finetuning it on our labeled training set for three-class sentiment classification. We then conduct the performance evaluation using 10-fold cross validation. Remarkably, the BERT model achieves an impressive accuracy of 84.12% on the test set. \n\nTo assess the effectiveness of the BERT-based classification approach, we compare it with commonly used classification algorithms from previous studies, including Support Vector Machines (SVM), Convolutional Neural Networks (CNN), and Long Short-Term Memory networks (LSTM), all trained on the same labeled dataset. For the experiment, we used Intel(R) Core(TM) i5-9300H CPU, 16g DDR4 memory, NVIDIA Geforce GTX 1660Ti, and Python 3.7 environment. The performance results are presented in Appendix Table 4. Notably, the BERT model outperforms all other algorithms in sentiment classification within the Guba discussion. This outcome underscores the predictive superiority of pre-trained models in this context.",
            "score": 0.4070122292167261,
            "section_title": "4) SENTIMENT CLASSIFICATION",
            "char_start_offset": 23849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1182
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "211208214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01056671142578125
        },
        {
            "corpus_id": "273532238",
            "title": "Beware of Calibration Data for Pruning Large Language Models",
            "text": "In the Section 3, our empirical study of the open-source DCLM-7B model demonstrates that selecting calibration data similar to the training data can yield better pruning performance. However, in practical scenarios, the training data of many LLMs is not publicly available to users. In this section, we will propose the \"self-generating then sampling\" strategy for sampling calibration data when the training data is unavailable. Formally, given a dataset D as the source of calibration data and an LLM M pre-trained on an inaccessible dataset D t , we aim to sample n instances from D as calibration data D c that has a similar distribution to D t . \n\nRecently, Xu et al. (2024b) disclosed that LLMs internalize patterns such as language structure, word distribution, and even commonsense knowledge from the training data during the training process. Due to their auto-regressive nature, LLMs leverage these internalized patterns when predicting the next token, producing the generated text similar to the training data. Thus, we propose using selfgenerated synthetic data as a proxy for the training data for calibration in post-training pruning. Specifically, for a sample from the source of calibration data D, we truncate the first t tokens as the prefix and then allow the LLM M to generate contextually relevant subsequent content: \n\n(2) \n\nAfter generating the data, we filter the synthetic data to prevent low-quality generated data from negatively impacting pruning effectiveness. We calculate each generated sample's perplexity and filter the k% samples with the highest perplexity. Higher perplexity indicates that the patterns are not well-fitted by the LLM and may differ significantly from the training data, making them unsuitable as calibration data.",
            "score": 0.40694506050319534,
            "section_title": "CALIBRATION DATA SAMPLING METHOD",
            "char_start_offset": 17354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1766
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "paperId": "b3b71c8e01bd487fe8e40d0fc0bd6faed970da96",
            "corpusId": 275213063,
            "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.01028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149467818",
                    "name": "Xinshuo Hu"
                },
                {
                    "authorId": "2257014318",
                    "name": "Zifei Shan"
                },
                {
                    "authorId": "2326046038",
                    "name": "Xinping Zhao"
                },
                {
                    "authorId": "2265614778",
                    "name": "Zetian Sun"
                },
                {
                    "authorId": "2230018369",
                    "name": "Zhenyu Liu"
                },
                {
                    "authorId": "2265618386",
                    "name": "Dongfang Li"
                },
                {
                    "authorId": "2330241155",
                    "name": "Shaolin Ye"
                },
                {
                    "authorId": "2338333120",
                    "name": "Xinyuan Wei"
                },
                {
                    "authorId": "2260297779",
                    "name": "Qian Chen"
                },
                {
                    "authorId": "2285172247",
                    "name": "Baotian Hu"
                },
                {
                    "authorId": "2340382675",
                    "name": "Haofen Wang"
                },
                {
                    "authorId": "2342081596",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2258690227",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with<1B parameters.",
            "corpus_id": "275213063",
            "text": "As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with<1B parameters.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0792236328125
        },
        {
            "paperId": "eee0d1b81257087430b45ee25b45281f2069df3b",
            "corpusId": 270440640,
            "title": "On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models",
            "venue": "Interspeech",
            "year": 2024,
            "referenceCount": 50,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2247862832",
                    "name": "Jinchuan Tian"
                },
                {
                    "authorId": "2111014429",
                    "name": "Yifan Peng"
                },
                {
                    "authorId": "2257117609",
                    "name": "William Chen"
                },
                {
                    "authorId": "2110459735",
                    "name": "Kwanghee Choi"
                },
                {
                    "authorId": "2924113",
                    "name": "Karen Livescu"
                },
                {
                    "authorId": "2308010819",
                    "name": "Shinji Watanabe"
                }
            ],
            "abstract": "The Open Whisper-style Speech Model (OWSM) series was introduced to achieve full transparency in building advanced speech-to-text (S2T) foundation models. To this end, OWSM models are trained on 25 public speech datasets, which are heterogeneous in multiple ways. In this study, we advance the OWSM series by introducing OWSM v3.2, which improves on prior models by investigating and addressing the impacts of this data heterogeneity. Our study begins with a detailed analysis of each dataset, from which we derive two key strategies: data filtering with proxy task to enhance data quality, and the incorporation of punctuation and true-casing using an open large language model (LLM). With all other configurations staying the same, OWSM v3.2 improves performance over the OWSM v3.1 baseline while using 15% less training data.",
            "corpus_id": "270440640",
            "text": "The Open Whisper-style Speech Model (OWSM) series was introduced to achieve full transparency in building advanced speech-to-text (S2T) foundation models. To this end, OWSM models are trained on 25 public speech datasets, which are heterogeneous in multiple ways. In this study, we advance the OWSM series by introducing OWSM v3.2, which improves on prior models by investigating and addressing the impacts of this data heterogeneity. Our study begins with a detailed analysis of each dataset, from which we derive two key strategies: data filtering with proxy task to enhance data quality, and the incorporation of punctuation and true-casing using an open large language model (LLM). With all other configurations staying the same, OWSM v3.2 improves performance over the OWSM v3.1 baseline while using 15% less training data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.042724609375
        },
        {
            "paperId": "413880f17759ffefbc15f0fdf81a83ffa15aa684",
            "corpusId": 245557232,
            "title": "Corporate Bankruptcy Prediction with BERT Model",
            "venue": "ECONLP",
            "year": 2023,
            "referenceCount": 62,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.econlp-1.4.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.03194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152213850",
                    "name": "Alex G. Kim"
                },
                {
                    "authorId": "88999805",
                    "name": "Sang-Yeong Yoon"
                }
            ],
            "abstract": "This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.",
            "corpus_id": "245557232",
            "text": "This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0103302001953125
        },
        {
            "paperId": "7a070cecef295098a529fb5ee40370584f1fe6c2",
            "corpusId": 277510325,
            "title": "MegaMath: Pushing the Limits of Open Math Corpora",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 55,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153433679",
                    "name": "Fan Zhou"
                },
                {
                    "authorId": "2353623252",
                    "name": "Zengzhi Wang"
                },
                {
                    "authorId": "2273563498",
                    "name": "Nikhil Ranjan"
                },
                {
                    "authorId": "2258309092",
                    "name": "Zhoujun Cheng"
                },
                {
                    "authorId": "2274221257",
                    "name": "Liping Tang"
                },
                {
                    "authorId": "2274904834",
                    "name": "Guowei He"
                },
                {
                    "authorId": "100468503",
                    "name": "Zhengzhong Liu"
                },
                {
                    "authorId": "2266839099",
                    "name": "Eric P. Xing"
                }
            ],
            "abstract": "Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.",
            "corpus_id": "277510325",
            "text": "Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.133056640625
        },
        {
            "paperId": "02a2d86c226bffb2a47785d18faa474b94948413",
            "corpusId": 279075783,
            "title": "Talking to Data: Designing Smart Assistants for Humanities Databases",
            "venue": "",
            "year": 2025,
            "referenceCount": 35,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.00986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2364747805",
                    "name": "Alexander Sergeev"
                },
                {
                    "authorId": "2115473005",
                    "name": "Valeriya Goloviznina"
                },
                {
                    "authorId": "2364749906",
                    "name": "Mikhail Melnichenko"
                },
                {
                    "authorId": "2304553048",
                    "name": "Evgeny Kotelnikov"
                }
            ],
            "abstract": "Access to humanities research databases is often hindered by the limitations of traditional interaction formats, particularly in the methods of searching and response generation. This study introduces an LLM-based smart assistant designed to facilitate natural language communication with digital humanities data. The assistant, developed in a chatbot format, leverages the RAG approach and integrates state-of-the-art technologies such as hybrid search, automatic query generation, text-to-SQL filtering, semantic database search, and hyperlink insertion. To evaluate the effectiveness of the system, experiments were conducted to assess the response quality of various language models. The testing was based on the Prozhito digital archive, which contains diary entries from predominantly Russian-speaking individuals who lived in the 20th century. The chatbot is tailored to support anthropology and history researchers, as well as non-specialist users with an interest in the field, without requiring prior technical training. By enabling researchers to query complex databases with natural language, this tool aims to enhance accessibility and efficiency in humanities research. The study highlights the potential of Large Language Models to transform the way researchers and the public interact with digital archives, making them more intuitive and inclusive. Additional materials are presented in GitHub repository: https://github.com/alekosus/talking-to-data-intersys2025.",
            "corpus_id": "279075783",
            "text": "Access to humanities research databases is often hindered by the limitations of traditional interaction formats, particularly in the methods of searching and response generation. This study introduces an LLM-based smart assistant designed to facilitate natural language communication with digital humanities data. The assistant, developed in a chatbot format, leverages the RAG approach and integrates state-of-the-art technologies such as hybrid search, automatic query generation, text-to-SQL filtering, semantic database search, and hyperlink insertion. To evaluate the effectiveness of the system, experiments were conducted to assess the response quality of various language models. The testing was based on the Prozhito digital archive, which contains diary entries from predominantly Russian-speaking individuals who lived in the 20th century. The chatbot is tailored to support anthropology and history researchers, as well as non-specialist users with an interest in the field, without requiring prior technical training. By enabling researchers to query complex databases with natural language, this tool aims to enhance accessibility and efficiency in humanities research. The study highlights the potential of Large Language Models to transform the way researchers and the public interact with digital archives, making them more intuitive and inclusive. Additional materials are presented in GitHub repository: https://github.com/alekosus/talking-to-data-intersys2025.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01267242431640625
        },
        {
            "paperId": "6a6c890306198fee39ad0ec2d218e11fbb320025",
            "corpusId": 271097644,
            "title": "Euclid preparation. LI. Forecasting the recovery of galaxy physical properties and their relations with template-fitting and machine-learning methods.",
            "venue": "Astronomy &amp; Astrophysics",
            "year": 2024,
            "referenceCount": 104,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.07940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310698212",
                    "name": "Euclid Collaboration A. Enia"
                },
                {
                    "authorId": "5919593",
                    "name": "M. Bolzonella"
                },
                {
                    "authorId": "117359478",
                    "name": "L. Pozzetti"
                },
                {
                    "authorId": "2256689083",
                    "name": "A. Humphrey"
                },
                {
                    "authorId": "2186737325",
                    "name": "P. Cunha"
                },
                {
                    "authorId": "144626501",
                    "name": "W. Hartley"
                },
                {
                    "authorId": "51241348",
                    "name": "F. Dubath"
                },
                {
                    "authorId": "4196114",
                    "name": "S. Paltani"
                },
                {
                    "authorId": "2302801252",
                    "name": "X. L. Lopez"
                },
                {
                    "authorId": "102545680",
                    "name": "S. Quai"
                },
                {
                    "authorId": "2276761789",
                    "name": "S. Bardelli"
                },
                {
                    "authorId": "31165175",
                    "name": "L. Bisigello"
                },
                {
                    "authorId": "2568179",
                    "name": "S. Cavuoti"
                },
                {
                    "authorId": "40809692",
                    "name": "G. Lucia"
                },
                {
                    "authorId": "102935009",
                    "name": "M. Ginolfi"
                },
                {
                    "authorId": "12897738",
                    "name": "A. Grazian"
                },
                {
                    "authorId": "2296712442",
                    "name": "M. Siudek"
                },
                {
                    "authorId": "2284220906",
                    "name": "C. Tortora"
                },
                {
                    "authorId": "2163163103",
                    "name": "G. Zamorani"
                },
                {
                    "authorId": "2993020",
                    "name": "N. Aghanim"
                },
                {
                    "authorId": "3437487",
                    "name": "B. Altieri"
                },
                {
                    "authorId": "2256440113",
                    "name": "A. Amara"
                },
                {
                    "authorId": "1748532",
                    "name": "S. Andreon"
                },
                {
                    "authorId": "2309852243",
                    "name": "N. Auricchio"
                },
                {
                    "authorId": "2488388",
                    "name": "C. Baccigalupi"
                },
                {
                    "authorId": "2247907098",
                    "name": "M. Baldi"
                },
                {
                    "authorId": "2256643411",
                    "name": "R. Bender"
                },
                {
                    "authorId": "2220127",
                    "name": "C. Bodendorf"
                },
                {
                    "authorId": "66835922",
                    "name": "D. Bonino"
                },
                {
                    "authorId": "4804744",
                    "name": "E. Branchini"
                },
                {
                    "authorId": "2260337097",
                    "name": "M. Brescia"
                },
                {
                    "authorId": "144090605",
                    "name": "J. Brinchmann"
                },
                {
                    "authorId": "49731796",
                    "name": "S. Camera"
                },
                {
                    "authorId": "108516655",
                    "name": "V. Capobianco"
                },
                {
                    "authorId": "37874349",
                    "name": "C. Carbone"
                },
                {
                    "authorId": "119894740",
                    "name": "J. Carretero"
                },
                {
                    "authorId": "144248743",
                    "name": "S. Casas"
                },
                {
                    "authorId": "4969574",
                    "name": "F. Castander"
                },
                {
                    "authorId": "2241604074",
                    "name": "M. Castellano"
                },
                {
                    "authorId": "102356560",
                    "name": "G. Castignani"
                },
                {
                    "authorId": "2205314691",
                    "name": "A. Cimatti"
                },
                {
                    "authorId": "1380246041",
                    "name": "C. Colodro-Conde"
                },
                {
                    "authorId": "48518696",
                    "name": "G. Congedo"
                },
                {
                    "authorId": "153934123",
                    "name": "C. Conselice"
                },
                {
                    "authorId": "8114978",
                    "name": "L. Conversi"
                },
                {
                    "authorId": "69010811",
                    "name": "Y. Copin"
                },
                {
                    "authorId": "52105032",
                    "name": "L. Corcione"
                },
                {
                    "authorId": "3687489",
                    "name": "F. Courbin"
                },
                {
                    "authorId": "2258955424",
                    "name": "H. Courtois"
                },
                {
                    "authorId": "2110269114",
                    "name": "A. D. Silva"
                },
                {
                    "authorId": "5456964",
                    "name": "H. Degaudenzi"
                },
                {
                    "authorId": "2261203614",
                    "name": "A. D. Giorgio"
                },
                {
                    "authorId": "145427654",
                    "name": "J. Dinis"
                },
                {
                    "authorId": "6729944",
                    "name": "X. Dupac"
                },
                {
                    "authorId": "6909880",
                    "name": "S. Dusini"
                },
                {
                    "authorId": "2130572674",
                    "name": "M. Fabricius"
                },
                {
                    "authorId": "117076357",
                    "name": "M. Farina"
                },
                {
                    "authorId": "103168094",
                    "name": "S. Farrens"
                },
                {
                    "authorId": "47118705",
                    "name": "S. Ferriol"
                },
                {
                    "authorId": "6536354",
                    "name": "P. Fosalba"
                },
                {
                    "authorId": "51249528",
                    "name": "S. Fotopoulou"
                },
                {
                    "authorId": "3073847",
                    "name": "M. Frailis"
                },
                {
                    "authorId": "46832563",
                    "name": "E. Franceschi"
                },
                {
                    "authorId": "52253858",
                    "name": "M. Fumana"
                },
                {
                    "authorId": "2574173",
                    "name": "S. Galeotta"
                },
                {
                    "authorId": "102716273",
                    "name": "B. Gillis"
                },
                {
                    "authorId": "51456607",
                    "name": "C. Giocoli"
                },
                {
                    "authorId": "1411206975",
                    "name": "F. Grupp"
                },
                {
                    "authorId": "69054246",
                    "name": "S. Haugan"
                },
                {
                    "authorId": "1599284720",
                    "name": "W. Holmes"
                },
                {
                    "authorId": "2249830907",
                    "name": "I. Hook"
                },
                {
                    "authorId": "12095927",
                    "name": "F. Hormuth"
                },
                {
                    "authorId": "46773613",
                    "name": "A. Hornstrup"
                },
                {
                    "authorId": "33445215",
                    "name": "K. Jahnke"
                },
                {
                    "authorId": "30125421",
                    "name": "B. Joachimi"
                },
                {
                    "authorId": "102922668",
                    "name": "E. Keihanen"
                },
                {
                    "authorId": "11647299",
                    "name": "S. Kermiche"
                },
                {
                    "authorId": "145717114",
                    "name": "A. Kiessling"
                },
                {
                    "authorId": "95237098",
                    "name": "B. Kubik"
                },
                {
                    "authorId": "152478113",
                    "name": "M. Kummel"
                },
                {
                    "authorId": "2270751471",
                    "name": "M. Kunz"
                },
                {
                    "authorId": "1380246077",
                    "name": "H. Kurki-Suonio"
                },
                {
                    "authorId": "66032887",
                    "name": "S. Ligori"
                },
                {
                    "authorId": "3926702",
                    "name": "P. Lilje"
                },
                {
                    "authorId": "103088323",
                    "name": "V. Lindholm"
                },
                {
                    "authorId": "12563398",
                    "name": "I. Lloro"
                },
                {
                    "authorId": "2068971344",
                    "name": "E. Maiorano"
                },
                {
                    "authorId": "3483703",
                    "name": "O. Mansutti"
                },
                {
                    "authorId": "52201814",
                    "name": "O. Marggraf"
                },
                {
                    "authorId": "119778629",
                    "name": "K. Markovi\u010d"
                },
                {
                    "authorId": "2275237204",
                    "name": "M. Martinelli"
                },
                {
                    "authorId": "48525489",
                    "name": "N. Martinet"
                },
                {
                    "authorId": "30800771",
                    "name": "F. Marulli"
                },
                {
                    "authorId": "2242178060",
                    "name": "R. Massey"
                },
                {
                    "authorId": "144015152",
                    "name": "H. McCracken"
                },
                {
                    "authorId": "15147676",
                    "name": "E. Medinaceli"
                },
                {
                    "authorId": "31025762",
                    "name": "S. Mei"
                },
                {
                    "authorId": "108020378",
                    "name": "M. Melchior"
                },
                {
                    "authorId": "3849545",
                    "name": "Y. Mellier"
                },
                {
                    "authorId": "2251961290",
                    "name": "M. Meneghetti"
                },
                {
                    "authorId": "2186182446",
                    "name": "E. Merlin"
                },
                {
                    "authorId": "6030188",
                    "name": "G. Meylan"
                },
                {
                    "authorId": "2568146",
                    "name": "Michele Moresco"
                },
                {
                    "authorId": "2302030921",
                    "name": "L. Moscardini"
                },
                {
                    "authorId": "77191711",
                    "name": "E. Munari"
                },
                {
                    "authorId": "2568705",
                    "name": "C. Neissner"
                },
                {
                    "authorId": "40465320",
                    "name": "S. Niemi"
                },
                {
                    "authorId": "102270538",
                    "name": "J. Nightingale"
                },
                {
                    "authorId": "2241600378",
                    "name": "C. Padilla"
                },
                {
                    "authorId": "3248528",
                    "name": "F. Pasian"
                },
                {
                    "authorId": "2256991407",
                    "name": "K. Pedersen"
                },
                {
                    "authorId": "3691085",
                    "name": "V. Pettorino"
                },
                {
                    "authorId": "6018607",
                    "name": "G. Polenta"
                },
                {
                    "authorId": "47788108",
                    "name": "M. Poncet"
                },
                {
                    "authorId": "144761488",
                    "name": "L. Popa"
                },
                {
                    "authorId": "103255899",
                    "name": "F. Raison"
                },
                {
                    "authorId": "2169602505",
                    "name": "R. Rebolo"
                },
                {
                    "authorId": "1992929",
                    "name": "A. Renzi"
                },
                {
                    "authorId": "2241967323",
                    "name": "J. Rhodes"
                },
                {
                    "authorId": "2264761576",
                    "name": "G. Riccio"
                },
                {
                    "authorId": "52154193",
                    "name": "E. Romelli"
                },
                {
                    "authorId": "89191647",
                    "name": "M. Roncarelli"
                },
                {
                    "authorId": "81693785",
                    "name": "E. Rossetti"
                },
                {
                    "authorId": "96237694",
                    "name": "R. Saglia"
                },
                {
                    "authorId": "108490464",
                    "name": "Z. Sakr"
                },
                {
                    "authorId": "8377552",
                    "name": "D. Sapone"
                },
                {
                    "authorId": "2058146233",
                    "name": "P. Schneider"
                },
                {
                    "authorId": "102268726",
                    "name": "T. Schrabback"
                },
                {
                    "authorId": "2272587126",
                    "name": "M. Scodeggio"
                },
                {
                    "authorId": "92964702",
                    "name": "A. Secroun"
                },
                {
                    "authorId": "102355644",
                    "name": "E. Sefusatti"
                },
                {
                    "authorId": "49328699",
                    "name": "G. Seidel"
                },
                {
                    "authorId": "2053825494",
                    "name": "S. Serrano"
                },
                {
                    "authorId": "47085800",
                    "name": "C. Sirignano"
                },
                {
                    "authorId": "15093029",
                    "name": "G. Sirri"
                },
                {
                    "authorId": "2224081916",
                    "name": "L. Stanco"
                },
                {
                    "authorId": "49347199",
                    "name": "J. Steinwagner"
                },
                {
                    "authorId": "2207075014",
                    "name": "C. Surace"
                },
                {
                    "authorId": "1380246072",
                    "name": "P. Tallada-Cresp'i"
                },
                {
                    "authorId": "73702643",
                    "name": "D. Tavagnacco"
                },
                {
                    "authorId": "2242242673",
                    "name": "A. Taylor"
                },
                {
                    "authorId": "2268760331",
                    "name": "H. Teplitz"
                },
                {
                    "authorId": "3440614",
                    "name": "I. Tereno"
                },
                {
                    "authorId": "1380245958",
                    "name": "R. Toledo-Moreo"
                },
                {
                    "authorId": "103759883",
                    "name": "F. Torradeflot"
                },
                {
                    "authorId": "5972836",
                    "name": "I. Tutusaus"
                },
                {
                    "authorId": "5725782",
                    "name": "L. Valenziano"
                },
                {
                    "authorId": "95807245",
                    "name": "T. Vassallo"
                },
                {
                    "authorId": "1833440",
                    "name": "G. V. Kleijn"
                },
                {
                    "authorId": "69904258",
                    "name": "A. Veropalumbo"
                },
                {
                    "authorId": "47905241",
                    "name": "Y. Wang"
                },
                {
                    "authorId": "2249429875",
                    "name": "J. Weller"
                },
                {
                    "authorId": "2246273762",
                    "name": "E. Zucca"
                },
                {
                    "authorId": "153549651",
                    "name": "A. Biviano"
                },
                {
                    "authorId": "81558107",
                    "name": "A. Boucaud"
                },
                {
                    "authorId": "2269510",
                    "name": "C. Burigana"
                },
                {
                    "authorId": "2255466480",
                    "name": "M. Calabrese"
                },
                {
                    "authorId": "2237423400",
                    "name": "J. Vigo"
                },
                {
                    "authorId": "1380246220",
                    "name": "J. Graci\u00e1-Carpio"
                },
                {
                    "authorId": "6390974",
                    "name": "N. Mauri"
                },
                {
                    "authorId": "2257329746",
                    "name": "A. Pezzotta"
                },
                {
                    "authorId": "14971047",
                    "name": "M. Pontinen"
                },
                {
                    "authorId": "6496054",
                    "name": "C. Porciani"
                },
                {
                    "authorId": "102260274",
                    "name": "V. Scottez"
                },
                {
                    "authorId": "1929870",
                    "name": "M. Tenti"
                },
                {
                    "authorId": "151141461",
                    "name": "M. Viel"
                },
                {
                    "authorId": "72486310",
                    "name": "M. Wiesmann"
                },
                {
                    "authorId": "2278171699",
                    "name": "Y. Akrami"
                },
                {
                    "authorId": "71360070",
                    "name": "V. Allevato"
                },
                {
                    "authorId": "51168263",
                    "name": "S. Anselmi"
                },
                {
                    "authorId": "92991858",
                    "name": "M. Ballardini"
                },
                {
                    "authorId": "102327618",
                    "name": "P. Bergamini"
                },
                {
                    "authorId": "8154187",
                    "name": "M. B\u00e9thermin"
                },
                {
                    "authorId": "2242064889",
                    "name": "A. Blanchard"
                },
                {
                    "authorId": "103103306",
                    "name": "L. Blot"
                },
                {
                    "authorId": "3838450",
                    "name": "S. Borgani"
                },
                {
                    "authorId": "2265382677",
                    "name": "S. Bruton"
                },
                {
                    "authorId": "122310377",
                    "name": "R. Cabanac"
                },
                {
                    "authorId": "2224886858",
                    "name": "A. Calabr\u00f2"
                },
                {
                    "authorId": "2249533065",
                    "name": "G. Ca\u00f1as-Herrera"
                },
                {
                    "authorId": "114283565",
                    "name": "A. Cappi"
                },
                {
                    "authorId": "143886458",
                    "name": "C. Carvalho"
                },
                {
                    "authorId": "151424547",
                    "name": "T. Castro"
                },
                {
                    "authorId": "2261246009",
                    "name": "K. Chambers"
                },
                {
                    "authorId": "89970784",
                    "name": "S. Contarini"
                },
                {
                    "authorId": "3288401",
                    "name": "T. Contini"
                },
                {
                    "authorId": "1399728540",
                    "name": "A. Cooray"
                },
                {
                    "authorId": "4817940",
                    "name": "O. Cucciati"
                },
                {
                    "authorId": "13109234",
                    "name": "S. Davini"
                },
                {
                    "authorId": "2079941755",
                    "name": "B. D. Caro"
                },
                {
                    "authorId": "103261487",
                    "name": "G. Desprez"
                },
                {
                    "authorId": "2164336044",
                    "name": "A. D'iaz-S'anchez"
                },
                {
                    "authorId": "2156301936",
                    "name": "S. Domizio"
                },
                {
                    "authorId": "65805413",
                    "name": "H. Dole"
                },
                {
                    "authorId": "5618143",
                    "name": "S. Escoffier"
                },
                {
                    "authorId": "2297188318",
                    "name": "A. Ferrari"
                },
                {
                    "authorId": "2265492779",
                    "name": "P. G. Ferreira"
                },
                {
                    "authorId": "104877970",
                    "name": "I. Ferrero"
                },
                {
                    "authorId": "5801502",
                    "name": "A. Finoguenov"
                },
                {
                    "authorId": "51398556",
                    "name": "F. Fornari"
                },
                {
                    "authorId": "80007770",
                    "name": "L. Gabarra"
                },
                {
                    "authorId": "50689914",
                    "name": "K. Ganga"
                },
                {
                    "authorId": "2067876207",
                    "name": "J. Garc'ia-Bellido"
                },
                {
                    "authorId": "2342779",
                    "name": "V. Gautard"
                },
                {
                    "authorId": "6009422",
                    "name": "E. Gazta\u00f1aga"
                },
                {
                    "authorId": "2265489759",
                    "name": "F. Giacomini"
                },
                {
                    "authorId": "48790236",
                    "name": "F. Gianotti"
                },
                {
                    "authorId": "88742255",
                    "name": "G. Gozaliasl"
                },
                {
                    "authorId": "2267490304",
                    "name": "A. Hall"
                },
                {
                    "authorId": "28279829",
                    "name": "S. Hemmati"
                },
                {
                    "authorId": "2261389594",
                    "name": "H. Hildebrandt"
                },
                {
                    "authorId": "2293615796",
                    "name": "J. Hjorth"
                },
                {
                    "authorId": "2248523660",
                    "name": "A. Mu\u00f1oz"
                },
                {
                    "authorId": "9916889",
                    "name": "S. Joudaki"
                },
                {
                    "authorId": "94502481",
                    "name": "J. Kajava"
                },
                {
                    "authorId": "2043319272",
                    "name": "Vanshika Kansal"
                },
                {
                    "authorId": "2256283372",
                    "name": "D. Karagiannis"
                },
                {
                    "authorId": "21540573",
                    "name": "C. Kirkpatrick"
                },
                {
                    "authorId": "2302795602",
                    "name": "J. L. Graet"
                },
                {
                    "authorId": "2256718360",
                    "name": "L. Legrand"
                },
                {
                    "authorId": "103127479",
                    "name": "A. Loureiro"
                },
                {
                    "authorId": "1399376020",
                    "name": "J. Mac\u00edas-P\u00e9rez"
                },
                {
                    "authorId": "2254516525",
                    "name": "G. Maggio"
                },
                {
                    "authorId": "2310631305",
                    "name": "M. Magliocchetti"
                },
                {
                    "authorId": "2282962920",
                    "name": "C. Mancini"
                },
                {
                    "authorId": "8389856",
                    "name": "F. Mannucci"
                },
                {
                    "authorId": "31063452",
                    "name": "R. Maoli"
                },
                {
                    "authorId": "2249531152",
                    "name": "C. Martins"
                },
                {
                    "authorId": "2296600815",
                    "name": "S. Matthew"
                },
                {
                    "authorId": "102479600",
                    "name": "L. Maurin"
                },
                {
                    "authorId": "143736725",
                    "name": "R. B. Metcalf"
                },
                {
                    "authorId": "2069976986",
                    "name": "P. Monaco"
                },
                {
                    "authorId": "2267727743",
                    "name": "C. Moretti"
                },
                {
                    "authorId": "4303406",
                    "name": "G. Morgante"
                },
                {
                    "authorId": "2267725803",
                    "name": "N. A. Walton"
                },
                {
                    "authorId": "11914711",
                    "name": "L. Patrizii"
                },
                {
                    "authorId": "2066308744",
                    "name": "V. Popa"
                },
                {
                    "authorId": "50126970",
                    "name": "D. Potter"
                },
                {
                    "authorId": "2085888229",
                    "name": "I. Risso"
                },
                {
                    "authorId": "101929864",
                    "name": "P. Rocci"
                },
                {
                    "authorId": "2133346881",
                    "name": "M. Sahl'en"
                },
                {
                    "authorId": "2071234234",
                    "name": "A. Schneider"
                },
                {
                    "authorId": "2265491322",
                    "name": "M. Schultheis"
                },
                {
                    "authorId": "98358467",
                    "name": "M. Sereno"
                },
                {
                    "authorId": "2052639391",
                    "name": "P. Simon"
                },
                {
                    "authorId": "50350073",
                    "name": "A. Mancini"
                },
                {
                    "authorId": "2181605284",
                    "name": "S. Stanford"
                },
                {
                    "authorId": "102557399",
                    "name": "K. Tanidis"
                },
                {
                    "authorId": "2302770495",
                    "name": "C. Tao"
                },
                {
                    "authorId": "113026221",
                    "name": "G. Testera"
                },
                {
                    "authorId": "2089999978",
                    "name": "R. Teyssier"
                },
                {
                    "authorId": "2267725970",
                    "name": "S. Toft"
                },
                {
                    "authorId": "2265382957",
                    "name": "S. Tosi"
                },
                {
                    "authorId": "152981643",
                    "name": "A. Troja"
                },
                {
                    "authorId": "2304787231",
                    "name": "M. Tucci"
                },
                {
                    "authorId": "92702151",
                    "name": "C. Valieri"
                },
                {
                    "authorId": "5048135",
                    "name": "J. Valiviita"
                },
                {
                    "authorId": "2309805135",
                    "name": "D. Vergani"
                },
                {
                    "authorId": "114039994",
                    "name": "G. Verza"
                },
                {
                    "authorId": "15511401",
                    "name": "I. A. Zinchenko"
                },
                {
                    "authorId": "6037679",
                    "name": "G. Rodighiero"
                },
                {
                    "authorId": "2310698526",
                    "name": "M. T. D. D. F. E. A. R. Bologna"
                },
                {
                    "authorId": "1422457674",
                    "name": "V. G. 932"
                },
                {
                    "authorId": "103220483",
                    "name": "40129 Bologna"
                },
                {
                    "authorId": "2251847707",
                    "name": "Italy"
                },
                {
                    "authorId": "102827193",
                    "name": "I. Bologna"
                },
                {
                    "authorId": "103230173",
                    "name": "V. G. 933"
                },
                {
                    "authorId": "88742463",
                    "name": "I. D. A. E. C. D. Espacco"
                },
                {
                    "authorId": "88740406",
                    "name": "U. Porto"
                },
                {
                    "authorId": "88741503",
                    "name": "Caup"
                },
                {
                    "authorId": "102718033",
                    "name": "Rua das Estrelas"
                },
                {
                    "authorId": "123753107",
                    "name": "P. Porto"
                },
                {
                    "authorId": "97953156",
                    "name": "Portugal"
                },
                {
                    "authorId": "2302802181",
                    "name": "DTx -- Digital Transformation CoLAB"
                },
                {
                    "authorId": "1928320330",
                    "name": "1. Building"
                },
                {
                    "authorId": "2302801433",
                    "name": "Azur'em Campus"
                },
                {
                    "authorId": "2285886998",
                    "name": "University of Minho"
                },
                {
                    "authorId": "2302800583",
                    "name": "4800-058 Guimaraes"
                },
                {
                    "authorId": "123595280",
                    "name": "F. Porto"
                },
                {
                    "authorId": "2240119175",
                    "name": "Rua do Campo Alegre"
                },
                {
                    "authorId": "2039660862",
                    "name": "4169-007 Porto"
                },
                {
                    "authorId": "102339908",
                    "name": "D. Astronomy"
                },
                {
                    "authorId": "52229751",
                    "name": "U. Geneva"
                },
                {
                    "authorId": "2087693883",
                    "name": "16 ch.d'Ecogia"
                },
                {
                    "authorId": "103000020",
                    "name": "1290 Versoix"
                },
                {
                    "authorId": "2242527384",
                    "name": "Switzerland"
                },
                {
                    "authorId": "152565545",
                    "name": "-INAF"
                },
                {
                    "authorId": "103309516",
                    "name": "Istituto di Radioastronomia"
                },
                {
                    "authorId": "102884995",
                    "name": "V. G. 101"
                },
                {
                    "authorId": "103313339",
                    "name": "D. Galilei'"
                },
                {
                    "authorId": "2228641567",
                    "name": "U. Padova"
                },
                {
                    "authorId": "102349302",
                    "name": "8. viaMarzolo"
                },
                {
                    "authorId": "2189423295",
                    "name": "35131 Padova"
                },
                {
                    "authorId": "90940924",
                    "name": "I. -. Capodimonte"
                },
                {
                    "authorId": "2157044864",
                    "name": "16 viaMoiariello"
                },
                {
                    "authorId": "2191896907",
                    "name": "80131 Napoli"
                },
                {
                    "authorId": "2157043002",
                    "name": "I. Naples"
                },
                {
                    "authorId": "2157044898",
                    "name": "6. ViaCinthia"
                },
                {
                    "authorId": "2191895395",
                    "name": "80126"
                },
                {
                    "authorId": "2241270084",
                    "name": "Napoli"
                },
                {
                    "authorId": "145906293",
                    "name": "Inaf Trieste"
                },
                {
                    "authorId": "1417420946",
                    "name": "11 ViaG.B.Tiepolo"
                },
                {
                    "authorId": "2037383460",
                    "name": "34127 Trieste"
                },
                {
                    "authorId": "102468963",
                    "name": "D. Astronomia"
                },
                {
                    "authorId": "102392194",
                    "name": "U. Firenze"
                },
                {
                    "authorId": "2161241487",
                    "name": "1. viaG.Sansone"
                },
                {
                    "authorId": "2161241489",
                    "name": "50019 Sesto Fiorentino"
                },
                {
                    "authorId": "90896560",
                    "name": "Firenze"
                },
                {
                    "authorId": "103369815",
                    "name": "Inaf - Arcetri"
                },
                {
                    "authorId": "1404104430",
                    "name": "5. LargoE.Fermi"
                },
                {
                    "authorId": "2162781628",
                    "name": "50125"
                },
                {
                    "authorId": "152168030",
                    "name": "I. Padova"
                },
                {
                    "authorId": "2157044529",
                    "name": "5. Viadell'Osservatorio"
                },
                {
                    "authorId": "2189423295",
                    "name": "35131 Padova"
                },
                {
                    "authorId": "1410640555",
                    "name": "Instituto de Astrof'isica de Canarias"
                },
                {
                    "authorId": "1460222454",
                    "name": "D. Astrof'isica"
                },
                {
                    "authorId": "145562748",
                    "name": "U. L. Laguna"
                },
                {
                    "authorId": "2223756665",
                    "name": "38200"
                },
                {
                    "authorId": "3984288",
                    "name": "La Laguna"
                },
                {
                    "authorId": "152262807",
                    "name": "Tenerife"
                },
                {
                    "authorId": "121444207",
                    "name": "Spain."
                },
                {
                    "authorId": "102837369",
                    "name": "Institute for Space Sciences"
                },
                {
                    "authorId": "102449811",
                    "name": "Campus Uab"
                },
                {
                    "authorId": "2157044640",
                    "name": "Carrer de Can Magrans"
                },
                {
                    "authorId": "102439013",
                    "name": "sn."
                },
                {
                    "authorId": "1388304977",
                    "name": "08193 Barcelona"
                },
                {
                    "authorId": "1388354217",
                    "name": "Universit'e Paris-Saclay"
                },
                {
                    "authorId": "88741801",
                    "name": "Cnrs"
                },
                {
                    "authorId": "88742276",
                    "name": "I. D. Spatiale"
                },
                {
                    "authorId": "2142464246",
                    "name": "91405"
                },
                {
                    "authorId": "152116034",
                    "name": "Orsay"
                },
                {
                    "authorId": "2244814133",
                    "name": "France"
                },
                {
                    "authorId": "103230873",
                    "name": "Esacesa"
                },
                {
                    "authorId": "2065337646",
                    "name": "Camino Bajo de Castillo"
                },
                {
                    "authorId": "102439013",
                    "name": "sn."
                },
                {
                    "authorId": "102800819",
                    "name": "Urb. Villafranca del Castillo"
                },
                {
                    "authorId": "1388304707",
                    "name": "28692 Villanueva de la Canada"
                },
                {
                    "authorId": "2252612614",
                    "name": "Madrid"
                },
                {
                    "authorId": "51954309",
                    "name": "S. O. Mathematics"
                },
                {
                    "authorId": "30064669",
                    "name": "Physics"
                },
                {
                    "authorId": "103192379",
                    "name": "University of Surrey"
                },
                {
                    "authorId": "80276246",
                    "name": "Guildford"
                },
                {
                    "authorId": "14622441",
                    "name": "Surrey"
                },
                {
                    "authorId": "1734786106",
                    "name": "GU2 7XH"
                },
                {
                    "authorId": "2258971425",
                    "name": "Uk"
                },
                {
                    "authorId": "152287482",
                    "name": "I. Brera"
                },
                {
                    "authorId": "2157044849",
                    "name": "28 ViaBrera"
                },
                {
                    "authorId": "103304654",
                    "name": "20133 Milano"
                },
                {
                    "authorId": "2154608445",
                    "name": "Ifpu"
                },
                {
                    "authorId": "1490888546",
                    "name": "Institute for Fundamental Physics of the Universe"
                },
                {
                    "authorId": "2047287443",
                    "name": "2. viaBeirut"
                },
                {
                    "authorId": "2037383460",
                    "name": "34127 Trieste"
                },
                {
                    "authorId": "152568243",
                    "name": "Infn"
                },
                {
                    "authorId": "102753457",
                    "name": "Sezione di Trieste"
                },
                {
                    "authorId": "1413659873",
                    "name": "2. ViaValerio"
                },
                {
                    "authorId": "2191896030",
                    "name": "TS 34127Trieste"
                },
                {
                    "authorId": "102442433",
                    "name": "Sissa"
                },
                {
                    "authorId": "102456232",
                    "name": "International School for Advanced Studies"
                },
                {
                    "authorId": "2157044594",
                    "name": "Via Bonomea 265"
                },
                {
                    "authorId": "2191897400",
                    "name": "TS 34136Trieste"
                },
                {
                    "authorId": "112871064",
                    "name": "U. Bologna"
                },
                {
                    "authorId": "1422457674",
                    "name": "V. G. 932"
                },
                {
                    "authorId": "102827178",
                    "name": "I. Bologna"
                },
                {
                    "authorId": "113354953",
                    "name": "62 vialeBertiPichat"
                },
                {
                    "authorId": "103220483",
                    "name": "40129 Bologna"
                },
                {
                    "authorId": "121021007",
                    "name": "M. F. Physics"
                },
                {
                    "authorId": "2157043508",
                    "name": "1. Giessenbachstr."
                },
                {
                    "authorId": "102943829",
                    "name": "85748 Garching"
                },
                {
                    "authorId": "2264654886",
                    "name": "Germany"
                },
                {
                    "authorId": "103210288",
                    "name": "Universitatssternwarte Munchen"
                },
                {
                    "authorId": "103024819",
                    "name": "F. Physik"
                },
                {
                    "authorId": "52244545",
                    "name": "Ludwig-Maximilians-Universitat Munchen"
                },
                {
                    "authorId": "102632447",
                    "name": "1. Scheinerstrasse"
                },
                {
                    "authorId": "1389868000",
                    "name": "8. Munchen"
                },
                {
                    "authorId": "2202029828",
                    "name": "I. Torino"
                },
                {
                    "authorId": "1387934705",
                    "name": "20 viaOsservatorio"
                },
                {
                    "authorId": "1382582906",
                    "name": "1. P. Torinese"
                },
                {
                    "authorId": "92909518",
                    "name": "D. Fisica"
                },
                {
                    "authorId": "102864912",
                    "name": "U. Genova"
                },
                {
                    "authorId": "1422213009",
                    "name": "33 viaDodecaneso"
                },
                {
                    "authorId": "2090357082",
                    "name": "16146"
                },
                {
                    "authorId": "103057906",
                    "name": "Genova"
                },
                {
                    "authorId": "152359268",
                    "name": "I. Genova"
                },
                {
                    "authorId": "2157042414",
                    "name": "Department of PhysicsE. Pancini"
                },
                {
                    "authorId": "102803639",
                    "name": "U. Federico"
                },
                {
                    "authorId": "102735570",
                    "name": "U. Torino"
                },
                {
                    "authorId": "1422586098",
                    "name": "1. ViaP.Giuria"
                },
                {
                    "authorId": "1387897930",
                    "name": "10125 Torino"
                },
                {
                    "authorId": "145872073",
                    "name": "I. Torino"
                },
                {
                    "authorId": "103287996",
                    "name": "Inaf-Iasf Milano"
                },
                {
                    "authorId": "2157042433",
                    "name": "12 ViaAlfonsoCorti"
                },
                {
                    "authorId": "103304654",
                    "name": "20133 Milano"
                },
                {
                    "authorId": "103205512",
                    "name": "Centro de Investigaciones Energ'eticas"
                },
                {
                    "authorId": "100460827",
                    "name": "Medioambientales y Tecnol'ogicas"
                },
                {
                    "authorId": "1491707808",
                    "name": "40 AvenidaComplutense"
                },
                {
                    "authorId": "102853551",
                    "name": "28014 Madrid"
                },
                {
                    "authorId": "2124367374",
                    "name": "Port d'Informaci'o Cient'ifica"
                },
                {
                    "authorId": "89410745",
                    "name": "C. Sn"
                },
                {
                    "authorId": "88727214",
                    "name": "08193 Bellaterra"
                },
                {
                    "authorId": "102462783",
                    "name": "I. F. Physics"
                },
                {
                    "authorId": "102232946",
                    "name": "Cosmology"
                },
                {
                    "authorId": "90037339",
                    "name": "Rwth Aachen University"
                },
                {
                    "authorId": "2191896531",
                    "name": "52056 Aachen"
                },
                {
                    "authorId": "152160569",
                    "name": "I. D. E. D. Catalunya"
                },
                {
                    "authorId": "2296602367",
                    "name": "Edifici Rdit"
                },
                {
                    "authorId": "97932901",
                    "name": "C. Upc"
                },
                {
                    "authorId": "2301018695",
                    "name": "08860 Castelldefels"
                },
                {
                    "authorId": "121003604",
                    "name": "Barcelona"
                },
                {
                    "authorId": "93001966",
                    "name": "I. Roma"
                },
                {
                    "authorId": "102689845",
                    "name": "33 viaFrascati"
                },
                {
                    "authorId": "102667878",
                    "name": "00078 Monte Porzio Catone"
                },
                {
                    "authorId": "1388952415",
                    "name": "D. Bologna"
                },
                {
                    "authorId": "1388360687",
                    "name": "C. V. L. sn"
                },
                {
                    "authorId": "2191896798",
                    "name": "38204"
                },
                {
                    "authorId": "2256690591",
                    "name": "San Crist'obal de La Laguna"
                },
                {
                    "authorId": "103004057",
                    "name": "Institute for Astronomy"
                },
                {
                    "authorId": "95994788",
                    "name": "U. Edinburgh"
                },
                {
                    "authorId": "1410615093",
                    "name": "R. Observatory"
                },
                {
                    "authorId": "41221445",
                    "name": "B. Hill"
                },
                {
                    "authorId": "152478613",
                    "name": "Edinburgh EH9 3HJ"
                },
                {
                    "authorId": "102268868",
                    "name": "J. B. C. F. Astrophysics"
                },
                {
                    "authorId": "2237985348",
                    "name": "Department of Astronomy Physics"
                },
                {
                    "authorId": "2254660649",
                    "name": "Astronomy"
                },
                {
                    "authorId": "102997085",
                    "name": "U. Manchester"
                },
                {
                    "authorId": "71491410",
                    "name": "Oxford Road"
                },
                {
                    "authorId": "1617917610",
                    "name": "M13 9PL"
                },
                {
                    "authorId": "2157044287",
                    "name": "European Space AgencyESRIN"
                },
                {
                    "authorId": "2157043244",
                    "name": "1. LargoGalileoGalilei"
                },
                {
                    "authorId": "1388397787",
                    "name": "00044 Frascati"
                },
                {
                    "authorId": "152556838",
                    "name": "Roma"
                },
                {
                    "authorId": "1382546848",
                    "name": "1. Universit'eClaudeBernardLyon"
                },
                {
                    "authorId": "152432549",
                    "name": "CNRSIN2p3"
                },
                {
                    "authorId": "103286831",
                    "name": "I. Lyon"
                },
                {
                    "authorId": "2157044503",
                    "name": "Umr 5822"
                },
                {
                    "authorId": "102946594",
                    "name": "Villeurbanne"
                },
                {
                    "authorId": "2301018486",
                    "name": "F-69100"
                },
                {
                    "authorId": "102462800",
                    "name": "I. O. Physics"
                },
                {
                    "authorId": "102345645",
                    "name": "L. O. Astrophysics"
                },
                {
                    "authorId": "102616163",
                    "name": "E. P. F. Lausanne"
                },
                {
                    "authorId": "102281895",
                    "name": "Observatoire de Sauverny"
                },
                {
                    "authorId": "2157043973",
                    "name": "1. UCBLyon"
                },
                {
                    "authorId": "51280923",
                    "name": "Iuf"
                },
                {
                    "authorId": "2142474323",
                    "name": "4. R. E. Fermi"
                },
                {
                    "authorId": "2211734560",
                    "name": "69622 Villeurbanne"
                },
                {
                    "authorId": "103378694",
                    "name": "D. D. F'isica"
                },
                {
                    "authorId": "101180427",
                    "name": "F. Ciencias"
                },
                {
                    "authorId": "52467992",
                    "name": "Universidade de Lisboa"
                },
                {
                    "authorId": "2162838174",
                    "name": "C8 Edif'icio"
                },
                {
                    "authorId": "2053992700",
                    "name": "C. Grande"
                },
                {
                    "authorId": "3878333",
                    "name": "P. Lisboa"
                },
                {
                    "authorId": "2170536169",
                    "name": "1049-001 Lisboa"
                },
                {
                    "authorId": "1823631848",
                    "name": "I. D. A. E. P. Spaziali"
                },
                {
                    "authorId": "103278669",
                    "name": "V. Cavaliere"
                },
                {
                    "authorId": "1391023356",
                    "name": "100"
                },
                {
                    "authorId": "1388360550",
                    "name": "00133 Roma"
                },
                {
                    "authorId": "1422423144",
                    "name": "INFN-Padova"
                },
                {
                    "authorId": "2162838041",
                    "name": "Universit'e de Paris Cit'e"
                },
                {
                    "authorId": "152234079",
                    "name": "Cea"
                },
                {
                    "authorId": "87063022",
                    "name": "Aim"
                },
                {
                    "authorId": "2184593566",
                    "name": "91191"
                },
                {
                    "authorId": "1388354189",
                    "name": "Gif-sur-Yvette"
                },
                {
                    "authorId": "123080358",
                    "name": "Institut de Ci'encies de l'Espai"
                },
                {
                    "authorId": "2164336876",
                    "name": "sn Cerdanyola del Vall'es"
                },
                {
                    "authorId": "89909107",
                    "name": "S. O. Physics"
                },
                {
                    "authorId": "2157045440",
                    "name": "H. P. Laboratory"
                },
                {
                    "authorId": "73133885",
                    "name": "U. Bristol"
                },
                {
                    "authorId": "103021225",
                    "name": "Tyndall Avenue"
                },
                {
                    "authorId": "2261706718",
                    "name": "Bristol"
                },
                {
                    "authorId": "121885560",
                    "name": "BS8 1TL"
                },
                {
                    "authorId": "102325608",
                    "name": "Istituto Nazionale Fisica Nucleare"
                },
                {
                    "authorId": "39568705",
                    "name": "Sezione Infn di Bologna"
                },
                {
                    "authorId": "2157044904",
                    "name": "46 ViaIrnerio"
                },
                {
                    "authorId": "103220483",
                    "name": "40129 Bologna"
                },
                {
                    "authorId": "102710800",
                    "name": "Institute of Theoretical Astrophysics"
                },
                {
                    "authorId": "102378200",
                    "name": "U. Oslo"
                },
                {
                    "authorId": "2157044459",
                    "name": "P. O. B. 1. Blindern"
                },
                {
                    "authorId": "1410624112",
                    "name": "0315 Oslo"
                },
                {
                    "authorId": "51953222",
                    "name": "Norway"
                },
                {
                    "authorId": "102835046",
                    "name": "Jet propulsion Laboratory"
                },
                {
                    "authorId": "102611508",
                    "name": "C. I. O. Technology."
                },
                {
                    "authorId": "2157043672",
                    "name": "4800 Oak Grove Drive"
                },
                {
                    "authorId": "119874209",
                    "name": "Pasadena"
                },
                {
                    "authorId": "2270789132",
                    "name": "Ca"
                },
                {
                    "authorId": "2157044915",
                    "name": "91109"
                },
                {
                    "authorId": "2237992692",
                    "name": "Usa"
                },
                {
                    "authorId": "1404750990",
                    "name": "L. University"
                },
                {
                    "authorId": "101253468",
                    "name": "Lancaster"
                },
                {
                    "authorId": "2157044596",
                    "name": "LA1 4YB"
                },
                {
                    "authorId": "2302796380",
                    "name": "Felix Hormuth Engineering"
                },
                {
                    "authorId": "2302795031",
                    "name": "17 Goethestr."
                },
                {
                    "authorId": "2302795786",
                    "name": "69181 Leimen"
                },
                {
                    "authorId": "12578472",
                    "name": "T. Denmark"
                },
                {
                    "authorId": "2157043404",
                    "name": "Elektrovej 327"
                },
                {
                    "authorId": "2157043299",
                    "name": "2800 Kgs. Lyngby"
                },
                {
                    "authorId": "113326763",
                    "name": "Denmark"
                },
                {
                    "authorId": "102481307",
                    "name": "Cosmic Dawn Center"
                },
                {
                    "authorId": "93336282",
                    "name": "M. Astronomie"
                },
                {
                    "authorId": "102701690",
                    "name": "17 Konigstuhl"
                },
                {
                    "authorId": "102917045",
                    "name": "69117 Heidelberg"
                },
                {
                    "authorId": "101245983",
                    "name": "U. London"
                }
            ],
            "abstract": "will collect an enormous amount of data during the mission's lifetime, observing billions of galaxies in the extragalactic sky. Along with traditional template-fitting methods, numerous machine learning (ML) algorithms have been presented for computing their photometric redshifts and physical parameters (PPs), requiring significantly less computing effort while producing equivalent performance measures. However, their performance is limited by the quality and amount of input information entering the model (the features), to a level where the recovery of some well-established physical relationships between parameters might not be guaranteed -- for example, the star-forming main sequence (SFMS). To forecast the reliability of photo-$z$s and PPs calculations, we produced two mock catalogs simulating the photometry with the UNIONS $ugriz$ and filters. We simulated the Euclid Wide Survey (EWS) and Euclid Deep Fields (EDF), alongside two auxiliary fields. We tested the performance of a template-fitting algorithm ( and four ML methods in recovering photo-$z$s, PPs (stellar masses and star formation rates), and the SFMS on the simulated fields. To mimic the processing as closely as possible, the models were trained with labels and tested on the simulated ground truth. For the EWS, we found that the best results are achieved with a mixed labels approach, training the models with wide survey features and labels from the results on deeper photometry, that is, with the best possible set of labels for a given photometry. This imposes a prior to the input features, helping the models to better discern cases in degenerate regions of feature space, that is, when galaxies have similar magnitudes and colors but different redshifts and PPs, with performance metrics even better than those found with We found no more than 3 performance degradation using a COSMOS-like reference sample or removing $u$ band data, which will not be available until after data release DR1. The best results are obtained for the EDF, with appropriate recovery of photo-$z$, PPs, and the SFMS.",
            "corpus_id": "271097644",
            "text": "will collect an enormous amount of data during the mission's lifetime, observing billions of galaxies in the extragalactic sky. Along with traditional template-fitting methods, numerous machine learning (ML) algorithms have been presented for computing their photometric redshifts and physical parameters (PPs), requiring significantly less computing effort while producing equivalent performance measures. However, their performance is limited by the quality and amount of input information entering the model (the features), to a level where the recovery of some well-established physical relationships between parameters might not be guaranteed -- for example, the star-forming main sequence (SFMS). To forecast the reliability of photo-$z$s and PPs calculations, we produced two mock catalogs simulating the photometry with the UNIONS $ugriz$ and filters. We simulated the Euclid Wide Survey (EWS) and Euclid Deep Fields (EDF), alongside two auxiliary fields. We tested the performance of a template-fitting algorithm ( and four ML methods in recovering photo-$z$s, PPs (stellar masses and star formation rates), and the SFMS on the simulated fields. To mimic the processing as closely as possible, the models were trained with labels and tested on the simulated ground truth. For the EWS, we found that the best results are achieved with a mixed labels approach, training the models with wide survey features and labels from the results on deeper photometry, that is, with the best possible set of labels for a given photometry. This imposes a prior to the input features, helping the models to better discern cases in degenerate regions of feature space, that is, when galaxies have similar magnitudes and colors but different redshifts and PPs, with performance metrics even better than those found with We found no more than 3 performance degradation using a COSMOS-like reference sample or removing $u$ band data, which will not be available until after data release DR1. The best results are obtained for the EDF, with appropriate recovery of photo-$z$, PPs, and the SFMS.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0018100738525390625
        },
        {
            "paperId": "f17f6d0b64cffa84196740766e0abfc41b67cf22",
            "corpusId": 3866300,
            "title": "Chemically Aware Model Builder (camb): an R package for property and bioactivity modelling of small molecules",
            "venue": "Journal of Cheminformatics",
            "year": 2015,
            "referenceCount": 47,
            "citationCount": 41,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-015-0086-2",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC4551546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49839952",
                    "name": "Daniel S. Murrell"
                },
                {
                    "authorId": "1397373631",
                    "name": "I. Cort\u00e9s-Ciriano"
                },
                {
                    "authorId": "2299548",
                    "name": "G. Westen"
                },
                {
                    "authorId": "40337478",
                    "name": "I. Stott"
                },
                {
                    "authorId": "144917032",
                    "name": "A. Bender"
                },
                {
                    "authorId": "1815975",
                    "name": "T. Malliavin"
                },
                {
                    "authorId": "1782438",
                    "name": "R. Glen"
                }
            ],
            "abstract": "AbstractBackgroundIn silico predictive models have proved to be valuable\n for the optimisation of compound potency, selectivity and safety profiles in the drug discovery process.Resultscamb is an R package that provides an environment for the rapid generation of quantitative Structure-Property and Structure-Activity models for small molecules (including QSAR, QSPR, QSAM, PCM) and is aimed at both advanced and beginner R users. camb's capabilities include the standardisation of chemical structure representation, computation of 905 one-dimensional and 14 fingerprint type descriptors for small molecules, 8 types of amino acid descriptors, 13 whole protein sequence descriptors, filtering methods for feature selection, generation of predictive models (using an interface to the R package caret), as well as techniques to create model ensembles using techniques from the R package caretEnsemble). Results can be visualised through high-quality, customisable plots (R package ggplot2).Conclusions Overall, camb constitutes an open-source framework to perform the following steps: (1) compound standardisation, (2) molecular and protein descriptor calculation, (3) descriptor pre-processing and model training, visualisation and validation, and (4) bioactivity/property prediction for new molecules. camb aims to speed model generation, in order to provide reproducibility and tests of robustness. QSPR and proteochemometric case studies are included which demonstrate camb's application.Graphical abstractFrom compounds and data to models: a complete model building workflow in one package.",
            "corpus_id": "3866300",
            "text": "AbstractBackgroundIn silico predictive models have proved to be valuable\n for the optimisation of compound potency, selectivity and safety profiles in the drug discovery process.Resultscamb is an R package that provides an environment for the rapid generation of quantitative Structure-Property and Structure-Activity models for small molecules (including QSAR, QSPR, QSAM, PCM) and is aimed at both advanced and beginner R users. camb's capabilities include the standardisation of chemical structure representation, computation of 905 one-dimensional and 14 fingerprint type descriptors for small molecules, 8 types of amino acid descriptors, 13 whole protein sequence descriptors, filtering methods for feature selection, generation of predictive models (using an interface to the R package caret), as well as techniques to create model ensembles using techniques from the R package caretEnsemble). Results can be visualised through high-quality, customisable plots (R package ggplot2).Conclusions Overall, camb constitutes an open-source framework to perform the following steps: (1) compound standardisation, (2) molecular and protein descriptor calculation, (3) descriptor pre-processing and model training, visualisation and validation, and (4) bioactivity/property prediction for new molecules. camb aims to speed model generation, in order to provide reproducibility and tests of robustness. QSPR and proteochemometric case studies are included which demonstrate camb's application.Graphical abstractFrom compounds and data to models: a complete model building workflow in one package.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0017824172973632812
        },
        {
            "paperId": "787af3b02cbd55010025431b56722c24f4e27e0f",
            "corpusId": 260649577,
            "title": "Multi-layer clustering-based residual sparsifying transform for low-dose CT image reconstruction.",
            "venue": "Medical Physics (Lancaster)",
            "year": 2023,
            "referenceCount": 10,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2203.11565",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/mp.16645?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/mp.16645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2230125180",
                    "name": "Ling Chen"
                },
                {
                    "authorId": "1686376629",
                    "name": "Xikai Yang"
                },
                {
                    "authorId": "2318717",
                    "name": "Zhishen Huang"
                },
                {
                    "authorId": "144615547",
                    "name": "Y. Long"
                },
                {
                    "authorId": "145233481",
                    "name": "S. Ravishankar"
                }
            ],
            "abstract": "PURPOSE\nThe recently proposed sparsifying transform (ST) models incur low computational cost and have been applied to medical imaging. Meanwhile, deep models with nested network structure reveal great potential for learning features in different layers. In this study, we propose a network-structured ST learning approach for X-ray computed tomography (CT), which we refer to as multi-layer clustering-based residual sparsifying transform (MCST) learning. The proposed MCST scheme learns multiple different unitary transforms in each layer by dividing each layer's input into several classes. We apply the MCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST model into the regularizer in penalized weighted least squares (PWLS) reconstruction.\n\n\nMETHODS\nThe proposed MCST model combines a multi-layer sparse representation structure with multiple clusters for the features in each layer that are modeled by a rich collection of transforms. We train the MCST model in an unsupervised manner via a block coordinate descent (BCD) algorithm. Since our method is patch-based, the training can be performed with a limited set of images. For CT image reconstruction, we devise a novel algorithm called PWLS-MCST by integrating the pre-learned MCST signal model with PWLS\u00a0optimization.\n\n\nRESULTS\nWe conducted LDCT reconstruction experiments on XCAT phantom data, Numerical Mayo Clinical CT dataset and \"LDCT image and projection dataset\" (Clinical LDCT dataset). We trained the MCST model with two (or three) layers and with five clusters in each layer. The learned transforms in the same layer showed rich features while additional information is extracted from representation residuals. Our simulation results and clinical results demonstrate that PWLS-MCST achieves better image reconstruction quality than the conventional filtered back-projection (FBP) method and PWLS with edge-preserving (EP) regularizer. It also outperformed recent advanced methods like PWLS with a learned multi-layer residual sparsifying transform (MARS) prior and PWLS with a union of learned transforms (ULTRA), especially for displaying clear edges and preserving subtle\u00a0details.\n\n\nCONCLUSIONS\nIn this work, a multi-layer sparse signal model with a nested network structure is proposed. We refer this novel model as the MCST model that exploits multi-layer residual maps to sparsify the underlying image and clusters the inputs in each layer for accurate sparsification. We presented a new PWLS framework with a learned MCST regularizer for LDCT reconstruction. Experimental results show that the proposed PWLS-MCST provides clearer reconstructions than several baseline methods. The code for PWLS-MCST is released at https://github.com/Xikai97/PWLS-MCST.",
            "corpus_id": "260649577",
            "text": "PURPOSE\nThe recently proposed sparsifying transform (ST) models incur low computational cost and have been applied to medical imaging. Meanwhile, deep models with nested network structure reveal great potential for learning features in different layers. In this study, we propose a network-structured ST learning approach for X-ray computed tomography (CT), which we refer to as multi-layer clustering-based residual sparsifying transform (MCST) learning. The proposed MCST scheme learns multiple different unitary transforms in each layer by dividing each layer's input into several classes. We apply the MCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST model into the regularizer in penalized weighted least squares (PWLS) reconstruction.\n\n\nMETHODS\nThe proposed MCST model combines a multi-layer sparse representation structure with multiple clusters for the features in each layer that are modeled by a rich collection of transforms. We train the MCST model in an unsupervised manner via a block coordinate descent (BCD) algorithm. Since our method is patch-based, the training can be performed with a limited set of images. For CT image reconstruction, we devise a novel algorithm called PWLS-MCST by integrating the pre-learned MCST signal model with PWLS\u00a0optimization.\n\n\nRESULTS\nWe conducted LDCT reconstruction experiments on XCAT phantom data, Numerical Mayo Clinical CT dataset and \"LDCT image and projection dataset\" (Clinical LDCT dataset). We trained the MCST model with two (or three) layers and with five clusters in each layer. The learned transforms in the same layer showed rich features while additional information is extracted from representation residuals. Our simulation results and clinical results demonstrate that PWLS-MCST achieves better image reconstruction quality than the conventional filtered back-projection (FBP) method and PWLS with edge-preserving (EP) regularizer. It also outperformed recent advanced methods like PWLS with a learned multi-layer residual sparsifying transform (MARS) prior and PWLS with a union of learned transforms (ULTRA), especially for displaying clear edges and preserving subtle\u00a0details.\n\n\nCONCLUSIONS\nIn this work, a multi-layer sparse signal model with a nested network structure is proposed. We refer this novel model as the MCST model that exploits multi-layer residual maps to sparsify the underlying image and clusters the inputs in each layer for accurate sparsification. We presented a new PWLS framework with a learned MCST regularizer for LDCT reconstruction. Experimental results show that the proposed PWLS-MCST provides clearer reconstructions than several baseline methods. The code for PWLS-MCST is released at https://github.com/Xikai97/PWLS-MCST.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00345611572265625
        }
    ],
    "quotes": {
        "cost": 0.11886000000000001,
        "quotes": [
            {
                "idx": 0,
                "key": "[261243909 | Gao et al. | 2023 | Citations: 5]",
                "snippets": "Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training.\n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data.\n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples.\n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training.\n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245124124 | Du et al. | 2021 | Citations: 826]": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",
                    "[248986979 | Hernandez et al. | 2022 | Citations: 118]": "Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.",
                    "[258832491 | Longpre et al. | 2023 | Citations: 166]": "Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development."
                },
                "metadata": [
                    {
                        "section_title": "Data Quantity and Quality",
                        "pdf_hash": "",
                        "start": 535,
                        "end": 2182,
                        "sentence_offsets": [
                            {
                                "start": 453,
                                "end": 561
                            },
                            {
                                "start": 562,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 802
                            },
                            {
                                "start": 805,
                                "end": 1197
                            },
                            {
                                "start": 1198,
                                "end": 1413
                            },
                            {
                                "start": 1416,
                                "end": 1443
                            },
                            {
                                "start": 1444,
                                "end": 1533
                            },
                            {
                                "start": 1534,
                                "end": 1654
                            },
                            {
                                "start": 1657,
                                "end": 1822
                            },
                            {
                                "start": 1825,
                                "end": 1952
                            },
                            {
                                "start": 1953,
                                "end": 2152
                            },
                            {
                                "start": 2153,
                                "end": 2311
                            }
                        ],
                        "ref_mentions": [
                            "248986979",
                            "245124124",
                            "258832491"
                        ],
                        "quote": "Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training.\n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data.\n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples.\n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training.\n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[264935645 | Chen et al. | 2023 | Citations: 7]",
                "snippets": "To extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusions and Future Work",
                        "pdf_hash": "",
                        "start": 9,
                        "end": 519,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 417
                            },
                            {
                                "start": 418,
                                "end": 519
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[265609639 | Wang et al. | 2023 | Citations: 14]",
                "snippets": "Despite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;(Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258832491 | Longpre et al. | 2023 | Citations: 166]": "Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development."
                },
                "metadata": [
                    {
                        "section_title": "Quality Filtering",
                        "pdf_hash": "",
                        "start": 645,
                        "end": 1685,
                        "sentence_offsets": [
                            {
                                "start": 645,
                                "end": 795
                            },
                            {
                                "start": 796,
                                "end": 1004
                            },
                            {
                                "start": 1005,
                                "end": 1205
                            },
                            {
                                "start": 1206,
                                "end": 1426
                            },
                            {
                                "start": 1427,
                                "end": 1554
                            },
                            {
                                "start": 1555,
                                "end": 1685
                            }
                        ],
                        "ref_mentions": [
                            "258832491",
                            "268096300"
                        ],
                        "quote": "Despite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;(Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[266755678 | Liu et al. | 2024 | Citations: 74]",
                "snippets": "Quality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText (Radford et al., 2019) to filter out low-quality datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data preprocessing",
                        "pdf_hash": "",
                        "start": 1539,
                        "end": 2103,
                        "sentence_offsets": [
                            {
                                "start": 1539,
                                "end": 1661
                            },
                            {
                                "start": 1662,
                                "end": 1760
                            },
                            {
                                "start": 1761,
                                "end": 1965
                            },
                            {
                                "start": 1966,
                                "end": 2103
                            }
                        ],
                        "ref_mentions": [
                            "160025533"
                        ],
                        "quote": "Quality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText (Radford et al., 2019) to filter out low-quality datasets."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[266977204 | Lucy et al. | 2024 | Citations: 17]",
                "snippets": "We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2019)(Laurenccon et al., 2023)Muennighoff et al., 2023;Marion et al., 2023)...Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870323 | Wenzek et al. | 2019 | Citations: 658]": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
                    "[257378329 | Laurenccon et al. | 2023 | Citations: 169]": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus."
                },
                "metadata": [
                    {
                        "section_title": "Pretraining Data Filters",
                        "pdf_hash": "",
                        "start": 405,
                        "end": 1197,
                        "sentence_offsets": [
                            {
                                "start": 405,
                                "end": 530
                            },
                            {
                                "start": 531,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 808
                            },
                            {
                                "start": 809,
                                "end": 949
                            },
                            {
                                "start": 950,
                                "end": 1198
                            }
                        ],
                        "ref_mentions": [
                            "207870323",
                            "257378329"
                        ],
                        "quote": "We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2019)(Laurenccon et al., 2023)Muennighoff et al., 2023;Marion et al., 2023)"
                    },
                    {
                        "section_title": "Pretraining Data Filters",
                        "pdf_hash": "",
                        "start": 1640,
                        "end": 1750,
                        "sentence_offsets": [
                            {
                                "start": 1640,
                                "end": 1749
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a)."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268680360 | Levine et al. | 2024 | Citations: 9]",
                "snippets": "Previous research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;(Dodge et al., 2021)Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237568724 | Dodge et al. | 2021 | Citations: 450]": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet."
                },
                "metadata": [
                    {
                        "section_title": "Foundation Model Training",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 378,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 189
                            },
                            {
                                "start": 189,
                                "end": 378
                            }
                        ],
                        "ref_mentions": [
                            "237568724"
                        ],
                        "quote": "Previous research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;(Dodge et al., 2021)Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[269187631 | Ding et al. | 2024 | Citations: 14]",
                "snippets": "There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2019), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2021)Marion et al., 2023;Chen et al., 2023;(Chowdhery et al., 2022)Touvron et al., 2023a;(Raffel et al., 2019)) propose various filtering strategies to improve data quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[235829052 | Lee et al. | 2021 | Citations: 635]": "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets\u2014for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.",
                    "[247951931 | Chowdhery et al. | 2022 | Citations: 6293]": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 81,
                        "end": 579,
                        "sentence_offsets": [
                            {
                                "start": 81,
                                "end": 354
                            },
                            {
                                "start": 354,
                                "end": 579
                            }
                        ],
                        "ref_mentions": [
                            "204838007",
                            "235829052",
                            "247951931",
                            "204838007"
                        ],
                        "quote": "There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2019), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2021)Marion et al., 2023;Chen et al., 2023;(Chowdhery et al., 2022)Touvron et al., 2023a;(Raffel et al., 2019)) propose various filtering strategies to improve data quality."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270357359 | Kong et al. | 2024 | Citations: 4]",
                "snippets": "It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023]...However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b]).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 191,
                        "end": 411,
                        "sentence_offsets": [
                            {
                                "start": 191,
                                "end": 384
                            },
                            {
                                "start": 384,
                                "end": 412
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023]"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1564,
                        "end": 2089,
                        "sentence_offsets": [
                            {
                                "start": 1564,
                                "end": 1848
                            },
                            {
                                "start": 1848,
                                "end": 2089
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b])."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270514462 | Enomoto et al. | 2024 | Citations: 3]",
                "snippets": "We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora. The experimental results show that the perplexity filtering method based on an N-gram language model is the best. We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022). The results show that massively strong filtering results in performance deterioration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1061,
                        "end": 1587,
                        "sentence_offsets": [
                            {
                                "start": 1053,
                                "end": 1287
                            },
                            {
                                "start": 1287,
                                "end": 1402
                            },
                            {
                                "start": 1402,
                                "end": 1515
                            },
                            {
                                "start": 1515,
                                "end": 1724
                            }
                        ],
                        "ref_mentions": [
                            "52967399"
                        ],
                        "quote": "We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora. The experimental results show that the perplexity filtering method based on an N-gram language model is the best. We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022). The results show that massively strong filtering results in performance deterioration."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[271874495 | Li et al. | 2024 | Citations: 4]",
                "snippets": "Quality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020)Gao et al., 2020;(Chowdhery et al., 2022) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[247951931 | Chowdhery et al. | 2022 | Citations: 6293]": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 393,
                        "end": 1300,
                        "sentence_offsets": [
                            {
                                "start": 393,
                                "end": 552
                            },
                            {
                                "start": 553,
                                "end": 664
                            },
                            {
                                "start": 665,
                                "end": 890
                            },
                            {
                                "start": 891,
                                "end": 1121
                            },
                            {
                                "start": 1122,
                                "end": 1300
                            }
                        ],
                        "ref_mentions": [
                            "218971783",
                            "247951931"
                        ],
                        "quote": "Quality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020)Gao et al., 2020;(Chowdhery et al., 2022) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272524632 | Thrush et al. | 2024 | Citations: 22]",
                "snippets": "According to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 322,
                        "end": 691,
                        "sentence_offsets": [
                            {
                                "start": 322,
                                "end": 691
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "According to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273323858 | Brandizzi et al. | 2024 | Citations: 2]",
                "snippets": "Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]...Traditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,(Kumar et al., 2020)(Yoo et al., 2021). One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53]31].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211987786 | Kumar et al. | 2020 | Citations: 356]": "Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.",
                    "[233296100 | Yoo et al. | 2021 | Citations: 242]": "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach."
                },
                "metadata": [
                    {
                        "section_title": "Quality Assessment",
                        "pdf_hash": "",
                        "start": 192,
                        "end": 375,
                        "sentence_offsets": [
                            {
                                "start": 192,
                                "end": 376
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]"
                    },
                    {
                        "section_title": "Quality Assessment",
                        "pdf_hash": "",
                        "start": 379,
                        "end": 953,
                        "sentence_offsets": [
                            {
                                "start": 379,
                                "end": 552
                            },
                            {
                                "start": 553,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 952
                            }
                        ],
                        "ref_mentions": [
                            "211987786",
                            "233296100"
                        ],
                        "quote": "Traditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,(Kumar et al., 2020)(Yoo et al., 2021). One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53]31]."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[274422859 | Zhang et al. | 2024 | Citations: 0]",
                "snippets": "Quality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 708,
                        "end": 1689,
                        "sentence_offsets": [
                            {
                                "start": 708,
                                "end": 727
                            },
                            {
                                "start": 728,
                                "end": 874
                            },
                            {
                                "start": 875,
                                "end": 1033
                            },
                            {
                                "start": 1034,
                                "end": 1184
                            },
                            {
                                "start": 1185,
                                "end": 1289
                            },
                            {
                                "start": 1290,
                                "end": 1477
                            },
                            {
                                "start": 1478,
                                "end": 1689
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Quality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[274514936 | Hausenloy et al. | 2024 | Citations: 2]",
                "snippets": "Existing Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Mandatory data filtering",
                        "pdf_hash": "",
                        "start": 1403,
                        "end": 1940,
                        "sentence_offsets": [
                            {
                                "start": 1403,
                                "end": 1530
                            },
                            {
                                "start": 1531,
                                "end": 1631
                            },
                            {
                                "start": 1632,
                                "end": 1725
                            },
                            {
                                "start": 1726,
                                "end": 1940
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Existing Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32]."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276394897 | Messmer et al. | 2025 | Citations: 2]",
                "snippets": "To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 246,
                        "end": 658,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[276776523 | Li et al. | 2025 | Citations: 2]",
                "snippets": "As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control...This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4...As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A Method Details",
                        "pdf_hash": "",
                        "start": 176,
                        "end": 470,
                        "sentence_offsets": [
                            {
                                "start": 176,
                                "end": 375
                            },
                            {
                                "start": 376,
                                "end": 471
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control"
                    },
                    {
                        "section_title": "A Method Details",
                        "pdf_hash": "",
                        "start": 1047,
                        "end": 1235,
                        "sentence_offsets": [
                            {
                                "start": 1047,
                                "end": 1171
                            },
                            {
                                "start": 1172,
                                "end": 1344
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4"
                    },
                    {
                        "section_title": "A Method Details",
                        "pdf_hash": "",
                        "start": 1662,
                        "end": 1833,
                        "sentence_offsets": [
                            {
                                "start": 1662,
                                "end": 1832
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[277113645 | Li et al. | 2025 | Citations: 0]",
                "snippets": "In terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1179,
                        "end": 1962,
                        "sentence_offsets": [
                            {
                                "start": 1179,
                                "end": 1454
                            },
                            {
                                "start": 1455,
                                "end": 1551
                            },
                            {
                                "start": 1552,
                                "end": 1752
                            },
                            {
                                "start": 1753,
                                "end": 1962
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[277955900 | Zhuang et al. | 2025 | Citations: 0]",
                "snippets": "More recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267681974 | Wettig et al. | 2024 | Citations: 79]": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1421,
                        "end": 1963,
                        "sentence_offsets": [
                            {
                                "start": 1421,
                                "end": 1537
                            },
                            {
                                "start": 1538,
                                "end": 1686
                            },
                            {
                                "start": 1687,
                                "end": 1853
                            },
                            {
                                "start": 1854,
                                "end": 1963
                            }
                        ],
                        "ref_mentions": [
                            "267681974"
                        ],
                        "quote": "More recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[278394813 | Wang et al. | 2025 | Citations: 1]",
                "snippets": "Current high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "FastText-based Quality Filtering",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 836,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 211
                            },
                            {
                                "start": 214,
                                "end": 304
                            },
                            {
                                "start": 305,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 836
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Current high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.022122,
        "cot": "Based on the user query, I need to organize information about the effectiveness of quality filtering for LLM pre-training data. The query specifically mentions advanced models like BERT and studies involving FastText for filtering.\n\nLet me plan the dimensions to address this query comprehensively:\n\n1. \"Introduction to Data Quality Filtering for LLMs\" - This should be a synthesis section providing background on why data quality filtering is important for LLM training. This sets the stage for understanding the subsequent sections.\n\n2. \"Types of Data Quality Filtering Approaches\" - This should be a synthesis section that categorizes and explains the main approaches to filtering pre-training data (classifier-based, rule-based, etc.). Many quotes discuss these different approaches.\n\n3. \"Classifier-Based Filtering Methods\" - This should be a synthesis section focused specifically on classifier methods (BERT, FastText, etc.) since the query specifically mentions these models. The quotes contain substantial information about various classifier implementations.\n\n4. \"Effectiveness and Impact of Quality Filtering\" - This should be a synthesis section addressing the central question of effectiveness, including studies showing improvements or potential limitations of filtering approaches.\n\n5. \"Case Studies of Quality Filtering in Major LLMs\" - This should be a list section showing specific examples of filtering strategies used in well-known LLM systems, providing concrete implementation examples.\n\nThe format choices are based on the need to explain complex concepts (synthesis) for most sections, while providing specific examples in an easy-to-scan format (list) for the case studies section.",
        "plan": {
            "Introduction to Data Quality Filtering for LLMs (synthesis)": [
                0,
                5,
                7,
                11
            ],
            "Types of Data Quality Filtering Approaches (synthesis)": [
                3,
                9,
                16
            ],
            "Classifier-Based Filtering Methods (synthesis)": [
                1,
                4,
                10,
                12,
                13,
                14,
                17,
                18
            ],
            "Effectiveness and Impact of Quality Filtering (synthesis)": [
                2,
                8,
                15
            ],
            "Case Studies of Quality Filtering in Major LLMs (list)": [
                0,
                6
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Data Quality Filtering for LLMs",
                "tldr": "Data quality filtering has become increasingly important in language model development, with researchers recognizing that high-quality data often outperforms sheer quantity. Various filtering techniques have been developed to remove noise, errors, and irrelevant content from pre-training corpora to improve model performance. (9 sources)",
                "text": "\nThe quality of pre-training data has emerged as a critical factor in the development and performance of large language models (LLMs). Recent research indicates a significant shift from simply accumulating vast amounts of training data to carefully curating high-quality datasets <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>. This paradigm shift emphasizes data excellence through thorough filtering and curation processes to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable models <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>.\n\nMultiple studies confirm that the quality of text data used in pre-training is crucial for improving LLM performance <Paper corpusId=\"268680360\" paperTitle=\"(Levine et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237568724\" paperTitle=\"(Dodge et al., 2021)\" isShortName></Paper>. Better data selection has been shown to enhance downstream performance and/or reduce the compute budget required to achieve equivalent results <Paper corpusId=\"270357359\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>. This is particularly important given that pre-training data often contains significant noise, including boilerplate text, templates, error messages, and other forms of repetitive or uninformative content that contributes little to model quality <Paper corpusId=\"270357359\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>.\n\nThe negative impact of poor quality data extends beyond wasted computational resources. When models are trained on noisy or low-quality data, they can learn and perpetuate irrelevant or redundant information, ultimately affecting their performance and generalization capabilities across various tasks <Paper corpusId=\"270357359\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>. Additionally, research has demonstrated that repeated data can lead to a \"double descent\" phenomenon, where test loss increases midway through training, and even a small fraction of repeated data can significantly degrade model performance <Paper corpusId=\"248986979\" paperTitle=\"(Hernandez et al., 2022)\" isShortName></Paper>.\n\nRecent trends in dataset curation reflect this growing emphasis on quality over quantity, with advanced filtering and deduplication techniques becoming increasingly prevalent <Paper corpusId=\"273323858\" paperTitle=\"(Brandizzi et al., 2024)\" isShortName></Paper>. The evaluation of data quality has also evolved from using intrinsic signals derived from the data itself (such as word length or sentence complexity) to leveraging pre-trained LLMs to judge content quality or even generate synthetic data <Paper corpusId=\"273323858\" paperTitle=\"(Brandizzi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211987786\" paperTitle=\"(Kumar et al., 2020)\" isShortName></Paper> <Paper corpusId=\"233296100\" paperTitle=\"(Yoo et al., 2021)\" isShortName></Paper>. One reason for the effectiveness of these methods is the strong alignment between human judgments of quality and the assessments made by LLMs <Paper corpusId=\"273323858\" paperTitle=\"(Brandizzi et al., 2024)\" isShortName></Paper>.\n\nThe importance of temporal alignment between pre-training and evaluation data has also been highlighted, with research showing that temporal shift between these datasets can lead to performance degradation that is not easily overcome by fine-tuning <Paper corpusId=\"258832491\" paperTitle=\"(Longpre et al., 2023)\" isShortName></Paper>. Furthermore, studies have revealed trade-offs between performance on standard benchmarks and the risk of toxic generations when applying quality and toxicity filters <Paper corpusId=\"258832491\" paperTitle=\"(Longpre et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Gao et al., 2023)",
                        "snippets": [
                            "Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training.\n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data.\n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples.\n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training.\n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality."
                        ],
                        "paper": {
                            "corpus_id": 261243909,
                            "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
                            "authors": [
                                {
                                    "authorId": "1944690382",
                                    "name": "Kaiyuan Gao"
                                },
                                {
                                    "authorId": "2112345574",
                                    "name": "Su He"
                                },
                                {
                                    "authorId": "2152990262",
                                    "name": "Zhenyu He"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiacheng Lin"
                                },
                                {
                                    "authorId": "2171652249",
                                    "name": "Qizhi Pei"
                                },
                                {
                                    "authorId": "2234370833",
                                    "name": "Jie Shao"
                                },
                                {
                                    "authorId": "2256597384",
                                    "name": "Wei Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.71533203125
                    },
                    {
                        "id": "(Levine et al., 2024)",
                        "snippets": [
                            "Previous research studies indicate that the quality of text data used for pre-training is critical to improving LLM performance (Rae et al., 2022;(Dodge et al., 2021)Chowdhery et al., 2022).Alongside these studies, we experiment with and develop data filtering techniques that allow us to further improve the quality of available internet-scale datasets for Japanese and English."
                        ],
                        "paper": {
                            "corpus_id": 268680360,
                            "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
                            "authors": [
                                {
                                    "authorId": "2061741965",
                                    "name": "Aaron Levine"
                                },
                                {
                                    "authorId": "2293356757",
                                    "name": "Connie Huang"
                                },
                                {
                                    "authorId": "2293351792",
                                    "name": "Chenguang Wang"
                                },
                                {
                                    "authorId": "2293313172",
                                    "name": "Eduardo Batista"
                                },
                                {
                                    "authorId": "2293311576",
                                    "name": "Ewa Szymanska"
                                },
                                {
                                    "authorId": "2293527287",
                                    "name": "Hongyi Ding"
                                },
                                {
                                    "authorId": "2121371065",
                                    "name": "Houwei Chou"
                                },
                                {
                                    "authorId": "2099242119",
                                    "name": "Jean-Fran\u00e7ois Pessiot"
                                },
                                {
                                    "authorId": "2293313121",
                                    "name": "Johanes Effendi"
                                },
                                {
                                    "authorId": "2293313467",
                                    "name": "Justin Chiu"
                                },
                                {
                                    "authorId": "147240287",
                                    "name": "Kai Torben Ohlhus"
                                },
                                {
                                    "authorId": "2293312430",
                                    "name": "Karan Chopra"
                                },
                                {
                                    "authorId": "40466108",
                                    "name": "Keiji Shinzato"
                                },
                                {
                                    "authorId": "2313561595",
                                    "name": "Koji Murakami"
                                },
                                {
                                    "authorId": "2293314500",
                                    "name": "Lee Xiong"
                                },
                                {
                                    "authorId": "2293403169",
                                    "name": "Lei Chen"
                                },
                                {
                                    "authorId": "2293314132",
                                    "name": "Maki Kubota"
                                },
                                {
                                    "authorId": "2295735234",
                                    "name": "Maksim Tkatchenko"
                                },
                                {
                                    "authorId": "2293317836",
                                    "name": "Miroku Lee"
                                },
                                {
                                    "authorId": "2293314089",
                                    "name": "Naoki Takahashi"
                                },
                                {
                                    "authorId": "35640774",
                                    "name": "Prathyusha Jwalapuram"
                                },
                                {
                                    "authorId": "2293313287",
                                    "name": "Ryutaro Tatsushima"
                                },
                                {
                                    "authorId": "2165228954",
                                    "name": "Saurabh Jain"
                                },
                                {
                                    "authorId": "49596302",
                                    "name": "Sunil Kumar Yadav"
                                },
                                {
                                    "authorId": "2293312307",
                                    "name": "Ting Cai"
                                },
                                {
                                    "authorId": "2271252858",
                                    "name": "Wei-Te Chen"
                                },
                                {
                                    "authorId": "3456546",
                                    "name": "Yandi Xia"
                                },
                                {
                                    "authorId": "2293302761",
                                    "name": "Yuki Nakayama"
                                },
                                {
                                    "authorId": "2293313297",
                                    "name": "Yutaka Higashiyama"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.57763671875
                    },
                    {
                        "id": "(Dodge et al., 2021)",
                        "snippets": [
                            "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet."
                        ],
                        "paper": {
                            "corpus_id": 237568724,
                            "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
                            "authors": [
                                {
                                    "authorId": "34176020",
                                    "name": "Jesse Dodge"
                                },
                                {
                                    "authorId": "2127450728",
                                    "name": "Ana Marasovic"
                                },
                                {
                                    "authorId": "2123694087",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "3458736",
                                    "name": "Dirk Groeneveld"
                                },
                                {
                                    "authorId": "49501003",
                                    "name": "Margaret Mitchell"
                                },
                                {
                                    "authorId": "40642935",
                                    "name": "Matt Gardner"
                                },
                                {
                                    "authorId": "2301202406",
                                    "name": "William Agnew"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 450
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kong et al., 2024)",
                        "snippets": [
                            "It is well known that better data selection leads to improved downstream performance and/or a reduced compute budget to achieve the same performance for large-scale model training efforts, e.g.LLMs Sorscher et al. [2023]",
                            "However, it is widely recognized that pre-training data is very noisy, and often includes boilerplate text, templates, error messages, and other forms of repetitive or not-informative content that does not contribute meaningfully to model quality as measured by downstream benchmarks.This poor quality data can lead models to learn and perpetuate irrelevant or redundant information, ultimately impacting their performance and generalization capabilities across various tasks (Raffel et al. [2020], Touvron et al. [2023a,b])."
                        ],
                        "paper": {
                            "corpus_id": 270357359,
                            "title": "Large Language Model-guided Document Selection",
                            "authors": [
                                {
                                    "authorId": "2291470563",
                                    "name": "Xiang Kong"
                                },
                                {
                                    "authorId": "2238621478",
                                    "name": "Tom Gunter"
                                },
                                {
                                    "authorId": "2238621132",
                                    "name": "Ruoming Pang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.51806640625
                    },
                    {
                        "id": "(Hernandez et al., 2022)",
                        "snippets": [
                            "Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance."
                        ],
                        "paper": {
                            "corpus_id": 248986979,
                            "title": "Scaling Laws and Interpretability of Learning from Repeated Data",
                            "authors": [
                                {
                                    "authorId": "39182747",
                                    "name": "Danny Hernandez"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2154608209",
                                    "name": "Tom Conerly"
                                },
                                {
                                    "authorId": "2142833890",
                                    "name": "Nova Dassarma"
                                },
                                {
                                    "authorId": "1943097969",
                                    "name": "Dawn Drain"
                                },
                                {
                                    "authorId": "1403602266",
                                    "name": "S. El-Showk"
                                },
                                {
                                    "authorId": "2866708",
                                    "name": "Nelson Elhage"
                                },
                                {
                                    "authorId": "1573482302",
                                    "name": "Zac Hatfield-Dodds"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "2162194147",
                                    "name": "Tristan Hume"
                                },
                                {
                                    "authorId": "2154610174",
                                    "name": "Scott Johnston"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "2287268442",
                                    "name": "Chris Olah"
                                },
                                {
                                    "authorId": "2061321863",
                                    "name": "Catherine Olsson"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                },
                                {
                                    "authorId": "2117706920",
                                    "name": "Nicholas Joseph"
                                },
                                {
                                    "authorId": "2053807409",
                                    "name": "Jared Kaplan"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 118
                        },
                        "score": 0
                    },
                    {
                        "id": "(Brandizzi et al., 2024)",
                        "snippets": [
                            "Recent trends in dataset curation reflect an increasing emphasis on quality over quantity, with techniques such as advanced filtering and deduplication becoming more prevalent [81,92]",
                            "Traditionally, data quality has been assessed through intrinsic signals derived from the data itself, such as average word length, sentence complexity, or overall structure. However, as the field has advanced, the focus has shifted towards leveraging pretrained LLMs to judge high-quality content or even generate synthetic data, a strategy that has proven effective in improving model performance on specific benchmarks [39,(Kumar et al., 2020)(Yoo et al., 2021). One reason for the success of these methods is the alignment between human judgments of quality and the assessments made by LLMs [91,53]31]."
                        ],
                        "paper": {
                            "corpus_id": 273323858,
                            "title": "Data Processing for the OpenGPT-X Model Family",
                            "authors": [
                                {
                                    "authorId": "2108867346",
                                    "name": "Nicolo\u2019 Brandizzi"
                                },
                                {
                                    "authorId": "2899331",
                                    "name": "Hammam Abdelwahab"
                                },
                                {
                                    "authorId": "2239019329",
                                    "name": "Anirban Bhowmick"
                                },
                                {
                                    "authorId": "2313826462",
                                    "name": "Lennard Helmer"
                                },
                                {
                                    "authorId": "2325731547",
                                    "name": "Benny Stein"
                                },
                                {
                                    "authorId": "2325730238",
                                    "name": "Pavel Denisov"
                                },
                                {
                                    "authorId": "2210670041",
                                    "name": "Qasid Saleem"
                                },
                                {
                                    "authorId": "2258551638",
                                    "name": "Michael Fromm"
                                },
                                {
                                    "authorId": "2258668067",
                                    "name": "Mehdi Ali"
                                },
                                {
                                    "authorId": "2258550842",
                                    "name": "Richard Rutmann"
                                },
                                {
                                    "authorId": "2325731187",
                                    "name": "Farzad Naderi"
                                },
                                {
                                    "authorId": "2325730227",
                                    "name": "Mohamad Saif Agy"
                                },
                                {
                                    "authorId": "2325730246",
                                    "name": "Alexander Schwirjow"
                                },
                                {
                                    "authorId": "2332085973",
                                    "name": "Fabian K\u00fcch"
                                },
                                {
                                    "authorId": "2335570562",
                                    "name": "Luzian Hahn"
                                },
                                {
                                    "authorId": "2123349725",
                                    "name": "Malte Ostendorff"
                                },
                                {
                                    "authorId": "2302563988",
                                    "name": "Pedro Ortiz Suarez"
                                },
                                {
                                    "authorId": "77903998",
                                    "name": "Georg Rehm"
                                },
                                {
                                    "authorId": "2313822383",
                                    "name": "Dennis Wegener"
                                },
                                {
                                    "authorId": "2347259072",
                                    "name": "Nicolas Flores-Herr"
                                },
                                {
                                    "authorId": "2330411614",
                                    "name": "Joachim K\u00f6hler"
                                },
                                {
                                    "authorId": "2258552277",
                                    "name": "Johannes Leveling"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.5
                    },
                    {
                        "id": "(Kumar et al., 2020)",
                        "snippets": [
                            "Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information."
                        ],
                        "paper": {
                            "corpus_id": 211987786,
                            "title": "Data Augmentation using Pre-trained Transformer Models",
                            "authors": [
                                {
                                    "authorId": "40574366",
                                    "name": "Varun Kumar"
                                },
                                {
                                    "authorId": "47992757",
                                    "name": "Ashutosh Choudhary"
                                },
                                {
                                    "authorId": "4006425",
                                    "name": "Eunah Cho"
                                }
                            ],
                            "year": 2020,
                            "venue": "LIFELONGNLP",
                            "n_citations": 356
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yoo et al., 2021)",
                        "snippets": [
                            "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach."
                        ],
                        "paper": {
                            "corpus_id": 233296100,
                            "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation",
                            "authors": [
                                {
                                    "authorId": "31760501",
                                    "name": "Kang Min Yoo"
                                },
                                {
                                    "authorId": "13453892",
                                    "name": "Dongju Park"
                                },
                                {
                                    "authorId": "35518563",
                                    "name": "Jaewook Kang"
                                },
                                {
                                    "authorId": "3226948",
                                    "name": "Sang-Woo Lee"
                                },
                                {
                                    "authorId": "2087289230",
                                    "name": "Woomyeong Park"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 242
                        },
                        "score": 0
                    },
                    {
                        "id": "(Longpre et al., 2023)",
                        "snippets": [
                            "Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development."
                        ],
                        "paper": {
                            "corpus_id": 258832491,
                            "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
                            "authors": [
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "32918271",
                                    "name": "Gregory Yauney"
                                },
                                {
                                    "authorId": "49849144",
                                    "name": "Emily Reif"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "38917723",
                                    "name": "David M. Mimno"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 166
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Data Quality Filtering Approaches",
                "tldr": "Data quality filtering techniques can be broadly categorized into heuristic-based approaches that use manually defined rules and model-based approaches that employ classifiers or language models to assess content quality. These approaches can be further classified as reference-dependent (comparing against high-quality seed datasets) or reference-free (using predefined metrics without comparison to reference data). (5 sources)",
                "text": "\nData quality filtering techniques for language model pre-training can be organized into several major categories, each with distinct methodologies and tradeoffs. At the highest level, these approaches can be divided into heuristic-based methods and model-based methods <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277113645\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nHeuristic-based methods rely on manually defined rules to identify and remove low-quality content. These include filters based on textual characteristics such as mean word length, stop word fraction, word repetitions, symbol-to-word ratios, and the presence of uppercase letters <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277113645\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. For example, researchers might establish rules to retain only text containing digits, discard sentences composed entirely of uppercase letters, or remove files where the ratio of symbols to words exceeds a certain threshold <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nModel-based approaches can be further categorized into reference-dependent and reference-free methods <Paper corpusId=\"271874495\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Reference-dependent methods compare data against high-quality seed datasets to determine what should be kept or filtered out. These include binary classification approaches, which have been employed in major language models like GPT-3 <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper> and PaLM <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>. While effective, reference-dependent methods can introduce biases present in the reference data, potentially limiting the diversity and representativeness of the resulting training corpus <Paper corpusId=\"271874495\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nIn contrast, reference-free methods evaluate data quality using predefined metrics without requiring comparison to reference datasets. For instance, perplexity gating uses scores from pre-trained models to assess content quality <Paper corpusId=\"271874495\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. These approaches can maintain greater diversity in the training data by avoiding the biases inherent in reference datasets.\n\nMore sophisticated model-based approaches directly employ language models similar to BERT or GPT to evaluate the quality or value of training data <Paper corpusId=\"277113645\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. These methods leverage the capabilities of existing language models to make more nuanced judgments about content quality, potentially capturing aspects of quality that might be missed by simpler heuristic approaches.",
                "citations": [
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Quality filtering: Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Heuristic methods involve employing manually defined rules to eliminate low-quality data [84; 72]. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. Classifier-based methods involve training a classifier on a high-quality dataset such as WebText (Radford et al., 2019) to filter out low-quality datasets."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.63134765625
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "In terms of data selection methods, the objective is to extract high-quality subsets from the original datasets such that training LLMs on these subsets can achieve similar or even superior performance on downstream tasks compared to training on the entire original datasets. There are two main categories of methods: heuristic-based approaches and model-based approaches. For the former, a variety of manually designed filters, including mean word length, stop word fraction, and word repetitions, are frequently applied to preprocess raw web data (Tirumala et al., 2023). For the latter, models similar to BERT (Devlin et al., 2019) are commonly used to evaluate and select data with higher scores, while other approaches directly employ GPT-like models to assess the data's value."
                        ],
                        "paper": {
                            "corpus_id": 277113645,
                            "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2260817933",
                                    "name": "Jiazheng Li"
                                },
                                {
                                    "authorId": "2350870800",
                                    "name": "Lu Yu"
                                },
                                {
                                    "authorId": "2279851906",
                                    "name": "Qing Cui"
                                },
                                {
                                    "authorId": "2344807347",
                                    "name": "Zhiqiang Zhang"
                                },
                                {
                                    "authorId": "2344948641",
                                    "name": "Jun Zhou"
                                },
                                {
                                    "authorId": "2093920413",
                                    "name": "Yanfang Ye"
                                },
                                {
                                    "authorId": "2117879943",
                                    "name": "Chuxu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.59228515625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Quality filters aim to extract high-quality data from a noisy raw corpus, thereby improving the language model's performance without increasing training costs. Existing filters are broadly classified into two categories: reference-dependent and reference-free approaches. Reference-dependent methods, such as binary classification (Brown et al., 2020)Gao et al., 2020;(Chowdhery et al., 2022) and DSIR (Xie et al., 2023), filter out low-quality data by comparing it with high-quality seed datasets. While effective, these methods inevitably introduce biases present in the reference data, such as specific writing styles or topics, thereby limiting the diversity and representativeness of training corpus (Soldaini et al., 2023). In contrast, reference-free methods, such as perplexity gating (Marion et al., 2023), assess data quality using predefined metrics like perplexity scores from pre-trained models."
                        ],
                        "paper": {
                            "corpus_id": 271874495,
                            "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2262451401",
                                    "name": "Ruihang Li"
                                },
                                {
                                    "authorId": "2107995927",
                                    "name": "Yixuan Wei"
                                },
                                {
                                    "authorId": "2273515255",
                                    "name": "Miaosen Zhang"
                                },
                                {
                                    "authorId": "2316146999",
                                    "name": "Nenghai Yu"
                                },
                                {
                                    "authorId": "2262465375",
                                    "name": "Han Hu"
                                },
                                {
                                    "authorId": "2243685256",
                                    "name": "Houwen Peng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.849609375
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chowdhery et al., 2022)",
                        "snippets": [
                            "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                        ],
                        "paper": {
                            "corpus_id": 247951931,
                            "title": "PaLM: Scaling Language Modeling with Pathways",
                            "authors": [
                                {
                                    "authorId": "2841893",
                                    "name": "Aakanksha Chowdhery"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2159632445",
                                    "name": "Gaurav Mishra"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "152399055",
                                    "name": "P. Barham"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "152549864",
                                    "name": "Charles Sutton"
                                },
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2620528",
                                    "name": "Parker Schuh"
                                },
                                {
                                    "authorId": "2362367",
                                    "name": "Kensen Shi"
                                },
                                {
                                    "authorId": "2160888237",
                                    "name": "Sasha Tsvyashchenko"
                                },
                                {
                                    "authorId": "2124977868",
                                    "name": "Joshua Maynez"
                                },
                                {
                                    "authorId": "1484043592",
                                    "name": "Abhishek Rao"
                                },
                                {
                                    "authorId": "80940648",
                                    "name": "Parker Barnes"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "49849144",
                                    "name": "Emily Reif"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "2161431901",
                                    "name": "Reiner Pope"
                                },
                                {
                                    "authorId": "2065251344",
                                    "name": "James Bradbury"
                                },
                                {
                                    "authorId": "2058365883",
                                    "name": "Jacob Austin"
                                },
                                {
                                    "authorId": "2090818",
                                    "name": "M. Isard"
                                },
                                {
                                    "authorId": "2284681044",
                                    "name": "Guy Gur-Ari"
                                },
                                {
                                    "authorId": "38253388",
                                    "name": "Pengcheng Yin"
                                },
                                {
                                    "authorId": "2145151992",
                                    "name": "Toju Duke"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                },
                                {
                                    "authorId": "1780892",
                                    "name": "S. Ghemawat"
                                },
                                {
                                    "authorId": "50991767",
                                    "name": "Sunipa Dev"
                                },
                                {
                                    "authorId": "47407464",
                                    "name": "H. Michalewski"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "40055795",
                                    "name": "Vedant Misra"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "2096916416",
                                    "name": "L. Fedus"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "150970919",
                                    "name": "D. Luan"
                                },
                                {
                                    "authorId": "8939217",
                                    "name": "Hyeontaek Lim"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "1572884723",
                                    "name": "A. Spiridonov"
                                },
                                {
                                    "authorId": "35474601",
                                    "name": "Ryan Sepassi"
                                },
                                {
                                    "authorId": "35363891",
                                    "name": "David Dohan"
                                },
                                {
                                    "authorId": "3504647",
                                    "name": "Shivani Agrawal"
                                },
                                {
                                    "authorId": "3175815",
                                    "name": "Mark Omernick"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2598683",
                                    "name": "Thanumalayan Sankaranarayana Pillai"
                                },
                                {
                                    "authorId": "97905921",
                                    "name": "Marie Pellat"
                                },
                                {
                                    "authorId": "102549875",
                                    "name": "Aitor Lewkowycz"
                                },
                                {
                                    "authorId": "2057453483",
                                    "name": "Erica Moreira"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "2636739",
                                    "name": "Oleksandr Polozov"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "2198519",
                                    "name": "Zongwei Zhou"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "4125424",
                                    "name": "Brennan Saeta"
                                },
                                {
                                    "authorId": "2152965375",
                                    "name": "Mark D\u00edaz"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "1754926",
                                    "name": "Michele Catasta"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1398655031",
                                    "name": "K. Meier-Hellstern"
                                },
                                {
                                    "authorId": "2396681",
                                    "name": "D. Eck"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "1754497",
                                    "name": "Slav Petrov"
                                },
                                {
                                    "authorId": "22640071",
                                    "name": "Noah Fiedel"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 6293
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Classifier-Based Filtering Methods",
                "tldr": "Classifier-based filtering methods employ machine learning models to assess text quality, with approaches ranging from lightweight models like FastText to more complex architectures like BERT. Despite their varying computational requirements, classifier-based methods have become foundational in modern LLM data filtering pipelines, offering more flexibility and robustness than rule-based approaches. (10 sources)",
                "text": "\nClassifier-based filtering methods have emerged as a sophisticated approach to evaluating pre-training data quality, offering greater flexibility and robustness compared to simple rule-based techniques <Paper corpusId=\"274422859\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. These methods leverage machine learning models to score and classify text based on quality metrics, enabling more nuanced filtering decisions.\n\nFastText-based classifiers have gained significant traction in production data filtering pipelines due to their computational efficiency. Recent surveys indicate that despite the development of more complex methods, a combination of FastText classifiers with English language filtering following extensive deduplication remains surprisingly effective and represents the current state-of-the-art approach for many tasks <Paper corpusId=\"272524632\" paperTitle=\"(Thrush et al., 2024)\" isShortName></Paper>. Major language models, including Llama-3, utilize FastText alongside RoBERTa-based models in their data filtering workflows <Paper corpusId=\"274514936\" paperTitle=\"(Hausenloy et al., 2024)\" isShortName></Paper>. The computational advantage of FastText is substantial - processing 15 trillion tokens with a FastText classifier requires approximately 1,000 CPU hours on an 80-CPU machine, whereas LLM-based classifiers would demand around 6,000 H100 GPU hours for the same task <Paper corpusId=\"278394813\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nMore sophisticated classifier approaches employ transformer-based models like BERT to evaluate text quality. For example, in developing high-quality Chinese pre-training datasets, researchers have implemented pipelines that combine handcrafted rules with BERT-based quality evaluation models that assign quality scores to each text <Paper corpusId=\"264935645\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. BERT has become widely adopted for quality evaluation due to its exceptional performance in text classification and understanding tasks, with its effectiveness stemming from pre-training objectives that enable powerful text representation capabilities <Paper corpusId=\"274422859\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nReference-based classification approaches train binary classifiers using high-quality corpora as positive examples and lower-quality data as negative examples. Researchers have experimented with various reference corpora for training these classifiers, including \"WIKIWEBBOOKS,\" \"OPENWEB,\" \"WIKI,\" and \"WIKIREFS,\" often following the approach used in GPT-3's quality classifier <Paper corpusId=\"266977204\" paperTitle=\"(Lucy et al., 2024)\" isShortName></Paper>. These methods typically filter data for a specific language, often using FastText language identification as a preliminary step <Paper corpusId=\"266977204\" paperTitle=\"(Lucy et al., 2024)\" isShortName></Paper> <Paper corpusId=\"207870323\" paperTitle=\"(Wenzek et al., 2019)\" isShortName></Paper>.\n\nRecent advances in classifier-based filtering have introduced more specialized approaches. WanjuanCC employs BERT-based classifiers to identify and remove data containing excessive advertisements or exhibiting lower fluency <Paper corpusId=\"277955900\" paperTitle=\"(Zhuang et al., 2025)\" isShortName></Paper>. QuRating represents a more sophisticated framework that simulates human-like text quality assessments using four criteria (writing style, required expertise, facts and trivia, and educational value) to guide data selection <Paper corpusId=\"277955900\" paperTitle=\"(Zhuang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267681974\" paperTitle=\"(Wettig et al., 2024)\" isShortName></Paper>. For multilingual datasets, specialized filtering frameworks have been developed that leverage both Transformer and FastText-based classifiers to identify structured and knowledge-rich samples across different languages <Paper corpusId=\"276394897\" paperTitle=\"(Messmer et al., 2025)\" isShortName></Paper>.\n\nRecent trends show that LLM-based classifiers are increasingly being used for data filtering, enabling more dynamic systems that require minimal prompting and eliminate the need for predefined heuristics <Paper corpusId=\"274514936\" paperTitle=\"(Hausenloy et al., 2024)\" isShortName></Paper>. However, the significant computational costs of LLM-based approaches mean that FastText-based methods remain competitive for many practical applications, offering a balance between performance and efficiency <Paper corpusId=\"278394813\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Quality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities."
                        ],
                        "paper": {
                            "corpus_id": 274422859,
                            "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
                            "authors": [
                                {
                                    "authorId": "2333395980",
                                    "name": "Wanyue Zhang"
                                },
                                {
                                    "authorId": "2333247185",
                                    "name": "Ziyong Li"
                                },
                                {
                                    "authorId": "2218735807",
                                    "name": "Wen Yang"
                                },
                                {
                                    "authorId": "2333234263",
                                    "name": "Chunlin Leng"
                                },
                                {
                                    "authorId": "2333317901",
                                    "name": "Yinan Bai"
                                },
                                {
                                    "authorId": "8134471",
                                    "name": "Qianlong Du"
                                },
                                {
                                    "authorId": "2064100826",
                                    "name": "Chengqing Zong"
                                },
                                {
                                    "authorId": "2283356334",
                                    "name": "Jiajun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.5185546875
                    },
                    {
                        "id": "(Thrush et al., 2024)",
                        "snippets": [
                            "According to a recent survey of data selection approaches by Li et al. (2024), the heavier-weight pretraining data selection methods have not shown large gains, and the current state-of-the-art across many tasks is primitive: a fixed fastText classifier (Joulin et al., 2016) combined with an English filter as a final layer after extensive deduplication and filtering."
                        ],
                        "paper": {
                            "corpus_id": 272524632,
                            "title": "Improving Pretraining Data Using Perplexity Correlations",
                            "authors": [
                                {
                                    "authorId": "1500242049",
                                    "name": "Tristan Thrush"
                                },
                                {
                                    "authorId": "2279335958",
                                    "name": "Christopher Potts"
                                },
                                {
                                    "authorId": "2214604036",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 22
                        },
                        "score": 0.7861328125
                    },
                    {
                        "id": "(Hausenloy et al., 2024)",
                        "snippets": [
                            "Existing Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or lowquality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and RoBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\", as in current safety filtering systems [32]."
                        ],
                        "paper": {
                            "corpus_id": 274514936,
                            "title": "Towards Data Governance of Frontier AI Models",
                            "authors": [
                                {
                                    "authorId": "2258553730",
                                    "name": "Jason Hausenloy"
                                },
                                {
                                    "authorId": "2333897525",
                                    "name": "Duncan McClements"
                                },
                                {
                                    "authorId": "2333900015",
                                    "name": "Madhavendra Thakur"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.509765625
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Current high-quality data classifiers are primarily divided into LLM-based (Penedo et al., 2024;Yu et al., 2025;Wang et al., 2024) and fastText-based (Li et al., 2024;Shao et al., 2024;Guo et al., 2024) methods. \n\nWhile LLM-based classifiers are effective, they need significantly higher inference costs. To address this, we adopt a fastText-based classifier, which significantly reduces inference costs while maintaining competitive performance under certain conditions. This approach not only minimizes resource consumption but also speeds up data filtering experiments. For instance, as shown in Table 2, processing 15T tokens with an LLM-based classifier requires approximately 6,000 H100 GPU hours, while fastText can complete the same task on a non-GPU machine with just 80 CPUs in 1,000 hours, significantly improving efficiency."
                        ],
                        "paper": {
                            "corpus_id": 278394813,
                            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
                            "authors": [
                                {
                                    "authorId": "2312864296",
                                    "name": "Yudong Wang"
                                },
                                {
                                    "authorId": "2354260586",
                                    "name": "Zixuan Fu"
                                },
                                {
                                    "authorId": "2295809950",
                                    "name": "Jie Cai"
                                },
                                {
                                    "authorId": "2359636324",
                                    "name": "Peijun Tang"
                                },
                                {
                                    "authorId": "2359634927",
                                    "name": "Hongya Lyu"
                                },
                                {
                                    "authorId": "2295846809",
                                    "name": "Yewei Fang"
                                },
                                {
                                    "authorId": "2295929465",
                                    "name": "Zhi Zheng"
                                },
                                {
                                    "authorId": "2295789325",
                                    "name": "Jie Zhou"
                                },
                                {
                                    "authorId": "1398454307",
                                    "name": "Guoyang Zeng"
                                },
                                {
                                    "authorId": "51131083",
                                    "name": "Chaojun Xiao"
                                },
                                {
                                    "authorId": "2324934386",
                                    "name": "Xu Han"
                                },
                                {
                                    "authorId": "2316519794",
                                    "name": "Zhiyuan Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.81103515625
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "To extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score."
                        ],
                        "paper": {
                            "corpus_id": 264935645,
                            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
                            "authors": [
                                {
                                    "authorId": "2264139789",
                                    "name": "Jianghao Chen"
                                },
                                {
                                    "authorId": "2329739061",
                                    "name": "Pu Jian"
                                },
                                {
                                    "authorId": "2264961304",
                                    "name": "Tengxiao Xi"
                                },
                                {
                                    "authorId": "2265214644",
                                    "name": "Yidong Yi"
                                },
                                {
                                    "authorId": "8134471",
                                    "name": "Qianlong Du"
                                },
                                {
                                    "authorId": "2264336906",
                                    "name": "Chenglin Ding"
                                },
                                {
                                    "authorId": "2894321",
                                    "name": "Guibo Zhu"
                                },
                                {
                                    "authorId": "2064100826",
                                    "name": "Chengqing Zong"
                                },
                                {
                                    "authorId": "1519293616",
                                    "name": "Jinqiao Wang"
                                },
                                {
                                    "authorId": "2124819243",
                                    "name": "Jiajun Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.55908203125
                    },
                    {
                        "id": "(Lucy et al., 2024)",
                        "snippets": [
                            "We experiment with quality filters that score text based on their similarity to some chosen \"high quality\" reference corpora. We name these filters based on the reference corpora used to train them: WIKIWEBBOOKS, OPENWEB, WIKI, and WIKIREFS (Table 3). We use Gururangan et al. (2022)'s replication of GPT-3's binary logistic regression quality classifier and only vary the positive \"high quality\" class. The negative class is a fixed set of tokens from the September 2019 dump of Common Crawl, and each class contains approximately 300M tokens. We also compare WIKI to a perplexity-based text scorer, WIKI ppl , which uses a 5-gram Kneser-Ney language model trained on Wikipedia instead of a classifier (Wenzek et al., 2019)(Laurenccon et al., 2023)Muennighoff et al., 2023;Marion et al., 2023)",
                            "Our data is pre-filtered to documents that fastText langID scores as likely English (Joulin et al., 2016b,a)."
                        ],
                        "paper": {
                            "corpus_id": 266977204,
                            "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
                            "authors": [
                                {
                                    "authorId": "15983089",
                                    "name": "Li Lucy"
                                },
                                {
                                    "authorId": "40895369",
                                    "name": "Suchin Gururangan"
                                },
                                {
                                    "authorId": "3328733",
                                    "name": "Luca Soldaini"
                                },
                                {
                                    "authorId": "2268272",
                                    "name": "Emma Strubell"
                                },
                                {
                                    "authorId": "2064411219",
                                    "name": "David Bamman"
                                },
                                {
                                    "authorId": "2279335512",
                                    "name": "Lauren Klein"
                                },
                                {
                                    "authorId": "34176020",
                                    "name": "Jesse Dodge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 17
                        },
                        "score": 0.529296875
                    },
                    {
                        "id": "(Wenzek et al., 2019)",
                        "snippets": [
                            "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia."
                        ],
                        "paper": {
                            "corpus_id": 207870323,
                            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
                            "authors": [
                                {
                                    "authorId": "2293203",
                                    "name": "Guillaume Wenzek"
                                },
                                {
                                    "authorId": "114952298",
                                    "name": "M. Lachaux"
                                },
                                {
                                    "authorId": "2480903",
                                    "name": "Alexis Conneau"
                                },
                                {
                                    "authorId": "113810201",
                                    "name": "Vishrav Chaudhary"
                                },
                                {
                                    "authorId": "2061585840",
                                    "name": "F. Guzm\u2019an"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 658
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhuang et al., 2025)",
                        "snippets": [
                            "More recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data."
                        ],
                        "paper": {
                            "corpus_id": 277955900,
                            "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
                            "authors": [
                                {
                                    "authorId": "2366068443",
                                    "name": "Xinlin Zhuang"
                                },
                                {
                                    "authorId": "2233445161",
                                    "name": "Jiahui Peng"
                                },
                                {
                                    "authorId": "2299118979",
                                    "name": "Ren Ma"
                                },
                                {
                                    "authorId": "2352285941",
                                    "name": "Yinfan Wang"
                                },
                                {
                                    "authorId": "2318978696",
                                    "name": "Tianyi Bai"
                                },
                                {
                                    "authorId": "2298376663",
                                    "name": "Xingjian Wei"
                                },
                                {
                                    "authorId": "2289911484",
                                    "name": "Jiantao Qiu"
                                },
                                {
                                    "authorId": "2325489246",
                                    "name": "Chi Zhang"
                                },
                                {
                                    "authorId": "2356585875",
                                    "name": "Ying Qian"
                                },
                                {
                                    "authorId": "2346476781",
                                    "name": "Conghui He"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.72216796875
                    },
                    {
                        "id": "(Wettig et al., 2024)",
                        "snippets": [
                            "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts&trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."
                        ],
                        "paper": {
                            "corpus_id": 267681974,
                            "title": "QuRating: Selecting High-Quality Data for Training Language Models",
                            "authors": [
                                {
                                    "authorId": "2127066887",
                                    "name": "Alexander Wettig"
                                },
                                {
                                    "authorId": "2284268826",
                                    "name": "Aatmik Gupta"
                                },
                                {
                                    "authorId": "2323513320",
                                    "name": "Saumya Malik"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 79
                        },
                        "score": 0
                    },
                    {
                        "id": "(Messmer et al., 2025)",
                        "snippets": [
                            "To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data."
                        ],
                        "paper": {
                            "corpus_id": 276394897,
                            "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
                            "authors": [
                                {
                                    "authorId": "2219037377",
                                    "name": "Bettina Messmer"
                                },
                                {
                                    "authorId": "2350460665",
                                    "name": "Vinko Sabolcec"
                                },
                                {
                                    "authorId": "2328413457",
                                    "name": "Martin Jaggi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.56005859375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effectiveness and Impact of Quality Filtering",
                "tldr": "Data quality filtering for LLM pre-training shows a complex relationship between filtering intensity and model performance, with moderate filtering generally improving results while excessive filtering can reduce data diversity and hurt performance. The effectiveness of filtering approaches varies based on filtering methods, evaluation tasks, and model architectures, with perplexity-based filtering methods showing particularly strong results. (4 sources)",
                "text": "\nThe effectiveness of quality filtering in LLM pre-training exhibits a nuanced relationship with model performance, revealing both benefits and potential drawbacks. While quality filtering generally improves model performance despite reducing data quantity, research indicates the existence of an optimal filtering intensity beyond which performance may deteriorate <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This non-linear relationship suggests that while filtering out low-quality data is beneficial, excessive filtering can remove valuable information and reduce data diversity.\n\nSeveral studies demonstrate the effectiveness of carefully filtered high-quality datasets in training lightweight LLMs with outstanding performance <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. However, researchers have also observed that aggressive filtering can lead to performance degradation across a wide range of tasks for GPT-like models due to poor representativity of the filtering proxy objectives <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This finding highlights the challenge of selecting appropriate filtering criteria that maintain data diversity while removing genuinely low-quality content.\n\nIn evaluating different filtering approaches, perplexity-based methods have shown particularly promising results. Experimental findings indicate that perplexity filtering based on N-gram language models can be especially effective for processing massive volumes of corpora <Paper corpusId=\"270514462\" paperTitle=\"(Enomoto et al., 2024)\" isShortName></Paper>. However, these studies also confirm that overly aggressive filtering using these methods can result in performance deterioration when evaluated on general language understanding tasks <Paper corpusId=\"270514462\" paperTitle=\"(Enomoto et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nBeyond pre-training, quality filtering has shown benefits for downstream tasks and model fine-tuning. For instance, heuristic negative sample filtering has been demonstrated to improve both faithfulness and correctness in question-answering models by preserving the integrity of faithfulness relations between positive and negative examples <Paper corpusId=\"276776523\" paperTitle=\"(Li et al._1, 2025)\" isShortName></Paper>. This suggests that targeted filtering approaches can enhance model performance on specific tasks beyond general language understanding.\n\nDespite these advances, researchers note that there remains a lack of well-established and theoretically efficient filtering strategies, indicating opportunities for further exploration in this area <Paper corpusId=\"265609639\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. The varying effectiveness of different filtering approaches across model architectures and evaluation tasks suggests that optimal filtering strategies may need to be tailored to specific applications and model designs rather than applying one-size-fits-all solutions.",
                "citations": [
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Despite the reduction of data quantity, quality filtering is usually proven to be beneficial in model performance improvement (Longpre et al., 2023). Several carefully filtered high-quality datasets are proposed to train lightweight LLMs with outstanding performances (Gunasekar et al., 2023;Li et al., 2023d;Javaheripi and Bubeck, 2023;(Penedo et al., 2023). However, Gao (2021) finds that aggressive filtering might lead to performance degradation on a wide range of tasks for GPT-like LLMs due to the poor representativity of the filtering proxy objectives. To address this issue, Marion et al. (2023) comprehensively examine different data quality estimators and find that pruning datasets based on perplexity performs better than more complicated techniques like memorization. Gan et al. (2023) develop data-centric scaling laws and show that improving semantic and grammatical quality is more effective. However, there still lacks a wellestablished and theoretically efficient filtering strategy, leaving room for further exploration."
                        ],
                        "paper": {
                            "corpus_id": 265609639,
                            "title": "Data Management For Training Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2238165141",
                                    "name": "Zige Wang"
                                },
                                {
                                    "authorId": "2249763710",
                                    "name": "Wanjun Zhong"
                                },
                                {
                                    "authorId": "46395829",
                                    "name": "Yufei Wang"
                                },
                                {
                                    "authorId": "2269768949",
                                    "name": "Qi Zhu"
                                },
                                {
                                    "authorId": "2258717400",
                                    "name": "Fei Mi"
                                },
                                {
                                    "authorId": "2239032344",
                                    "name": "Baojun Wang"
                                },
                                {
                                    "authorId": "2238661808",
                                    "name": "Lifeng Shang"
                                },
                                {
                                    "authorId": "2257942536",
                                    "name": "Xin Jiang"
                                },
                                {
                                    "authorId": "2249841180",
                                    "name": "Qun Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 14
                        },
                        "score": 0.77734375
                    },
                    {
                        "id": "(Enomoto et al., 2024)",
                        "snippets": [
                            "We test the perplexity of a language model and a relatively fast classifier to process a massive volume of corpora. The experimental results show that the perplexity filtering method based on an N-gram language model is the best. We also pretrain BERT (Devlin et al., 2019) on a Japanese Web corpus filtered by the N-gram language model and evaluated it on Japanese General Language Understanding Evaluation, JGLUE (Kurihara et al., 2022). The results show that massively strong filtering results in performance deterioration."
                        ],
                        "paper": {
                            "corpus_id": 270514462,
                            "title": "Investigating Web Corpus Filtering Methods for Language Model Development in Japanese",
                            "authors": [
                                {
                                    "authorId": "2306632898",
                                    "name": "Rintaro Enomoto"
                                },
                                {
                                    "authorId": "153616317",
                                    "name": "A. Tolmachev"
                                },
                                {
                                    "authorId": "2301579122",
                                    "name": "Takuro Niitsuma"
                                },
                                {
                                    "authorId": "2306632538",
                                    "name": "Shuhei Kurita"
                                },
                                {
                                    "authorId": "2306632595",
                                    "name": "Daisuke Kawahara"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.67919921875
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2025)",
                        "snippets": [
                            "As a result, it is possible that \u03b8 0 can answer the question correctly and faithfully even without access to the evidence passages, provided it has encountered similar information during pretraining. To mitigate this, we involve a heuristic negative sample filtering process for quality control",
                            "This filtering process tries to preserve the integrity of our faithfulness relation between positive and negative sentences. The effectiveness of pre-stage filtering is reported in Tab. 4",
                            "As demonstrated in the Tab. 4, heuristic negative sample filtering improves both faithfulness and correctness, with filtered outperforming non-filterd across all metrics."
                        ],
                        "paper": {
                            "corpus_id": 276776523,
                            "title": "Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution",
                            "authors": [
                                {
                                    "authorId": "2185631323",
                                    "name": "Kun Li"
                                },
                                {
                                    "authorId": "2146333115",
                                    "name": "Tianhua Zhang"
                                },
                                {
                                    "authorId": "2155851529",
                                    "name": "Yunxiang Li"
                                },
                                {
                                    "authorId": "1944274",
                                    "name": "Hongyin Luo"
                                },
                                {
                                    "authorId": "2348476303",
                                    "name": "Abdalla Moustafa"
                                },
                                {
                                    "authorId": "2107999711",
                                    "name": "Xixin Wu"
                                },
                                {
                                    "authorId": "2303402875",
                                    "name": "James Glass"
                                },
                                {
                                    "authorId": "2273659859",
                                    "name": "Helen M. Meng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.54931640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Case Studies of Quality Filtering in Major LLMs",
                "tldr": "Major language models employ diverse data filtering approaches, with GPT-3 pioneering classifier-based methods and subsequent models like PaLM and Llama introducing increasingly sophisticated filtering pipelines. These case studies demonstrate the industry's shift toward prioritizing data quality over quantity, with different models employing unique combinations of classifier-based filtering, rule-based filtering, and deduplication techniques. (7 sources)",
                "text": "\n## GPT-3\nGPT-3 pioneered comprehensive data quality filtering by implementing an automated approach to eliminate low-quality documents from Common Crawl. Its data preprocessing pipeline employed a classifier trained on high-quality corpora (WebText, Wikipedia, and web books) to prioritize documents with higher quality scores. Additionally, GPT-3 implemented fuzzy deduplication within each dataset and removed WebText from Common Crawl to further enhance data quality. <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>\n\n## PaLM\nPaLM (Pathways Language Model) adopted sophisticated filtering strategies to improve training data quality. The 540-billion parameter model's preprocessing pipeline incorporated various filtering techniques to ensure high-quality inputs, which contributed to its state-of-the-art few-shot learning results across hundreds of language understanding and generation benchmarks. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>\n\n## Llama Models\nThe Llama family of models employed comprehensive data filtering approaches, including both classifier-based and rule-based methods. As mentioned in earlier sections, recent versions utilize FastText alongside RoBERTa-based models in their data filtering workflows, balancing computational efficiency with filtering effectiveness.\n\n## GLaM\nGLaM (Generalist Language Model) focused on data quality despite using a sparsely activated mixture-of-experts architecture. While GLaM was 7x larger than GPT-3 with 1.2 trillion parameters, it achieved better overall zero-shot and one-shot performance across 29 NLP tasks while consuming only 1/3 of the energy used to train GPT-3. This performance improvement can be attributed in part to its data quality filtering processes. <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245124124\" paperTitle=\"(Du et al., 2021)\" isShortName></Paper>\n\n## Publicly Available Filtered Datasets\nSeveral high-quality pre-training datasets have been made publicly available, providing researchers with already-filtered data resources:\n- C4 (Colossal Clean Crawled Corpus): Introduced comprehensive filtering processes to clean web-crawled data. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>\n- The Pile: A diverse, high-quality dataset incorporating multiple filtering layers. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n- RefinedWeb: Focused on quality refinement of web-crawled data. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n- RedPajama: Provided filtered data specifically designed for open-source language models. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n- The Stack: Incorporated various filtering strategies to improve code and text quality. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>\n\n## Deduplication-Focused Approaches\nSeveral models and researchers have prioritized deduplication as a key quality filtering strategy. Researchers found that existing language modeling datasets contained many near-duplicate examples and long repetitive substrings, with over 1% of unprompted output from models being copied verbatim from training data. Improved deduplication tools allowed for removing highly repetitive content (such as a single 61-word English sentence repeated over 60,000 times in C4). Models trained on deduplicated data showed significant improvements, emitting memorized text ten times less frequently while requiring fewer training steps to achieve the same or better accuracy. <Paper corpusId=\"269187631\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235829052\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper>\n\n## Impact of Temporal Alignment\nModels that carefully considered the temporal relationship between pre-training and evaluation data showed better performance. Research demonstrated that temporal shift between evaluation data and pre-training data leads to performance degradation that is not overcome by fine-tuning, highlighting the importance of considering data freshness in filtering strategies. <Paper corpusId=\"261243909\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258832491\" paperTitle=\"(Longpre et al., 2023)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Gao et al., 2023)",
                        "snippets": [
                            "Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training.\n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data.\n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples.\n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training.\n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality."
                        ],
                        "paper": {
                            "corpus_id": 261243909,
                            "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
                            "authors": [
                                {
                                    "authorId": "1944690382",
                                    "name": "Kaiyuan Gao"
                                },
                                {
                                    "authorId": "2112345574",
                                    "name": "Su He"
                                },
                                {
                                    "authorId": "2152990262",
                                    "name": "Zhenyu He"
                                },
                                {
                                    "authorId": null,
                                    "name": "Jiacheng Lin"
                                },
                                {
                                    "authorId": "2171652249",
                                    "name": "Qizhi Pei"
                                },
                                {
                                    "authorId": "2234370833",
                                    "name": "Jie Shao"
                                },
                                {
                                    "authorId": "2256597384",
                                    "name": "Wei Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.71533203125
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2019), Pile (Gao et al., 2021), RefinedWeb (Penedo et al., 2023), RedPajama (Computer, 2023), and the Stack (Kocetkov et al., 2022;Lozhkov et al., 2024).On top of these, multiple papers (e.g., (Lee et al., 2021)Marion et al., 2023;Chen et al., 2023;(Chowdhery et al., 2022)Touvron et al., 2023a;(Raffel et al., 2019)) propose various filtering strategies to improve data quality."
                        ],
                        "paper": {
                            "corpus_id": 269187631,
                            "title": "Fewer Truncations Improve Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2113455281",
                                    "name": "Hantian Ding"
                                },
                                {
                                    "authorId": "2259065741",
                                    "name": "Zijian Wang"
                                },
                                {
                                    "authorId": "2296990653",
                                    "name": "Giovanni Paolini"
                                },
                                {
                                    "authorId": "40574366",
                                    "name": "Varun Kumar"
                                },
                                {
                                    "authorId": "1713801",
                                    "name": "Anoop Deoras"
                                },
                                {
                                    "authorId": "2258962983",
                                    "name": "Dan Roth"
                                },
                                {
                                    "authorId": "2264070792",
                                    "name": "Stefano Soatto"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 14
                        },
                        "score": 0.576171875
                    },
                    {
                        "id": "(Chowdhery et al., 2022)",
                        "snippets": [
                            "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                        ],
                        "paper": {
                            "corpus_id": 247951931,
                            "title": "PaLM: Scaling Language Modeling with Pathways",
                            "authors": [
                                {
                                    "authorId": "2841893",
                                    "name": "Aakanksha Chowdhery"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2159632445",
                                    "name": "Gaurav Mishra"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "152399055",
                                    "name": "P. Barham"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "152549864",
                                    "name": "Charles Sutton"
                                },
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2620528",
                                    "name": "Parker Schuh"
                                },
                                {
                                    "authorId": "2362367",
                                    "name": "Kensen Shi"
                                },
                                {
                                    "authorId": "2160888237",
                                    "name": "Sasha Tsvyashchenko"
                                },
                                {
                                    "authorId": "2124977868",
                                    "name": "Joshua Maynez"
                                },
                                {
                                    "authorId": "1484043592",
                                    "name": "Abhishek Rao"
                                },
                                {
                                    "authorId": "80940648",
                                    "name": "Parker Barnes"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "49849144",
                                    "name": "Emily Reif"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "2161431901",
                                    "name": "Reiner Pope"
                                },
                                {
                                    "authorId": "2065251344",
                                    "name": "James Bradbury"
                                },
                                {
                                    "authorId": "2058365883",
                                    "name": "Jacob Austin"
                                },
                                {
                                    "authorId": "2090818",
                                    "name": "M. Isard"
                                },
                                {
                                    "authorId": "2284681044",
                                    "name": "Guy Gur-Ari"
                                },
                                {
                                    "authorId": "38253388",
                                    "name": "Pengcheng Yin"
                                },
                                {
                                    "authorId": "2145151992",
                                    "name": "Toju Duke"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                },
                                {
                                    "authorId": "1780892",
                                    "name": "S. Ghemawat"
                                },
                                {
                                    "authorId": "50991767",
                                    "name": "Sunipa Dev"
                                },
                                {
                                    "authorId": "47407464",
                                    "name": "H. Michalewski"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "40055795",
                                    "name": "Vedant Misra"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "2096916416",
                                    "name": "L. Fedus"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "150970919",
                                    "name": "D. Luan"
                                },
                                {
                                    "authorId": "8939217",
                                    "name": "Hyeontaek Lim"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "1572884723",
                                    "name": "A. Spiridonov"
                                },
                                {
                                    "authorId": "35474601",
                                    "name": "Ryan Sepassi"
                                },
                                {
                                    "authorId": "35363891",
                                    "name": "David Dohan"
                                },
                                {
                                    "authorId": "3504647",
                                    "name": "Shivani Agrawal"
                                },
                                {
                                    "authorId": "3175815",
                                    "name": "Mark Omernick"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2598683",
                                    "name": "Thanumalayan Sankaranarayana Pillai"
                                },
                                {
                                    "authorId": "97905921",
                                    "name": "Marie Pellat"
                                },
                                {
                                    "authorId": "102549875",
                                    "name": "Aitor Lewkowycz"
                                },
                                {
                                    "authorId": "2057453483",
                                    "name": "Erica Moreira"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "2636739",
                                    "name": "Oleksandr Polozov"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "2198519",
                                    "name": "Zongwei Zhou"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "4125424",
                                    "name": "Brennan Saeta"
                                },
                                {
                                    "authorId": "2152965375",
                                    "name": "Mark D\u00edaz"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "1754926",
                                    "name": "Michele Catasta"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1398655031",
                                    "name": "K. Meier-Hellstern"
                                },
                                {
                                    "authorId": "2396681",
                                    "name": "D. Eck"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "1754497",
                                    "name": "Slav Petrov"
                                },
                                {
                                    "authorId": "22640071",
                                    "name": "Noah Fiedel"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 6293
                        },
                        "score": 0
                    },
                    {
                        "id": "(Du et al., 2021)",
                        "snippets": [
                            "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks."
                        ],
                        "paper": {
                            "corpus_id": 245124124,
                            "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
                            "authors": [
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2145438541",
                                    "name": "Yanping Huang"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2058177533",
                                    "name": "Simon Tong"
                                },
                                {
                                    "authorId": "150077954",
                                    "name": "Dmitry Lepikhin"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "2048712",
                                    "name": "M. Krikun"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "40625240",
                                    "name": "Adams Wei Yu"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "2096916416",
                                    "name": "L. Fedus"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "1389392654",
                                    "name": "Zongwei Zhou"
                                },
                                {
                                    "authorId": null,
                                    "name": "Tao Wang"
                                },
                                {
                                    "authorId": "2153608756",
                                    "name": "Yu Emma Wang"
                                },
                                {
                                    "authorId": "20825661",
                                    "name": "Kellie Webster"
                                },
                                {
                                    "authorId": "97905921",
                                    "name": "Marie Pellat"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "1398655031",
                                    "name": "K. Meier-Hellstern"
                                },
                                {
                                    "authorId": "2145151992",
                                    "name": "Toju Duke"
                                },
                                {
                                    "authorId": "2065639113",
                                    "name": "Lucas Dixon"
                                },
                                {
                                    "authorId": "1556095165",
                                    "name": "Kun Zhang"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                },
                                {
                                    "authorId": "2545358",
                                    "name": "Z. Chen"
                                },
                                {
                                    "authorId": "2052275005",
                                    "name": "Claire Cui"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 826
                        },
                        "score": 0
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2021)",
                        "snippets": [
                            "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets\u2014for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets."
                        ],
                        "paper": {
                            "corpus_id": 235829052,
                            "title": "Deduplicating Training Data Makes Language Models Better",
                            "authors": [
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "2064161903",
                                    "name": "A. Nystrom"
                                },
                                {
                                    "authorId": "151505981",
                                    "name": "Chiyuan Zhang"
                                },
                                {
                                    "authorId": "2396681",
                                    "name": "D. Eck"
                                },
                                {
                                    "authorId": "1763608",
                                    "name": "Chris Callison-Burch"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 635
                        },
                        "score": 0
                    },
                    {
                        "id": "(Longpre et al., 2023)",
                        "snippets": [
                            "Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development."
                        ],
                        "paper": {
                            "corpus_id": 258832491,
                            "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
                            "authors": [
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "32918271",
                                    "name": "Gregory Yauney"
                                },
                                {
                                    "authorId": "49849144",
                                    "name": "Emily Reif"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "38917723",
                                    "name": "David M. Mimno"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 166
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.129186
    }
}